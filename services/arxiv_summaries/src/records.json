[
    {
        "identifier": "oai:arXiv.org:2104.00118",
        "abstract_url": "http://arxiv.org/abs/2104.00118",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Peipei"
            },
            {
                "last_name": "Rupp",
                "first_name": "Andreas"
            },
            {
                "last_name": "Kanschat",
                "first_name": "Guido"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Uniform convergence of the geometric multigrid V-cycle is proven for HDGmethods with a new set of assumptions on the injection operators from coarserto finer meshes. The scheme involves standard smoothers and local solvers whichare bounded, convergent, and consistent. Elliptic regularity is used in theproofs. The new assumptions admit injection operators local to a single coarsegrid cell. Examples for admissible injection operators are given. The analysisapplies to the hybridized local discontinuous Galerkin method, hybridizedRaviart-Thomas, and hybridized Brezzi-Douglas-Marini mixed element methods.Numerical experiments are provided to confirm the theoretical results.",
        "title": "Analysis of injection operators in multigrid solvers for hybridized  discontinuous Galerkin methods",
        "date": "2021-03-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.11448",
        "abstract_url": "http://arxiv.org/abs/2104.11448",
        "authors": [
            {
                "last_name": "Vizca\u00edno",
                "first_name": "Aurora"
            },
            {
                "last_name": "de Guzm\u00e1n",
                "first_name": "Ignacio Garc\u00eda-Rodr\u00edguez"
            },
            {
                "last_name": "Manjavacas",
                "first_name": "Antonio"
            },
            {
                "last_name": "Garc\u00eda",
                "first_name": "F\u00e9lix"
            },
            {
                "last_name": "Cruz-Lemus",
                "first_name": "Jos\u00e9 A."
            },
            {
                "last_name": "Serrano",
                "first_name": "Manuel \u00c1ngel"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Technology has changed both our way of life and the way in which we learn.Students now attend lectures with laptops and mobile phones, and this situationis accentuated in the case of students on Computer Science degrees, since theyrequire their computers in order to participate in both theoretical andpractical lessons. Problems, however, arise when the students' social networksare opened on their computers and they receive notifications that interrupttheir work. We set up a workshop regarding time, thoughts and attentionmanagement with the objective of teaching our students techniques that wouldallow them to manage interruptions, concentrate better and definitively makebetter use of their time. Those who took part in the workshop were thenevaluated to discover its effects. The results obtained are quite optimisticand are described in this paper with the objective of encouraging otheruniversities to perform similar initiatives.",
        "title": "How to help university students to manage their interruptions and  improve their attention and time management",
        "date": "2021-04-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2112.06333",
        "abstract_url": "http://arxiv.org/abs/2112.06333",
        "authors": [
            {
                "last_name": "Bradshaw",
                "first_name": "Peter"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            ""
        ],
        "abstract": "  We consider single-conflict colorings, a variant of graph colorings in whicheach edge of a graph has a single forbidden color pair. We show that for anyassignment of forbidden color pairs to the edges of a $d$-degenerate graph $G$on $n$ vertices of edge-multiplicity at most $\\log \\log n$, $O(\\sqrt{ d } \\logn)$ colors are always enough to color the vertices of $G$ in a way that avoidsevery forbidden color pair. This answers a question of Dvo\\v{r}\\'ak, Esperet,Kang, and Ozeki for simple graphs (Journal of Graph Theory 2021).",
        "title": "Single-conflict colorings of degenerate graphs",
        "date": "2021-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.12673",
        "abstract_url": "http://arxiv.org/abs/2201.12673",
        "authors": [
            {
                "last_name": "Rasetto",
                "first_name": "Marco"
            },
            {
                "last_name": "Wan",
                "first_name": "Qingzhou"
            },
            {
                "last_name": "Akolkar",
                "first_name": "Himanshu"
            },
            {
                "last_name": "Shi",
                "first_name": "Bertram"
            },
            {
                "last_name": "Xiong",
                "first_name": "Feng"
            },
            {
                "last_name": "Benosman",
                "first_name": "Ryad"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Neuromorphic engineering has led to the necessary process of rethinking ofhow we process and integrate information, analyze data, and use the resultinginsights to improve computation and avoid the current high power and latency ofArtificial Intelligence (AI) hardware. Current neuromorphic processors are,however, limited by digital technologies, which cannot reproduce the abilitiesof biological neural computation in terms of power, latency and area cost. Inthis paper, we show that the combined use of the dynamic properties ofmemristors to implement a model of synaptic integration and the determinationof the correct level of abstraction of biological neural networks has thepotential to open a new range of capabilities for neuromorphic processors. Wetest this approach using a novel three-terminal LixWO3 electrochemicalmemristor, by deriving its conductance model and using it to emulate synaptictemporal kernel computation in the context of a pattern recognition task. Weshow that these devices allow for robust results with no loss in precisionwhile opening the path for an energy efficient approach to build novelbio-inspired processing units in silicon.",
        "title": "The Challenges Ahead for Bio-inspired Neuromorphic Event Processors: How  Memristors Dynamic Properties Could Revolutionize Machine Learning",
        "date": "2022-01-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2202.12438",
        "abstract_url": "http://arxiv.org/abs/2202.12438",
        "authors": [
            {
                "last_name": "Dvo\u0159\u00e1k",
                "first_name": "Pavel"
            },
            {
                "last_name": "Krawczyk",
                "first_name": "Monika"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Novotn\u00e1",
                "first_name": "Jana"
            },
            {
                "last_name": "Rz\u0105\u017cewski",
                "first_name": "Pawe\u0142"
            },
            {
                "last_name": "\u017buk",
                "first_name": "Aneta"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  A locally surjective homomorphism from a graph $G$ to a graph $H$ is anedge-preserving mapping from $V(G)$ to $V(H)$ that is surjective in theneighborhood of each vertex in $G$. In the list locally surjective homomorphismproblem, denoted by LLSHom($H$), the graph $H$ is fixed and the instanceconsists of a graph $G$ whose every vertex is equipped with a subset of $V(H)$,called list. We ask for the existence of a locally surjective homomorphism from$G$ to $H$, where every vertex of $G$ is mapped to a vertex from its list. Inthis paper, we study the complexity of the LLSHom($H$) problem in $F$-freegraphs, i.e., graphs that exclude a fixed graph $F$ as an induced subgraph. Weaim to understand for which pairs $(H,F)$ the problem can be solved insubexponential time.  We show that for all graphs $H$, for which the problem is NP-hard in generalgraphs, it cannot be solved in subexponential time in $F$-free graphs unless$F$ is a bounded-degree forest or the ETH fails. The initial study reveals thata natural subfamily of bounded-degree forests $F$ that might lead to sometractability results is the family $\\mathcal S$ consisting of forests whoseevery component has at most three leaves. In this case, we exhibit thefollowing dichotomy theorem: besides the cases that are polynomial-timesolvable in general graphs, the graphs $H \\in \\{P_3,C_4\\}$ are the onlyconnected ones that allow for a subexponential-time algorithm in $F$-freegraphs for every $F \\in \\mathcal S$ (unless the ETH fails).",
        "title": "List Locally Surjective Homomorphisms in Hereditary Graph Classes",
        "date": "2022-02-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2203.05103",
        "abstract_url": "http://arxiv.org/abs/2203.05103",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Wei",
                "first_name": "Shikui"
            },
            {
                "last_name": "Lu",
                "first_name": "Qiming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Neural Ordinary Differential Equations (Neural ODEs) construct the continuousdynamics of hidden units using ordinary differential equations specified by aneural network, demonstrating promising results on many tasks. However, NeuralODEs still do not perform well on image recognition tasks. The possible reasonis that the one-hot encoding vector commonly used in Neural ODEs can notprovide enough supervised information. We propose a new training based onknowledge distillation to construct more powerful and robust Neural ODEsfitting image recognition tasks. Specially, we model the training of NeuralODEs into a teacher-student learning process, in which we propose ResNets asthe teacher model to provide richer supervised information. The experimentalresults show that the new training manner can improve the classificationaccuracy of Neural ODEs by 24% on CIFAR10 and 5% on SVHN. In addition, we alsoquantitatively discuss the effect of both knowledge distillation and timehorizon in Neural ODEs on robustness against adversarial examples. Theexperimental analysis concludes that introducing the knowledge distillation andincreasing the time horizon can improve the robustness of Neural ODEs againstadversarial examples.",
        "title": "Improving Neural ODEs via Knowledge Distillation",
        "date": "2022-03-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2205.02065",
        "abstract_url": "http://arxiv.org/abs/2205.02065",
        "authors": [
            {
                "last_name": "Posso",
                "first_name": "Julien"
            },
            {
                "last_name": "Bois",
                "first_name": "Guy"
            },
            {
                "last_name": "Savaria",
                "first_name": "Yvon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Spacecraft pose estimation is an essential computer vision application thatcan improve the autonomy of in-orbit operations. An ESA/Stanford competitionbrought out solutions that seem hardly compatible with the constraints imposedon spacecraft onboard computers. URSONet is among the best in the competitionfor its generalization capabilities but at the cost of a tremendous number ofparameters and high computational complexity. In this paper, we proposeMobile-URSONet: a spacecraft pose estimation convolutional neural network with178 times fewer parameters while degrading accuracy by no more than four timescompared to URSONet.",
        "title": "Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose  Estimation",
        "date": "2022-05-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.10943",
        "abstract_url": "http://arxiv.org/abs/2206.10943",
        "authors": [
            {
                "last_name": "Linders",
                "first_name": "Viktor"
            },
            {
                "last_name": "Birken",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Conservation and consistency are fundamental properties of discretizations ofsystems of hyperbolic conservation laws. Here, these concepts are extended tothe realm of iterative methods by formally defining locally conservative andflux consistent iterations. These concepts are of both theoretical andpractical importance: Based on recent work by the authors, it is shown thatpseudo-time iterations using explicit Runge-Kutta methods are locallyconservative but not necessarily flux consistent. An extension of theLax-Wendroff theorem is presented, revealing convergence towards weak solutionsof a temporally retarded system of conservation laws. Each equation is modifiedin the same way, namely by a particular scalar factor multiplying the spatialflux terms. A technique for enforcing flux consistency, and thereby recoveringconvergence, is presented. Further, local conservation is established for allKrylov subspace methods, with and without restarts, and for Newton's methodunder certain assumptions on the discretization. Thus it is shown thatNewton-Krylov methods are locally conservative, although not necessarily fluxconsistent. Numerical experiments with the 2D compressible Euler equationscorroborate the theoretical results. Further numerical investigations of theimpact of flux consistency on Newton-Krylov methods indicate that its effect iscase dependent, and diminishes as the number of iterations grow.",
        "title": "Locally conservative and flux consistent iterative methods",
        "date": "2022-06-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.07425",
        "abstract_url": "http://arxiv.org/abs/2207.07425",
        "authors": [
            {
                "last_name": "Hatzel",
                "first_name": "Meike"
            },
            {
                "last_name": "Jaffke",
                "first_name": "Lars"
            },
            {
                "last_name": "Lima",
                "first_name": "Paloma T."
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Sharma",
                "first_name": "Roohani"
            },
            {
                "last_name": "Sorge",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We show fixed-parameter tractability of the Directed Multicut problem withthree terminal pairs (with a randomized algorithm). This problem, given adirected graph $G$, pairs of vertices (called terminals) $(s_1,t_1)$,$(s_2,t_2)$, and $(s_3,t_3)$, and an integer $k$, asks to find a set of at most$k$ non-terminal vertices in $G$ that intersect all $s_1t_1$-paths, all$s_2t_2$-paths, and all $s_3t_3$-paths. The parameterized complexity of thiscase has been open since Chitnis, Cygan, Hajiaghayi, and Marx provedfixed-parameter tractability of the 2-terminal-pairs case at SODA 2012, andPilipczuk and Wahlstr\\\"{o}m proved the W[1]-hardness of the 4-terminal-pairscase at SODA 2016.  On the technical side, we use two recent developments in parameterizedalgorithms. Using the technique of directed flow-augmentation [Kim, Kratsch,Pilipczuk, Wahlstr\\\"{o}m, STOC 2022] we cast the problem as a CSP problem withfew variables and constraints over a large ordered domain.We observe that thisproblem can be in turn encoded as an FO model-checking task over a structureconsisting of a few 0-1 matrices. We look at this problem through the lenses oftwin-width, a recently introduced structural parameter [Bonnet, Kim,Thomass\\'{e}, Watrigant, FOCS 2020]: By a recent characterization [Bonnet,Giocanti, Ossona de Mendes, Simon, Thomass\\'{e}, Toru\\'{n}czyk, STOC 2022] thesaid FO model-checking task can be done in FPT time if the said matrices havebounded grid rank. To complete the proof, we show an irrelevant vertex rule: Ifany of the matrices in the said encoding has a large grid minor, a vertexcorresponding to the ``middle'' box in the grid minor can be proclaimedirrelevant -- not contained in the sought solution -- and thus reduced.",
        "title": "Fixed-parameter tractability of Directed Multicut with three terminal  pairs parameterized by the size of the cutset: twin-width meets  flow-augmentation",
        "date": "2022-07-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.07426",
        "abstract_url": "http://arxiv.org/abs/2207.07426",
        "authors": [
            {
                "last_name": "Jaffke",
                "first_name": "Lars"
            },
            {
                "last_name": "Lima",
                "first_name": "Paloma T."
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Souza",
                "first_name": "Ueverton S."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  We study a generalization of the classic Global Min-Cut problem, calledGlobal Label Min-Cut (or sometimes Global Hedge Min-Cut): the edges of theinput (multi)graph are labeled (or partitioned into color classes or hedges),and removing all edges of the same label (color or from the same hedge) costsone. The problem asks to disconnect the graph at minimum cost.  While the $st$-cut version of the problem is known to be NP-hard, the aboveglobal cut version is known to admit a quasi-polynomial randomized $n^{O(\\log\\mathrm{OPT})}$-time algorithm due to Ghaffari, Karger, and Panigrahi [SODA2017]. They consider this as ``strong evidence that this problem is in P''. Weshow that this is actually not the case. We complete the study of thecomplexity of the Global Label Min-Cut problem by showing that thequasi-polynomial running time is probably optimal: We show that the existenceof an algorithm with running time $(np)^{o(\\log n/ (\\log \\log n)^2)}$ wouldcontradict the Exponential Time Hypothesis, where $n$ is the number ofvertices, and $p$ is the number of labels in the input. The key step for thelower bound is a proof that Global Label Min-Cut is W[1]-hard whenparameterized by the number of uncut labels. In other words, the problem isdifficult in the regime where almost all labels need to be cut to disconnectthe graph. To turn this lower bound into a quasi-polynomial-time lower bound,we also needed to revisit the framework due to Marx [Theory Comput. 2010] ofproving lower bounds assuming Exponential Time Hypothesis through the SubgraphIsomorphism problem parameterized by the number of edges of the pattern. Here,we provide an alternative simplified proof of the hardness of this problem thatis more versatile with respect to the choice of the regimes of the parameters.",
        "title": "A tight quasi-polynomial bound for Global Label Min-Cut",
        "date": "2022-07-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.14403",
        "abstract_url": "http://arxiv.org/abs/2208.14403",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Ayoosh"
            },
            {
                "last_name": "Kim",
                "first_name": "Hunmin"
            },
            {
                "last_name": "Yu",
                "first_name": "Simon"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Hovakimyan",
                "first_name": "Naira"
            },
            {
                "last_name": "Caccamo",
                "first_name": "Marco"
            },
            {
                "last_name": "Sha",
                "first_name": "Lui"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Perception of obstacles remains a critical safety concern for autonomousvehicles. Real-world collisions have shown that the autonomy faults leading tofatal collisions originate from obstacle existence detection. Open sourceautonomous driving implementations show a perception pipeline with complexinterdependent Deep Neural Networks. These networks are not fully verifiable,making them unsuitable for safety-critical tasks.  In this work, we present a safety verification of an existing LiDAR basedclassical obstacle detection algorithm. We establish strict bounds on thecapabilities of this obstacle detection algorithm. Given safety standards, suchbounds allow for determining LiDAR sensor properties that would reliablysatisfy the standards. Such analysis has as yet been unattainable for neuralnetwork based perception systems. We provide a rigorous analysis of theobstacle detection system with empirical results based on real-world sensordata.",
        "title": "Verifiable Obstacle Detection",
        "date": "2022-08-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2209.06171",
        "abstract_url": "http://arxiv.org/abs/2209.06171",
        "authors": [
            {
                "last_name": "Cook",
                "first_name": "Linda"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Reinald",
                "first_name": "Amadeus"
            },
            {
                "last_name": "Souza",
                "first_name": "U\u00e9verton S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            ""
        ],
        "abstract": "  An oriented graph is a digraph that does not contain a directed cycle oflength two. An (oriented) graph $D$ is $H$-free if $D$ does not contain $H$ asan induced sub(di)graph. The Gy\\'arf\\'as-Sumner conjecture is a widely-openconjecture on simple graphs, which states that for any forest $F$, there issome function $f$ such that every $F$-free graph $G$ with clique number$\\omega(G)$ has chromatic number at most $f(\\omega(G))$. Aboulker, Charbit, andNaserasr [Extension of Gy\\'arf\\'as-Sumner Conjecture to Digraphs; E-JC 2021]proposed an analogue of this conjecture to the dichromatic number of orientedgraphs. The dichromatic number of a digraph $D$ is the minimum number of colorsrequired to color the vertex set of $D$ so that no directed cycle in $D$ ismonochromatic.  Aboulker, Charbit, and Naserasr's $\\overrightarrow{\\chi}$-boundednessconjecture states that for every oriented forest $F$, there is some function$f$ such that every $F$-free oriented graph $D$ has dichromatic number at most$f(\\omega(D))$, where $\\omega(D)$ is the size of a maximum clique in the graphunderlying $D$. In this paper, we perform the first step towards provingAboulker, Charbit, and Naserasr's $\\overrightarrow{\\chi}$-boundednessconjecture by showing that it holds when $F$ is any orientation of a path onfour vertices.",
        "title": "Proving a directed analogue of the Gy\\'arf\\'as-Sumner conjecture for  orientations of $P_4$",
        "date": "2022-09-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.09813",
        "abstract_url": "http://arxiv.org/abs/2302.09813",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Juexiao"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Liao",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bin"
            },
            {
                "last_name": "He",
                "first_name": "Wenjia"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongxiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Longxi"
            },
            {
                "last_name": "Gao",
                "first_name": "Xin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Revoking personal private data is one of the basic human rights, which hasalready been sheltered by several privacy-preserving laws in many countries.However, with the development of data science, machine learning and deeplearning techniques, this right is usually neglected or violated as more andmore patients' data are being collected and used for model training, especiallyin intelligent healthcare, thus making intelligent healthcare a sector wheretechnology must meet the law, regulations, and privacy principles to ensurethat the innovation is for the common good. In order to secure patients' rightto be forgotten, we proposed a novel solution by using auditing to guide theforgetting process, where auditing means determining whether a dataset has beenused to train the model and forgetting requires the information of a querydataset to be forgotten from the target model. We unified these two tasks byintroducing a new approach called knowledge purification. To implement oursolution, we developed AFS, a unified open-source software, which is able toevaluate and revoke patients' private data from pre-trained deep learningmodels. We demonstrated the generality of AFS by applying it to four tasks ondifferent datasets with various data sizes and architectures of deep learningnetworks. The software is publicly available at\\url{https://github.com/JoshuaChou2018/AFS}.",
        "title": "Audit to Forget: A Unified Method to Revoke Patients' Private Data in  Intelligent Healthcare",
        "date": "2023-02-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.11571",
        "abstract_url": "http://arxiv.org/abs/2302.11571",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Juexiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Longxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Chu",
                "first_name": "Yuetan"
            },
            {
                "last_name": "Han",
                "first_name": "Wenkai"
            },
            {
                "last_name": "Gao",
                "first_name": "Xin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Heterogeneous data is endemic due to the use of diverse models and settingsof devices by hospitals in the field of medical imaging. However, there are fewopen-source frameworks for federated heterogeneous medical image analysis withpersonalization and privacy protection simultaneously without the demand tomodify the existing model structures or to share any private data. In thispaper, we proposed PPPML-HMI, an open-source learning paradigm for personalizedand privacy-preserving federated heterogeneous medical image analysis. To ourbest knowledge, personalization and privacy protection were achievedsimultaneously for the first time under the federated scenario by integratingthe PerFedAvg algorithm and designing our novel cyclic secure aggregation withthe homomorphic encryption algorithm. To show the utility of PPPML-HMI, weapplied it to a simulated classification task namely the classification ofhealthy people and patients from the RAD-ChestCT Dataset, and one real-worldsegmentation task namely the segmentation of lung infections from COVID-19 CTscans. For the real-world task, PPPML-HMI achieved $\\sim$5\\% higher Dice scoreon average compared to conventional FL under the heterogeneous scenario.Meanwhile, we applied the improved deep leakage from gradients to simulateadversarial attacks and showed the solid privacy-preserving capability ofPPPML-HMI. By applying PPPML-HMI to both tasks with different neural networks,a varied number of users, and sample sizes, we further demonstrated the strongrobustness of PPPML-HMI.",
        "title": "Personalized and privacy-preserving federated heterogeneous medical  image analysis with PPPML-HMI",
        "date": "2023-02-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10567",
        "abstract_url": "http://arxiv.org/abs/2303.10567",
        "authors": [
            {
                "last_name": "Jeong",
                "first_name": "Jinyeong"
            },
            {
                "last_name": "Kim",
                "first_name": "Min Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a decentralized passive impedance control scheme forcollaborative grasping using under-actuated aerial manipulators (AMs). The AMsystem is formulated, using a proper coordinate transformation, as aninertially decoupled dynamics with which a passivity-based control design isconducted. Since the interaction for grasping can be interpreted as a feedbackinterconnection of passive systems, an arbitrary number of AMs can be modularlycombined, leading to a decentralized control scheme. Another interestingconsequence of the passivity property is that the AMs automatically converge toa certain configuration to accomplish the grasping. Collaborative graspingusing 10 AMs is presented in simulation.",
        "title": "Passivity-based Decentralized Control for Collaborative Grasping of  Under-Actuated Aerial Manipulators",
        "date": "2023-03-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.14853",
        "abstract_url": "http://arxiv.org/abs/2304.14853",
        "authors": [
            {
                "last_name": "Manjunath",
                "first_name": "Shashank"
            },
            {
                "last_name": "Perea",
                "first_name": "Jose A."
            },
            {
                "last_name": "Sathyanarayana",
                "first_name": "Aarti"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CG"
        ],
        "abstract": "  Topological data analysis (TDA) is an emerging technique for biologicalsignal processing. TDA leverages the invariant topological features of signalsin a metric space for robust analysis of signals even in the presence of noise.In this paper, we leverage TDA on brain connectivity networks derived fromelectroencephalogram (EEG) signals to identify statistical differences betweenpediatric patients with obstructive sleep apnea (OSA) and pediatric patientswithout OSA. We leverage a large corpus of data, and show that TDA enables usto see a statistical difference between the brain dynamics of the two groups.",
        "title": "Topological Data Analysis of Electroencephalogram Signals for Pediatric  Obstructive Sleep Apnea",
        "date": "2023-04-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.10945",
        "abstract_url": "http://arxiv.org/abs/2305.10945",
        "authors": [
            {
                "last_name": "Heisler",
                "first_name": "Marcel"
            },
            {
                "last_name": "Becker-Asano",
                "first_name": "Christian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper describes, how current Machine Learning (ML) techniques combinedwith simple rule-based animation routines make an android robot head anembodied conversational agent with ChatGPT as its core component. The androidrobot head is described, technical details are given of how lip-sync animationis being achieved, and general software design decisions are presented. Apublic presentation of the system revealed improvement opportunities that arereported and that lead our iterative implementation approach.",
        "title": "An Android Robot Head as Embodied Conversational Agent",
        "date": "2023-05-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11292",
        "abstract_url": "http://arxiv.org/abs/2305.11292",
        "authors": [
            {
                "last_name": "Vinod",
                "first_name": "Vivin"
            },
            {
                "last_name": "Maity",
                "first_name": "Sayan"
            },
            {
                "last_name": "Zaspel",
                "first_name": "Peter"
            },
            {
                "last_name": "Kleinekath\u00f6fer",
                "first_name": "Ulrich"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  The accurate but fast calculation of molecular excited states is still a verychallenging topic. For many applications, detailed knowledge of the energyfunnel in larger molecular aggregates is of key importance requiring highlyaccurate excited state energies. To this end, machine learning techniques canbe an extremely useful tool though the cost of generating highly accuratetraining datasets still remains a severe challenge. To overcome this hurdle,this work proposes the use of multi-fidelity machine learning where very littletraining data from high accuracies is combined with cheaper and less accuratedata to achieve the accuracy of the costlier level. In the present study, theapproach is employed to predict the first excited state energies for threemolecules of increasing size, namely, benzene, naphthalene, and anthracene. Theenergies are trained and tested for conformations stemming from classicalmolecular dynamics simulations and from real-time density functionaltight-binding calculations. It can be shown that the multi-fidelity machinelearning model can achieve the same accuracy as a machine learning model builtonly on high cost training data while having a much lower computational effortto generate the data. The numerical gain observed in these benchmark testcalculations was over a factor of 30 but certainly can be much higher for highaccuracy data.",
        "title": "Multi-Fidelity Machine Learning for Excited State Energies of Molecules",
        "date": "2023-05-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11728",
        "abstract_url": "http://arxiv.org/abs/2305.11728",
        "authors": [
            {
                "last_name": "Tabatabaei",
                "first_name": "Zahra"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adrian"
            },
            {
                "last_name": "Moll",
                "first_name": "Javier Oliver"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Digital pathology has revolutionized cancer diagnosis by leveragingContent-Based Medical Image Retrieval (CBMIR) for analyzing histopathologicalWhole Slide Images (WSIs). CBMIR enables searching for similar content,enhancing diagnostic reliability and accuracy. In 2020, breast and prostatecancer constituted 11.7% and 14.1% of cases, respectively, as reported by theGlobal Cancer Observatory (GCO). The proposed Unsupervised CBMIR (UCBMIR)replicates the traditional cancer diagnosis workflow, offering a dependablemethod to support pathologists in WSI-based diagnostic conclusions. Thisapproach alleviates pathologists' workload, potentially enhancing diagnosticefficiency. To address the challenge of the lack of labeled histopathologicalimages in CBMIR, a customized unsupervised Convolutional Auto Encoder (CAE) wasdeveloped, extracting 200 features per image for the search engine component.UCBMIR was evaluated using widely-used numerical techniques in CBMIR, alongsidevisual evaluation and comparison with a classifier. The validation involvedthree distinct datasets, with an external evaluation demonstrating itseffectiveness. UCBMIR outperformed previous studies, achieving a top 5 recallof 99% and 80% on BreaKHis and SICAPv2, respectively, using the firstevaluation technique. Precision rates of 91% and 70% were achieved for BreaKHisand SICAPv2, respectively, using the second evaluation technique. Furthermore,UCBMIR demonstrated the capability to identify various patterns in patches,achieving an 81% accuracy in the top 5 when tested on an external image fromArvaniti.",
        "title": "Towards More Transparent and Accurate Cancer Diagnosis with an  Unsupervised CAE Approach",
        "date": "2023-05-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.12722",
        "abstract_url": "http://arxiv.org/abs/2305.12722",
        "authors": [
            {
                "last_name": "Nilsson",
                "first_name": "Gustav"
            },
            {
                "last_name": "Aquino",
                "first_name": "Alejandro D. Owen"
            },
            {
                "last_name": "Coogan",
                "first_name": "Samuel"
            },
            {
                "last_name": "Molzahn",
                "first_name": "Daniel K."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The ongoing electrification of the transportation fleet will increase theload on the electric power grid. Since both the transportation network and thepower grid already experience periods of significant stress, joint analyses ofboth infrastructures will most likely be necessary to ensure acceptableoperation in the future. To enable such analyses, this paper presents anopen-source testbed that jointly simulates high-fidelity models of both theelectric distribution system and the transportation network. The testbedutilizes two open-source simulators, OpenDSS to simulate the electricdistribution system and the microscopic traffic simulator SUMO to simulate thetraffic dynamics. Electric vehicle charging links the electric distributionsystem and the transportation network models at vehicle locations determinedusing publicly available parcel data. Leveraging high-fidelity syntheticelectric distribution system data from the SMART-DS project and transportationsystem data from OpenStreetMap, this testbed models the city of Greensboro, NCdown to the household level. Moreover, the methodology and the supportingscripts released with the testbed allow adaption to other areas wherehigh-fidelity geolocated OpenDSS datasets are available. After describing thecomponents and usage of the testbed, we exemplify applications enabled by thetestbed via two scenarios modeling the extreme stresses encountered duringevacuations.",
        "title": "GreenEVT: Greensboro Electric Vehicle Testbed",
        "date": "2023-05-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.15099",
        "abstract_url": "http://arxiv.org/abs/2305.15099",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Yang",
                "first_name": "Meng"
            },
            {
                "last_name": "Feng",
                "first_name": "Minwei"
            },
            {
                "last_name": "Yin",
                "first_name": "Jingcheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinbing"
            },
            {
                "last_name": "Leng",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhouhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The transformer model is known to be computationally demanding, andprohibitively costly for long sequences, as the self-attention module uses aquadratic time and space complexity with respect to sequence length. Manyresearchers have focused on designing new forms of self-attention orintroducing new parameters to overcome this limitation, however a large portionof them prohibits the model to inherit weights from large pretrained models. Inthis work, the transformer's inefficiency has been taken care of from anotherperspective. We propose Fourier Transformer, a simple yet effective approach byprogressively removing redundancies in hidden sequence using the ready-madeFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation(DCT). Fourier Transformer is able to significantly reduce computational costswhile retain the ability to inherit from various large pretrained models.Experiments show that our model achieves state-of-the-art performances amongall transformer-based models on the long-range modeling benchmark LRA withsignificant improvement in both speed and space. For generative seq-to-seqtasks including CNN/DailyMail and ELI5, by inheriting the BART weights ourmodel outperforms the standard BART and other efficient models. \\footnote{Ourcode is publicly available at\\url{https://github.com/LUMIA-Group/FourierTransformer}}",
        "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence  Redundancy with FFT Operator",
        "date": "2023-05-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.17594",
        "abstract_url": "http://arxiv.org/abs/2305.17594",
        "authors": [
            {
                "last_name": "Bian",
                "first_name": "Sizhen"
            },
            {
                "last_name": "Rupp",
                "first_name": "Alexander"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  In recent years, working out in the gym has gotten increasingly moredata-focused and many gym enthusiasts are recording their exercises to have abetter overview of their historical gym activities and to make a betterexercise plan for the future. As a side effect, this recording process has ledto a lot of time spent painstakingly operating these apps by plugging in usedtypes of equipment and repetitions. This project aims to automate this processusing an Internet of Things (IoT) approach. Specifically, beacons with embeddedultra-low-power inertial measurement units (IMUs) are attached to the types ofequipment to recognize the usage and transmit the information to gym-goers andmanagers. We have created a small ecosystem composed of beacons, a gateway,smartwatches, android/iPhone applications, a firebase cloud server, and adashboard, all communicating over a mixture of Bluetooth and Wifi to distributecollected data from machines to users and gym managers in a compact andmeaningful way. The system we have implemented is a working prototype of abigger end goal and is supposed to initialize progress toward a smarter, moreefficient, and still privacy-respect gym environment in the future. Asmall-scale real-life test shows 94.6\\% accuracy in user gym session recording,which can reach up to 100\\% easily with a more suitable assembling of thebeacons. This promising result shows the potential of a fully automaticexercise recording system, which enables comprehensive monitoring and analysisof the exercise sessions and frees the user from manual recording. Theestimated battery life of the beacon is 400 days with a 210 mAh coin battery.We also discussed the shortcoming of the current demonstration system and thefuture work for a reliable and ready-to-deploy automatic gym workout recordingsystem.",
        "title": "Fully Automatic Gym Exercises Recording: An IoT Solution",
        "date": "2023-05-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.00265",
        "abstract_url": "http://arxiv.org/abs/2307.00265",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Ying"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingqing"
            },
            {
                "last_name": "Chen",
                "first_name": "Wen"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Li",
                "first_name": "Ming"
            },
            {
                "last_name": "da Costa",
                "first_name": "Daniel Benevides"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper studies an intelligent reflecting surface (IRS)-aidedmulti-antenna simultaneous wireless information and power transfer (SWIPT)system where an $M$-antenna access point (AP) serves $K$ single-antennainformation users (IUs) and $J$ single-antenna energy users (EUs) with the aidof an IRS with phase errors. We explicitly concentrate on overloaded scenarioswhere $K + J > M$ and $K \\geq M$. Our goal is to maximize the minimumthroughput among all the IUs by optimizing the allocation of resources(including time, transmit beamforming at the AP, and reflect beamforming at theIRS), while guaranteeing the minimum amount of harvested energy at each EU.Towards this goal, we propose two user grouping (UG) schemes, namely, thenon-overlapping UG scheme and the overlapping UG scheme, where the differencelies in whether identical IUs can exist in multiple groups. Different IU groupsare served in orthogonal time dimensions, while the IUs in the same group areserved simultaneously with all the EUs via spatial multiplexing. The twoproblems corresponding to the two UG schemes are mixed-integer non-convexoptimization problems and difficult to solve optimally. We propose efficientalgorithms for these two problems based on the big-M formulation, the penaltymethod, the block coordinate descent, and the successive convex approximation.Simulation results show that: 1) the non-robust counterparts of the proposedrobust designs are unsuitable for practical IRS-aided SWIPT systems with phaseerrors since the energy harvesting constraints cannot be satisfied; 2) theproposed UG strategies can significantly improve the max-min throughput overthe benchmark schemes without UG or adopting random UG; 3) the overlapping UGscheme performs much better than its non-overlapping counterpart when theabsolute difference between $K$ and $M$ is small and the EH constraints are notstringent.",
        "title": "IRS-Aided Overloaded Multi-Antenna Systems: Joint User Grouping and  Resource Allocation",
        "date": "2023-07-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.03299",
        "abstract_url": "http://arxiv.org/abs/2308.03299",
        "authors": [
            {
                "last_name": "Gaba",
                "first_name": "Aimen"
            },
            {
                "last_name": "Kaufman",
                "first_name": "Zhanna"
            },
            {
                "last_name": "Chueng",
                "first_name": "Jason"
            },
            {
                "last_name": "Shvakel",
                "first_name": "Marie"
            },
            {
                "last_name": "Hall",
                "first_name": "Kyle Wm."
            },
            {
                "last_name": "Brun",
                "first_name": "Yuriy"
            },
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Machine learning technology has become ubiquitous, but, unfortunately, oftenexhibits bias. As a consequence, disparate stakeholders need to interact withand make informed decisions about using machine learning models in everydaysystems. Visualization technology can support stakeholders in understanding andevaluating trade-offs between, for example, accuracy and fairness of models.This paper aims to empirically answer \"Can visualization design choices affecta stakeholder's perception of model bias, trust in a model, and willingness toadopt a model?\" Through a series of controlled, crowd-sourced experiments withmore than 1,500 participants, we identify a set of strategies people follow indeciding which models to trust. Our results show that men and women prioritizefairness and performance differently and that visual design choicessignificantly affect that prioritization. For example, women trust fairermodels more often than men do, participants value fairness more when it isexplained using text than as a bar chart, and being explicitly told a model isbiased has a bigger impact than showing past biased performance. We test thegeneralizability of our results by comparing the effect of multiple textual andvisual design choices and offer potential explanations of the cognitivemechanisms behind the difference in fairness perception and trust. Our researchguides design considerations to support future work developing visualizationsystems for machine learning.",
        "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and  Perceived Bias in Machine Learning",
        "date": "2023-08-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.06085",
        "abstract_url": "http://arxiv.org/abs/2308.06085",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jinqiang"
            },
            {
                "last_name": "Dwarka",
                "first_name": "Vandana"
            },
            {
                "last_name": "Vuik",
                "first_name": "Cornelis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  The Helmholtz equation is related to seismic exploration, sonar, antennas,and medical imaging applications. It is one of the most challenging problems tosolve in terms of accuracy and convergence due to the scalability issues of thenumerical solvers. For 3D large-scale applications, high-performance parallelsolvers are also needed. In this paper, a matrix-free parallel iterative solveris presented for the three-dimensional (3D) heterogeneous Helmholtz equation.We consider the preconditioned Krylov subspace methods for solving the linearsystem obtained from finite-difference discretization. The Complex ShiftedLaplace Preconditioner (CSLP) is employed since it results in a linear increasein the number of iterations as a function of the wavenumber. The preconditioneris approximately inverted using one parallel 3D multigrid cycle. For parallelcomputing, the global domain is partitioned blockwise. The matrix-vectormultiplication and preconditioning operator are implemented in a matrix-freeway instead of constructing large, memory-consuming coefficient matrices.Numerical experiments of 3D model problems demonstrate the robustness andoutstanding strong scaling of our matrix-free parallel solution method.Moreover, the weak parallel scalability indicates our approach is suitable forrealistic 3D heterogeneous Helmholtz problems with minimized pollution error.",
        "title": "A matrix-free parallel solution method for the three-dimensional  heterogeneous Helmholtz equation",
        "date": "2023-08-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.13694",
        "abstract_url": "http://arxiv.org/abs/2308.13694",
        "authors": [
            {
                "last_name": "McDermott",
                "first_name": "Matthew"
            },
            {
                "last_name": "Rife",
                "first_name": "Jason"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Because scanning-LIDAR sensors require finite time to create a point cloud,sensor motion during a scan warps the resulting image, a phenomenon known asmotion distortion or rolling shutter. Motion-distortion correction methodsexist, but they rely on external measurements or Bayesian filtering overmultiple LIDAR scans. In this paper we propose a novel algorithm that performssnapshot processing to obtain a motion-distortion correction. Snapshotprocessing, which registers a current LIDAR scan to a reference image withoutusing external sensors or Bayesian filtering, is particularly relevant forlocalization to a high-definition (HD) map. Our approach, which we callVelocity-corrected Iterative Compact Ellipsoidal Transformation (VICET),extends the well-known Normal Distributions Transform (NDT) algorithm to solvejointly for both a 6 Degree-of-Freedom (DOF) rigid transform between two LIDARscans and a set of 6DOF motion states that describe distortion within thecurrent LIDAR scan. Using experiments, we show that VICET achievessignificantly higher accuracy than NDT or Iterative Closest Point (ICP)algorithms when localizing a distorted raw LIDAR scan against an undistorted HDMap. We recommend the reader explore our open-source code and visualizations athttps://github.com/mcdermatt/VICET, which supplements this manuscript.",
        "title": "Correcting Motion Distortion for LIDAR HD-Map Localization",
        "date": "2023-08-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.02045",
        "abstract_url": "http://arxiv.org/abs/2310.02045",
        "authors": [
            {
                "last_name": "Rogenmoser",
                "first_name": "Michael"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  One of the key challenges when operating microcontrollers in harshenvironments such as space is radiation-induced Single Event Upsets (SEUs),which can lead to errors in computation. Common countermeasures rely onproprietary radiation-hardened technologies, low density technologies, orextensive replication, leading to high costs and low performance andefficiency. To combat this, we present Trikarenos, a fault-tolerant 32-bitRISC-V microcontroller SoC in an advanced TSMC 28nm technology. Trikarenosalleviates the replication cost by employing a configurable triple-corelockstep configuration, allowing three Ibex cores to execute applicationsreliably, operating on ECC-protected memory. If reliability is not needed for agiven application, the cores can operate independently in parallel for higherperformance and efficiency. Trikarenos consumes 15.7mW at 250MHz executing afault-tolerant matrix-matrix multiplication, a 21.5x efficiency gain overstate-of-the-art, and performance is increased by 2.96x when reliability is notneeded for processing, with a 2.36x increase in energy efficiency.",
        "title": "Trikarenos: A Fault-Tolerant RISC-V-based Microcontroller for CubeSats  in 28nm",
        "date": "2023-10-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.08261",
        "abstract_url": "http://arxiv.org/abs/2310.08261",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Wei",
                "first_name": "Haiyue"
            },
            {
                "last_name": "Bai",
                "first_name": "Lin"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  LiDAR and cameras are complementary sensors for 3D object detection inautonomous driving. However, it is challenging to explore the unnaturalinteraction between point clouds and images, and the critical factor is how toconduct feature alignment of heterogeneous modalities. Currently, many methodsachieve feature alignment by projection calibration only, without consideringthe problem of coordinate conversion accuracy errors between sensors, leadingto sub-optimal performance. In this paper, we present GraphAlign, a moreaccurate feature alignment strategy for 3D object detection by graph matching.Specifically, we fuse image features from a semantic segmentation encoder inthe image branch and point cloud features from a 3D Sparse CNN in the LiDARbranch. To save computation, we construct the nearest neighbor relationship bycalculating Euclidean distance within the subspaces that are divided into thepoint cloud features. Through the projection calibration between the image andpoint cloud, we project the nearest neighbors of point cloud features onto theimage features. Then by matching the nearest neighbors with a single pointcloud to multiple images, we search for a more appropriate feature alignment.In addition, we provide a self-attention module to enhance the weights ofsignificant relations to fine-tune the feature alignment between heterogeneousmodalities. Extensive experiments on nuScenes benchmark demonstrate theeffectiveness and efficiency of our GraphAlign.",
        "title": "GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for  Multi-Modal 3D Object Detection",
        "date": "2023-10-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.13320",
        "abstract_url": "http://arxiv.org/abs/2310.13320",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shaoan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Mingzhu"
            },
            {
                "last_name": "Hu",
                "first_name": "Yaoqing"
            },
            {
                "last_name": "Li",
                "first_name": "Dongyue"
            },
            {
                "last_name": "Yuan",
                "first_name": "Fusong"
            },
            {
                "last_name": "Yu",
                "first_name": "Junzhi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "RO"
        ],
        "abstract": "  High-precision pose estimation based on visual markers has been a thrivingresearch topic in the field of computer vision. However, the suitability oftraditional flat markers on curved objects is limited due to the diverse shapesof curved surfaces, which hinders the development of high-precision poseestimation for curved objects. Therefore, this paper proposes a novel visualmarker called CylinderTag, which is designed for developable curved surfacessuch as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmlyattached to objects with a cylindrical shape. Leveraging the manifoldassumption, the cross-ratio in projective invariance is utilized for encodingin the direction of zero curvature on the surface. Additionally, to facilitatethe usage of CylinderTag, we propose a heuristic search-based marker generatorand a high-performance recognizer as well. Moreover, an all-encompassingevaluation of CylinderTag properties is conducted by means of extensiveexperimentation, covering detection rate, detection speed, dictionary size,localization jitter, and pose estimation accuracy. CylinderTag showcasessuperior detection performance from varying view angles in comparison totraditional visual markers, accompanied by higher localization accuracy.Furthermore, CylinderTag boasts real-time detection capability and an extensivemarker dictionary, offering enhanced versatility and practicality in a widerange of applications. Experimental results demonstrate that the CylinderTag isa highly promising visual marker for use on cylindrical-like surfaces, thusoffering important guidance for future research on high-precision visuallocalization of cylinder-shaped objects. The code is available at:https://github.com/wsakobe/CylinderTag.",
        "title": "CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects  Pose Estimation Based on Projective Invariants",
        "date": "2023-10-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.01773",
        "abstract_url": "http://arxiv.org/abs/2311.01773",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Yin",
                "first_name": "Fukun"
            },
            {
                "last_name": "Fan",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Li",
                "first_name": "Hui"
            },
            {
                "last_name": "Chen",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Wen"
            },
            {
                "last_name": "Lu",
                "first_name": "Chongshan"
            },
            {
                "last_name": "YU",
                "first_name": "Gang"
            },
            {
                "last_name": "Chen",
                "first_name": "Tao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advances in implicit neural representations have achieved impressiveresults by sampling and fusing individual points along sampling rays in thesampling space. However, due to the explosively growing sampling space, finelyrepresenting and synthesizing detailed textures remains a challenge forunbounded large-scale outdoor scenes. To alleviate the dilemma of usingindividual points to perceive the entire colossal space, we explore learningthe surface distribution of the scene to provide structural priors and reducethe samplable space and propose a Point Diffusion implicit Function, PDF, forlarge-scale scene neural representation. The core of our method is alarge-scale point cloud super-resolution diffusion module that enhances thesparse point cloud reconstructed from several training images into a densepoint cloud as an explicit prior. Then in the rendering stage, only samplingpoints with prior points within the sampling radius are retained. That is, thesampling space is reduced from the unbounded space to the scene surface.Meanwhile, to fill in the background of the scene that cannot be provided bypoint clouds, the region sampling based on Mip-NeRF 360 is employed to modelthe background representation. Expensive experiments have demonstrated theeffectiveness of our method for large-scale scene novel view synthesis, whichoutperforms relevant state-of-the-art baselines.",
        "title": "PDF: Point Diffusion Implicit Function for Large-scale Scene Neural  Representation",
        "date": "2023-11-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.06067",
        "abstract_url": "http://arxiv.org/abs/2311.06067",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Xin"
            },
            {
                "last_name": "Chen",
                "first_name": "Shikun"
            },
            {
                "last_name": "Cao",
                "first_name": "Yichao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xin"
            },
            {
                "last_name": "Lu",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "CV"
        ],
        "abstract": "  In recent years, hashing methods have been popular in the large-scale mediasearch for low storage and strong representation capabilities. To describeobjects with similar overall appearance but subtle differences, more and morestudies focus on hashing-based fine-grained image retrieval. Existing hashingnetworks usually generate both local and global features through attentionguidance on the same deep activation tensor, which limits the diversity offeature representations. To handle this limitation, we substitute convolutionaldescriptors for attention-guided features and propose an Attributes Groupingand Mining Hashing (AGMH), which groups and embeds the category-specific visualattributes in multiple descriptors to generate a comprehensive featurerepresentation for efficient fine-grained image retrieval. Specifically, anAttention Dispersion Loss (ADL) is designed to force the descriptors to attendto various local regions and capture diverse subtle details. Moreover, wepropose a Stepwise Interactive External Attention (SIEA) to mine criticalattributes in each descriptor and construct correlations between fine-grainedattributes and objects. The attention mechanism is dedicated to learningdiscrete attributes, which will not cost additional computations in hash codesgeneration. Finally, the compact binary codes are learned by preservingpairwise similarities. Experimental results demonstrate that AGMH consistentlyyields the best performance against state-of-the-art methods on fine-grainedbenchmark datasets.",
        "title": "Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval",
        "date": "2023-11-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10653",
        "abstract_url": "http://arxiv.org/abs/2311.10653",
        "authors": [
            {
                "last_name": "Keyvanian",
                "first_name": "Shafagh"
            },
            {
                "last_name": "Johnson",
                "first_name": "Michelle J."
            },
            {
                "last_name": "Figueroa",
                "first_name": "Nadia"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  A realistic human kinematic model that satisfies anatomical constraints isessential for human-robot interaction, biomechanics and robot-assistedrehabilitation. Modeling realistic joint constraints, however, is challengingas human arm motion is constrained by joint limits, inter- and intra-jointdependencies, self-collisions, individual capabilities and muscular orneurological constraints which are difficult to represent. Hence, physiciansand researchers have relied on simple box-constraints, ignoring importantanatomical factors. In this paper, we propose a data-driven method to learnrealistic anatomically constrained upper-limb range of motion (RoM) boundariesfrom motion capture data. This is achieved by fitting a one-class supportvector machine to a dataset of upper-limb joint space exploration motions withan efficient hyper-parameter tuning scheme. Our approach outperforms similarworks focused on valid RoM learning. Further, we propose an impairment index(II) metric that offers a quantitative assessment of capability/impairment whencomparing healthy and impaired arms. We validate the metric on healthy subjectsphysically constrained to emulate hemiplegia and different disability levels asstroke patients.",
        "title": "Learning Realistic Joint Space Boundaries for Range of Motion Analysis  of Healthy and Impaired Human Arms",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10765",
        "abstract_url": "http://arxiv.org/abs/2311.10765",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yufeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The challenge of improving translation accuracy in GPT-4 is being addressedby harnessing a method known as in-context learning. This paper introduces astrategic approach to utilize in-context learning specifically for machinetranslation, aiming to significantly boost accuracy. The crux of this methodlies in the judicious selection of demonstrations that are most effective forin-context learning. By selecting these examples carefully, GPT-4 can utilizethem to achieve remarkably accurate machine translations, eliminating the needfor task-specific fine-tuning. This technique is anchored in the semanticsimilarities between the user's prompt and the chosen dataset. Sentences fromthis dataset, carefully picked for their relevance and clarity, serve as potentdemonstrations for in-context learning. This approach not only enhancestranslation accuracy but also enriches the understanding of nuanced linguisticstructures. It represents a significant step forward in machine learning,leveraging the inherent capabilities of GPT-4 to provide translations that arenot only accurate but also contextually rich and linguistically sophisticated.This method demonstrates the potential of in-context learning in overcominglanguage barriers, opening new avenues for cross-cultural communication andglobal collaboration.",
        "title": "Enhancing Machine Translation through Advanced In-Context Learning: A  Methodological Strategy for GPT-4 Improvement",
        "date": "2023-11-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.02937",
        "abstract_url": "http://arxiv.org/abs/2312.02937",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Ayoosh"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhu",
                "first_name": "James"
            },
            {
                "last_name": "Cheng",
                "first_name": "Sheng"
            },
            {
                "last_name": "Gu",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Yoon",
                "first_name": "Hyung-Jin"
            },
            {
                "last_name": "Kim",
                "first_name": "Hunmin"
            },
            {
                "last_name": "Hovakimyan",
                "first_name": "Naira"
            },
            {
                "last_name": "Sha",
                "first_name": "Lui"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Perception, Planning, and Control form the essential components of autonomyin advanced air mobility. This work advances the holistic integration of thesecomponents to enhance the performance and robustness of the completecyber-physical system. We adapt Perception Simplex, a system for verifiablecollision avoidance amidst obstacle detection faults, to the vertical landingmaneuver for autonomous air mobility vehicles. We improve upon this system byreplacing static assumptions of control capabilities with dynamic confirmation,i.e., real-time confirmation of control limitations of the system, ensuringreliable fulfillment of safety maneuvers and overrides, without dependence onoverly pessimistic assumptions. Parameters defining control system capabilitiesand limitations, e.g., maximum deceleration, are continuously tracked withinthe system and used to make safety-critical decisions. We apply thesetechniques to propose a verifiable collision avoidance solution for autonomousaerial mobility vehicles operating in cluttered and potentially unsafeenvironments.",
        "title": "Synergistic Perception and Control Simplex for Verifiable Safe Vertical  Landing",
        "date": "2023-12-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.16814",
        "abstract_url": "http://arxiv.org/abs/2312.16814",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Jindan"
            },
            {
                "last_name": "Xu",
                "first_name": "Wei"
            },
            {
                "last_name": "Yuen",
                "first_name": "Chau"
            },
            {
                "last_name": "Swindlehurst",
                "first_name": "A. Lee"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chunming"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Reconfigurable intelligent surface (RIS) technology is emerging as apromising technique for performance enhancement for next-generation wirelessnetworks. This paper investigates the physical layer security of anRIS-assisted multiple-antenna communication system in the presence of randomspatially distributed eavesdroppers. The RIS-to-ground channels are assumed toexperience Rician fading. Using stochastic geometry, exact distributions of thereceived signal-to-noise-ratios (SNRs) at the legitimate user and theeavesdroppers located according to a Poisson point process (PPP) are derived,and closed-form expressions for the secrecy outage probability (SOP) and theergodic secrecy capacity (ESC) are obtained to provide insightful guidelinesfor system design. First, the secrecy diversity order is obtained as$\\frac{2}{\\alpha_2}$, where $\\alpha_2$ denotes the path loss exponent of theRIS-to-ground links. Then, it is revealed that the secrecy performance ismainly affected by the number of RIS reflecting elements, $N$, and the impactof the number of transmit antennas and transmit power at the base station ismarginal. In addition, when the locations of the randomly located eavesdroppersare unknown, deploying the RIS closer to the legitimate user rather than to thebase station is shown to be more efficient. Moreover, it is also found that thedensity of randomly located eavesdroppers, $\\lambda_e$, has an additive effecton the asymptotic ESC performance given by$\\log_2{\\left({1}/{\\lambda_e}\\right)}$. Finally, numerical simulations areconducted to verify the accuracy of these theoretical observations.",
        "title": "On Secrecy Performance of RIS-Assisted MISO Systems over Rician Channels  with Spatially Random Eavesdroppers",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01060",
        "abstract_url": "http://arxiv.org/abs/2401.01060",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Shuzheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Wenxin"
            },
            {
                "last_name": "Gao",
                "first_name": "Cuiyun"
            },
            {
                "last_name": "Li",
                "first_name": "Li"
            },
            {
                "last_name": "Hu",
                "first_name": "Xing"
            },
            {
                "last_name": "Xia",
                "first_name": "Xin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Michael R."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Pre-trained code models have recently achieved substantial improvements inmany code intelligence tasks. These models are first pre-trained on large-scaleunlabeled datasets in a task-agnostic manner using self-supervised learning,and then fine-tuned on labeled datasets in downstream tasks. However, thelabeled datasets are usually limited in size (i.e., human intensive efforts),which may hinder the performance of pre-trained code models in specific tasks.To mitigate this, one possible solution is to leverage the large-scaleunlabeled data in the tuning stage by pseudo-labeling. However, directlyemploying the pseudo-labeled data can bring a large amount of noise, i.e.,incorrect labels, leading to suboptimal performance. How to effectivelyleverage the noisy pseudo-labeled data is a challenging yet under-exploredproblem.In this paper, we propose a novel approach named HINT to improvepre-trained code models with large-scale unlabeled datasets by better utilizingthe pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeleddata selection and Noise-tolerant Training. In the hybrid pseudo-data selectionmodule, considering the robustness issue, apart from directly measuring thequality of pseudo labels through training loss, we further propose to employ aretrieval-based method to filter low-quality pseudo-labeled data. Thenoise-tolerant training module aims to further mitigate the influence of errorsin pseudo labels by training the model with a noise-tolerant loss function andby regularizing the consistency of model predictions.The experimental resultsshow that HINT can better leverage those unlabeled data in a task-specific wayand provide complementary benefits for pre-trained models, e.g., improving thebest baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defectdetection, and assertion generation, respectively.",
        "title": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively  Tuning Pre-trained Code Models",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01753",
        "abstract_url": "http://arxiv.org/abs/2401.01753",
        "authors": [
            {
                "last_name": "Vaidya",
                "first_name": "Amal"
            },
            {
                "last_name": "Vankayalapati",
                "first_name": "Mohan Krishna"
            },
            {
                "last_name": "Chan",
                "first_name": "Jacky"
            },
            {
                "last_name": "Ibraimoski",
                "first_name": "Senad"
            },
            {
                "last_name": "Moran",
                "first_name": "Sean"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We present a tool that leverages generative AI to accelerate the migration ofon-premises applications to the cloud. The Cloud Migration LLM accepts inputfrom the user specifying the parameters of their migration, and outputs amigration strategy with an architecture diagram. A user study suggests that themigration LLM can assist inexperienced users in finding the right cloudmigration profile, while avoiding complexities of a manual approach.",
        "title": "A Generative AI Assistant to Accelerate Cloud Migration",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02702",
        "abstract_url": "http://arxiv.org/abs/2401.02702",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Xie",
                "first_name": "Jun"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            },
            {
                "last_name": "Xu",
                "first_name": "Shaoqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhepeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  LiDAR-camera fusion can enhance the performance of 3D object detection byutilizing complementary information between depth-aware LiDAR points andsemantically rich images. Existing voxel-based methods face significantchallenges when fusing sparse voxel features with dense image features in aone-to-one manner, resulting in the loss of the advantages of images, includingsemantic and continuity information, leading to sub-optimal detectionperformance, especially at long distances. In this paper, we presentVoxelNextFusion, a multi-modal 3D object detection framework specificallydesigned for voxel-based methods, which effectively bridges the gap betweensparse point clouds and dense images. In particular, we propose a voxel-basedimage pipeline that involves projecting point clouds onto images to obtain bothpixel- and patch-level features. These features are then fused using aself-attention to obtain a combined representation. Moreover, to address theissue of background features present in patches, we propose a featureimportance module that effectively distinguishes between foreground andbackground features, thus minimizing the impact of the background features.Extensive experiments were conducted on the widely used KITTI and nuScenes 3Dobject detection benchmarks. Notably, our VoxelNextFusion achieved around+3.20% in AP@0.7 improvement for car detection in hard level compared to theVoxel R-CNN baseline on the KITTI test dataset",
        "title": "VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework  for Multi-Modal 3D Object Detection",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02899",
        "abstract_url": "http://arxiv.org/abs/2401.02899",
        "authors": [
            {
                "last_name": "Lan\u010da",
                "first_name": "Luka"
            },
            {
                "last_name": "Jakac",
                "first_name": "Karlo"
            },
            {
                "last_name": "Ivi\u0107",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA",
            ""
        ],
        "abstract": "  This research addresses the challenge of executing multi-UAV survey missionsover diverse terrains characterized by varying elevations. The approachintegrates advanced two-dimensional ergodic search technique with modelpredictive control of UAV altitude and velocity. Optimization of altitude andvelocity is performed along anticipated UAV ground routes, considering multipleobjectives and constraints. This yields a flight regimen tailored to theterrain, as well as the motion and sensing characteristics of the UAVs. Theproposed UAV motion control strategy is assessed through simulations ofrealistic search missions and actual terrain models. Results demonstrate thesuccessful integration of model predictive altitude and velocity control with atwo-dimensional potential field-guided ergodic search. Adjusting UAV altitudesto near-ideal levels facilitates the utilization of sensing ranges, therebyenhancing the effectiveness of the search. Furthermore, the control algorithmis capable of real-time computation, encouraging its practical application inreal-world scenarios.",
        "title": "Model predictive altitude and velocity control in ergodic potential  field directed multi-UAV search",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03231",
        "abstract_url": "http://arxiv.org/abs/2401.03231",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Seongbeom"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "",
            ""
        ],
        "abstract": "  Many countries around the world, including Korea, use the school choicelottery system. However, this method has a problem in that many students areassigned to less-preferred schools based on the lottery results. In addition,the task of finding a good assignment with ties often has a time complexity ofNP, making it a very difficult problem to improve the quality of theassignment.  In this paper, we prove that the problem of finding a stable matching thatmaximizes the student-oriented preference utility in a two-sided market withone-sided preference can be solved in polynomial time, and we verify throughexperiments that the quality of assignment is improved. The main contributionsof this paper are as follows. We found that stable student-oriented allocationin a two-sided market with one-sided preferences is the same as stableallocation in a two-sided market with symmetric preferences. In addition, wedefined a method to quantify the quality of allocation from a preferenceutilitarian perspective. Based on the above two, it was proven that the problemof finding a stable match that maximizes the preference utility in a two-sidedmarket with homogeneous preferences can be reduced to an allocation problem. Inthis paper, through an experiment, we quantitatively verified that optimalstudent assignment assigns more students to schools of higher preference, evenin situations where many students are assigned to schools of low preferenceusing the existing assignment method.",
        "title": "Stable Marriage with One-Sided Preference",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03904",
        "abstract_url": "http://arxiv.org/abs/2401.03904",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yongjie"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Yang",
                "first_name": "Liying"
            },
            {
                "last_name": "Nie",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Chaoxiong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yiwen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Time-optimal control of a multi-rotor remains an open problem due to theunder-actuation and nonlinearity of its dynamics, which make it difficult tosolve this problem directly. In this paper, the time-optimal control problem ofthe multi-rotor is studied. Firstly, a thrust limit optimal decompositionmethod is proposed, which can reasonably decompose the limited thrust intothree directions according to the current state and the target state. As aresult, the thrust limit constraint is decomposed as a linear constraint. Withthe linear constraint and decoupled dynamics, a time-optimal guidancetrajectory can be obtained. Then, a cost function is defined based on thetime-optimal guidance trajectory, which has a quadratic form and can be used toevaluate the time-optimal performance of the system outputs. Finally, based onthe cost function, the time-optimal control problem is reformulated as an MPC(Model Predictive Control) problem. The experimental results demonstrate thefeasibility and validity of the proposed methods.",
        "title": "Guided Time-optimal Model Predictive Control of a Multi-rotor",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03991",
        "abstract_url": "http://arxiv.org/abs/2401.03991",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Fangjun"
            },
            {
                "last_name": "Hogg",
                "first_name": "David C."
            },
            {
                "last_name": "Cohn",
                "first_name": "Anthony G."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "DB",
            "LO"
        ],
        "abstract": "  Artificial intelligence (AI) has made remarkable progress across variousdomains, with large language models like ChatGPT gaining substantial attentionfor their human-like text-generation capabilities. Despite these achievements,spatial reasoning remains a significant challenge for these models. Benchmarkslike StepGame evaluate AI spatial reasoning, where ChatGPT has shownunsatisfactory performance. However, the presence of template errors in thebenchmark has an impact on the evaluation results. Thus there is potential forChatGPT to perform better if these template errors are addressed, leading tomore accurate assessments of its spatial reasoning capabilities. In this study,we refine the StepGame benchmark, providing a more accurate dataset for modelevaluation. We analyze GPT's spatial reasoning performance on the rectifiedbenchmark, identifying proficiency in mapping natural language text to spatialrelations but limitations in multi-hop reasoning. We provide a flawlesssolution to the benchmark by combining template-to-relation mapping withlogic-based reasoning. This combination demonstrates proficiency in performingqualitative reasoning on StepGame without encountering any errors. We thenaddress the limitations of GPT models in spatial reasoning. We deployChain-of-thought and Tree-of-thoughts prompting strategies, offering insightsinto GPT's ``cognitive process\", and achieving remarkable improvements inaccuracy. Our investigation not only sheds light on model deficiencies but alsoproposes enhancements, contributing to the advancement of AI with more robustspatial reasoning capabilities.",
        "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth  Evaluation and Enhancement Using the StepGame Benchmark",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04068",
        "abstract_url": "http://arxiv.org/abs/2401.04068",
        "authors": [
            {
                "last_name": "Mathiesen",
                "first_name": "Frederik Baymler"
            },
            {
                "last_name": "Lahijanian",
                "first_name": "Morteza"
            },
            {
                "last_name": "Laurenti",
                "first_name": "Luca"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LO"
        ],
        "abstract": "  In this paper, we present IntervalMDP.jl, a Julia package for probabilisticanalysis of interval Markov Decision Processes (IMDPs). IntervalMDP.jlfacilitates the synthesis of optimal strategies and verification of IMDPsagainst reachability specifications and discounted reward properties. Thelibrary supports sparse matrices and is compatible with common tools foranalysis of probabilistic models, such as PRISM. A key feature ofIntervalMDP.jl is that it presents both a multi-threaded CPU and aGPU-accelerated implementation of value iteration algorithms for IMDPs. Inparticular, IntervalMDP.jl takes advantage of the Julia type system and theinherently parallelizable nature of value iteration to improve the efficiencyof performing analysis of IMDPs. On a set of examples, we show thatIntervalMDP.jl substantially outperforms existing tools for verification andstrategy synthesis for IMDPs in both computation time and memory consumption.",
        "title": "IntervalMDP.jl: Accelerated Value Iteration for Interval Markov Decision  Processes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04316",
        "abstract_url": "http://arxiv.org/abs/2401.04316",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Dai",
                "first_name": "Bo"
            },
            {
                "last_name": "Gu",
                "first_name": "Feng"
            },
            {
                "last_name": "Han",
                "first_name": "Jianda"
            },
            {
                "last_name": "Liu",
                "first_name": "Guangjun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Aerial manipulator, which is composed of an UAV (Unmanned Aerial Vehicle) anda multi-link manipulator and can perform aerial manipulation, has shown greatpotential of applications. However, dynamic coupling between the UAV and themanipulator makes it difficult to control the aerial manipulator with highperformance. In this paper, system modeling and control problem of the aerialmanipulator are studied. Firstly, an UAV dynamic model is proposed withconsideration of the dynamic coupling from an attached manipulator, which istreated as disturbance for the UAV. In the dynamic model, the disturbance isaffected by the variable inertia parameters of the aerial manipulator system.Then, based on the proposed dynamic model, a disturbance compensation robust$H_{\\infty}$ controller is designed to stabilize flight of the UAV while themanipulator is in operation. Finally, experiments are conducted and theexperimental results demonstrate the feasibility and validity of the proposedcontrol scheme.",
        "title": "Robust Control of An Aerial Manipulator Based on A Variable Inertia  Parameters Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04732",
        "abstract_url": "http://arxiv.org/abs/2401.04732",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Manpreet"
            },
            {
                "last_name": "Pasricha",
                "first_name": "Ravdeep"
            },
            {
                "last_name": "Singh",
                "first_name": "Nitish"
            },
            {
                "last_name": "Kondapalli",
                "first_name": "Ravi Prasad"
            },
            {
                "last_name": "R",
                "first_name": "Manoj"
            },
            {
                "last_name": "R",
                "first_name": "Kiran"
            },
            {
                "last_name": "Bou\u00e9",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "LG"
        ],
        "abstract": "  In this paper, we design a real-time question-answering system specificallytargeted for helping sellers get relevant material/documentation they can sharelive with their customers or refer to during a call. Taking the Seismic contentrepository as a relatively large scale example of a diverse dataset of salesmaterial, we demonstrate how LLM embeddings of sellers' queries can be matchedwith the relevant content. We achieve this by engineering prompts in anelaborate fashion that makes use of the rich set of meta-features available fordocuments and sellers. Using a bi-encoder with cross-encoder re-rankerarchitecture, we show how the solution returns the most relevant contentrecommendations in just a few seconds even for large datasets. Our recommendersystem is deployed as an AML endpoint for real-time inferencing and has beenintegrated into a Copilot interface that is now deployed in the productionversion of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.",
        "title": "A case study of Generative AI in MSX Sales Copilot: Improving seller  productivity with a real-time question-answering system for content  recommendation",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04734",
        "abstract_url": "http://arxiv.org/abs/2401.04734",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Xiaofan"
            },
            {
                "last_name": "Khan",
                "first_name": "Muhammad Aadil"
            },
            {
                "last_name": "Onori",
                "first_name": "Simona"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  A key challenge that is currently hindering the widespread deployment and useof retired electric vehicle (EV) batteries for second-life (SL) applications isthe ability to accurately estimate and monitor their state of health (SOH).Second-life battery systems can be sourced from different battery packs with alack of knowledge of their historical usage.  To facilitate the on-the-field use of SL batteries, this paper introduces anonline adaptive health estimation strategy with guaranteed stability. Thismethod relies exclusively on operational data that can be accessed in real-timefrom SL batteries. The adaptation algorithm is designed to ensurebounded-input-bounded-output (BIBO) stability. The effectiveness of theproposed approach is shown on a laboratory-aged experimental data set ofretired EV batteries. The estimator gains are dynamically adapted toaccommodate the distinct characteristics of each individual cell, making it apromising candidate for future SL battery management systems (BMS2).",
        "title": "Online Adaptive Data-driven State-of-health Estimation for Second-life  Batteries with BIBO Stability Guarantees",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04736",
        "abstract_url": "http://arxiv.org/abs/2401.04736",
        "authors": [
            {
                "last_name": "Choudhury",
                "first_name": "Tashfique Hasnine"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  The extensive use of distributed vehicle platoon controllers has resulted inseveral benefits for transportation systems, such as increased traffic flow,fuel efficiency, and decreased pollution. The rising reliance on interconnectedsystems and communication networks, on the other hand, exposes thesecontrollers to potential cyber-attacks, which may compromise their safety andfunctionality. This thesis aims to improve the security of distributed vehicleplatoon controllers by investigating attack scenarios and assessing theirinfluence on system performance. Various attack techniques, includingman-in-the-middle (MITM) and false data injection (FDI), are simulated usingModel Predictive Control (MPC) controller to identify vulnerabilities andweaknesses of the platoon controller. Countermeasures are offered and tested,that includes attack analysis and reinforced communication protocols usingMachine Learning techniques for detection. The findings emphasize thesignificance of integrating security issues into their design andimplementation, which helps to construct safe and resilient distributed platooncontrollers.",
        "title": "Exploring Attack Resilience in Distributed Platoon Controllers with  Model Predictive Control",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04737",
        "abstract_url": "http://arxiv.org/abs/2401.04737",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Yigang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  In recent years, various well-designed algorithms have empowered musicplatforms to provide content based on one's preferences. Music genres aredefined through various aspects, including acoustic features and culturalconsiderations. Music genre classification works well with content-basedfiltering, which recommends content based on music similarity to users. Given aconsiderable dataset, one premise is automatic annotation using machinelearning or deep learning methods that can effectively classify audio files.The effectiveness of systems largely depends on feature and model selection, asdifferent architectures and features can facilitate each other and yielddifferent results. In this study, we conduct a comparative study investigatingthe performances of three models: a proposed convolutional neural network(CNN), the VGG16 with fully connected layers (FC), and an eXtreme GradientBoosting (XGBoost) approach on different features: 30-second Mel spectrogramand 3-second Mel-frequency cepstral coefficients (MFCCs). The results show thatthe MFCC XGBoost model outperformed the others. Furthermore, applying datasegmentation in the data preprocessing phase can significantly enhance theperformance of the CNNs.",
        "title": "Music Genre Classification: A Comparative Analysis of CNN and XGBoost  Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04739",
        "abstract_url": "http://arxiv.org/abs/2401.04739",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            },
            {
                "last_name": "Gao",
                "first_name": "Eryang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In recent years, the recognition of free-hand sketches has remained a populartask. However, in some special fields such as the military field, free-handsketches are difficult to sample on a large scale. Common data augmentation andimage generation techniques are difficult to produce images with variousfree-hand sketching styles. Therefore, the recognition and segmentation tasksin related fields are limited. In this paper, we propose a novel adversarialgenerative network that can accurately generate realistic free-hand sketcheswith various styles. We explore the performance of the model, including usingstyles randomly sampled from a prior normal distribution to generate imageswith various free-hand sketching styles, disentangling the painters' stylesfrom known free-hand sketches to generate images with specific styles, andgenerating images of unknown classes that are not in the training set. Wefurther demonstrate with qualitative and quantitative evaluations ouradvantages in visual quality, content accuracy, and style imitation onSketchIME.",
        "title": "Content-Conditioned Generation of Stylized Free hand Sketches",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04740",
        "abstract_url": "http://arxiv.org/abs/2401.04740",
        "authors": [
            {
                "last_name": "Chenna",
                "first_name": "Dwith"
            },
            {
                "last_name": "Bhogawar",
                "first_name": "Suyash"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Brain extraction and removal of skull artifacts from magnetic resonanceimages (MRI) is an important preprocessing step in neuroimaging analysis. Thereare many tools developed to handle human fMRI images, which could involvemanual steps for verifying results from brain segmentation that makes it timeconsuming and inefficient. In this study, we will use the segment anythingmodel (SAM), a freely available neural network released by Meta[4], which hasshown promising results in many generic segmentation applications. We willanalyze the efficiency of SAM for neuroimaging brain segmentation by removingskull artifacts. The results of the experiments showed promising results thatexplore using automated segmentation algorithms for neuroimaging without theneed to train on custom medical imaging dataset.",
        "title": "Segment anything model (SAM) for brain extraction in fMRI studies",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04741",
        "abstract_url": "http://arxiv.org/abs/2401.04741",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yuanchi"
            },
            {
                "last_name": "He",
                "first_name": "Hui"
            },
            {
                "last_name": "Lei",
                "first_name": "Zhongxiang"
            },
            {
                "last_name": "Niu",
                "first_name": "Zhendong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph clustering algorithms with autoencoder structures have recently gainedpopularity due to their efficient performance and low training cost. However,for existing graph autoencoder clustering algorithms based on GCN or GAT, notonly do they lack good generalization ability, but also the number of clustersclustered by such autoencoder models is difficult to determine automatically.To solve this problem, we propose a new framework called Graph Clustering withMasked Autoencoders (GCMA). It employs our designed fusion autoencoder based onthe graph masking method for the fusion coding of graph. It introduces ourimproved density-based clustering algorithm as a second decoder while decodingwith multi-target reconstruction. By decoding the mask embedding, our model cancapture more generalized and comprehensive knowledge. The number of clustersand clustering results can be output end-to-end while improving thegeneralization ability. As a nonparametric class method, extensive experimentsdemonstrate the superiority of \\textit{GCMA} over state-of-the-art baselines.",
        "title": "Masked AutoEncoder for Graph Clustering without Pre-defined Cluster  Number k",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04744",
        "abstract_url": "http://arxiv.org/abs/2401.04744",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            },
            {
                "last_name": "Hefenbrock",
                "first_name": "Michael"
            },
            {
                "last_name": "Prenat",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Anghel",
                "first_name": "Lorena"
            },
            {
                "last_name": "Tahoori",
                "first_name": "Mehdi B."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "AR",
            "LG"
        ],
        "abstract": "  Bayesian Neural Networks (BayNNs) can inherently estimate predictiveuncertainty, facilitating informed decision-making. Dropout-based BayNNs areincreasingly implemented in spintronics-based computation-in-memoryarchitectures for resource-constrained yet high-performance safety-criticalapplications. Although uncertainty estimation is important, the reliability ofDropout generation and BayNN computation is equally important for targetapplications but is overlooked in existing works. However, testing BayNNs issignificantly more challenging compared to conventional NNs, due to theirstochastic nature. In this paper, we present for the first time the model ofthe non-idealities of the spintronics-based Dropout module and analyze theirimpact on uncertainty estimates and accuracy. Furthermore, we propose a testingframework based on repeatability ranking for Dropout-based BayNN with up to$100\\%$ fault coverage while using only $0.2\\%$ of training data as testvectors.",
        "title": "Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian  Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04746",
        "abstract_url": "http://arxiv.org/abs/2401.04746",
        "authors": [
            {
                "last_name": "Himel",
                "first_name": "Galib Muhammad Shahriar"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Masudul"
            },
            {
                "last_name": "Al-Aff",
                "first_name": "Kh Abdullah"
            },
            {
                "last_name": "Karim",
                "first_name": "Shams Ibne"
            },
            {
                "last_name": "Sikder",
                "first_name": "Md. Kabir Uddin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Skin cancer is a global health concern, necessitating early and accuratediagnosis for improved patient outcomes. This study introduces a groundbreakingapproach to skin cancer classification, employing the Vision Transformer, astate-of-the-art deep learning architecture renowned for its success in diverseimage analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulouslyannotated skin lesion images, the model undergoes preprocessing for enhancedrobustness. The Vision Transformer, adapted to the skin cancer classificationtask, leverages the self-attention mechanism to capture intricate spatialdependencies, achieving superior performance over traditional deep learningarchitectures. Segment Anything Model aids in precise segmentation of cancerousareas, attaining high IOU and Dice Coefficient. Extensive experiments highlightthe model's supremacy, particularly the Google-based ViT patch-32 variant,which achieves 96.15% accuracy and showcases potential as an effective tool fordermatologists in skin cancer diagnosis, contributing to advancements indermatological practices.",
        "title": "Skin Cancer Segmentation and Classification Using Vision Transformer for  Automatic Analysis in Dermatoscopy-based Non-invasive Digital System",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04747",
        "abstract_url": "http://arxiv.org/abs/2401.04747",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junming"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Ailing"
            },
            {
                "last_name": "Li",
                "first_name": "Yu"
            },
            {
                "last_name": "Chen",
                "first_name": "Qifeng"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "CV",
            "GR",
            ""
        ],
        "abstract": "  We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3DExpression and Gesture generation with arbitrary length. While previous worksfocused on co-speech gesture or expression generation individually, the jointgeneration of synchronized expressions and gestures remains barely explored. Toaddress this, our diffusion-based co-speech motion generation transformerenables uni-directional information flow from expression to gesture,facilitating improved matching of joint expression-gesture distributions.Furthermore, we introduce an outpainting-based sampling strategy for arbitrarylong sequence generation in diffusion models, offering flexibility andcomputational efficiency. Our method provides a practical solution thatproduces high-quality synchronized expression and gesture generation driven byspeech. Evaluated on two public datasets, our approach achievesstate-of-the-art performance both quantitatively and qualitatively.Additionally, a user study confirms the superiority of DiffSHEG over priorapproaches. By enabling the real-time generation of expressive and synchronizedmotions, DiffSHEG showcases its potential for various applications in thedevelopment of digital humans and embodied agents.",
        "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven  Holistic 3D Expression and Gesture Generation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04748",
        "abstract_url": "http://arxiv.org/abs/2401.04748",
        "authors": [
            {
                "last_name": "Olisah",
                "first_name": "Chollette C."
            },
            {
                "last_name": "Trewhella",
                "first_name": "Ben"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Smith",
                "first_name": "Melvyn L."
            },
            {
                "last_name": "Winstone",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Whitfield",
                "first_name": "E. Charles"
            },
            {
                "last_name": "Fern\u00e1ndez",
                "first_name": "Felicidad Fern\u00e1ndez"
            },
            {
                "last_name": "Duncalfe",
                "first_name": "Harriet"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Fruit ripeness estimation models have for decades depended on spectral indexfeatures or colour-based features, such as mean, standard deviation, skewness,colour moments, and/or histograms for learning traits of fruit ripeness.Recently, few studies have explored the use of deep learning techniques toextract features from images of fruits with visible ripeness cues. However, theblackberry (Rubus fruticosus) fruit does not show obvious and reliable visibletraits of ripeness when mature and therefore poses great difficulty to fruitpickers. The mature blackberry, to the human eye, is black before, during, andpost-ripening. To address this engineering application challenge, this paperproposes a novel multi-input convolutional neural network (CNN) ensembleclassifier for detecting subtle traits of ripeness in blackberry fruits. Themulti-input CNN was created from a pre-trained visual geometry group 16-layerdeep convolutional network (VGG16) model trained on the ImageNet dataset. Thefully connected layers were optimized for learning traits of ripeness of matureblackberry fruits. The resulting model served as the base for buildinghomogeneous ensemble learners that were ensemble using the stack generalizationensemble (SGE) framework. The input to the network is images acquired with astereo sensor using visible and near-infrared (VIS-NIR) spectral filters atwavelengths of 700 nm and 770 nm. Through experiments, the proposed modelachieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-fieldconditions. Further experiments reveal that machine sensory is highly andpositively correlated to human sensory over blackberry fruit skin texture.",
        "title": "Convolutional Neural Network Ensemble Learning for Hyperspectral  Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm  Environment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04749",
        "abstract_url": "http://arxiv.org/abs/2401.04749",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieqiao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bo"
            },
            {
                "last_name": "peng",
                "first_name": "Junran"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "SE"
        ],
        "abstract": "  Log anomaly detection is a key component in the field of artificialintelligence for IT operations (AIOps). Considering log data of variantdomains, retraining the whole network for unknown domains is inefficient inreal industrial scenarios. However, previous deep models merely focused onextracting the semantics of log sequences in the same domain, leading to poorgeneralization on multi-domain logs. To alleviate this issue, we propose aunified Transformer-based framework for Log anomaly detection (LogFormer) toimprove the generalization ability across different domains, where we establisha two-stage process including the pre-training and adapter-based tuning stage.Specifically, our model is first pre-trained on the source domain to obtainshared semantic knowledge of log data. Then, we transfer such knowledge to thetarget domain via shared parameters. Besides, the Log-Attention module isproposed to supplement the information ignored by the log-paring. The proposedmethod is evaluated on three public and one real-world datasets. Experimentalresults on multiple benchmarks demonstrate the effectiveness of our LogFormerwith fewer trainable parameters and lower training costs.",
        "title": "LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04750",
        "abstract_url": "http://arxiv.org/abs/2401.04750",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shengli"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Lin",
                "first_name": "Sen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While dust significantly affects the environmental perception of automatedagricultural machines, the existing deep learning-based methods for dustremoval require further research and improvement in this area to improve theperformance and reliability of automated agricultural machines in agriculture.We propose an end-to-end trainable learning network (DedustNet) to solve thereal-world agricultural dust removal task. To our knowledge, DedustNet is thefirst time Swin Transformer-based units have been used in wavelet networks foragricultural image dusting. Specifically, we present the frequency-dominatedblock (DWTFormer block and IDWTFormer block) by adding a spatial featuresaggregation scheme (SFAS) to the Swin Transformer and combining it with thewavelet transform, the DWTFormer block and IDWTFormer block, alleviating thelimitation of the global receptive field of Swin Transformer when dealing withcomplex dusty backgrounds. Furthermore, We propose a cross-level informationfusion module to fuse different levels of features and effectively captureglobal and long-range feature relationships. In addition, we present a dilatedconvolution module to capture contextual information guided by wavelettransform at multiple scales, which combines the advantages of wavelettransform and dilated convolution. Our algorithm leverages deep learningtechniques to effectively remove dust from images while preserving the originalstructural and textural features. Compared to existing state-of-the-artmethods, DedustNet achieves superior performance and more reliable results inagricultural image dedusting, providing strong support for the application ofagricultural machinery in dusty environments. Additionally, the impressiveperformance on real-world hazy datasets and application tests highlightsDedustNet superior generalization ability and computer vision-relatedapplication performance.",
        "title": "DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network  for Agricultural Dust Removal",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04751",
        "abstract_url": "http://arxiv.org/abs/2401.04751",
        "authors": [
            {
                "last_name": "Howard",
                "first_name": "Daniel Anthony"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "PF",
            ""
        ],
        "abstract": "  Improving energy efficiency in industrial production processes is crucial forcompetitiveness, and compliance with climate policies. This paper introduces adata-driven approach to identify optimal melting patterns in inductionfurnaces. Through time-series K-means clustering the melting patterns could beclassified into distinct clusters based on temperature profiles. Using theelbow method, 12 clusters were identified, representing the range of meltingpatterns. Performance parameters such as melting time, energy-specificperformance, and carbon cost were established for each cluster, indicatingfurnace efficiency and environmental impact. Multiple criteria decision-makingmethods including Simple Additive Weighting, Multiplicative ExponentialWeighting, Technique for Order of Preference by Similarity to Ideal Solution,modified TOPSIS, and VlseKriterijumska Optimizacija I Kompromisno Resenje wereutilized to determine the best-practice cluster. The study successfullyidentified the cluster with the best performance. Implementing the bestpractice operation resulted in an 8.6 % reduction in electricity costs,highlighting the potential energy savings in the foundry.",
        "title": "Identifying Best Practice Melting Patterns in Induction Furnaces: A  Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria  Decision Making",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04757",
        "abstract_url": "http://arxiv.org/abs/2401.04757",
        "authors": [
            {
                "last_name": "Owen",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  We investigate large language model performance across five orders ofmagnitude of compute scaling in eleven recent model architectures. We show thataverage benchmark performance, aggregating over many individual tasks andevaluations as in the commonly-used BIG-Bench dataset, is decently predictableas a function of training compute scale. Specifically, when extrapolatingBIG-Bench Hard performance across one order of magnitude in compute, we observeaverage absolute errors of 6 percentage points (pp). By contrast, extrapolationfor individual BIG-Bench tasks across an order of magnitude in compute yieldshigher average errors of 18pp. Nonetheless, individual task performance remainssignificantly more predictable than chance. Overall, our work suggests computescaling provides a promising basis to forecast AI capabilities in diversebenchmarks, though predicting performance in specific tasks poses challenges.",
        "title": "How predictable is language model benchmark performance?",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04758",
        "abstract_url": "http://arxiv.org/abs/2401.04758",
        "authors": [
            {
                "last_name": "Gatterbauer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Dunne",
                "first_name": "Cody"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "HC",
            "LO"
        ],
        "abstract": "  Comparing relational languages by their logical expressiveness is wellunderstood. Less well understood is how to compare relational languages bytheir ability to represent relational query patterns. Indeed, what are querypatterns other than \"a certain way of writing a query\"? And how can querypatterns be defined across procedural and declarative languages, irrespectiveof their syntax? To the best of our knowledge, we provide the first semanticdefinition of relational query patterns by using a variant ofstructure-preserving mappings between the relational tables of queries. Thisformalism allows us to analyze the relative pattern expressiveness ofrelational language fragments and create a hierarchy of languages with equallogical expressiveness yet different pattern expressiveness. Notably, for thenon-disjunctive language fragment, we show that relational calculus can expressa larger class of patterns than the basic operators of relational algebra.  Our language-independent definition of query patterns opens novel paths forassisting database users. For example, these patterns could be leveraged tocreate visual query representations that faithfully represent query patterns,speed up interpretation, and provide visual feedback during query editing. As aconcrete example, we propose Relational Diagrams, a complete and sounddiagrammatic representation of safe relational calculus that is provably (i)unambiguous, (ii) relationally complete, and (iii) able to represent all querypatterns for unions of non-disjunctive queries. Among all diagrammaticrepresentations for relational queries that we are aware of, ours is the onlyone with these three properties. Furthermore, our anonymously preregistereduser study shows that Relational Diagrams allow users to recognize patternsmeaningfully faster and more accurately than SQL.",
        "title": "On The Reasonable Effectiveness of Relational Diagrams: Explaining  Relational Query Patterns and the Pattern Expressiveness of Relational  Languages",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04771",
        "abstract_url": "http://arxiv.org/abs/2401.04771",
        "authors": [
            {
                "last_name": "Smiley",
                "first_name": "Octavious"
            },
            {
                "last_name": "Hoffmann",
                "first_name": "Till"
            },
            {
                "last_name": "Onnela",
                "first_name": "Jukka-Pekka"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            ""
        ],
        "abstract": "  Network science explores intricate connections among objects, employed indiverse domains like social interactions, fraud detection, and disease spread.Visualization of networks facilitates conceptualizing research questions andforming scientific hypotheses. Networks, as mathematical high-dimensionalobjects, require dimensionality reduction for (planar) visualization.Visualizing empirical networks present additional challenges. They oftencontain false positive (spurious) and false negative (missing) edges.Traditional visualization methods don't account for errors in observation,potentially biasing interpretations. Moreover, contemporary network dataincludes rich nodal attributes. However, traditional methods neglect theseattributes when computing node locations. Our visualization approach aims toleverage nodal attribute richness to compensate for network data limitations.We employ a statistical model estimating the probability of edge connectionsbetween nodes based on their covariates. We enhance the Fruchterman-Reingoldalgorithm to incorporate estimated dyad connection probabilities, allowingpractitioners to balance reliance on observed versus estimated edges. Weexplore optimal smoothing levels, offering a natural way to include relevantnodal information in layouts. Results demonstrate the effectiveness of ourmethod in achieving robust network visualization, providing insights forimproved analysis.",
        "title": "Network Layout Algorithm with Covariate Smoothing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04778",
        "abstract_url": "http://arxiv.org/abs/2401.04778",
        "authors": [
            {
                "last_name": "Br\u00fcck",
                "first_name": "Florian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  In this work, we provide a simulation algorithm to simulate from a(multivariate) characteristic function, which is only accessible in a black-boxformat. We construct a generative neural network, whose loss function exploitsa specific representation of the Maximum-Mean-Discrepancy metric to directlyincorporate the targeted characteristic function. The construction is universalin the sense that it is independent of the dimension and that it does notrequire any assumptions on the given characteristic function. Furthermore,finite sample guarantees on the approximation quality in terms of theMaximum-Mean Discrepancy metric are derived. The method is illustrated in ashort simulation study.",
        "title": "Generative neural networks for characteristic functions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04781",
        "abstract_url": "http://arxiv.org/abs/2401.04781",
        "authors": [
            {
                "last_name": "Mazaev",
                "first_name": "A. V."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In the development of materials for structural purposes, the main focus is onthe advantageous combination of mechanical and volume-mass properties. Due tothe development of production, solid plates are increasingly being replaced bymodern composite materials with improved properties, one of the varieties ofwhich is layered composites with a honeycomb core. The most widespread arethree-layer plates with solid face layers and a hexagonal honeycomb core.However, with the development of new technologies, including 3D printing,honeycombs with new geometries are gaining popularity, the mechanicalproperties of which make it possible to obtain layered composites with improvedfeatures. In this paper, three-layer plates with solid face layers and atetrachiral honeycomb core are investigated. The influence of discretization(number of unit cells), relative density and thickness of the honeycomb core onthe stress state of three-layer composites subjected to static bending undervarious boundary conditions is studied. Mathematical modeling is carried outwithin the framework of the theory of elasticity by the finite element methodvia three-dimensional modeling in the Comsol Multiphysics system, as well asusing algorithms developed by the author for analyzing the stress state ofmultilayer plates with tetrachiral honeycombs by solving a plane problem of thetheory of elasticity. As a result, good agreement is shown between thenumerical results obtained using algorithms for solving a plane problem and viathree-dimensional finite element modeling in the Comsol Multiphysics system,while the numerical results are qualitatively consistent with laboratory testdata.",
        "title": "Mathematical modeling of mechanical behavior of three-layer plates with  tetrachiral honeycomb core",
        "date": "2023-11-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04783",
        "abstract_url": "http://arxiv.org/abs/2401.04783",
        "authors": [
            {
                "last_name": "Christlieb",
                "first_name": "Andrew J."
            },
            {
                "last_name": "Ding",
                "first_name": "Mingchang"
            },
            {
                "last_name": "Huang",
                "first_name": "Juntao"
            },
            {
                "last_name": "Krupansky",
                "first_name": "Nicholas A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We introduce a hyperbolic closure for the Grad moment expansion of theBhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trainedon BGK's moment data. This closure is motivated by the exact closure for thefree streaming limit that we derived in our paper on closures in transport\\cite{Huang2022-RTE1}. The exact closure relates the gradient of the highestmoment to the gradient of four lower moments. As with our past work, the modelpresented here learns the gradient of the highest moment in terms of thecoefficients of gradients for all lower ones. By necessity, this means that theresulting hyperbolic system is not conservative in the highest moment. Forstability, the output layers of the NN are designed to enforce hyperbolicityand Galilean invariance. This ensures the model can be run outside of thetraining window of the NN. Unlike our previous work on radiation transport thatdealt with linear models, the BGK model's nonlinearity demanded advancedtraining tools. These comprised an optimal learning rate discovery, one cycletraining, batch normalization in each neural layer, and the use of the\\texttt{AdamW} optimizer. To address the non-conservative structure of thehyperbolic model, we adopt the FORCE numerical method to achieve robustsolutions. This results in a comprehensive computing model combining learnedclosures with methods for solving hyperbolic models. The proposed model cancapture accurate moment solutions across a broad spectrum of Knudsen numbers.Our paper details the multi-scale model construction and is run on a range oftest problems.",
        "title": "Hyperbolic Machine Learning Moment Closures for the BGK Equations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04787",
        "abstract_url": "http://arxiv.org/abs/2401.04787",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Shih-Chi"
            },
            {
                "last_name": "Heide",
                "first_name": "A. Leonid"
            },
            {
                "last_name": "Hemati",
                "first_name": "Maziar S."
            },
            {
                "last_name": "Seiler",
                "first_name": "Peter J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Quadratic systems with lossless quadratic terms arise in many applications,including models of atmosphere and incompressible fluid flows. Such systemshave a trapping region if all trajectories eventually converge to and staywithin a bounded set. Conditions for the existence and characterization oftrapping regions have been established in prior works for boundedness analysis.However, prior solutions have used non-convex optimization methods, resultingin conservative estimates. In this paper, we build on this prior work andprovide a convex semidefinite programming condition for the existence of atrapping region. The condition allows precise verification or falsification ofthe existence of a trapping region. If a trapping region exists, then weprovide a second semidefinite program to compute the least conservativetrapping region in the form of a ball. Two low-dimensional systems are providedas examples to illustrate the results. A third high-dimensional example is alsoincluded to demonstrate that the computation required for the analysis can bescaled to systems of up to $\\sim O(100)$ states. The proposed method provides aprecise and computationally efficient numerical approach for computing trappingregions. We anticipate this work will benefit future studies on modeling andcontrol of lossless quadratic dynamical systems.",
        "title": "A Convex Optimization Approach to Compute Trapping Regions for Lossless  Quadratic Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04791",
        "abstract_url": "http://arxiv.org/abs/2401.04791",
        "authors": [
            {
                "last_name": "Kinnari",
                "first_name": "Jouko"
            },
            {
                "last_name": "Thomas",
                "first_name": "Annika"
            },
            {
                "last_name": "Lusk",
                "first_name": "Parker"
            },
            {
                "last_name": "Kondo",
                "first_name": "Kota"
            },
            {
                "last_name": "How",
                "first_name": "Jonathan P."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  We present a novel framework for open-set Simultaneous Localization andMapping (SLAM) in unstructured environments that uses segmentation to create amap of objects and geometric relationships between objects for localization.Our system consists of 1) a front-end mapping pipeline using a zero-shotsegmentation model to extract object masks from images and track them acrossframes to generate an object-based map and 2) a frame alignment pipeline thatuses the geometric consistency of objects to efficiently localize within mapstaken in a variety of conditions. This approach is shown to be more robust tochanges in lighting and appearance than traditional feature-based SLAM systemsor global descriptor methods. This is established by evaluating SOS-SLAM on theBatvik seasonal dataset which includes drone flights collected over a coastalplot of southern Finland during different seasons and lighting conditions.Across flights during varying environmental conditions, our approach achieveshigher recall than benchmark methods with precision of 1.0. SOS-SLAM localizeswithin a reference map up to 14x faster than other feature based approaches andhas a map size less than 0.4% the size of the most compact other maps. Whenconsidering localization performance from varying viewpoints, our approachoutperforms all benchmarks from the same viewpoint and most benchmarks fromdifferent viewpoints. SOS-SLAM is a promising new approach for SLAM inunstructured environments that is robust to changes in lighting and appearanceand is more computationally efficient than other approaches. We release ourcode and datasets: https://acl.mit.edu/SOS-SLAM/.",
        "title": "SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04792",
        "abstract_url": "http://arxiv.org/abs/2401.04792",
        "authors": [
            {
                "last_name": "Hamad",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Finkenzeller",
                "first_name": "Andreas"
            },
            {
                "last_name": "K\u00fchr",
                "first_name": "Michael"
            },
            {
                "last_name": "Roberts",
                "first_name": "Andrew"
            },
            {
                "last_name": "Maennel",
                "first_name": "Olaf"
            },
            {
                "last_name": "Prevelakis",
                "first_name": "Vassilis"
            },
            {
                "last_name": "Steinhorst",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Autonomous and connected vehicles are rapidly evolving, integrating numeroustechnologies and software. This progress, however, has made them appealingtargets for cybersecurity attacks. As the risk of cyber threats escalates withthis advancement, the focus is shifting from solely preventing these attacks toalso mitigating their impact. Current solutions rely on vehicle securityoperation centers, where attack information is analyzed before deciding on aresponse strategy. However, this process can be time-consuming and facesscalability challenges, along with other issues stemming from vehicleconnectivity. This paper proposes a dynamic intrusion response systemintegrated within the vehicle. This system enables the vehicle to respond to avariety of incidents almost instantly, thereby reducing the need forinteraction with the vehicle security operation center. The system offers acomprehensive list of potential responses, a methodology for responseevaluation, and various response selection methods. The proposed solution wasimplemented on an embedded platform. Two distinct cyberattack use cases servedas the basis for evaluating the system. The evaluation highlights the system'sadaptability, its ability to respond swiftly, its minimal memory footprint, andits capacity for dynamic system parameter adjustments. The proposed solutionunderscores the necessity and feasibility of incorporating dynamic responsemechanisms in smart vehicles. This is a crucial factor in ensuring the safetyand resilience of future smart mobility.",
        "title": "REACT: Autonomous Intrusion Response System for Intelligent Vehicles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04795",
        "abstract_url": "http://arxiv.org/abs/2401.04795",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Gauri"
            },
            {
                "last_name": "Kapila",
                "first_name": "Ritvik"
            },
            {
                "last_name": "Chopra",
                "first_name": "Ayush"
            },
            {
                "last_name": "Raskar",
                "first_name": "Ramesh"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "LG",
            "SI",
            ""
        ],
        "abstract": "  Pandemics, notably the recent COVID-19 outbreak, have impacted both publichealth and the global economy. A profound understanding of disease progressionand efficient response strategies is thus needed to prepare for potentialfuture outbreaks. In this paper, we emphasize the potential of Agent-BasedModels (ABM) in capturing complex infection dynamics and understanding theimpact of interventions. We simulate realistic pharmaceutical, behavioral, anddigital interventions that mirror challenges in real-world policy adoption andsuggest a holistic combination of these interventions for pandemic response.Using these simulations, we study the trends of emergent behavior on alarge-scale population based on real-world socio-demographic and geo-censusdata from Kings County in Washington. Our analysis reveals the pivotal role ofthe initial 100 days in dictating a pandemic's course, emphasizing theimportance of quick decision-making and efficient policy development. Further,we highlight that investing in behavioral and digital interventions can reducethe burden on pharmaceutical interventions by reducing the total number ofinfections and hospitalizations, and by delaying the pandemic's peak. We alsoinfer that allocating the same amount of dollars towards extensive testing withcontact tracing and self-quarantine offers greater cost efficiency compared tospending the entire budget on vaccinations.",
        "title": "First 100 days of pandemic; an interplay of pharmaceutical, behavioral  and digital interventions -- A study using agent based modeling",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04799",
        "abstract_url": "http://arxiv.org/abs/2401.04799",
        "authors": [
            {
                "last_name": "Owonikoko",
                "first_name": "Waheed"
            },
            {
                "last_name": "Elsaadany",
                "first_name": "Mazen"
            },
            {
                "last_name": "Pandey",
                "first_name": "Amritanshu"
            },
            {
                "last_name": "Almassalkhi",
                "first_name": "Mads R."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  High penetration of renewable resources results in a power system with lowerinertia and higher frequency sensitivity to power imbalances. Such systems arebecoming increasingly susceptible to frequency collapse during extremedisturbances. Under-Frequency Load Shedding (UFLS) is a last-resort protectionscheme and acts as an emergency brake by shedding load to arrest frequencydecline. Current and emerging efforts to optimize UFLS settings and frequencythresholds are mostly network agnostic, ignoring network spatial information.With the prevalence of Distributed Energy Resources (DERs) in thehigh-renewable paradigm, the power grid is becoming more bidirectional, makingsome locations in the network less effective for UFLS action than others. Thiswork proposes a Mixed Integer Linear Program that optimizes the UFLS setpoints(prioritizing one location over another) to minimize frequency deviation andload-shed for a given disturbance. The formulation considers system informationand DER generation mix at different network locations, increasing modelfidelity. The formulation also captures the discrete nature and practical timedelays and deadbands associated with UFLS using a minimal set of binaryvariables, reducing problem complexity. We empirically validate theoptimization approach on the dynamic IEEE 39-bus system for performancemetrics, including frequency nadir, steady-state frequency and total load shed.",
        "title": "Optimization-based Framework for Selecting Under-frequency Load Shedding  Parameters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04801",
        "abstract_url": "http://arxiv.org/abs/2401.04801",
        "authors": [
            {
                "last_name": "Vance",
                "first_name": "Nathan"
            },
            {
                "last_name": "Flynn",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Model architecture refinement is a challenging task in deep learning researchfields such as remote photoplethysmography (rPPG). One architecturalconsideration, the depth of the model, can have significant consequences on theresulting performance. In rPPG models that are overprovisioned with more layersthan necessary, redundancies exist, the removal of which can result in fastertraining and reduced computational load at inference time. With too few layersthe models may exhibit sub-optimal error rates. We apply Centered KernelAlignment (CKA) to an array of rPPG architectures of differing depths,demonstrating that shallower models do not learn the same representations asdeeper models, and that after a certain depth, redundant layers are addedwithout significantly increased functionality. An empirical study confirmsthese findings and shows how this method could be used to refine rPPGarchitectures.",
        "title": "Refining Remote Photoplethysmography Architectures using CKA and  Empirical Methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04805",
        "abstract_url": "http://arxiv.org/abs/2401.04805",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Clifton Paul"
            },
            {
                "last_name": "Uvaydov",
                "first_name": "Daniel"
            },
            {
                "last_name": "D'Oro",
                "first_name": "Salvatore"
            },
            {
                "last_name": "Melodia",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Spectrum sensing is an essential component of modern wireless networks as itoffers a tool to characterize spectrum usage and better utilize it. DeepLearning (DL) has become one of the most used techniques to perform spectrumsensing as they are capable of delivering high accuracy and reliability.However, current techniques suffer from ad-hoc implementations and highcomplexity, which makes them unsuited for practical deployment on wirelesssystems where flexibility and fast inference time are necessary to supportreal-time spectrum sensing. In this paper, we introduce DeepSweep, a novelDL-based transceiver design that allows scalable, accurate, and fast spectrumsensing while maintaining a high level of customizability to adapt its designto a broad range of application scenarios and use cases. DeepSweep is designedto be seamlessly integrated with well-established transceiver designs andleverages shallow convolutional neural network (CNN) to \"sweep\" the spectrumand process captured IQ samples fast and reliably without interrupting ongoingdemodulation and decoding operations. DeepSweep reduces training and inferencetimes by more than 2 times and 10 times respectively, achieves up to 98 percentaccuracy in locating spectrum activity, and produces outputs in less than 1 ms,thus showing that DeepSweep can be used for a broad range of spectrum sensingapplications and scenarios.",
        "title": "DeepSweep: Parallel and Scalable Spectrum Sensing via Convolutional  Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04810",
        "abstract_url": "http://arxiv.org/abs/2401.04810",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Eugene"
            },
            {
                "last_name": "Lawrie",
                "first_name": "Dawn"
            },
            {
                "last_name": "Mayfield",
                "first_name": "James"
            },
            {
                "last_name": "Oard",
                "first_name": "Douglas W."
            },
            {
                "last_name": "Miller",
                "first_name": "Scott"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Prior work on English monolingual retrieval has shown that a cross-encodertrained using a large number of relevance judgments for query-document pairscan be used as a teacher to train more efficient, but similarly effective,dual-encoder student models. Applying a similar knowledge distillation approachto training an efficient dual-encoder model for Cross-Language InformationRetrieval (CLIR), where queries and documents are in different languages, ischallenging due to the lack of a sufficiently large training collection whenthe query and document languages differ. The state of the art for CLIR thusrelies on translating queries, documents, or both from the large English MSMARCO training set, an approach called Translate-Train. This paper proposes analternative, Translate-Distill, in which knowledge distillation from either amonolingual cross-encoder or a CLIR cross-encoder is used to train adual-encoder CLIR student model. This richer design space enables the teachermodel to perform inference in an optimized setting, while training the studentmodel directly for CLIR. Trained models and artifacts are publicly available onHuggingface.",
        "title": "Translate-Distill: Learning Cross-Language Dense Retrieval by  Translation and Distillation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04812",
        "abstract_url": "http://arxiv.org/abs/2401.04812",
        "authors": [
            {
                "last_name": "Zhai",
                "first_name": "Yaoguang"
            },
            {
                "last_name": "Qin",
                "first_name": "Zhizhen"
            },
            {
                "last_name": "Gao",
                "first_name": "Sicun"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Standard approaches for global optimization of non-convex functions, such asbranch-and-bound, maintain partition trees to systematically prune the domain.The tree size grows exponentially in the number of dimensions. We propose newsampling-based methods for non-convex optimization that adapts Monte Carlo TreeSearch (MCTS) to improve efficiency. Instead of the standard use of visitationcount in Upper Confidence Bounds, we utilize numerical overapproximations ofthe objective as an uncertainty metric, and also take into account of sampledestimates of first-order and second-order information. The Monte Carlo tree inour approach avoids the usual fixed combinatorial patterns in growing the tree,and aggressively zooms into the promising regions, while still balancingexploration and exploitation. We evaluate the proposed algorithms onhigh-dimensional non-convex optimization benchmarks against competitivebaselines and analyze the effects of the hyper parameters.",
        "title": "Sample-and-Bound for Non-Convex Optimization",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04820",
        "abstract_url": "http://arxiv.org/abs/2401.04820",
        "authors": [
            {
                "last_name": "\u00c7olhak",
                "first_name": "Furkan"
            },
            {
                "last_name": "Ecevit",
                "first_name": "Mert \u0130lhan"
            },
            {
                "last_name": "U\u00e7ar",
                "first_name": "Bilal Emir"
            },
            {
                "last_name": "Creutzburg",
                "first_name": "Reiner"
            },
            {
                "last_name": "Da\u011f",
                "first_name": "Hasan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The way we communicate and work has changed significantly with the rise ofthe Internet. While it has opened up new opportunities, it has also broughtabout an increase in cyber threats. One common and serious threat is phishing,where cybercriminals employ deceptive methods to steal sensitiveinformation.This study addresses the pressing issue of phishing by introducingan advanced detection model that meticulously focuses on HTML content. Ourproposed approach integrates a specialized Multi-Layer Perceptron (MLP) modelfor structured tabular data and two pretrained Natural Language Processing(NLP) models for analyzing textual features such as page titles and content.The embeddings from these models are harmoniously combined through a novelfusion process. The resulting fused embeddings are then input into a linearclassifier. Recognizing the scarcity of recent datasets for comprehensivephishing research, our contribution extends to the creation of an up-to-datedataset, which we openly share with the community. The dataset is meticulouslycurated to reflect real-life phishing conditions, ensuring relevance andapplicability. The research findings highlight the effectiveness of theproposed approach, with the CANINE demonstrating superior performance inanalyzing page titles and the RoBERTa excelling in evaluating page content. Thefusion of two NLP and one MLP model,termed MultiText-LP, achieves impressiveresults, yielding a 96.80 F1 score and a 97.18 accuracy score on our researchdataset. Furthermore, our approach outperforms existing methods on theCatchPhish HTML dataset, showcasing its efficacies.",
        "title": "Phishing Website Detection through Multi-Model Analysis of HTML Content",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04821",
        "abstract_url": "http://arxiv.org/abs/2401.04821",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Haotian"
            },
            {
                "last_name": "Liu",
                "first_name": "Yihong"
            },
            {
                "last_name": "Ma",
                "first_name": "Chunlan"
            },
            {
                "last_name": "Sch\u00fctze",
                "first_name": "Hinrich"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Transformer-based pre-trained language models (PLMs) have achieved remarkableperformance in various natural language processing (NLP) tasks. However,pre-training such models can take considerable resources that are almost onlyavailable to high-resource languages. On the contrary, static word embeddingsare easier to train in terms of computing resources and the amount of datarequired. In this paper, we introduce MoSECroT Model Stitching with Static WordEmbeddings for Crosslingual Zero-shot Transfer), a novel and challenging taskthat is especially relevant to low-resource languages for which static wordembeddings are available. To tackle the task, we present the first frameworkthat leverages relative representations to construct a common space for theembeddings of a source language PLM and the static word embeddings of a targetlanguage. In this way, we can train the PLM on source-language training dataand perform zero-shot transfer to the target language by simply swapping theembedding layer. However, through extensive experiments on two classificationdatasets, we show that although our proposed framework is competitive with weakbaselines when addressing MoSECroT, it fails to achieve competitive resultscompared with some strong baselines. In this paper, we attempt to explain thisnegative result and provide several thoughts on possible improvement.",
        "title": "MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual  Zero-shot Transfer",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04823",
        "abstract_url": "http://arxiv.org/abs/2401.04823",
        "authors": [
            {
                "last_name": "\u0160petl\u00edk",
                "first_name": "Martin"
            },
            {
                "last_name": "B\u0159ezina",
                "first_name": "Jan"
            },
            {
                "last_name": "Laloy",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Simulating 2D flow in fractured crystalline rock requires 2D stochasticdiscrete-fracture matrix (DFM) models. To obtain the simulation statistics ofinterest at an affordable computational cost, we aim to use the multilevelMonte Carlo method. To use this multiscale approach, one needs to upscale thehydraulic conductivity of the fractures by numerical homogenization. In thiswork, we substitute numerical homogenization with a surrogate model to speed upthe computations. In particular, we resort to a deep convolutional neuralnetwork (CNN) connected to a deep feed-forward neural network. The equivalenthydraulic conductivity tensor $K_{eq}$ is predicted based on an input spatialrandom field (SRF) of hydraulic conductivity tensors, cross-section, andhydraulic conductivity of fractures. Three independent surrogates with the samearchitecture are trained using data from DFM models with three different ratiosof hydraulic conductivities of fracture and bulk $K_f/K_b$. As the $K_f/K_b$ratio increases, the multivariate $K_{eq}$ distribution becomes more complex,and thus, the prediction accuracy of the trained surrogates deteriorates.Regardless of $K_f/K_b$, however, an improvement in the prediction accuracy ofthe trained surrogates is noted as the considered fracture density of themodeling setup decreases. We also investigate prediction accuracy on input SRFsof different correlation lengths. Upscaling by numerical homogenization and bysurrogate modeling is compared on two practical problems: upscaling of thehydraulic conductivity tensor and groundwater flow through a given surface. Weobtained equally accurate results for the equivalent hydraulic tensorcalculation of upscaled DFM models regardless of the upscaling method. For thegroundwater flow problem, the accuracy of quantity of interest imitates theaccuracy of $K_{eq}$ predictions.",
        "title": "Deep learning surrogate for predicting hydraulic conductivity tensors  from stochastic discrete fracture-matrix models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04827",
        "abstract_url": "http://arxiv.org/abs/2401.04827",
        "authors": [
            {
                "last_name": "Barrett",
                "first_name": "Christopher"
            },
            {
                "last_name": "Bura",
                "first_name": "Andrei"
            },
            {
                "last_name": "Huang",
                "first_name": "Fenix"
            },
            {
                "last_name": "Reidys",
                "first_name": "Christian"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A new perspective is introduced regarding the analysis of Multiple SequenceAlignments (MSA), representing aligned data defined over a finite alphabet ofsymbols. The framework is designed to produce a block decomposition of an MSA,where each block is comprised of sequences exhibiting a certain site-coherence.The key component of this framework is an information theoretical potentialdefined on pairs of sites (links) within the MSA. This potential quantifies theexpected drop in variation of information between the two constituent sites,where the expectation is taken with respect to all possible sub-alignments,obtained by removing a finite, fixed collection of rows. It is proved that thepotential is zero for linked sites representing columns, whose symbols are inbijective correspondence and it is strictly positive, otherwise. It isfurthermore shown that the potential assumes its unique minimum for links atwhich each symbol pair appears with the same multiplicity. Finally, anapplication is presented regarding anomaly detection in an MSA, composed ofinverse fold solutions of a fixed tRNA secondary structure, where the anomaliesare represented by inverse fold solutions of a different RNA structure.",
        "title": "The site linkage spectrum of data arrays",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04829",
        "abstract_url": "http://arxiv.org/abs/2401.04829",
        "authors": [
            {
                "last_name": "Akkas",
                "first_name": "Selahattin"
            },
            {
                "last_name": "Azad",
                "first_name": "Ariful"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Graph neural networks (GNNs) are popular machine learning models for graphswith many applications across scientific domains. However, GNNs are consideredblack box models, and it is challenging to understand how the model makespredictions. Game theory-based Shapley value approaches are popular explanationmethods in other domains but are not well-studied for graphs. Some studies haveproposed Shapley value-based GNN explanations, yet they have severallimitations: they consider limited samples to approximate Shapley values; somemainly focus on small and large coalition sizes, and they are an order ofmagnitude slower than other explanation methods, making them inapplicable toeven moderate-size graphs. In this work, we propose GNNShap, which providesexplanations for edges since they provide more natural explanations for graphsand more fine-grained explanations. We overcome the limitations by samplingfrom all coalition sizes, parallelizing the sampling on GPUs, and speeding upmodel predictions by batching. GNNShap gives better fidelity scores and fasterexplanations than baselines on real-world datasets.",
        "title": "GNNShap: Fast and Accurate GNN Explanations using Shapley Values",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04831",
        "abstract_url": "http://arxiv.org/abs/2401.04831",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Jaeyoung"
            },
            {
                "last_name": "Achermann",
                "first_name": "Florian"
            },
            {
                "last_name": "Girod",
                "first_name": "Rik"
            },
            {
                "last_name": "Lawrance",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Siegwart",
                "first_name": "Roland"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Fixed-wing aerial vehicles provide an efficient way to navigate longdistances or cover large areas for environmental monitoring applications. Bydesign, they also require large open spaces due to limited maneuverability.However, strict regulatory and safety altitude limits constrain the availablespace. Especially in complex, confined, or steep terrain, ensuring the vehicledoes not enter an inevitable collision state(ICS) can be challenging. In thiswork, we propose a strategy to find safe paths that do not enter an ICS whilenavigating within tight altitude constraints. The method uses periodic paths toefficiently classify ICSs. A sampling-based planner creates collision-free andkinematically feasible paths that begin and end in safe periodic (circular)paths. We show that, in realistic terrain, using circular periodic paths cansimplify the goal selection process by making it yaw agnostic and constrainingyaw. We demonstrate our approach by dynamically planning safe paths inreal-time while navigating steep terrain on a flight test in complex alpineterrain.",
        "title": "Safe Low-Altitude Navigation in Steep Terrain with Fixed-Wing Aerial  Vehicles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04836",
        "abstract_url": "http://arxiv.org/abs/2401.04836",
        "authors": [
            {
                "last_name": "Raje",
                "first_name": "Saurabh"
            },
            {
                "last_name": "Xu",
                "first_name": "Yufan"
            },
            {
                "last_name": "Rountev",
                "first_name": "Atanas"
            },
            {
                "last_name": "Valeev",
                "first_name": "Edward F."
            },
            {
                "last_name": "Sadayappan",
                "first_name": "Saday"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "DC",
            "PF"
        ],
        "abstract": "  Sparse tensor networks are commonly used to represent contractions oversparse tensors. Tensor contractions are higher-order analogs of matrixmultiplication. Tensor networks arise commonly in many domains of scientificcomputing and data science. After a transformation into a tree of binarycontractions, the network is implemented as a sequence of individualcontractions. Several critical aspects must be considered in the generation ofefficient code for a contraction tree, including sparse tensor layout modeorder, loop fusion to reduce intermediate tensors, and the interdependence ofloop order, mode order, and contraction order. We propose CoNST, a novelapproach that considers these factors in an integrated manner using a singleformulation. Our approach creates a constraint system that encodes thesedecisions and their interdependence, while aiming to produce reduced-orderintermediate tensors via fusion. The constraint system is solved by the Z3 SMTsolver and the result is used to create the desired fused loop structure andtensor mode layouts for the entire contraction tree. This structure is loweredto the IR of the TACO compiler, which is then used to generate executable code.Our experimental evaluation demonstrates very significant (sometimes orders ofmagnitude) performance improvements over current state-of-the-art sparse tensorcompiler/library alternatives.",
        "title": "CoNST: Code Generator for Sparse Tensor Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04837",
        "abstract_url": "http://arxiv.org/abs/2401.04837",
        "authors": [
            {
                "last_name": "Belgiovine",
                "first_name": "Mauro"
            },
            {
                "last_name": "Groen",
                "first_name": "Joshua"
            },
            {
                "last_name": "Sirera",
                "first_name": "Miquel"
            },
            {
                "last_name": "Tassie",
                "first_name": "Chinenye"
            },
            {
                "last_name": "Yildiz",
                "first_name": "Ayberk Yarkin"
            },
            {
                "last_name": "Trudeau",
                "first_name": "Sage"
            },
            {
                "last_name": "Ioannidis",
                "first_name": "Stratis"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Kaushik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI",
            ""
        ],
        "abstract": "  Spectrum sharing allows different protocols of the same standard (e.g.,802.11 family) or different standards (e.g., LTE and DVB) to coexist inoverlapping frequency bands. As this paradigm continues to spread, wirelesssystems must also evolve to identify active transmitters and unauthorizedwaveforms in real time under intentional distortion of preambles, extremely lowsignal-to-noise ratios and challenging channel conditions. We overcomelimitations of correlation-based preamble matching methods in such conditionsthrough the design of T-PRIME: a Transformer-based machine learning approach.T-PRIME learns the structural design of transmitted frames through itsattention mechanism, looking at sequence patterns that go beyond the preamblealone. The paper makes three contributions: First, it compares Transformermodels and demonstrates their superiority over traditional methods andstate-of-the-art neural networks. Second, it rigorously analyzes T-PRIME'sreal-time feasibility on DeepWave's AIR-T platform. Third, it utilizes anextensive 66 GB dataset of over-the-air (OTA) WiFi transmissions for training,which is released along with the code for community use. Results reveal nearlyperfect (i.e. $>98\\%$) classification accuracy under simulated scenarios,showing $100\\%$ detection improvement over legacy methods in low SNR ranges,$97\\%$ classification accuracy for OTA single-protocol transmissions and up to$75\\%$ double-protocol classification accuracy in interference scenarios.",
        "title": "T-PRIME: Transformer-based Protocol Identification for Machine-learning  at the Edge",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04842",
        "abstract_url": "http://arxiv.org/abs/2401.04842",
        "authors": [
            {
                "last_name": "Arabzadeh",
                "first_name": "Negar"
            },
            {
                "last_name": "Bigdeli",
                "first_name": "Amin"
            },
            {
                "last_name": "Clarke",
                "first_name": "Charles L. A."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large language models can now directly generate answers to many factualquestions without referencing external sources. Unfortunately, relativelylittle attention has been paid to methods for evaluating the quality andcorrectness of these answers, for comparing the performance of one model toanother, or for comparing one prompt to another. In addition, the quality ofgenerated answers are rarely directly compared to the quality of retrievedanswers. As models evolve and prompts are modified, we have no systematic wayto measure improvements without resorting to expensive human judgments. Toaddress this problem we adapt standard retrieval benchmarks to evaluate answersgenerated by large language models. Inspired by the BERTScore metric forsummarization, we explore two approaches. In the first, we base our evaluationon the benchmark relevance judgments. We empirically run experiments on howinformation retrieval relevance judgments can be utilized as an anchor toevaluating the generated answers. In the second, we compare generated answersto the top results retrieved by a diverse set of retrieval models, ranging fromtraditional approaches to advanced methods, allowing us to measure improvementswithout human judgments. In both cases, we measure the similarity between anembedded representation of the generated answer and an embedded representationof a known, or assumed, relevant passage from the retrieval benchmark.",
        "title": "Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04846",
        "abstract_url": "http://arxiv.org/abs/2401.04846",
        "authors": [
            {
                "last_name": "Glinsky",
                "first_name": "Michael E."
            },
            {
                "last_name": "Sievert",
                "first_name": "Sharon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper will examine what makes a being intelligent, whether that be abiological being or an artificial silicon being on a computer. Specialattention will be paid to the being having the ability to characterize andcontrol a collective system of many identical conservative sub-systemsconservatively interacting. The essence of intelligence will be found to be thegolden rule -- \"the collective acts as one\" or \"knowing the global consequencesof local actions\". The flow of the collective is a small set of twinklingtextures, that are governed by a puppeteer who is pulling a small number ofstrings according to a geodesic motion of least action, determined by thesymmetries. Controlling collective conservative systems is difficult and hashistorically been done by adding significant viscosity to the system tostabilize the desirable meta stable equilibriums of maximum performance, but itdegrades or destroys them in the process. There is an alternative. Once theoptimum twinkling textures of the meta stable equilibriums are identified bythe intelligent being (that is the collective system is characterized), thecollective system can be moved by the intelligent being to the optimumtwinkling textures, then quickly vibrated by the intelligent being according tothe textures so that the collective system remains at the meta stableequilibrium. Well educated intelligence knows the global consequences of itslocal actions so that it will not take short term actions that will lead topoor long term outcomes. In contrast, trained intelligence or trained stupiditywill optimize its short term actions, leading to poor long term outcomes. Welleducated intelligence is inherently good, but trained stupidity is inherentlyevil and should be feared. Particular attention is paid to the control andoptimization of economic and social collectives.",
        "title": "The inherent goodness of well educated intelligence",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04848",
        "abstract_url": "http://arxiv.org/abs/2401.04848",
        "authors": [
            {
                "last_name": "Skiredj",
                "first_name": "Abderrahman"
            },
            {
                "last_name": "Berrada",
                "first_name": "Ismail"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Automatic diacritization of Arabic text involves adding diacritical marks(diacritics) to the text. This task poses a significant challenge withnoteworthy implications for computational processing and comprehension. In thispaper, we introduce PTCAD (Pre-FineTuned Token Classification for ArabicDiacritization, a novel two-phase approach for the Arabic Text Diacritizationtask. PTCAD comprises a pre-finetuning phase and a finetuning phase, treatingArabic Text Diacritization as a token classification task for pre-trainedmodels. The effectiveness of PTCAD is demonstrated through evaluations on twobenchmark datasets derived from the Tashkeela dataset, where it achievesstate-of-the-art results, including a 20\\% reduction in Word Error Rate (WER)compared to existing benchmarks and superior performance over GPT-4 in ATDtasks.",
        "title": "Arabic Text Diacritization In The Age Of Transfer Learning: Token  Classification Is All You Need",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04849",
        "abstract_url": "http://arxiv.org/abs/2401.04849",
        "authors": [
            {
                "last_name": "Hao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Existing Spatial Interaction Models (SIMs) are limited in capturing thecomplex and context-aware interactions between business clusters and tradeareas. To address the limitation, we propose a SIM-GAT model to predictspatiotemporal visitation flows between community business clusters and theirtrade areas. The model innovatively represents the integrated system ofbusiness clusters, trade areas, and transportation infrastructure within anurban region using a connected graph. Then, a graph-based deep learning model,i.e., Graph AttenTion network (GAT), is used to capture the complexity andinterdependencies of business clusters. We developed this model with datacollected from the Miami metropolitan area in Florida. We then demonstrated itseffectiveness in capturing varying attractiveness of business clusters todifferent residential neighborhoods and across scenarios with an eXplainable AIapproach. We contribute a novel method supplementing conventional SIMs topredict and analyze the dynamics of inter-connected community businessclusters. The analysis results can inform data-evidenced and place-specificplanning strategies helping community business clusters better accommodatetheir customers across scenarios, and hence improve the resilience of communitybusinesses.",
        "title": "A Deep Learning Representation of Spatial Interaction Model for  Resilient Spatial Planning of Community Business Clusters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04850",
        "abstract_url": "http://arxiv.org/abs/2401.04850",
        "authors": [
            {
                "last_name": "Abdelmoniem",
                "first_name": "Ahmed M."
            },
            {
                "last_name": "Bensaou",
                "first_name": "Brahim"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The peculiar congestion patterns in data centers are caused by the bursty andcomposite nature of traffic, the small bandwidth-delay product, and the tinyswitch buffers. It is not practical to modify TCP to adapt to data centers,especially in public clouds where multiple congestion control protocolscoexist. In this work, we design a switch-based method to address suchcongestion issues; our approach does not require any modification to TCP, whichenables easy and seamless deployment in public data centers via switch softwareupdate. We first present a simple analysis to demonstrate the stability andeffectiveness of the scheme, and then we discuss a hardware NetFPGAswitch-based prototype. The experimental results from real deployments in asmall testbed cluster show the effectiveness of our approach.",
        "title": "FairQ: Fair and Fast Rate Allocation in Data Centers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04851",
        "abstract_url": "http://arxiv.org/abs/2401.04851",
        "authors": [
            {
                "last_name": "Paul",
                "first_name": "Steve"
            },
            {
                "last_name": "Witter",
                "first_name": "Jhoel"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Souma"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "LG"
        ],
        "abstract": "  This paper develops a graph reinforcement learning approach to onlineplanning of the schedule and destinations of electric aircraft that comprise anurban air mobility (UAM) fleet operating across multiple vertiports. This fleetscheduling problem is formulated to consider time-varying demand, constraintsrelated to vertiport capacity, aircraft capacity and airspace safetyguidelines, uncertainties related to take-off delay, weather-induced routeclosures, and unanticipated aircraft downtime. Collectively, such a formulationpresents greater complexity, and potentially increased realism, than inexisting UAM fleet planning implementations. To address these complexities, anew policy architecture is constructed, primary components of which include:graph capsule conv-nets for encoding vertiport and aircraft-fleet states bothabstracted as graphs; transformer layers encoding time series information ondemand and passenger fare; and a Multi-head Attention-based decoder that usesthe encoded information to compute the probability of selecting each availabledestination for an aircraft. Trained with Proximal Policy Optimization, thispolicy architecture shows significantly better performance in terms of dailyaveraged profits on unseen test scenarios involving 8 vertiports and 40aircraft, when compared to a random baseline and genetic algorithm-derivedoptimal solutions, while being nearly 1000 times faster in execution than thelatter.",
        "title": "Graph Learning-based Fleet Scheduling for Urban Air Mobility under  Operational Constraints, Varying Demand & Uncertainties",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04852",
        "abstract_url": "http://arxiv.org/abs/2401.04852",
        "authors": [
            {
                "last_name": "Askari",
                "first_name": "Arian"
            },
            {
                "last_name": "Yang",
                "first_name": "Zihui"
            },
            {
                "last_name": "Ren",
                "first_name": "Zhaochun"
            },
            {
                "last_name": "Verberne",
                "first_name": "Suzan"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The task of answer retrieval in the legal domain aims to help users to seekrelevant legal advice from massive amounts of professional responses. Two mainchallenges hinder applying existing answer retrieval approaches in otherdomains to the legal domain: (1) a huge knowledge gap between lawyers andnon-professionals; and (2) a mix of informal and formal content on legal QAwebsites. To tackle these challenges, we propose CE_FS, a novel cross-encoder(CE) re-ranker based on the fine-grained structured inputs. CE_FS usesadditional structured information in the CQA data to improve the effectivenessof cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-worldbenchmark dataset for evaluating answer retrieval in the legal domain.Experiments conducted on LegalQA show that our proposed method significantlyoutperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novelfinding is that adding the question tags of each question besides the questiondescription and title into the input of cross-encoder re-rankers structurallyboosts the rankers' effectiveness. While we study our proposed method in thelegal domain, we believe that our method can be applied in similar applicationsin other domains.",
        "title": "Answer Retrieval in Legal Community Question Answering",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04853",
        "abstract_url": "http://arxiv.org/abs/2401.04853",
        "authors": [
            {
                "last_name": "Babaian",
                "first_name": "Tamara"
            },
            {
                "last_name": "Xu",
                "first_name": "Jennifer"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Extraction of concepts and entities of interest from non-formal texts such associal media posts and informal communication is an important capability fordecision support systems in many domains, including healthcare, customerrelationship management, and others. Despite the recent advances in traininglarge language models for a variety of natural language processing tasks, thedeveloped models and techniques have mainly focused on formal texts and do notperform as well on colloquial data, which is characterized by a number ofdistinct challenges. In our research, we focus on the healthcare domain andinvestigate the problem of symptom recognition from colloquial texts bydesigning and evaluating several training strategies for BERT-based modelfine-tuning. These strategies are distinguished by the choice of the basemodel, the training corpora, and application of term perturbations in thetraining data. The best-performing models trained using these strategiesoutperform the state-of-the-art specialized symptom recognizer by a largemargin. Through a series of experiments, we have found specific patterns ofmodel behavior associated with the training strategies we designed. We presentdesign principles for training strategies for effective entity recognition incolloquial texts based on our findings.",
        "title": "Entity Recognition from Colloquial Text",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04854",
        "abstract_url": "http://arxiv.org/abs/2401.04854",
        "authors": [
            {
                "last_name": "Lederman",
                "first_name": "Harvey"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Are LLMs cultural technologies like photocopiers or printing presses, whichtransmit information but cannot create new content? A challenge for this idea,which we call bibliotechnism, is that LLMs often do generate entirely noveltext. We begin by defending bibliotechnism against this challenge, showing hownovel text may be meaningful only in a derivative sense, so that the content ofthis generated text depends in an important sense on the content of originalhuman text. We go on to present a different, novel challenge forbibliotechnism, stemming from examples in which LLMs generate \"novelreference\", using novel names to refer to novel entities. Such examples couldbe smoothly explained if LLMs were not cultural technologies but possessed alimited form of agency (beliefs, desires, and intentions). According tointerpretationism in the philosophy of mind, a system has beliefs, desires andintentions if and only if its behavior is well-explained by the hypothesis thatit has such states. In line with this view, we argue that cases of novelreference provide evidence that LLMs do in fact have beliefs, desires, andintentions, and thus have a limited form of agency.",
        "title": "Are Language Models More Like Libraries or Like Librarians?  Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04855",
        "abstract_url": "http://arxiv.org/abs/2401.04855",
        "authors": [
            {
                "last_name": "Agarwal",
                "first_name": "Saurav"
            },
            {
                "last_name": "Muthukrishnan",
                "first_name": "Ramya"
            },
            {
                "last_name": "Gosrich",
                "first_name": "Walker"
            },
            {
                "last_name": "Ribeiro",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vijay"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Coverage control is the problem of navigating a robot swarm tocollaboratively monitor features or a phenomenon of interest not known apriori. The problem is challenging in decentralized settings with robots thathave limited communication and sensing capabilities. This paper proposes alearnable Perception-Action-Communication (LPAC) architecture for the coveragecontrol problem. In the proposed solution, a convolution neural network (CNN)processes localized perception of the environment; a graph neural network (GNN)enables communication of relevant information between neighboring robots;finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNNin the communication module enables collaboration in the robot swarm bycomputing what information to communicate with neighbors and how to usereceived information to take appropriate actions. We train models usingimitation learning with a centralized clairvoyant algorithm that is aware ofthe entire environment. Evaluations show that the LPAC models outperformstandard decentralized and centralized coverage control algorithms. The learnedpolicy generalizes to environments different from the training dataset,transfers to larger environments with an increased number of robots, and isrobust to noisy position estimates. The results indicate that LPACarchitectures are well-suited for decentralized navigation in robot swarms toachieve collaborative behavior.",
        "title": "LPAC: Learnable Perception-Action-Communication Loops with Applications  to Coverage Control",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04856",
        "abstract_url": "http://arxiv.org/abs/2401.04856",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Sixu"
            },
            {
                "last_name": "Chen",
                "first_name": "Shi"
            },
            {
                "last_name": "Li",
                "first_name": "Qin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Score-based Generative Models (SGMs) is one leading method in generativemodeling, renowned for their ability to generate high-quality samples fromcomplex, high-dimensional data distributions. The method enjoys empiricalsuccess and is supported by rigorous theoretical convergence properties. Inparticular, it has been shown that SGMs can generate samples from adistribution that is close to the ground-truth if the underlying score functionis learned well, suggesting the success of SGM as a generative model. Weprovide a counter-example in this paper. Through the sample complexityargument, we provide one specific setting where the score function is learnedwell. Yet, SGMs in this setting can only output samples that are Gaussianblurring of training data points, mimicking the effects of kernel densityestimation. The finding resonates a series of recent finding that reveal thatSGMs can demonstrate strong memorization effect and fail to generate.",
        "title": "A Good Score Does not Lead to A Good Generative Model",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04857",
        "abstract_url": "http://arxiv.org/abs/2401.04857",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Haotian"
            },
            {
                "last_name": "Jacobs",
                "first_name": "Tim"
            },
            {
                "last_name": "Kaminsky",
                "first_name": "Philip"
            },
            {
                "last_name": "Guo",
                "first_name": "Xin"
            },
            {
                "last_name": "Li",
                "first_name": "Xinyu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Currently, Amazon relies on third parties for transportation marketplace rateforecasts, despite the poor quality and lack of interpretability of theseforecasts. While transportation marketplace rates are typically verychallenging to forecast accurately, we have developed a novel signature-basedstatistical technique to address these challenges and built a predictive andadaptive model to forecast marketplace rates. This novel technique is based ontwo key properties of the signature transform. The first is its universalnonlinearity which linearizes the feature space and hence translates theforecasting problem into a linear regression analysis; the second is thesignature kernel which allows for comparing computationally efficientlysimilarities between time series data. Combined, these properties allow forefficient feature generation and more precise identification of seasonality andregime switching in the forecasting process. Preliminary result by the modelshows that this new technique leads to far superior forecast accuracy versuscommercially available industry models with better interpretability, evenduring the period of Covid-19 and with the sudden onset of the Ukraine war.",
        "title": "Transportation Market Rate Forecast Using Signature Transform",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04858",
        "abstract_url": "http://arxiv.org/abs/2401.04858",
        "authors": [
            {
                "last_name": "Doddapaneni",
                "first_name": "Sumanth"
            },
            {
                "last_name": "Sayana",
                "first_name": "Krishna"
            },
            {
                "last_name": "Jash",
                "first_name": "Ambarish"
            },
            {
                "last_name": "Sodhi",
                "first_name": "Sukhdeep"
            },
            {
                "last_name": "Kuzmin",
                "first_name": "Dima"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "IR",
            "LG"
        ],
        "abstract": "  Modeling long histories plays a pivotal role in enhancing recommendationsystems, allowing to capture user's evolving preferences, resulting in moreprecise and personalized recommendations. In this study we tackle thechallenges of modeling long user histories for preference understanding innatural language. Specifically, we introduce a new User Embedding Module (UEM)that efficiently processes user history in free-form text by compressing andrepresenting them as embeddings, to use them as soft prompts to a LM. Ourexperiments demonstrate the superior capability of this approach in handlingsignificantly longer histories compared to conventional text based promptingmethods, yielding substantial improvements in predictive performance. The maincontribution of this research is to demonstrate the ability to bias languagemodels with user signals represented as embeddings.",
        "title": "User Embedding Model for Personalized Language Prompting",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04859",
        "abstract_url": "http://arxiv.org/abs/2401.04859",
        "authors": [
            {
                "last_name": "Buvoli",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Southworth",
                "first_name": "Ben S."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This work introduces a new class of Runge-Kutta methods for solvingnonlinearly partitioned initial value problems. These new methods, namednonlinearly partitioned Runge-Kutta (NPRK), generalize existing additive andcomponent-partitioned Runge-Kutta methods, and allow one to distributedifferent types of implicitness within nonlinear terms. The paper introducesthe NPRK framework and discusses order conditions, linear stability, and thederivation of implicit-explicit and implicit-implicit NPRK integrators. Thepaper concludes with numerical experiments that demonstrate the utility of NPRKmethods for solving viscous Burger's and the gray thermal radiation transportequations.",
        "title": "A New Class of Runge-Kutta Methods for Nonlinearly Partitioned Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04860",
        "abstract_url": "http://arxiv.org/abs/2401.04860",
        "authors": [
            {
                "last_name": "Lyou",
                "first_name": "Eunyi"
            },
            {
                "last_name": "Lee",
                "first_name": "Doyeon"
            },
            {
                "last_name": "Kim",
                "first_name": "Jooeun"
            },
            {
                "last_name": "Lee",
                "first_name": "Joonseok"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Zero-shot learning offers an efficient solution for a machine learning modelto treat unseen categories, avoiding exhaustive data collection. Zero-shotSketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where itis hard and costly to collect paired sketch-photo samples. We propose a novelframework that indirectly aligns sketches and photos by contrasting themthrough texts, removing the necessity of access to sketch-photo pairs. With anexplicit modality encoding learned from data, our approach disentanglesmodality-agnostic semantics from modality-specific information, bridging themodality gap and enabling effective cross-modal content retrieval within ajoint latent space. From comprehensive experiments, we verify the efficacy ofthe proposed model on ZS-SBIR, and it can be also applied to generalized andfine-grained settings.",
        "title": "Modality-Aware Representation Learning for Zero-shot Sketch-based Image  Retrieval",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04861",
        "abstract_url": "http://arxiv.org/abs/2401.04861",
        "authors": [
            {
                "last_name": "Miao",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Bai",
                "first_name": "Yang"
            },
            {
                "last_name": "Duan",
                "first_name": "Haoran"
            },
            {
                "last_name": "Huang",
                "first_name": "Yawen"
            },
            {
                "last_name": "Wan",
                "first_name": "Fan"
            },
            {
                "last_name": "Long",
                "first_name": "Yang"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yefeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The goal of our work is to generate high-quality novel views from monocularvideos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, haveshown impressive performance by leveraging time-varying dynamic radiationfields. However, these methods have limitations when it comes to accuratelymodeling the motion of complex objects, which can lead to inaccurate and blurryrenderings of details. To address this limitation, we propose a novel approachthat builds upon a recent generalization NeRF, which aggregates nearby viewsonto new viewpoints. However, such methods are typically only effective forstatic scenes. To overcome this challenge, we introduce a module that operatesin both the time and frequency domains to aggregate the features of objectmotion. This allows us to learn the relationship between frames and generatehigher-quality images. Our experiments demonstrate significant improvementsover state-of-the-art methods on dynamic scene datasets. Specifically, ourapproach outperforms existing methods in terms of both the accuracy and visualquality of the synthesized views.",
        "title": "CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from  Monocular Video",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04864",
        "abstract_url": "http://arxiv.org/abs/2401.04864",
        "authors": [
            {
                "last_name": "Charleston",
                "first_name": "M. A."
            },
            {
                "last_name": "Chowdhury",
                "first_name": "S. M."
            },
            {
                "last_name": "Marashdeh",
                "first_name": "Q. M."
            },
            {
                "last_name": "Straiton",
                "first_name": "B. J."
            },
            {
                "last_name": "Teixeira",
                "first_name": "F. L."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The use of capacitance sensors for fuel mass gauging has been inconsideration since the early days of manned space flight. However, certaindifficulties arise when considering tanks in microgravity environments. Surfacetension effects lead to fluid wetting of the interior surface of the tank,leaving large interior voids, while thrust/settling effects can lead todispersed two-phase mixtures. With the exception of Electrical CapacitanceVolume Tomography (ECVT), few sensing technologies are well suited formeasuring annular, stratified, and dispersed fluid configurations as well ashandling the additional complications of mechanical installation inside aspherical tank. To optimize the design of future ECVT based spherical tank massgauging sensors, different electrode plate layouts are considered, and theireffect on the performance of the sensor as a fuel mass gauge is analyzedthrough the use of imaging and averaging techniques.",
        "title": "Microgravity Mass Gauging with Capacitance Sensing: Sensor Design and  Experiment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04867",
        "abstract_url": "http://arxiv.org/abs/2401.04867",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Koji"
            },
            {
                "last_name": "Lala",
                "first_name": "Divesh"
            },
            {
                "last_name": "Ochi",
                "first_name": "Keiko"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Skantze",
                "first_name": "Gabriel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC"
        ],
        "abstract": "  Establishing evaluation schemes for spoken dialogue systems is important, butit can also be challenging. While subjective evaluations are commonly used inuser experiments, objective evaluations are necessary for research comparisonand reproducibility. To address this issue, we propose a framework forindirectly but objectively evaluating systems based on users' behaviours. Inthis paper, to this end, we investigate the relationship between userbehaviours and subjective evaluation scores in social dialogue tasks: attentivelistening, job interview, and first-meeting conversation. The results revealthat in dialogue tasks where user utterances are primary, such as attentivelistening and job interview, indicators like the number of utterances and wordsplay a significant role in evaluation. Observing disfluency also can indicatethe effectiveness of formal tasks, such as job interview. On the other hand, indialogue tasks with high interactivity, such as first-meeting conversation,behaviours related to turn-taking, like average switch pause length, becomemore important. These findings suggest that selecting appropriate userbehaviours can provide valuable insights for objective evaluation in eachsocial dialogue task.",
        "title": "An Analysis of User Behaviours for Objectively Evaluating Spoken  Dialogue Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04868",
        "abstract_url": "http://arxiv.org/abs/2401.04868",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Koji"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bing'er"
            },
            {
                "last_name": "Ekstedt",
                "first_name": "Erik"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Skantze",
                "first_name": "Gabriel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC",
            "SD",
            ""
        ],
        "abstract": "  A demonstration of a real-time and continuous turn-taking prediction systemis presented. The system is based on a voice activity projection (VAP) model,which directly maps dialogue stereo audio to future voice activities. The VAPmodel includes contrastive predictive coding (CPC) and self-attentiontransformers, followed by a cross-attention transformer. We examine the effectof the input context audio length and demonstrate that the proposed system canoperate in real-time with CPU settings, with minimal performance degradation.",
        "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity  Projection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04872",
        "abstract_url": "http://arxiv.org/abs/2401.04872",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuexin"
            },
            {
                "last_name": "Li",
                "first_name": "Kunming"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yongliang"
            },
            {
                "last_name": "Worrall",
                "first_name": "Stewart"
            },
            {
                "last_name": "Li",
                "first_name": "You-Fu"
            },
            {
                "last_name": "Kong",
                "first_name": "He"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "RO"
        ],
        "abstract": "  Predicting pedestrian motion trajectories is crucial for path planning andmotion control of autonomous vehicles. Accurately forecasting crowdtrajectories is challenging due to the uncertain nature of human motions indifferent environments. For training, recent deep learning-based predictionapproaches mainly utilize information like trajectory history and interactionsbetween pedestrians, among others. This can limit the prediction performanceacross various scenarios since the discrepancies between training datasets havenot been properly incorporated. To overcome this limitation, this paperproposes a graph transformer structure to improve prediction performance,capturing the differences between the various sites and scenarios contained inthe datasets. In particular, a self-attention mechanism and a domain adaptionmodule have been designed to improve the generalization ability of the model.Moreover, an additional metric considering cross-dataset sequences isintroduced for training and performance evaluation purposes. The proposedframework is validated and compared against existing methods using popularpublic datasets, i.e., ETH and UCY. Experimental results demonstrate theimproved performance of our proposed scheme.",
        "title": "Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04874",
        "abstract_url": "http://arxiv.org/abs/2401.04874",
        "authors": [
            {
                "last_name": "Mu",
                "first_name": "Xinying"
            },
            {
                "last_name": "Kon",
                "first_name": "Mark"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  A machine learning (ML) feature network is a graph that connects ML featuresin learning tasks based on their similarity. This network representation allowsus to view feature vectors as functions on the network. By leveraging functionoperations from Fourier analysis and from functional analysis, one can easilygenerate new and novel features, making use of the graph structure imposed onthe feature vectors. Such network structures have previously been studiedimplicitly in image processing and computational biology. We thus describefeature networks as graph structures imposed on feature vectors, and provideapplications in machine learning. One application involves graph-basedgeneralizations of convolutional neural networks, involving structured deeplearning with hierarchical representations of features that have varying depthor complexity. This extends also to learning algorithms that are able togenerate useful new multilevel features. Additionally, we discuss the use offeature networks to engineer new features, which can enhance the expressivenessof the model. We give a specific example of a deep tree-structured featurenetwork, where hierarchical connections are formed through feature clusteringand feed-forward learning. This results in low learning complexity andcomputational efficiency. Unlike \"standard\" neural features which are limitedto modulated (thresholded) linear combinations of adjacent ones, featurenetworks offer more general feedforward dependencies among features. Forexample, radial basis functions or graph structure-based dependencies betweenfeatures can be utilized.",
        "title": "Feature Network Methods in Machine Learning and Applications",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04875",
        "abstract_url": "http://arxiv.org/abs/2401.04875",
        "authors": [
            {
                "last_name": "Kobayashi",
                "first_name": "Tsutomu"
            },
            {
                "last_name": "Bondu",
                "first_name": "Martin"
            },
            {
                "last_name": "Ishikawa",
                "first_name": "Fuyuki"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring the safety of autonomous vehicles (AVs) is the key requisite fortheir acceptance in society. This complexity is the core challenge in formallyproving their safety conditions with AI-based black-box controllers andsurrounding objects under various traffic scenarios. This paper describes ourstrategy and experience in modelling, deriving, and proving the safetyconditions of AVs with the Event-B refinement mechanism to reduce complexity.Our case study targets the state-of-the-art model of goal-awareresponsibility-sensitive safety to argue over interactions with surroundingvehicles. We also employ the Simplex architecture to involve advanced black-boxAI controllers. Our experience has demonstrated that the refinement mechanismcan be effectively used to gradually develop the complex system over scenariovariations.",
        "title": "Formal Modelling of Safety Architecture for Responsibility-Aware  Autonomous Vehicle via Event-B Refinement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04881",
        "abstract_url": "http://arxiv.org/abs/2401.04881",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Zi"
            },
            {
                "last_name": "Hua",
                "first_name": "Nan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As LLMs have become capable of processing more complex types of inputs,researchers have recently studied how to efficiently and affordably processpossibly arbitrarily long sequences. One effective approach is to use a FIFOmemory to store keys and values of an attention sublayer from past chunks toallow subsequent queries to attend. However, this approach requires a largememory and/or takes into the consideration the specific LM architecture.Moreover, due to the causal nature between the key-values in prior context andthe queries at present, this approach cannot be extended to bidirectionalattention such as in an encoder-decoder or PrefixLM decoder-only architecture.In this paper, we propose to use eviction policies, such as LRA and LFA, toreduce the memory size and adapt to various architectures, and we also proposethe Attendre layer, a wait-to-attend mechanism by retrieving the key-valuememory (K/V memory) with evicted queries in the query memory (Q memory). As afirst step, we evaluate this method in the context length extension setup usingthe TriviaQA reading comprehension task, and show the effectiveness of theapproach.",
        "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in  Memory-Based Transformers for Long Context Processing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04883",
        "abstract_url": "http://arxiv.org/abs/2401.04883",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Manqing"
            },
            {
                "last_name": "Ting",
                "first_name": "Paishun"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yijian"
            },
            {
                "last_name": "Xu",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Julia"
            },
            {
                "last_name": "Lin",
                "first_name": "Jianzhe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancements in large language models (LLMs) have provided a newavenue for chatbot development, while most existing research has primarilycentered on single-user chatbots that focus on deciding \"What\" to answer afteruser inputs. In this paper, we identified that multi-user chatbots have morecomplex 3W design dimensions -- \"What\" to say, \"When\" to respond, and \"Who\" toanswer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is anLLM-based framework for chatbots specifically designed for group discussions.MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, andUtterance Strategies Arbitrator. These modules jointly determine suitableresponse contents, timings, and the appropriate recipients. To make theoptimizing process for MUCA easier, we further propose an LLM-based Multi-UserSimulator (MUS) that can mimic real user behavior. This enables fastersimulation of a conversation between the chatbot and simulated users, makingthe early development of the chatbot framework much more efficient. MUCAdemonstrates effectiveness, including appropriate chime-in timing, relevantcontent, and positive user engagement, in goal-oriented conversations with asmall to medium number of participants, as evidenced by case studies andexperimental results from user studies.",
        "title": "Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate  Group Conversations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04884",
        "abstract_url": "http://arxiv.org/abs/2401.04884",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junren"
            },
            {
                "last_name": "Scarlett",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "DM",
            "",
            ""
        ],
        "abstract": "  In recent years, the mathematical limits and algorithmic bounds forprobabilistic group testing having become increasingly well-understood, withexact asymptotic thresholds now being known in general scaling regimes for thenoiseless setting. In the noisy setting where each test outcome is flipped withconstant probability, there have been similar developments, but the overallunderstanding has lagged significantly behind the noiseless setting. In thispaper, we substantially narrow this gap by deriving exact asymptotic thresholdsfor the noisy setting under two widely-studied random test designs: i.i.d.Bernoulli and near-constant tests-per-item. These thresholds are established bycombining components of an existing information-theoretic threshold decoderwith a novel analysis of maximum-likelihood decoding (upper bounds), andderiving a novel set of impossibility results by analyzing certain failureevents for optimal maximum-likelihood decoding (lower bounds). Our results showthat existing algorithmic upper bounds for the noisy setting are strictlysuboptimal, and leave open the interesting question of whether our thresholdscan be attained using computationally efficient algorithms.",
        "title": "Exact Thresholds for Noisy Non-Adaptive Group Testing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04885",
        "abstract_url": "http://arxiv.org/abs/2401.04885",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Xie",
                "first_name": "Conghui"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Ding",
                "first_name": "Cunsheng"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Sum-rank codes have known applications in the multishot network coding, thedistributed storage and the construction of space-time codes. U.Mart\\'{\\i}nez-Pe\\~{n}as introduced the cyclic-skew-cyclic sum-rank codes andproposed the BCH bound on the cyclic-skew-cyclic sum-rank codes in his paperpublished in IEEE Trans. Inf. Theory, vol. 67, no. 8, 2021. Afterwards, manysum-rank BCH codes with lower bounds on their dimensions and minimum sum-rankdistances were constructed. Sum-rank Hartmann-Tzeng bound and sum-rank Roosbound on cyclic-skew-cyclic codes were proposed and proved by G. N. Alfarano,F. J. Lobillo, A. Neri, and A. Wachter-Zeh in 2022. In this paper, cyclic,negacyclic and constacyclic sum-rank codes are introduced and a directconstruction of cyclic, negacyclic and constacyclic sum-rank codes of thematrix size $m \\times m$ from cyclic, negacyclic and constacyclic codes over${\\bf F}_{q^m}$ in the Hamming metric is proposed. The cyclic-skew-cylicsum-rank codes are special cyclic sum-rank codes. In addition, BCH andHartmann-Tzeng bounds for a type of cyclic sum-rank codes are developed.Specific constructions of cyclic, negacyclic and constacyclic sum-rank codeswith known dimensions and controllable minimum sum-rank distances are proposed.Moreover, many distance-optimal binary sum-rank codes and an infinite family ofdistance-optimal binary cyclic sum-rank codes with minimum sum-rank distancefour are constructed. This is the first infinite family of distance-optimalsum-rank codes with minimum sum-rank distance four in the literature.",
        "title": "Cyclic and Negacyclic Sum-Rank Codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04887",
        "abstract_url": "http://arxiv.org/abs/2401.04887",
        "authors": [
            {
                "last_name": "Escamilla",
                "first_name": "Emily"
            },
            {
                "last_name": "Klein",
                "first_name": "Martin"
            },
            {
                "last_name": "Cooper",
                "first_name": "Talya"
            },
            {
                "last_name": "Rampin",
                "first_name": "Vicky"
            },
            {
                "last_name": "Weigle",
                "first_name": "Michele C."
            },
            {
                "last_name": "Nelson",
                "first_name": "Michael L."
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  One in five arXiv articles published in 2021 contained a URI to a Git HostingPlatform (GHP), which demonstrates the growing prevalence of GHP URIs inscholarly publications. However, GHP URIs are vulnerable to the same referencerot that plagues the Web at large. The disappearance of software hostingplatforms, like Gitorious and Google Code, and the source code they containthreatens research reproducibility. Archiving the source code and developmenthistory available in GHPs enables the long-term reproducibility of research.Software Heritage and Web archives contain archives of GHP URI resources.However, are the GHP URIs referenced by scholarly publications contained withinthe Software Heritage and Web archive collections? We analyzed a dataset of GHPURIs extracted from scholarly publications to determine (1) is the URI stillpublicly available on the live Web?, (2) has the URI been archived by SoftwareHeritage?, and (3) has the URI been archived by Web archives? Of all GHP URIs,we found that 93.98% were still publicly available on the live Web, 68.39% hadbeen archived by Software Heritage, and 81.43% had been archived by Webarchives.",
        "title": "Cited But Not Archived: Analyzing the Status of Code References in  Scholarly Articles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04890",
        "abstract_url": "http://arxiv.org/abs/2401.04890",
        "authors": [
            {
                "last_name": "Lachapelle",
                "first_name": "S\u00e9bastien"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Pau Rodr\u00edguez"
            },
            {
                "last_name": "Sharma",
                "first_name": "Yash"
            },
            {
                "last_name": "Everett",
                "first_name": "Katie"
            },
            {
                "last_name": "Priol",
                "first_name": "R\u00e9mi Le"
            },
            {
                "last_name": "Lacoste",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Lacoste-Julien",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  This work introduces a novel principle for disentanglement we call mechanismsparsity regularization, which applies when the latent factors of interestdepend sparsely on observed auxiliary variables and/or past latent factors. Wepropose a representation learning method that induces disentanglement bysimultaneously learning the latent factors and the sparse causal graphicalmodel that explains them. We develop a nonparametric identifiability theorythat formalizes this principle and shows that the latent factors can berecovered by regularizing the learned causal graph to be sparse. Moreprecisely, we show identifiablity up to a novel equivalence relation we call\"consistency\", which allows some latent factors to remain entangled (hence theterm partial disentanglement). To describe the structure of this entanglement,we introduce the notions of entanglement graphs and graph preserving functions.We further provide a graphical criterion which guarantees completedisentanglement, that is identifiability up to permutations and element-wisetransformations. We demonstrate the scope of the mechanism sparsity principleas well as the assumptions it relies on with several worked out examples. Forinstance, the framework shows how one can leverage multi-node interventionswith unknown targets on the latent factors to disentangle them. We further drawconnections between our nonparametric results and the now popular exponentialfamily assumption. Lastly, we propose an estimation procedure based onvariational autoencoders and a sparsity constraint and demonstrate it onvarious synthetic datasets. This work is meant to be a significantly extendedversion of Lachapelle et al. (2022).",
        "title": "Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse  Actions, Interventions and Sparse Temporal Dependencies",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04898",
        "abstract_url": "http://arxiv.org/abs/2401.04898",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Bingchao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Recently, various Large Language Models (LLMs) evaluation datasets haveemerged, but most of them have issues with distorted rankings and difficulty inmodel capabilities analysis. Addressing these concerns, this paper introducesANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes\\textit{Keypoint} categorization standard for the first time, each question inANGO can correspond to multiple keypoints, effectively enhancinginterpretability of evaluation results. Base on performance of real humans, webuild a quantifiable question difficulty standard and divide ANGO questionsinto 9 difficulty levels, which provide more precise guidance for modeltraining. To minimize data leakage impact and fully leverage ANGO's innovativefeatures, we have engineered exclusive sampling strategies and a new evaluationframework that support swift testset iteration. Our experiments demonstratethat ANGO poses a stronger challenge to models and reveals more details inevaluation result compared to existing benchmarks.",
        "title": "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language  Models In Chinese Domain",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04900",
        "abstract_url": "http://arxiv.org/abs/2401.04900",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Mengmeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Fan"
            },
            {
                "last_name": "Bu",
                "first_name": "Yude"
            },
            {
                "last_name": "Li",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Yi",
                "first_name": "Zhenping"
            },
            {
                "last_name": "Liu",
                "first_name": "Meng"
            },
            {
                "last_name": "Kong",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  The age and mass of red giants are essential for understanding the structureand evolution of the Milky Way. Traditional isochrone methods for theseestimations are inherently limited due to overlapping isochrones in theHertzsprung-Russell diagram, while asteroseismology, though more precise,requires high-precision, long-term observations. In response to thesechallenges, we developed a novel framework, Spectral Transformer (SPT), topredict the age and mass of red giants aligned with asteroseismology from theirspectra. A key component of SPT, the Multi-head Hadamard Self-Attentionmechanism, designed specifically for spectra, can capture complex relationshipsacross different wavelength. Further, we introduced a Mahalanobisdistance-based loss function to address scale imbalance and interaction modeloss, and incorporated Monte Carlo dropout for quantitative analysis ofprediction uncertainty.Trained and tested on 3,880 red giant spectra fromLAMOST, the SPT achieved remarkable age and mass estimations with averagepercentage errors of 17.64% and 6.61%, respectively, and provided uncertaintiesfor each corresponding prediction. The results significantly outperform thoseof traditional machine learning algorithms and demonstrate a high level ofconsistency with asteroseismology methods and isochrone fitting techniques. Inthe future, our work will leverage datasets from the Chinese Space StationTelescope and the Large Synoptic Survey Telescope to enhance the precision ofthe model and broaden its applicability in the field of astronomy andastrophysics.",
        "title": "SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04903",
        "abstract_url": "http://arxiv.org/abs/2401.04903",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jianqiao"
            },
            {
                "last_name": "Su",
                "first_name": "Yudi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Zeng",
                "first_name": "Zequn"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhengjue"
            },
            {
                "last_name": "Chen",
                "first_name": "Bo"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Captioning (VC) is a challenging multi-modal task since it requiresdescribing the scene in language by understanding various and complex videos.For machines, the traditional VC follows the\"imaging-compression-decoding-and-then-captioning\" pipeline, where compressionis pivot for storage and transmission. However, in such a pipeline, somepotential shortcomings are inevitable, i.e., information redundancy resultingin low efficiency and information loss during the sampling process forcaptioning. To address these problems, in this paper, we propose a novel VCpipeline to generate captions directly from the compressed measurement, whichcan be captured by a snapshot compressive sensing camera and we dub our modelSnapCap. To be more specific, benefiting from the signal simulation, we haveaccess to obtain abundant measurement-video-annotation data pairs for ourmodel. Besides, to better extract language-related visual representations fromthe compressed measurement, we propose to distill the knowledge from videos viaa pre-trained CLIP with plentiful language-vision associations to guide thelearning of our SnapCap. To demonstrate the effectiveness of SnapCap, weconduct experiments on two widely-used VC datasets. Both the qualitative andquantitative results verify the superiority of our pipeline over conventionalVC pipelines. In particular, compared to the \"caption-after-reconstruction\"methods, our SnapCap can run at least 3$\\times$ faster, and achieve bettercaption results.",
        "title": "SnapCap: Efficient Snapshot Compressive Video Captioning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04904",
        "abstract_url": "http://arxiv.org/abs/2401.04904",
        "authors": [
            {
                "last_name": "Akar",
                "first_name": "Nail"
            },
            {
                "last_name": "Liyanaarachchi",
                "first_name": "Sahan"
            },
            {
                "last_name": "Ulukus",
                "first_name": "Sennur"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI",
            "PF"
        ],
        "abstract": "  We study cyclic scheduling for generate-at-will (GAW) multi-source statusupdate systems with heterogeneous service times and packet drop probabilities,with the aim of minimizing the weighted sum age of information (AoI), calledsystem AoI, or the weighted sum peak AoI (PAoI), called system PAoI. Inparticular, we obtain well-performing cyclic schedulers which can easily scaleto thousands of information sources and which also have low onlineimplementation complexity. The proposed schedulers are comparatively studiedagainst existing scheduling algorithms in terms of computational load andsystem AoI/PAoI performance, to validate their effectiveness.",
        "title": "Scalable Cyclic Schedulers for Age of Information Optimization in  Large-Scale Status Update Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04906",
        "abstract_url": "http://arxiv.org/abs/2401.04906",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xinxin"
            },
            {
                "last_name": "Gao",
                "first_name": "Lei"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Device-to-device (D2D) technology is one of the key research areas in 5G/6Gnetworks, and full-duplex (FD) D2D will further enhance its spectral efficiency(SE). In recent years, deep learning approaches have shown remarkableperformance in D2D resource allocation tasks. However, most schemes only modelthe channel state information (CSI) as an independent feature, neglecting thespatial relationships among multiple channels and users within the scenario. Inthis paper, we first design an objective function for FD D2D communicationresource allocation, which aims to maximize the SE of D2D users while ensuringthe minimal required SE of cellular users. Then, considering the complex CSIconstituted by all the users in different channels as a three-dimensionalvector, a centralized resource allocation model based on multi-dimensionalspatial convolutional networks and attention mechanisms (SP-Conv-Att) isproposed. To alleviate the burden of base station, we develop two distributedmodels, Dist-Att and Dist-Att-Conv, to facilitate users to perform channel andpower allocation locally, based on attention and multi-user convolutionalnetworks respectively. Numerical results demonstrate that our models outperformtraditional schemes and recent deep neural network models, significantlyapproximating the optimal solution computed by exhaustive algorithm withextremely low latency.",
        "title": "Deep Learning Based Resource Allocation for Full-duplex Device-to-Device  Communication",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04908",
        "abstract_url": "http://arxiv.org/abs/2401.04908",
        "authors": [
            {
                "last_name": "Mei",
                "first_name": "Haoran"
            },
            {
                "last_name": "Peng",
                "first_name": "Limei"
            },
            {
                "last_name": "Ho",
                "first_name": "Pin-Han"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  Grant-free access (GFA) has been envisioned to play an active role in massiveMachine Type Communication (mMTC) under 5G and Beyond mobile systems, whichtargets at achieving significant reduction of signaling overhead and accesslatency in the presence of sporadic traffic and small-size data. The paperfocuses on a novel K-repetition GFA (K-GFA) scheme by incorporatingReed-Solomon (RS) code with the contention resolution diversity slotted ALOHA(CRDSA), aiming to achieve high-reliability and low-latency access in thepresence of massive uncoordinated MTC devices (MTCDs). We firstly defines a MAClayer transmission structure at each MTCD for supporting message-level RScoding on a data message of $Q$ packets, where a RS code of $KQ$ packets isgenerated and sent in a super time frame (STF) that is composed of $Q$ timeframes. The access point (AP) can recover the original $Q$ packets of the datamessage if at least $Q$ out of the $KQ$ packets of the RS code are successfullyreceived. The AP buffers the received MTCD signals of each resource block (RB)within an STF and exercises the CRDSA based multi-user detection (MUD) byexploring signal-level inter-RB correlation via iterative interferencecancellation (IIC). With the proposed CRDSA based K-GFA scheme, we provide thecomplexity analysis, and derive a closed-form analytical model on the accessprobability for each MTCD as well as its simplified approximate form. Extensivenumerical experiments are conducted to validate its effectiveness on theproposed CRDSA based K-GFA scheme and gain deep understanding on itsperformance regarding various key operational parameters.",
        "title": "On Achieving High-Fidelity Grant-free Non-Orthogonal Multiple Access",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04912",
        "abstract_url": "http://arxiv.org/abs/2401.04912",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhongyan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhifang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Node repair is a crucial problem in erasure-code-based distributed storagesystems. An important metric for repair efficiency is the I/O cost which equalsthe total amount of data accessed at helper nodes to repair a failed node. Inthis work, a general formula for computing the I/O cost of linear repairschemes is derived from a new perspective, i.e., by investigating the Hammingweight of a related linear space. Applying the formula to Reed-Solomon (RS)codes, we obtain lower bounds on the I/O cost for full-length RS codes with twoand three parities. Furthermore, we build linear repair schemes for the RScodes with improved I/O cost. For full-length RS codes with two parities, ourscheme meets the lower bound on the I/O cost.",
        "title": "A Formula for the I/O Cost of Linear Repair Schemes and Application to  Reed-Solomon Codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04914",
        "abstract_url": "http://arxiv.org/abs/2401.04914",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Guohui"
            },
            {
                "last_name": "Li",
                "first_name": "Jianjun"
            },
            {
                "last_name": "Wang",
                "first_name": "Chaoyang"
            },
            {
                "last_name": "Shi",
                "first_name": "Si"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Learning precise representations of users and items to fit observedinteraction data is the fundamental task of collaborative filtering. Existingstudies usually infer entangled representations to fit such interaction data,neglecting to model the diverse matching relationships between users and itemsbehind their interactions, leading to limited performance and weakinterpretability. To address this problem, we propose a Dual DisentangledVariational AutoEncoder (DualVAE) for collaborative recommendation, whichcombines disentangled representation learning with variational inference tofacilitate the generation of implicit interaction data. Specifically, we firstimplement the disentangling concept by unifying an attention-aware dualdisentanglement and disentangled variational autoencoder to infer thedisentangled latent representations of users and items. Further, to encouragethe correspondence and independence of disentangled representations of usersand items, we design a neighborhood-enhanced representation constraint with acustomized contrastive mechanism to improve the representation quality.Extensive experiments on three real-world benchmarks show that our proposedmodel significantly outperforms several recent state-of-the-art baselines.Further empirical experimental results also illustrate the interpretability ofthe disentangled representations learned by DualVAE.",
        "title": "DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04915",
        "abstract_url": "http://arxiv.org/abs/2401.04915",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Erica"
            },
            {
                "last_name": "Simek",
                "first_name": "Olga"
            },
            {
                "last_name": "Miller",
                "first_name": "Benjamin A."
            },
            {
                "last_name": "Sullivan-Pao",
                "first_name": "Danielle"
            },
            {
                "last_name": "Young",
                "first_name": "Evan"
            },
            {
                "last_name": "Smith",
                "first_name": "Christopher L."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  We propose a pipeline for identifying important entities from intelligencereports that constructs a knowledge graph, where nodes correspond to entitiesof fine-grained types (e.g. traffickers) extracted from the text and edgescorrespond to extracted relations between entities (e.g. cartel membership).The important entities in intelligence reports then map to central nodes in theknowledge graph. We introduce a novel method that extracts fine-grainedentities in a few-shot setting (few labeled examples), given limited resourcesavailable to label the frequently changing entity types that intelligenceanalysts are interested in. It outperforms other state-of-the-art methods.Next, we identify challenges facing previous evaluations of zero-shot (nolabeled examples) methods for extracting relations, affecting the step ofpopulating edges. Finally, we explore the utility of the pipeline: given thegoal of identifying important entities, we evaluate the impact of relationextraction errors on the identification of central nodes in several real andsynthetic networks. The impact of these errors varies significantly by graphtopology, suggesting that confidence in measurements based on automaticallyextracted relations should depend on observed network features.",
        "title": "From low resource information extraction to identifying influential  nodes in knowledge graphs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04916",
        "abstract_url": "http://arxiv.org/abs/2401.04916",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Kan"
            },
            {
                "last_name": "Mei",
                "first_name": "Jie"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojun"
            },
            {
                "last_name": "Hou",
                "first_name": "Lu"
            },
            {
                "last_name": "Ma",
                "first_name": "Siwei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Vehicles are no longer isolated entities in traffic environments, thanks tothe development of IoV powered by 5G networks and their evolution into 6G.However, it is not enough for vehicles in a highly dynamic and complex trafficenvironment to make reliable and efficient decisions. As a result, this paperproposes a cloud-edge-end computing system with multi-streams for IoV, referredto as Vehicular Digital Retina (VDR). Local computing and edge computing areeffectively integrated in the VDR system through the aid ofvehicle-to-everything (V2X) networks, resulting in a heterogeneous computingenvironment that improves vehicles' perception and decision-making abilitieswith collaborative strategies. Once the system framework is presented, variousimportant functions in the VDR system are explained in detail, includingV2X-aided collaborative perception, V2X-aided stream sharing for collaborativelearning, and V2X-aided secured collaboration. All of them enable thedevelopment of efficient mechanisms of data sharing and information interactionwith high security for collaborative intelligent driving. We also present acase study with simulation results to demonstrate the effectiveness of theproposed VDR system.",
        "title": "Digital Retina for IoV Towards 6G: Architecture, Opportunities, and  Challenges",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04918",
        "abstract_url": "http://arxiv.org/abs/2401.04918",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Kaitao"
            },
            {
                "last_name": "Masouros",
                "first_name": "Christos"
            },
            {
                "last_name": "Chen",
                "first_name": "Guangji"
            },
            {
                "last_name": "Liu",
                "first_name": "Fan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this study, we explore integrated sensing and communication (ISAC)networks to strike a more effective balance between sensing and communication(S&C) performance at the network scale. We leverage stochastic geometry toanalyze the S&C performance, shedding light on critical cooperativedependencies of ISAC networks. According to the derived expressions of networkperformance, we optimize the user/target loads and the cooperative base stationcluster sizes for S&C to achieve a flexible trade-off between network-scale S&Cperformance. It is observed that the optimal strategy emphasizes the fullutilization of spatial resources to enhance multiplexing and diversity gainwhen maximizing communication ASE. In contrast, for sensing objectives, partsof spatial resources are allocated to cancel inter-cell sensing interference tomaximize sensing ASE. Simulation results validate that the proposed ISAC schemerealizes a remarkable enhancement in overall S&C network performance.",
        "title": "BS Coordination Optimization in Integrated Sensing and Communication: A  Stochastic Geometric View",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04921",
        "abstract_url": "http://arxiv.org/abs/2401.04921",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Wang",
                "first_name": "Yong"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengyuan"
            },
            {
                "last_name": "Wu",
                "first_name": "Doudou"
            },
            {
                "last_name": "Liu",
                "first_name": "Peng"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xinlin"
            },
            {
                "last_name": "Yang",
                "first_name": "Wenming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed toenhance pose accuracy by generating multiple hypotheses. However, most of thehypotheses generated deviate substantially from the true pose. Compared todeterministic models, the excessive uncertainty in probabilistic models leadsto weaker performance in single-hypothesis prediction. To address these twochallenges, we propose a diffusion-based refinement framework called DRPose,which refines the output of deterministic models by reverse diffusion andachieves more suitable multi-hypothesis prediction for the current posebenchmark by multi-step refinement with multiple noises. To this end, wepropose a Scalable Graph Convolution Transformer (SGCT) and a Pose RefinementModule (PRM) for denoising and refining. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-artperformance on both single and multi-hypothesis 3DHPE. Code is available athttps://github.com/KHB1698/DRPose.",
        "title": "Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D  Human Pose Estimaiton",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04923",
        "abstract_url": "http://arxiv.org/abs/2401.04923",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Ruiyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Ouyang"
            },
            {
                "last_name": "Guo",
                "first_name": "Yunhui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Active learning is a commonly used approach that reduces the labeling effortrequired to train deep neural networks. However, the effectiveness of currentactive learning methods is limited by their closed-world assumptions, whichassume that all data in the unlabeled pool comes from a set of predefined knownclasses. This assumption is often not valid in practical situations, as theremay be unknown classes in the unlabeled data, leading to the active open-setannotation problem. The presence of unknown classes in the data cansignificantly impact the performance of existing active learning methods due tothe uncertainty they introduce. To address this issue, we propose a noveldata-centric active learning method called NEAT that actively annotatesopen-set data. NEAT is designed to label known classes data from a pool of bothknown and unknown classes unlabeled data. It utilizes the clusterability oflabels to identify the known classes from the unlabeled pool and selectsinformative samples from those classes based on a consistency criterion thatmeasures inconsistencies between model predictions and local featuredistribution. Unlike the recently proposed learning-centric method for the sameproblem, NEAT is much more computationally efficient and is a data-centricactive open-set annotation method. Our experiments demonstrate that NEATachieves significantly better performance than state-of-the-art active learningmethods for active open-set annotation.",
        "title": "Inconsistency-Based Data-Centric Active Open-Set Annotation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04925",
        "abstract_url": "http://arxiv.org/abs/2401.04925",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Yu",
                "first_name": "Qinkai"
            },
            {
                "last_name": "shu",
                "first_name": "Dong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Hua",
                "first_name": "Wenyue"
            },
            {
                "last_name": "Meng",
                "first_name": "Yanda"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongfeng"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Chain of Thought (CoT) is significant in improving the reasoning abilities oflarge language models (LLMs). However, the correlation between theeffectiveness of CoT and the length of reasoning steps in prompts remainslargely unknown. To shed light on this, we have conducted several empiricalexperiments to explore the relations. Specifically, we design experiments thatexpand and compress the rationale reasoning steps within CoT demonstrations,while keeping all other factors constant. We have the following key findings.First, the results indicate that lengthening the reasoning steps in prompts,even without adding new information into the prompt, considerably enhancesLLMs' reasoning abilities across multiple datasets. Alternatively, shorteningthe reasoning steps, even while preserving the key information, significantlydiminishes the reasoning abilities of models. This finding highlights theimportance of the number of steps in CoT prompts and provides practicalguidance to make better use of LLMs' potential in complex problem-solvingscenarios. Second, we also investigated the relationship between theperformance of CoT and the rationales used in demonstrations. Surprisingly, theresult shows that even incorrect rationales can yield favorable outcomes ifthey maintain the requisite length of inference. Third, we observed that theadvantages of increasing reasoning steps are task-dependent: simpler tasksrequire fewer steps, whereas complex tasks gain significantly from longerinference sequences.",
        "title": "The Impact of Reasoning Step Length on Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04928",
        "abstract_url": "http://arxiv.org/abs/2401.04928",
        "authors": [
            {
                "last_name": "Seo",
                "first_name": "Seonguk"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinkyu"
            },
            {
                "last_name": "Kim",
                "first_name": "Geeho"
            },
            {
                "last_name": "Han",
                "first_name": "Bohyung"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a novel contrastive learning framework to effectively address thechallenges of data heterogeneity in federated learning. We first analyze theinconsistency of gradient updates across clients during local training andestablish its dependence on the distribution of feature representations,leading to the derivation of the supervised contrastive learning (SCL)objective to mitigate local deviations. In addition, we show that a na\\\"iveadoption of SCL in federated learning leads to representation collapse,resulting in slow convergence and limited performance gains. To address thisissue, we introduce a relaxed contrastive learning loss that imposes adivergence penalty on excessively similar sample pairs within each class. Thisstrategy prevents collapsed representations and enhances featuretransferability, facilitating collaborative training and leading to significantperformance improvements. Our framework outperforms all existing federatedlearning approaches by huge margins on the standard benchmarks throughextensive experimental results.",
        "title": "Relaxed Contrastive Learning for Federated Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04929",
        "abstract_url": "http://arxiv.org/abs/2401.04929",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Haonan"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Tu"
            },
            {
                "last_name": "Wang",
                "first_name": "An"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG"
        ],
        "abstract": "  Machine learning models, in particular deep neural networks, are currently anintegral part of various applications, from healthcare to finance. However,using sensitive data to train these models raises concerns about privacy andsecurity. One method that has emerged to verify if the trained models areprivacy-preserving is Membership Inference Attacks (MIA), which allowsadversaries to determine whether a specific data point was part of a model'straining dataset. While a series of MIAs have been proposed in the literature,only a few can achieve high True Positive Rates (TPR) in the low False PositiveRate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIAto be practically useful in real-world settings. In this paper, we present anovel approach to MIA that is aimed at significantly improving TPR at low FPRs.Our method, named learning-based difficulty calibration for MIA(LDC-MIA),characterizes data records by their hardness levels using a neural networkclassifier to determine membership. The experiment results show that LDC-MIAcan improve TPR at low FPR by up to 4x compared to the other difficultycalibration based MIAs. It also has the highest Area Under ROC curve (AUC)across all datasets. Our method's cost is comparable with most of the existingMIAs, but is orders of magnitude more efficient than one of thestate-of-the-art methods, LiRA, while achieving similar performance.",
        "title": "Learning-Based Difficulty Calibration for Enhanced Membership Inference  Attacks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04933",
        "abstract_url": "http://arxiv.org/abs/2401.04933",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Sicong"
            },
            {
                "last_name": "He",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Lui",
                "first_name": "Kry Yik Chau"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While likelihood is attractive in theory, its estimates by deep generativemodels (DGMs) are often broken in practice, and perform poorly for out ofdistribution (OOD) Detection. Various recent works started to consideralternative scores and achieved better performances. However, such recipes donot come with provable guarantees, nor is it clear that their choices extractsufficient information.  We attempt to change this by conducting a case study on variationalautoencoders (VAEs). First, we introduce the likelihood path (LPath) principle,generalizing the likelihood principle. This narrows the search for informativesummary statistics down to the minimal sufficient statistics of VAEs'conditional likelihoods. Second, introducing new theoretic tools such as nearlyessential support, essential distance and co-Lipschitzness, we obtainnon-asymptotic provable OOD detection guarantees for certain distillation ofthe minimal sufficient statistics. The corresponding LPath algorithmdemonstrates SOTA performances, even using simple and small VAEs with poorlikelihood estimates. To our best knowledge, this is the first provableunsupervised OOD method that delivers excellent empirical results, better thanany other VAEs based techniques. We use the same model as\\cite{xiao2020likelihood}, open sourced from:https://github.com/XavierXiao/Likelihood-Regret",
        "title": "Rethinking Test-time Likelihood: The Likelihood Path Principle and Its  Application to OOD Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04934",
        "abstract_url": "http://arxiv.org/abs/2401.04934",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Jiechuan"
            },
            {
                "last_name": "Su",
                "first_name": "Kefan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zongqing"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "LG"
        ],
        "abstract": "  Cooperative multi-agent reinforcement learning is a powerful tool to solvemany real-world cooperative tasks, but restrictions of real-world applicationsmay require training the agents in a fully decentralized manner. Due to thelack of information about other agents, it is challenging to derive algorithmsthat can converge to the optimal joint policy in a fully decentralized setting.Thus, this research area has not been thoroughly studied. In this paper, weseek to systematically review the fully decentralized methods in two settings:maximizing a shared reward of all agents and maximizing the sum of individualrewards of all agents, and discuss open questions and future researchdirections.",
        "title": "Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A  Survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04935",
        "abstract_url": "http://arxiv.org/abs/2401.04935",
        "authors": [
            {
                "last_name": "Vosoughi",
                "first_name": "Ali"
            },
            {
                "last_name": "Bondi",
                "first_name": "Luca"
            },
            {
                "last_name": "Wu",
                "first_name": "Ho-Hsiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenliang"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Conventional audio classification relied on predefined classes, lacking theability to learn from free-form text. Recent methods unlock learning jointaudio-text embeddings from raw audio-text pairs describing audio in naturallanguage. Despite recent advancements, there is little exploration ofsystematic methods to train models for recognizing sound events and sources inalternative scenarios, such as distinguishing fireworks from gunshots atoutdoor events in similar situations. This study introduces causal reasoningand counterfactual analysis in the audio domain. We use counterfactualinstances and include them in our model across different aspects. Our modelconsiders acoustic characteristics and sound source information fromhuman-annotated reference texts. To validate the effectiveness of our model, weconducted pre-training utilizing multiple audio captioning datasets. We thenevaluate with several common downstream tasks, demonstrating the merits of theproposed method as one of the first works leveraging counterfactual informationin audio domain. Specifically, the top-1 accuracy in open-ended language-basedaudio retrieval task increased by more than 43%.",
        "title": "Learning Audio Concepts from Counterfactual Natural Language",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04936",
        "abstract_url": "http://arxiv.org/abs/2401.04936",
        "authors": [
            {
                "last_name": "De Sterck",
                "first_name": "H."
            },
            {
                "last_name": "Falgout",
                "first_name": "R. D."
            },
            {
                "last_name": "Krzysik",
                "first_name": "O. A."
            },
            {
                "last_name": "Schroder",
                "first_name": "J. B."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We consider the parallel-in-time solution of scalar nonlinear conservationlaws in one spatial dimension. The equations are discretized in space with aconservative finite-volume method using weighted essentially non-oscillatory(WENO) reconstructions, and in time with high-order explicit Runge-Kuttamethods. The solution of the global, discretized space-time problem is soughtvia a nonlinear iteration that uses a novel linearization strategy in cases ofnon-differentiable equations. Under certain choices of discretization andalgorithmic parameters, the nonlinear iteration coincides with Newton's method,although, more generally, it is a preconditioned residual correction scheme. Ateach nonlinear iteration, the linearized problem takes the form of a certaindiscretization of a linear conservation law over the space-time domain inquestion. An approximate parallel-in-time solution of the linearized problem iscomputed with a single multigrid reduction-in-time (MGRIT) iteration. The MGRITiteration employs a novel coarse-grid operator that is a modified conservativesemi-Lagrangian discretization and generalizes those we have developedpreviously for non-conservative scalar linear hyperbolic problems. Numericaltests are performed for the inviscid Burgers and Buckley--Leverett equations.For many test problems, the solver converges in just a handful of iterationswith convergence rate independent of mesh resolution, including problems with(interacting) shocks and rarefactions.",
        "title": "Parallel-in-time solution of scalar nonlinear conservation laws",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04938",
        "abstract_url": "http://arxiv.org/abs/2401.04938",
        "authors": [
            {
                "last_name": "Fatima",
                "first_name": "Rumsha"
            },
            {
                "last_name": "Younis",
                "first_name": "Shahzad"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Faraz"
            },
            {
                "last_name": "Imran",
                "first_name": "Hamna"
            },
            {
                "last_name": "Sultan",
                "first_name": "Haseeb"
            },
            {
                "last_name": "Rasool",
                "first_name": "Shahzad"
            },
            {
                "last_name": "Rafiq",
                "first_name": "Mehak"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "LG"
        ],
        "abstract": "  The reliable diagnosis of cardiac conditions through electrocardiogram (ECG)analysis critically depends on accurately detecting P waves and measuring thePR interval. However, achieving consistent and generalizable diagnoses acrossdiverse populations presents challenges due to the inherent global variationsobserved in ECG signals. This paper is focused on applying the Q learningreinforcement algorithm to the various ECG datasets available in thePhysioNet/Computing in Cardiology Challenge (CinC). Five ECG beats, includingNormal Sinus Rhythm, Atrial Flutter, Atrial Fibrillation, 1st DegreeAtrioventricular Block, and Left Atrial Enlargement, are included to studyvariations of P waves and PR Interval on Lead II and Lead V1. Q-Agentclassified 71,672 beat samples in 8,867 patients with an average accuracy of90.4% and only 9.6% average hamming loss over misclassification. The averageclassification time at the 100th episode containing around 40,000 samples is0.04 seconds. An average training reward of 344.05 is achieved at an alpha,gamma, and SoftMax temperature rate of 0.001, 0.9, and 0.1, respectively.",
        "title": "Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform  Variations Related to P Wave and PR Interval",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04941",
        "abstract_url": "http://arxiv.org/abs/2401.04941",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Gaojun"
            },
            {
                "last_name": "Ezerman",
                "first_name": "Martianus Frederic"
            },
            {
                "last_name": "G\u00fcneri",
                "first_name": "Cem"
            },
            {
                "last_name": "Ling",
                "first_name": "San"
            },
            {
                "last_name": "\u00d6zbudak",
                "first_name": "Ferruh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The $b$-symbol metric is a generalization of the Hamming metric. Linearcodes, in the $b$-symbol metric, have been used in the read channel whoseoutputs consist of $b$ consecutive symbols. The Griesmer bound outperforms theSingleton bound for $\\mathbb{F}_q$-linear codes in the Hamming metric, when $q$is fixed and the length is large enough. This scenario is also applicable inthe $b$-symbol metric. Shi, Zhu, and Helleseth recently made a conjecture oncyclic codes in the $b$-symbol metric. In this paper, we present the $b$-symbolGriesmer bound for linear codes by concatenating linear codes and simplexcodes. Based on cyclic codes and extended cyclic codes, we propose two familiesof distance-optimal linear codes with respect to the $b$-symbol Griesmer bound.",
        "title": "Griesmer Bound and Constructions of Linear Codes in $b$-Symbol Metric",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04942",
        "abstract_url": "http://arxiv.org/abs/2401.04942",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Beiwen"
            },
            {
                "last_name": "Gao",
                "first_name": "Huan-ang"
            },
            {
                "last_name": "Cui",
                "first_name": "Leiyao"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yupeng"
            },
            {
                "last_name": "Luo",
                "first_name": "Lan"
            },
            {
                "last_name": "Wang",
                "first_name": "Baofeng"
            },
            {
                "last_name": "Zhi",
                "first_name": "Rong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guyue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the past several years, road anomaly segmentation is actively explored inthe academia and drawing growing attention in the industry. The rationalebehind is straightforward: if the autonomous car can brake before hitting ananomalous object, safety is promoted. However, this rationale naturally callsfor a temporally informed setting while existing methods and benchmarks aredesigned in an unrealistic frame-wise manner. To bridge this gap, we contributethe first video anomaly segmentation dataset for autonomous driving. Sinceplacing various anomalous objects on busy roads and annotating them in everyframe are dangerous and expensive, we resort to synthetic data. To improve therelevance of this synthetic dataset to real-world applications, we train agenerative adversarial network conditioned on rendering G-buffers forphotorealism enhancement. Our dataset consists of 120,000 high-resolutionframes at a 60 FPS framerate, as recorded in 7 different towns. As an initialbenchmarking, we provide baselines using latest supervised and unsupervisedroad anomaly segmentation methods. Apart from conventional ones, we focus ontwo new metrics: temporal consistency and latencyaware streaming accuracy. Webelieve the latter is valuable as it measures whether an anomaly segmentationalgorithm can truly prevent a car from crashing in a temporally informedsetting.",
        "title": "Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic  Dataset and New Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04946",
        "abstract_url": "http://arxiv.org/abs/2401.04946",
        "authors": [
            {
                "last_name": "Mustapha",
                "first_name": "Kassem"
            },
            {
                "last_name": "McLean",
                "first_name": "William"
            },
            {
                "last_name": "Dick",
                "first_name": "Josef"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We investigate a second-order accurate time-stepping scheme for solving atime-fractional diffusion equation with a Caputo derivative of order~$\\alpha\\in (0,1)$. The basic idea of our scheme is based on local integration followedby linear interpolation. It reduces to the standard Crank--Nicolson scheme inthe classical diffusion case, that is, as $\\alpha\\to 1$. Using a novelapproach, we show that the proposed scheme is $\\alpha$-robust and second-orderaccurate in the $L^2(L^2)$-norm, assuming a suitable time-graded mesh. Forcompleteness, we use the Galerkin finite element method for the spatialdiscretization and discuss the error analysis under reasonable regularityassumptions on the given data. Some numerical results are presented at the end.",
        "title": "An $\\alpha$-robust second-order accurate scheme for a subdiffusion  equation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04947",
        "abstract_url": "http://arxiv.org/abs/2401.04947",
        "authors": [
            {
                "last_name": "Hassan-Montero",
                "first_name": "Yusef"
            },
            {
                "last_name": "Herrero-Solana",
                "first_name": "Victor"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Tagging-based systems enable users to categorize web resources by means oftags (freely chosen keywords), in order to refinding these resources later.Tagging is implicitly also a social indexing process, since users share theirtags and resources, constructing a social tag index, so-called folksonomy. Atthe same time of tagging-based system, has been popularised an interface modelfor visual information retrieval known as Tag-Cloud. In this model, the mostfrequently used tags are displayed in alphabetical order. This paper presents anovel approach to Tag-Cloud's tags selection, and proposes the use ofclustering algorithms for visual layout, with the aim of improve browsingexperience. The results suggest that presented approach reduces the semanticdensity of tag set, and improves the visual consistency of Tag-Cloud layout.",
        "title": "Improving Tag-Clouds as Visual Information Retrieval Interfaces",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04950",
        "abstract_url": "http://arxiv.org/abs/2401.04950",
        "authors": [
            {
                "last_name": "Hristopulos",
                "first_name": "Dionissios T."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "IT",
            ""
        ],
        "abstract": "  Causal inference seeks to identify cause-and-effect interactions in coupledsystems. A recently proposed method by Liang detects causal relations byquantifying the direction and magnitude of information flow between timeseries. The theoretical formulation of information flow for stochasticdynamical systems provides a general expression and a data-driven statistic forthe rate of entropy transfer between different system units. To advanceunderstanding of information flow rate in terms of intuitive concepts andphysically meaningful parameters, we investigate statistical properties of thedata-driven information flow rate between coupled stochastic processes. Wederive relations between the expectation of the information flow rate statisticand properties of the auto- and cross-correlation functions. Thus, we elucidatethe dependence of the information flow rate on the analytical properties andcharacteristic times of the correlation functions. Our analysis providesinsight into the influence of the sampling step, the strength ofcross-correlations, and the temporal delay of correlations on information flowrate. We support the theoretical results with numerical simulations ofcorrelated Gaussian processes.",
        "title": "Information Flow Rate for Cross-Correlated Stochastic Processes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04952",
        "abstract_url": "http://arxiv.org/abs/2401.04952",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Zekun"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Some argue that the essence of humanity, such as creativity and sentiment,can never be mimicked by machines. This paper casts doubt on this belief bystudying a vital question: Can AI compose poetry as well as humans? To answerthe question, we propose ProFTAP, a novel evaluation framework inspired byTuring test to assess AI's poetry writing capability. We apply it on currentlarge language models (LLMs) and find that recent LLMs do indeed possess theability to write classical Chinese poems nearly indistinguishable from those ofhumans. We also reveal that various open-source LLMs can outperform GPT-4 onthis task.",
        "title": "Can AI Write Classical Chinese Poetry like Humans? An Empirical Study  Inspired by Turing Test",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04954",
        "abstract_url": "http://arxiv.org/abs/2401.04954",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jian-Guo"
            },
            {
                "last_name": "Witelski",
                "first_name": "Thomas"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaoqian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaqi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In this paper, we investigate the tumor instability by employing bothanalytical and numerical techniques to validate previous results and extend theanalytical findings presented in a prior study by Feng et al 2023. Buildingupon the insights derived from the analytical reconstruction of key results inthe aforementioned work in one dimension (1D) and two dimensions (2D), weextend our analysis to three dimensions (3D). Specifically, we focus on thedetermination of boundary instability using perturbation and asymptoticanalysis along with spherical harmonics. Additionally, we have validated ouranalytical results in a two-dimensional framework by implementing theAlternating Directional Implicit (ADI) method, as detailed in Witelski andBowen (2003). Our primary focus has been on ensuring that the numericalsimulation of the propagation speed aligns accurately with the analyticalfindings. Furthermore, we have matched the simulated boundary stability withthe analytical predictions derived from the evolution function, which will bedefined in subsequent sections of our paper. These alignment is essential foraccurately determining the stability or instability of tumor boundaries.",
        "title": "A Three-dimensional tumor growth model and its boundary instability",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04956",
        "abstract_url": "http://arxiv.org/abs/2401.04956",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Huafeng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Jin",
                "first_name": "Xin"
            },
            {
                "last_name": "Song",
                "first_name": "Qun"
            },
            {
                "last_name": "El-Yacoubi",
                "first_name": "Mounim A."
            },
            {
                "last_name": "Gao",
                "first_name": "Xinbo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR"
        ],
        "abstract": "  Eye movement (EM) is a new highly secure biometric behavioral modality thathas received increasing attention in recent years. Although deep neuralnetworks, such as convolutional neural network (CNN), have recently achievedpromising performance, current solutions fail to capture local and globaltemporal dependencies within eye movement data. To overcome this problem, wepropose in this paper a mixed transformer termed EmMixformer to extract timeand frequency domain information for eye movement recognition. To this end, wepropose a mixed block consisting of three modules, transformer, attention Longshort-term memory (attention LSTM), and Fourier transformer. We are the firstto attempt leveraging transformer to learn long temporal dependencies withineye movement. Second, we incorporate the attention mechanism into LSTM topropose attention LSTM with the aim to learn short temporal dependencies.Third, we perform self attention in the frequency domain to learn globalfeatures. As the three modules provide complementary feature representations interms of local and global dependencies, the proposed EmMixformer is capable ofimproving recognition accuracy. The experimental results on our eye movementdataset and two public eye movement datasets show that the proposed EmMixformeroutperforms the state of the art by achieving the lowest verification error.",
        "title": "EmMixformer: Mix transformer for eye movement recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04958",
        "abstract_url": "http://arxiv.org/abs/2401.04958",
        "authors": [
            {
                "last_name": "Mubasshir",
                "first_name": "Kazi Samin"
            },
            {
                "last_name": "Karim",
                "first_name": "Imtiaz"
            },
            {
                "last_name": "Bertino",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Fake base stations (FBSes) pose a significant security threat byimpersonating legitimate base stations. Though efforts have been made to defeatthis threat, up to this day, the presence of FBSes and the multi-step attacks(MSAs) stemming from them can lead to unauthorized surveillance, interceptionof sensitive information, and disruption of network services for legitimateusers. Therefore, detecting these malicious entities is crucial to ensure thesecurity and reliability of cellular networks. Traditional detection methodsoften rely on additional hardware, predefined rules, signal scanning, changingprotocol specifications, or cryptographic mechanisms that have limitations andincur huge infrastructure costs in accurately identifying FBSes. In this paper,we develop FBSDetector-an effective and efficient detection solution that canreliably detect FBSes and MSAs from layer-3 network traces using machinelearning (ML) at the user equipment (UE) side. To develop FBSDetector, wecreated FBSAD and MSAD, the first-ever high-quality and large-scale datasetsfor training machine learning models capable of detecting FBSes and MSAs. Thesedatasets capture the network traces in different real-world cellular networkscenarios (including mobility and different attacker capabilities)incorporating legitimate base stations and FBSes. The combined network tracehas a volume of 6.6 GB containing 751963 packets. Our novel ML models,specially designed to detect FBSes and MSAs, can effectively detect FBSes withan accuracy of 92% and a false positive rate of 5.96% and recognize MSAs withan accuracy of 86% and a false positive rate of 7.82%. We deploy FBSDetector asa real-world solution to protect end-users through an Android app and validatein a controlled lab environment. Compared to the existing solutions that failto detect FBSes, FBSDetector can detect FBSes in the wild in real time.",
        "title": "FBSDetector: Fake Base Station and Multi Step Attack Detection in  Cellular Networks using Machine Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04960",
        "abstract_url": "http://arxiv.org/abs/2401.04960",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hanli"
            },
            {
                "last_name": "Srikanthan",
                "first_name": "Anusha"
            },
            {
                "last_name": "Folk",
                "first_name": "Spencer"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vijay"
            },
            {
                "last_name": "Matni",
                "first_name": "Nikolai"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG",
            ""
        ],
        "abstract": "  Motivated by the increasing use of quadrotors for payload delivery, weconsider a joint trajectory generation and feedback control design problem fora quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic dragforces from carried payloads can lead to catastrophic outcomes. Prior workmodel aerodynamic effects as residual dynamics or external disturbances in thecontrol problem leading to a reactive policy that could be catastrophic.Moreover, redesigning controllers and tuning control gains on hardwareplatforms is a laborious effort. In this paper, we argue that adapting thetrajectory generation component keeping the controller fixed can improvetrajectory tracking for quadrotor systems experiencing drag forces. To achievethis, we formulate a drag-aware planning problem by applying a suitablerelaxation to an optimal quadrotor control problem, introducing a tracking costfunction which measures the ability of a controller to follow a referencetrajectory. This tracking cost function acts as a regularizer in trajectorygeneration and is learned from data obtained from simulation. Our experimentsin both simulation and on the Crazyflie hardware platform show that changingthe planner reduces tracking error by as much as 83%. Evaluation on hardwaredemonstrates that our planned path, as opposed to a baseline, avoids controllersaturation and catastrophic outcomes during aggressive maneuvers.",
        "title": "Why Change Your Controller When You Can Change Your Planner: Drag-Aware  Trajectory Generation for Quadrotor Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04961",
        "abstract_url": "http://arxiv.org/abs/2401.04961",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Yuncheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zixun"
            },
            {
                "last_name": "Hu",
                "first_name": "Yiwen"
            },
            {
                "last_name": "Li",
                "first_name": "Guanbin"
            },
            {
                "last_name": "Wan",
                "first_name": "Xiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Song"
            },
            {
                "last_name": "Cui",
                "first_name": "Shuguang"
            },
            {
                "last_name": "Huang",
                "first_name": "Silin"
            },
            {
                "last_name": "Li",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate polyp detection is critical for early colorectal cancer diagnosis.Although remarkable progress has been achieved in recent years, the complexcolon environment and concealed polyps with unclear boundaries still posesevere challenges in this area. Existing methods either involve computationallyexpensive context aggregation or lack prior modeling of polyps, resulting inpoor performance in challenging cases. In this paper, we propose the EnhancedCenterNet with Contrastive Learning (ECC-PolypDet), a two-stage training \\&end-to-end inference framework that leverages images and bounding boxannotations to train a general model and fine-tune it based on the inferencescore to obtain a final robust model. Specifically, we conduct Box-assistedContrastive Learning (BCL) during training to minimize the intra-classdifference and maximize the inter-class difference between foreground polypsand backgrounds, enabling our model to capture concealed polyps. Moreover, toenhance the recognition of small polyps, we design the Semantic Flow-guidedFeature Pyramid Network (SFFPN) to aggregate multi-scale features and theHeatmap Propagation (HP) module to boost the model's attention on polyptargets. In the fine-tuning stage, we introduce the IoU-guided SampleRe-weighting (ISR) mechanism to prioritize hard samples by adaptively adjustingthe loss weight for each sample during fine-tuning. Extensive experiments onsix large-scale colonoscopy datasets demonstrate the superiority of our modelcompared with previous state-of-the-art detectors.",
        "title": "ECC-PolypDet: Enhanced CenterNet with Contrastive Learning for Automatic  Polyp Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04962",
        "abstract_url": "http://arxiv.org/abs/2401.04962",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Kailong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Xia",
                "first_name": "Qianchen"
            },
            {
                "last_name": "Liu",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Keyframe extraction aims to sum up a video's semantics with the minimumnumber of its frames. This paper puts forward a Large Model based SequentialKeyframe Extraction for video summarization, dubbed LMSKE, which contains threestages as below. First, we use the large model \"TransNetV21\" to cut the videointo consecutive shots, and employ the large model \"CLIP2\" to generate eachframe's visual feature within each shot; Second, we develop an adaptiveclustering algorithm to yield candidate keyframes for each shot, with eachcandidate keyframe locating nearest to a cluster center; Third, we furtherreduce the above candidate keyframes via redundancy elimination within eachshot, and finally concatenate them in accordance with the sequence of shots asthe final sequential keyframes. To evaluate LMSKE, we curate a benchmarkdataset and conduct rich experiments, whose results exhibit that LMSKE performsmuch better than quite a few SOTA competitors with average F1 of 0.5311,average fidelity of 0.8141, and average compression ratio of 0.9922.",
        "title": "Large Model based Sequential Keyframe Extraction for Video Summarization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04964",
        "abstract_url": "http://arxiv.org/abs/2401.04964",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zechen"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haolin"
            },
            {
                "last_name": "Yan",
                "first_name": "YuJie"
            },
            {
                "last_name": "Wu",
                "first_name": "Xihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD",
            ""
        ],
        "abstract": "  Relating speech to EEG holds considerable importance but challenging. In thisstudy, deep convolutional network was employed to extract spatiotemporalfeatures from EEG data. Self-supervised speech representation and contextualtext embedding were used as speech features. Contrastive learning was used torelated EEG features to speech features. The experimental results demonstratethe benefits of using self-supervised speech representation and contextual textembedding. Through feature fusion and model ensemble, an accuracy of 60.29% wasachieved, and the performance was ranked as No.2 in Task1 of the Auditory EEGChallenge (ICASSP 2024).",
        "title": "Self-supervised speech representation and contextual text embedding for  match-mismatch classification with EEG recording",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04965",
        "abstract_url": "http://arxiv.org/abs/2401.04965",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Xiran"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Yan",
                "first_name": "Yujie"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haolin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zechen"
            },
            {
                "last_name": "Wu",
                "first_name": "Xihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  To investigate the processing of speech in the brain, simple linear modelsare commonly used to establish a relationship between brain signals and speechfeatures. However, these linear models are ill-equipped to model a highlydynamic and complex non-linear system like the brain. Although non-linearmethods with neural networks have been developed recently, reconstructingunseen stimuli from unseen subjects' EEG is still a highly challenging task.This work presents a novel method, ConvConcatNet, to reconstruct mel-specgramsfrom EEG, in which the deep convolution neural network and extensiveconcatenation operation were combined. With our ConvConcatNet model, thePearson correlation between the reconstructed and the target mel-spectrogramcan achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEGChallenge. The codes and models to implement our work will be available onGithub: https://github.com/xuxiran/ConvConcatNet",
        "title": "ConvConcatNet: a deep convolutional neural network to reconstruct mel  spectrogram from the EEG",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04966",
        "abstract_url": "http://arxiv.org/abs/2401.04966",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Chenguang"
            },
            {
                "last_name": "Sun",
                "first_name": "Jie"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Don",
                "first_name": "WaiSun"
            },
            {
                "last_name": "Ju",
                "first_name": "Lili"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  A high-order multi-time-step (MTS) scheme for the bond-based peridynamic (PD)model, an extension of classical continuous mechanics widely used for analyzingdiscontinuous problems like cracks, is proposed. The MTS scheme discretizes thespatial domain with a meshfree method and advances in time with a high-orderRunge-Kutta method. To effectively handle discontinuities (cracks) that appearin a local subdomain in the solution, the scheme employs the Taylor expansionand Lagrange interpolation polynomials with a finer time step size, that is,coarse and fine time step sizes for smooth and discontinuous subdomains,respectively, to achieve accurate and efficient simulations. By eliminatingunnecessary fine-scale resolution imposed on the entire domain, the MTS schemeoutperforms the standard PD scheme by significantly reducing computationalcosts, particularly for problems with discontinuous solutions, as demonstratedby comprehensive theoretical analysis and numerical experiments.",
        "title": "A high-order multi-time-step scheme for bond-based peridynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04968",
        "abstract_url": "http://arxiv.org/abs/2401.04968",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Zhenmin"
            },
            {
                "last_name": "Shen",
                "first_name": "Shaojie"
            },
            {
                "last_name": "Ma",
                "first_name": "Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Cooperative decision-making of Connected Autonomous Vehicles (CAVs) presentsa longstanding challenge due to its inherent nonlinearity, non-convexity, anddiscrete characteristics, compounded by the diverse road topologies encounteredin real-world traffic scenarios. The majority of current methodologies are onlyapplicable to a single and specific scenario, predicated on scenario-specificassumptions. Consequently, their application in real-world environments isrestricted by the innumerable nature of traffic scenarios. In this study, wepropose a unified optimization approach that exhibits the potential to addresscooperative decision-making problems related to traffic scenarios with genericroad topologies. This development is grounded in the premise that thetopologies of various traffic scenarios can be universally represented asDirected Acyclic Graphs (DAGs). Particularly, the reference paths and timeprofiles for all involved CAVs are determined in a fully cooperative manner,taking into account factors such as velocities, accelerations, conflictresolutions, and overall traffic efficiency. The cooperative decision-making ofCAVs is approximated as a mixed-integer linear programming (MILP) problembuilding on the DAGs of road topologies. This favorably facilitates the use ofstandard numerical solvers and the global optimality can be attained throughthe optimization. Case studies corresponding to different multi-lane trafficscenarios featuring diverse topologies are scheduled as the test itineraries,and the efficacy of our proposed methodology is corroborated.",
        "title": "A Universal Cooperative Decision-Making Framework for Connected  Autonomous Vehicles with Generic Road Topologies",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04971",
        "abstract_url": "http://arxiv.org/abs/2401.04971",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shu"
            },
            {
                "last_name": "Xu",
                "first_name": "Zitao"
            },
            {
                "last_name": "Pan",
                "first_name": "Weike"
            },
            {
                "last_name": "Yang",
                "first_name": "Qiang"
            },
            {
                "last_name": "Ming",
                "first_name": "Zhong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Cross-domain sequential recommendation (CDSR) shifts the modeling of userpreferences from flat to stereoscopic by integrating and learning interactioninformation from multiple domains at different granularities (ranging frominter-sequence to intra-sequence and from single-domain to cross-domain).Inthis survey, we initially define the CDSR problem using a four-dimensionaltensor and then analyze its multi-type input representations undermultidirectional dimensionality reductions. Following that, we provide asystematic overview from both macro and micro views. From a macro view, weabstract the multi-level fusion structures of various models across domains anddiscuss their bridges for fusion. From a micro view, focusing on the existingmodels, we specifically discuss the basic technologies and then explain theauxiliary learning technologies. Finally, we exhibit the available publicdatasets and the representative experimental results as well as provide someinsights into future directions for research in CDSR.",
        "title": "A Survey on Cross-Domain Sequential Recommendation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04972",
        "abstract_url": "http://arxiv.org/abs/2401.04972",
        "authors": [
            {
                "last_name": "Stewart",
                "first_name": "Ian"
            },
            {
                "last_name": "Mihalcea",
                "first_name": "Rada"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Machine translation often suffers from biased data and algorithms that canlead to unacceptable errors in system output. While bias in gender norms hasbeen investigated, less is known about whether MT systems encode bias aboutsocial relationships, e.g. sentences such as \"the lawyer kissed her wife.\" Weinvestigate the degree of bias against same-gender relationships in MT systems,using generated template sentences drawn from several noun-gender languages(e.g. Spanish). We find that three popular MT services consistently fail toaccurately translate sentences concerning relationships between nouns of thesame gender. The error rate varies considerably based on the context, e.g.same-gender sentences referencing high female-representation occupations aretranslated with lower accuracy. We provide this work as a case study in theevaluation of intrinsic bias in NLP systems, with respect to socialrelationships.",
        "title": "Whose wife is it anyway? Assessing bias against same-gender  relationships in machine translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04973",
        "abstract_url": "http://arxiv.org/abs/2401.04973",
        "authors": [
            {
                "last_name": "Ju",
                "first_name": "Lili"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Lu",
                "first_name": "Junke"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Existing nonlocal diffusion models are predominantly classified into twocategories: bond-based models, which involve a single-fold integral and usuallysimulate isotropic diffusion, and state-based models, which contain adouble-fold integral and can additionally prototype anisotropic diffusion.While bond-based models exhibit computational efficiency, they are somewhatlimited in their modeling capabilities. In this paper, we develop a novelbond-based nonlocal diffusion model with matrix-valued coefficients innon-divergence form. Our approach incorporates the coefficients into acovariance matrix and employs the multivariate Gaussian function withtruncation to define the kernel function, and subsequently model the nonlocaldiffusion process through the bond-based formulation. We successfully establishthe well-posedness of the proposed model along with deriving some of itsproperties on maximum principle and mass conservation. Furthermore, anefficient linear collocation scheme is designed for numerical solution of ourmodel. Comprehensive experiments in two and three dimensions are conducted toshowcase application of the proposed nonlocal model to both isotropic andanisotropic diffusion problems and to demonstrate numerical accuracy andeffective asymptotic compatibility of the proposed collocation scheme.",
        "title": "A novel bond-based nonlocal diffusion model with matrix-valued  coefficients in non-divergence form and its collocation discretization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04975",
        "abstract_url": "http://arxiv.org/abs/2401.04975",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Qian"
            },
            {
                "last_name": "Cui",
                "first_name": "Ruoxuan"
            },
            {
                "last_name": "Li",
                "first_name": "Yuke"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haoqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Action recognition in videos poses a challenge due to its high computationalcost, especially for Joint Space-Time video transformers (Joint VT). Despitetheir effectiveness, the excessive number of tokens in such architecturessignificantly limits their efficiency. In this paper, we propose HaltingVT, anefficient video transformer adaptively removing redundant video patch tokens,which is primarily composed of a Joint VT and a Glimpser module. Specifically,HaltingVT applies data-adaptive token reduction at each layer, resulting in asignificant reduction in the overall computational cost. Besides, the Glimpsermodule quickly removes redundant tokens in shallow transformer layers, whichmay even be misleading for video recognition tasks based on our observations.To further encourage HaltingVT to focus on the key motion-related informationin videos, we design an effective Motion Loss during training. HaltingVTacquires video analysis capabilities and token halting compression strategiessimultaneously in a unified training process, without requiring additionaltraining procedures or sub-networks. On the Mini-Kinetics dataset, we achieved75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremelylow 9.9 GFLOPs. The code is available athttps://github.com/dun-research/HaltingVT.",
        "title": "HaltingVT: Adaptive Token Halting Transformer for Efficient Video  Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04976",
        "abstract_url": "http://arxiv.org/abs/2401.04976",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Haobo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Mu",
                "first_name": "Da"
            },
            {
                "last_name": "Dang",
                "first_name": "Yonghao"
            },
            {
                "last_name": "Yin",
                "first_name": "Jianqin"
            },
            {
                "last_name": "Tang",
                "first_name": "Jin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Recently, 2D convolution has been found unqualified in sound event detection(SED). It enforces translation equivariance on sound events along frequencyaxis, which is not a shift-invariant dimension. To address this issue, dynamicconvolution is used to model the frequency dependency of sound events. In thispaper, we proposed the first full-dynamic method named \\emph{full-frequencydynamic convolution} (FFDConv). FFDConv generates frequency kernels for everyfrequency band, which is designed directly in the structure forfrequency-dependent modeling. It physically furnished 2D convolution with thecapability of frequency-dependent modeling. FFDConv outperforms not only thebaseline by 6.6\\% in DESED real validation dataset in terms of PSDS1, butoutperforms the other full-dynamic methods. In addition, by visualizingfeatures of sound events, we observed that FFDConv could effectively extractcoherent features in specific frequency bands, consistent with the vocalcontinuity of sound events. This proves that FFDConv has greatfrequency-dependent perception ability.",
        "title": "Full-frequency dynamic convolution: a physical frequency-dependent  convolution for sound event detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04978",
        "abstract_url": "http://arxiv.org/abs/2401.04978",
        "authors": [
            {
                "last_name": "Wetzel",
                "first_name": "Sebastian Johann"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  I introduce a unified framework for interpreting neural network classifierstailored toward automated scientific discovery. In contrast to neuralnetwork-based regression, for classification, it is in general impossible tofind a one-to-one mapping from the neural network to a symbolic equation evenif the neural network itself bases its classification on a quantity that can bewritten as a closed-form equation. In this paper, I embed a trained neuralnetwork into an equivalence class of classifying functions that base theirdecisions on the same quantity. I interpret neural networks by finding anintersection between this equivalence class and human-readable equationsdefined by the search space of symbolic regression. The approach is not limitedto classifiers or full neural networks and can be applied to arbitrary neuronsin hidden layers or latent spaces or to simplify the process of interpretingneural network regressors.",
        "title": "Closed-Form Interpretation of Neural Network Classifiers with Symbolic  Regression Gradients",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04979",
        "abstract_url": "http://arxiv.org/abs/2401.04979",
        "authors": [
            {
                "last_name": "Oh",
                "first_name": "YongKyung"
            },
            {
                "last_name": "Lim",
                "first_name": "Dongyoung"
            },
            {
                "last_name": "Kim",
                "first_name": "Sungil"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  To handle the complexities of irregular and incomplete time series data, wepropose an invertible solution of Neural Differential Equations (NDE)-basedmethod. While NDE-based methods are a powerful method for analyzingirregularly-sampled time series, they typically do not guarantee reversibletransformations in their standard form. Our method suggests the variation ofNeural Controlled Differential Equations (Neural CDEs) with Neural Flow, whichensures invertibility while maintaining a lower computational burden.Additionally, it enables the training of a dual latent space, enhancing themodeling of dynamic temporal dynamics. Our research presents an advancedframework that excels in both classification and interpolation tasks. At thecore of our approach is an enhanced dual latent states architecture, carefullydesigned for high precision across various time series tasks. Empiricalanalysis demonstrates that our method significantly outperforms existingmodels. This work significantly advances irregular time series analysis,introducing innovative techniques and offering a versatile tool for diversepractical applications.",
        "title": "Invertible Solution of Neural Differential Equations for Analysis of  Irregularly-Sampled Time Series",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04980",
        "abstract_url": "http://arxiv.org/abs/2401.04980",
        "authors": [
            {
                "last_name": "Attard",
                "first_name": "Daniel"
            },
            {
                "last_name": "Bajada",
                "first_name": "Josef"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  In recent years, significant advancements have been made in the field ofautonomous driving with the aim of increasing safety and efficiency. However,research that focuses on tractor-trailer vehicles is relatively sparse. Due tothe physical characteristics and articulated joints, such vehicles requiretailored models. While turning, the back wheels of the trailer turn at atighter radius and the truck often has to deviate from the centre of the laneto accommodate this. Due to the lack of publicly available models, this workdevelops truck and trailer models using the high-fidelity simulation softwareCARLA, together with several roundabout scenarios, to establish a baselinedataset for benchmarks. Using a twin-q soft actor-critic algorithm, we train aquasi-end-to-end autonomous driving model which is able to achieve a 73%success rate on different roundabouts.",
        "title": "Autonomous Navigation of Tractor-Trailer Vehicles through Roundabout  Intersections",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04984",
        "abstract_url": "http://arxiv.org/abs/2401.04984",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Luanyuan"
            },
            {
                "last_name": "Du",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            },
            {
                "last_name": "Tang",
                "first_name": "Jinhui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Learning correspondences aims to find correct correspondences (inliers) fromthe initial correspondence set with an uneven correspondence distribution and alow inlier rate, which can be regarded as graph data. Recent advances usuallyuse graph neural networks (GNNs) to build a single type of graph or simplystack local graphs into the global one to complete the task. But they ignorethe complementary relationship between different types of graphs, which caneffectively capture potential relationships among sparse correspondences. Toaddress this problem, we propose MGNet to effectively combine multiplecomplementary graphs. To obtain information integrating implicit and explicitlocal graphs, we construct local graphs from implicit and explicit aspects andcombine them effectively, which is used to build a global graph. Moreover, wepropose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparsecorrespondence information at once in the global graph, which can capture andamplify discriminative features. Extensive experiments demonstrate that MGNetoutperforms state-of-the-art methods in different visual tasks. The code isprovided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.",
        "title": "MGNet: Learning Correspondences via Multiple Graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04986",
        "abstract_url": "http://arxiv.org/abs/2401.04986",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Miyatake",
                "first_name": "Yuto"
            },
            {
                "last_name": "Cui",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Wei",
                "first_name": "Shikui"
            },
            {
                "last_name": "Furihata",
                "first_name": "Daisuke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, there has been growing interest in using physics-informed neuralnetworks (PINNs) to solve differential equations. However, the preservation ofstructure, such as energy and stability, in a suitable manner has yet to beestablished. This limitation could be a potential reason why the learningprocess for PINNs is not always efficient and the numerical results may suggestnonphysical behavior. Besides, there is little research on their applicationson downstream tasks. To address these issues, we propose structure-preservingPINNs to improve their performance and broaden their applications fordownstream tasks. Firstly, by leveraging prior knowledge about the physicalsystem, a structure-preserving loss function is designed to assist the PINN inlearning the underlying structure. Secondly, a framework that utilizesstructure-preserving PINN for robust image recognition is proposed. Here,preserving the Lyapunov structure of the underlying system ensures thestability of the system. Experimental results demonstrate that the proposedmethod improves the numerical accuracy of PINNs for partial differentialequations. Furthermore, the robustness of the model against adversarialperturbations in image data is enhanced.",
        "title": "Structure-Preserving Physics-Informed Neural Networks With Energy or  Lyapunov Structure",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04988",
        "abstract_url": "http://arxiv.org/abs/2401.04988",
        "authors": [
            {
                "last_name": "Jeziorek",
                "first_name": "Kamil"
            },
            {
                "last_name": "Wzorek",
                "first_name": "Piotr"
            },
            {
                "last_name": "Blachut",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Pinna",
                "first_name": "Andrea"
            },
            {
                "last_name": "Kryjak",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Event-based vision is an emerging research field involving processing datagenerated by Dynamic Vision Sensors (neuromorphic cameras). One of the latestproposals in this area are Graph Convolutional Networks (GCNs), which allow toprocess events in its original sparse form while maintaining high detection andclassification performance. In this paper, we present the hardwareimplementation of a~graph generation process from an event camera data stream,taking into account both the advantages and limitations of FPGAs. We proposevarious ways to simplify the graph representation and use scaling andquantisation of values. We consider both undirected and directed graphs thatenable the use of PointNet convolution. The results obtained show that byappropriately modifying the graph representation, it is possible to createa~hardware module for graph generation. Moreover, the proposed modificationshave no significant impact on object detection performance, only 0.08% mAP lessfor the base model and the N-Caltech data set.Finally, we describe the proposedhardware architecture of the graph generation module.",
        "title": "Optimising Graph Representation for Hardware Implementation of Graph  Convolutional Networks for Event-based Vision",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04993",
        "abstract_url": "http://arxiv.org/abs/2401.04993",
        "authors": [
            {
                "last_name": "Hamidi",
                "first_name": "Shayan Mohajer"
            },
            {
                "last_name": "Yang",
                "first_name": "En-Hui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Federated learning (FL) is a promising technology via which some edgedevices/clients collaboratively train a machine learning model orchestrated bya server. Learning an unfair model is known as a critical problem in federatedlearning, where the trained model may unfairly advantage or disadvantage someof the devices. To tackle this problem, in this work, we propose AdaFed. Thegoal of AdaFed is to find an updating direction for the server along which (i)all the clients' loss functions are decreasing; and (ii) more importantly, theloss functions for the clients with larger values decrease with a higher rate.AdaFed adaptively tunes this common direction based on the values of localgradients and loss functions. We validate the effectiveness of AdaFed on asuite of federated datasets, and demonstrate that AdaFed outperformsstate-of-the-art fair FL methods.",
        "title": "AdaFed: Fair Federated Learning via Adaptive Common Descent Direction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04996",
        "abstract_url": "http://arxiv.org/abs/2401.04996",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanyuan"
            },
            {
                "last_name": "Su",
                "first_name": "Lili"
            },
            {
                "last_name": "Joe-Wong",
                "first_name": "Carlee"
            },
            {
                "last_name": "Yeh",
                "first_name": "Edmund"
            },
            {
                "last_name": "Ioannidis",
                "first_name": "Stratis"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  As edge computing capabilities increase, model learning deployments indiverse edge environments have emerged. In experimental design networks,introduced recently, network routing and rate allocation are designed to aidthe transfer of data from sensors to heterogeneous learners. We designefficient experimental design network algorithms that are (a) distributed and(b) use multicast transmissions. This setting poses significant challenges asclassic decentralization approaches often operate on (strictly) concaveobjectives under differentiable constraints. In contrast, the problem we studyhere has a non-convex, continuous DR-submodular objective, while multicasttransmissions naturally result in non-differentiable constraints. From atechnical standpoint, we propose a distributed Frank-Wolfe and a distributedprojected gradient ascent algorithm that, coupled with a relaxation ofnon-differentiable constraints, yield allocations within a $1-1/e$ factor fromthe optimal. Numerical evaluations show that our proposed algorithms outperformcompetitors with respect to model learning quality.",
        "title": "Distributed Experimental Design Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04997",
        "abstract_url": "http://arxiv.org/abs/2401.04997",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Lanling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Li",
                "first_name": "Bingqian"
            },
            {
                "last_name": "Wang",
                "first_name": "Jinpeng"
            },
            {
                "last_name": "Cai",
                "first_name": "Mingchen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recently, large language models such as ChatGPT have showcased remarkableabilities in solving general tasks, demonstrating the potential forapplications in recommender systems. To assess how effectively LLMs can be usedin recommendation tasks, our study primarily focuses on employing LLMs asrecommender systems through prompting engineering. We propose a generalframework for utilizing LLMs in recommendation tasks, focusing on thecapabilities of LLMs as recommenders. To conduct our analysis, we formalize theinput of LLMs for recommendation into natural language prompts with two keyaspects, and explain how our framework can be generalized to variousrecommendation scenarios. As for the use of LLMs as recommenders, we analyzethe impact of public availability, tuning strategies, model architecture,parameter scale, and context length on recommendation results based on theclassification of LLMs. As for prompt engineering, we further analyze theimpact of four important components of prompts, \\ie task descriptions, userinterest modeling, candidate items construction and prompting strategies. Ineach section, we first define and categorize concepts in line with the existingliterature. Then, we propose inspiring research questions followed byexperiments to systematically analyze the impact of different factors on twopublic datasets. Finally, we summarize promising directions to shed lights onfuture research.",
        "title": "Prompting Large Language Models for Recommender Systems: A Comprehensive  Framework and Empirical Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04998",
        "abstract_url": "http://arxiv.org/abs/2401.04998",
        "authors": [
            {
                "last_name": "Jing",
                "first_name": ""
            },
            {
                "last_name": "Lin",
                "first_name": ""
            },
            {
                "last_name": "Silfvenius",
                "first_name": "Christofer"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In the era of sustainable transportation, the significance of electricvehicles (EVs) and their battery technology is becoming increasingly paramount.This study addresses the critical aspect of EV battery reliability, anessential factor in the vehicles' sustainability, performance, and longevity.Current efforts to enhance EV battery reliability tend to focus on isolatedareas, often missing the broader, interconnected challenges within the system.This research investigates these challenges across micro, meso, and macrolevels, presenting a novel lifecycle framework that includes \"Zero\"-Lifereliability and phases such as use, reuse, repurpose, and recycling. Byadopting a holistic approach and delving into system cognition, the study aimsto bridge the gap between isolated improvements and comprehensive systemoptimization, aligning with global sustainability goals and contributing to theadvancement of sustainable transportation and EV technology.",
        "title": "Some Critical Thinking on EV Battery Reliability: from Enhancement to  Optimization -- comprehensive perspectives, lifecycle innovation, system  cognation, and strategic insights",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05007",
        "abstract_url": "http://arxiv.org/abs/2401.05007",
        "authors": [
            {
                "last_name": "Mukendi",
                "first_name": "Christian Mulomba"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  he evaluation of the impact of actions undertaken is essential in management.This paper assesses the impact of efforts considered to mitigate risk andcreate safe environments on a global scale. We measure this impact by lookingat the probability of improvement over a specific short period of time. Usingthe World Risk Index, we conduct a temporal analysis of global disaster riskdynamics from 2011 to 2021. This temporal exploration through the lens of theWorld Risk Index provides insights into the complex dynamics of disaster risk.We found that, despite sustained efforts, the global landscape remains dividedinto two main clusters: high susceptibility and moderate susceptibility,regardless of geographical location. This clustering was achieved using asemi-supervised approach through the Label Spreading algorithm, with 98%accuracy. We also found that the prediction of clusters achieved throughsupervised learning on the period considered in this study (one, three, andfive years) showed that the Logistic regression (almost 99% at each stage)performed better than other classifiers. This suggests that the currentpolicies and mechanisms are not effective in helping countries move from ahazardous position to a safer one during the period considered. In fact,statistical projections using a scenario analysis indicate that there is only a1% chance of such a shift occurring within a five-year timeframe. This soberingreality highlights the need for a paradigm shift. Traditional long-termdisaster management strategies are not effective for countries that are highlyvulnerable. Our findings indicate the need for an innovative approach that istailored to the specific vulnerabilities of these nations. As the threat ofvulnerability persists, our research calls for the development of newstrategies that can effectively address the ongoing challenges of disaster riskmanagement",
        "title": "Temporal Analysis of World Disaster Risk:A Machine Learning Approach to  Cluster Dynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05010",
        "abstract_url": "http://arxiv.org/abs/2401.05010",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Chunpeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Haishuai"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xilu"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhi"
            },
            {
                "last_name": "Bu",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Few-shot Learning aims to learn and distinguish new categories with a verylimited number of available images, presenting a significant challenge in therealm of deep learning. Recent researchers have sought to leverage theadditional textual or linguistic information of these rare categories with apre-trained language model to facilitate learning, thus partially alleviatingthe problem of insufficient supervision signals. However, the full potential ofthe textual information and pre-trained language model have been underestimatedin the few-shot learning till now, resulting in limited performanceenhancements. To address this, we propose a simple but effective framework forfew-shot learning tasks, specifically designed to exploit the textualinformation and language model. In more detail, we explicitly exploit thezero-shot capability of the pre-trained language model with the learnableprompt. And we just add the visual feature with the textual feature forinference directly without the intricate designed fusion modules in previousworks. Additionally, we apply the self-ensemble and distillation to furtherenhance these components. Our extensive experiments conducted across fourwidely used few-shot datasets demonstrate that our simple framework achievesimpressive results. Particularly noteworthy is its outstanding performance inthe 1-shot learning task, surpassing state-of-the-art methods by an average of3.0\\% in classification accuracy. \\footnote{We will make the source codes ofthe proposed framework publicly available upon acceptance. }.",
        "title": "Less is More : A Closer Look at Multi-Modal Few-Shot Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05011",
        "abstract_url": "http://arxiv.org/abs/2401.05011",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Na"
            },
            {
                "last_name": "Chen",
                "first_name": "Weiling"
            },
            {
                "last_name": "Ma",
                "first_name": "Keng Teck"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semi-supervised 3D object detection is a promising yet under-exploreddirection to reduce data annotation costs, especially for cluttered indoorscenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve thistask by utilizing a teacher model to generate pseudo-labels for unlabeledsamples. However, the availability of unlabeled samples in the 3D domain isrelatively limited compared to its 2D counterpart due to the greater effortrequired to collect 3D data. Moreover, the loose consistency regularization inSESS and restricted pseudo-label selection strategy in 3DIoUMatch lead toeither low-quality supervision or a limited amount of pseudo labels. To addressthese issues, we present a novel Dual-Perspective Knowledge Enrichment approachnamed DPKE for semi-supervised 3D object detection. Our DPKE enriches theknowledge of limited training data, particularly unlabeled data, from twoperspectives: data-perspective and feature-perspective. Specifically, from thedata-perspective, we propose a class-probabilistic data augmentation methodthat augments the input data with additional instances based on the varyingdistribution of class probabilities. Our DPKE achieves feature-perspectiveknowledge enrichment by designing a geometry-aware feature matching method thatregularizes feature-level similarity between object proposals from the studentand teacher models. Extensive experiments on the two benchmark datasetsdemonstrate that our DPKE achieves superior performance over existingstate-of-the-art approaches under various label ratio conditions. The sourcecode will be made available to the public.",
        "title": "Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05012",
        "abstract_url": "http://arxiv.org/abs/2401.05012",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Shubao"
            },
            {
                "last_name": "Jin",
                "first_name": "Ming"
            },
            {
                "last_name": "Hou",
                "first_name": "Zhaoxiang"
            },
            {
                "last_name": "Yang",
                "first_name": "Chengyi"
            },
            {
                "last_name": "Li",
                "first_name": "Zengxiang"
            },
            {
                "last_name": "Wen",
                "first_name": "Qingsong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Time series forecasting is crucial and challenging in the real world. Therecent surge in interest regarding time series foundation models, which caterto a diverse array of downstream tasks, is noteworthy. However, existingmethods often overlook the multi-scale nature of time series, an aspect crucialfor precise forecasting. To bridge this gap, we propose HiMTM, a hierarchicalmulti-scale masked time series modeling method designed for long-termforecasting. Specifically, it comprises four integral components: (1)hierarchical multi-scale transformer (HMT) to capture temporal information atdifferent scales; (2) decoupled encoder-decoder (DED) forces the encoder tofocus on feature extraction, while the decoder to focus on pretext tasks; (3)multi-scale masked reconstruction (MMR) provides multi-stage supervisionsignals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) tocapture dependencies between different scales for forecasting. Collectively,these components enhance multi-scale feature extraction capabilities in maskedtime series modeling and contribute to improved prediction accuracy. We conductextensive experiments on 7 mainstream datasets to prove that HiMTM has obviousadvantages over contemporary self-supervised and end-to-end learning methods.The effectiveness of HiMTM is further showcased by its application in theindustry of natural gas demand forecasting.",
        "title": "HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for  Long-Term Forecasting",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05014",
        "abstract_url": "http://arxiv.org/abs/2401.05014",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Jinjing"
            },
            {
                "last_name": "Chen",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Source-free cross-modal knowledge transfer is a crucial yet challenging task,which aims to transfer knowledge from one source modality (e.g., RGB) to thetarget modality (e.g., depth or infrared) with no access to the task-relevant(TR) source data due to memory and privacy concerns. A recent attempt leveragesthe paired task-irrelevant (TI) data and directly matches the features fromthem to eliminate the modality gap. However, it ignores a pivotal clue that thepaired TI data could be utilized to effectively estimate the source datadistribution and better facilitate knowledge transfer to the target modality.To this end, we propose a novel yet concise framework to unlock the potentialof paired TI data for enhancing source-free cross-modal knowledge transfer. Ourwork is buttressed by two key technical components. Firstly, to better estimatethe source data distribution, we introduce a Task-irrelevant data-GuidedModality Bridging (TGMB) module. It translates the target modality data (e.g.,infrared) into the source-like RGB images based on paired TI data and theguidance of the available source model to alleviate two key gaps: 1)inter-modality gap between the paired TI data; 2) intra-modality gap between TIand TR target data. We then propose a Task-irrelevant data-Guided KnowledgeTransfer (TGKT) module that transfers knowledge from the source model to thetarget model by leveraging the paired TI data. Notably, due to theunavailability of labels for the TR target data and its less reliableprediction from the source model, our TGKT model incorporates a self-supervisedpseudo-labeling approach to enable the target model to learn from itspredictions. Extensive experiments show that our method achievesstate-of-the-art performance on three datasets (RGB-to-depth andRGB-to-infrared).",
        "title": "Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential  of Task-Irrelevant Data",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05015",
        "abstract_url": "http://arxiv.org/abs/2401.05015",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Farnia",
                "first_name": "Farzan"
            },
            {
                "last_name": "Leung",
                "first_name": "Ho-fung"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Reinforcement learning (RL) problems where the learner attempts to infer anunobserved reward from some feedback variables have been studied in severalrecent papers. The setting of Interaction-Grounded Learning (IGL) is an exampleof such feedback-based reinforcement learning tasks where the learner optimizesthe return by inferring latent binary rewards from the interaction with theenvironment. In the IGL setting, a relevant assumption used in the RLliterature is that the feedback variable $Y$ is conditionally independent ofthe context-action $(X,A)$ given the latent reward $R$. In this work, wepropose Variational Information-based IGL (VI-IGL) as an information-theoreticmethod to enforce the conditional independence assumption in the IGL-based RLproblem. The VI-IGL framework learns a reward decoder using aninformation-based objective based on the conditional mutual information (MI)between the context-action $(X,A)$ and the feedback variable $Y$ observed fromthe environment. To estimate and optimize the information-based terms for thecontinuous random variables in the RL problem, VI-IGL leverages the variationalrepresentation of mutual information and results in a min-max optimizationproblem. Furthermore, we extend the VI-IGL framework to general $f$-Informationmeasures in the information theory literature, leading to the generalized$f$-VI-IGL framework to address the RL problem under the IGL condition.Finally, we provide the empirical results of applying the VI-IGL method toseveral reinforcement learning settings, which indicate an improved performancein comparison to the previous IGL-based RL algorithm.",
        "title": "An Information Theoretic Approach to Interaction-Grounded Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05018",
        "abstract_url": "http://arxiv.org/abs/2401.05018",
        "authors": [
            {
                "last_name": "Idrees",
                "first_name": "Sarmad"
            },
            {
                "last_name": "Choi",
                "first_name": "Jongeun"
            },
            {
                "last_name": "Sohn",
                "first_name": "Seokman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To achieve seamless collaboration between robots and humans in a sharedenvironment, accurately predicting future human movements is essential. Humanmotion prediction has traditionally been approached as a sequence predictionproblem, leveraging historical human motion data to estimate future poses.Beginning with vanilla recurrent networks, the research community hasinvestigated a variety of methods for learning human motion dynamics,encompassing graph-based and generative approaches. Despite these efforts,achieving accurate long-term predictions continues to be a significantchallenge. In this regard, we present the Adversarial Motion Transformer(AdvMT), a novel model that integrates a transformer-based motion encoder and atemporal continuity discriminator. This combination effectively capturesspatial and temporal dependencies simultaneously within frames. Withadversarial training, our method effectively reduces the unwanted artifacts inpredictions, thereby ensuring the learning of more realistic and fluid humanmotions. The evaluation results indicate that AdvMT greatly enhances theaccuracy of long-term predictions while also delivering robust short-termpredictions",
        "title": "AdvMT: Adversarial Motion Transformer for Long-term Human Motion  Prediction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05019",
        "abstract_url": "http://arxiv.org/abs/2401.05019",
        "authors": [
            {
                "last_name": "Xin",
                "first_name": "Jinghao"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinwoo"
            },
            {
                "last_name": "Chu",
                "first_name": "Shengjia"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Existing Global Path Planning (GPP) algorithms predominantly presume planningin a static environment. This assumption immensely limits their applications toUnmanned Surface Vehicles (USVs) that typically navigate in dynamicenvironments. To address this limitation, we present OkayPlan, a GPP algorithmcapable of generating safe and short paths in dynamic scenarios at a real-timeexecuting speed (125 Hz on a desktop-class computer). Specifically, we approachthe challenge of dynamic obstacle avoidance by formulating the path planningproblem as an obstacle kinematics augmented optimization problem, which can beefficiently resolved through a PSO-based optimizer at a real-time speed.Meanwhile, a Dynamic Prioritized Initialization (DPI) mechanism that adaptivelyinitializes potential solutions for the optimization problem is established tofurther ameliorate the solution quality. Additionally, a relaxation strategythat facilitates the autonomous tuning of OkayPlan's hyperparameters in dynamicenvironments is devised. Comparative experiments involving canonical andcontemporary GPP algorithms, along with ablation studies, have been conductedto substantiate the efficacy of our approach. Results indicate that OkayPlanoutstrips existing methods in terms of path safety, length optimality, andcomputational efficiency, establishing it as a potent GPP technique for dynamicenvironments. The video and code associated with this paper are accessible athttps://github.com/XinJingHao/OkayPlan.",
        "title": "OkayPlan: Obstacle Kinematics Augmented Dynamic Real-time Path Planning  via Particle Swarm Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05031",
        "abstract_url": "http://arxiv.org/abs/2401.05031",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenchao"
            },
            {
                "last_name": "Hong",
                "first_name": "Zicong"
            },
            {
                "last_name": "Guo",
                "first_name": "Song"
            },
            {
                "last_name": "Wang",
                "first_name": "Haozhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Zeng",
                "first_name": "Deze"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Transformer model empowered architectures have become a pillar of cloudservices that keeps reshaping our society. However, the dynamic query loads andheterogeneous user requirements severely challenge current transformer servingsystems, which rely on pre-training multiple variants of a foundation model,i.e., with different sizes, to accommodate varying service demands.Unfortunately, such a mechanism is unsuitable for large transformer models dueto the additional training costs and excessive I/O delay. In this paper, weintroduce OTAS, the first elastic serving system specially tailored fortransformer models by exploring lightweight token management. We develop anovel idea called token adaptation that adds prompting tokens to improveaccuracy and removes redundant tokens to accelerate inference. To cope withfluctuating query loads and diverse user requests, we enhance OTAS withapplication-aware selective batching and online token adaptation. OTAS firstbatches incoming queries with similar service-level objectives to improve theingress throughput. Then, to strike a tradeoff between the overhead of tokenincrement and the potentials for accuracy improvement, OTAS adaptively adjuststhe token execution strategy by solving an optimization problem. We implementand evaluate a prototype of OTAS with multiple datasets, which show that OTASimproves the system utility by at least 18.2%.",
        "title": "OTAS: An Elastic Transformer Serving System via Token Adaptation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05033",
        "abstract_url": "http://arxiv.org/abs/2401.05033",
        "authors": [
            {
                "last_name": "Ulmer",
                "first_name": "Dennis"
            },
            {
                "last_name": "Mansimov",
                "first_name": "Elman"
            },
            {
                "last_name": "Lin",
                "first_name": "Kaixiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Justin"
            },
            {
                "last_name": "Gao",
                "first_name": "Xibin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) are powerful dialogue agents, but specializingthem towards fulfilling a specific function can be challenging. Instructingtuning, i.e. tuning models on instruction and sample responses generated byhumans (Ouyang et al., 2022), has proven as an effective method to do so, yetrequires a number of data samples that a) might not be available or b) costlyto generate. Furthermore, this cost increases when the goal is to make the LLMfollow a specific workflow within a dialogue instead of single instructions.Inspired by the self-play technique in reinforcement learning and the use ofLLMs to simulate human agents, we propose a more effective method for datacollection through LLMs engaging in a conversation in various roles. Thisapproach generates a training data via \"self-talk\" of LLMs that can be refinedand utilized for supervised fine-tuning. We introduce an automated way tomeasure the (partial) success of a dialogue. This metric is used to filter thegenerated conversational data that is fed back in LLM for training. Based onour automated and human evaluations of conversation quality, we demonstratethat such self-talk data improves results. In addition, we examine the variouscharacteristics that showcase the quality of generated dialogues and how theycan be connected to their potential utility as training data.",
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05039",
        "abstract_url": "http://arxiv.org/abs/2401.05039",
        "authors": [
            {
                "last_name": "Hsieh",
                "first_name": "Chou-Ying"
            },
            {
                "last_name": "Chang",
                "first_name": "Chia-Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Po-Hsiu"
            },
            {
                "last_name": "Kuo",
                "first_name": "Sy-Yen"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Maximal Biclique Enumeration (MBE) holds critical importance in graph theorywith applications extending across fields such as bioinformatics, socialnetworks, and recommendation systems. However, its computational complexitypresents barriers for efficiently scaling to large graphs. To address thesechallenges, we introduce cuMBE, a GPU-optimized parallel algorithm for MBE.Utilizing a unique data structure, called compact array, cuMBE eradicates theneed for recursion, thereby significantly minimizing dynamic memoryrequirements and computational overhead. The algorithm utilizes a hybridparallelism approach, in which GPU thread blocks handle coarse-grained tasksassociated with part of the search process. Besides, we implement threefine-grained optimizations within each thread block to enhance performance.Further, we integrate a work-stealing mechanism to mitigate workload imbalancesamong thread blocks. Our experiments reveal that cuMBE achieves an geometricmean speedup of 4.02x and 4.13x compared to the state-of-the-art serialalgorithm and parallel CPU-based algorithm on both common and real-worlddatasets, respectively.",
        "title": "Accelerating Maximal Biclique Enumeration on GPUs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05041",
        "abstract_url": "http://arxiv.org/abs/2401.05041",
        "authors": [
            {
                "last_name": "Iommazzo",
                "first_name": "Gabriele"
            },
            {
                "last_name": "D'Ambrosio",
                "first_name": "Claudia"
            },
            {
                "last_name": "Frangioni",
                "first_name": "Antonio"
            },
            {
                "last_name": "Liberti",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We discuss the issue of finding a good mathematical programming solverconfiguration for a particular instance of a given problem, and we propose atwo-phase approach to solve it. In the first phase we learn the relationshipsbetween the instance, the configuration and the performance of the configuredsolver on the given instance. A specific difficulty of learning a good solverconfiguration is that parameter settings may not all be independent; thisrequires enforcing (hard) constraints, something that many widely usedsupervised learning methods cannot natively achieve. We tackle this issue inthe second phase of our approach, where we use the learnt information toconstruct and solve an optimization problem having an explicit representationof the dependency/consistency constraints on the configuration parametersettings. We discuss computational results for two different instantiations ofthis approach on a unit commitment problem arising in the short-term planningof hydro valleys. We use logistic regression as the supervised learningmethodology and consider CPLEX as the solver of interest.",
        "title": "Learning to Configure Mathematical Programming Solvers by Mathematical  Programming",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05043",
        "abstract_url": "http://arxiv.org/abs/2401.05043",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Kaizheng"
            },
            {
                "last_name": "Shariatmadar",
                "first_name": "Keivan"
            },
            {
                "last_name": "Manchingal",
                "first_name": "Shireen Kudukkil"
            },
            {
                "last_name": "Cuzzolin",
                "first_name": "Fabio"
            },
            {
                "last_name": "Moens",
                "first_name": "David"
            },
            {
                "last_name": "Hallez",
                "first_name": "Hans"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Uncertainty estimation is increasingly attractive for improving thereliability of neural networks. In this work, we present novel credal-setinterval neural networks (CreINNs) designed for classification tasks. CreINNspreserve the traditional interval neural network structure, capturing weightuncertainty through deterministic intervals, while forecasting credal setsusing the mathematical framework of probability intervals. Experimentalvalidations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN)showcase that CreINNs outperform epistemic uncertainty estimation when comparedto variational Bayesian neural networks (BNNs) and deep ensembles (DEs).Furthermore, CreINNs exhibit a notable reduction in computational complexitycompared to variational BNNs and demonstrate smaller model sizes than DEs.",
        "title": "CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation  in Classification Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05045",
        "abstract_url": "http://arxiv.org/abs/2401.05045",
        "authors": [
            {
                "last_name": "Barletta",
                "first_name": "Luca"
            },
            {
                "last_name": "Dytso",
                "first_name": "Alex"
            },
            {
                "last_name": "Shamai",
                "first_name": "Shlomo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers a discrete-time Poisson noise channel with an inputamplitude constraint $\\mathsf{A}$ and a dark current parameter $\\lambda$. It isknown that the capacity-achieving distribution for this channel is discretewith finitely many points. Recently, for $\\lambda=0$, a lower bound of order$\\sqrt{\\mathsf{A}}$ and an upper bound of order $\\mathsf{A} \\log^2(\\mathsf{A})$have been demonstrated on the cardinality of the support of the optimal inputdistribution.  In this work, we improve these results in several ways. First, we provideupper and lower bounds that hold for non-zero dark current. Second, we producea sharper upper bound with a far simpler technique. In particular, for$\\lambda=0$, we sharpen the upper bound from the order of $\\mathsf{A}\\log^2(\\mathsf{A})$ to the order of $\\mathsf{A}$. Finally, some otheradditional information about the location of the support is provided.",
        "title": "Improved Bounds on the Number of Support Points of the  Capacity-Achieving Input for Amplitude Constrained Poisson Channels",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05049",
        "abstract_url": "http://arxiv.org/abs/2401.05049",
        "authors": [
            {
                "last_name": "Vargis",
                "first_name": "Tom Richard"
            },
            {
                "last_name": "Ghiasvand",
                "first_name": "Siavash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This work prioritizes building a modular pipeline that utilizes existingmodels to systematically restore images, rather than creating new restorationmodels from scratch. Restoration is carried out at an object-specific level,with each object regenerated using its corresponding class label information.The approach stands out by providing complete user control over the entirerestoration process. Users can select models for specialized restoration steps,customize the sequence of steps to meet their needs, and refine the resultingregenerated image with depth awareness. The research provides two distinctpathways for implementing image regeneration, allowing for a comparison oftheir respective strengths and limitations. The most compelling aspect of thisversatile system is its adaptability. This adaptability enables users to targetparticular object categories, including medical images, by providing modelsthat are trained on those object classes.",
        "title": "Content-Aware Depth-Adaptive Image Restoration",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05054",
        "abstract_url": "http://arxiv.org/abs/2401.05054",
        "authors": [
            {
                "last_name": "Jinnai",
                "first_name": "Yuu"
            },
            {
                "last_name": "Honda",
                "first_name": "Ukyo"
            },
            {
                "last_name": "Morimura",
                "first_name": "Tetsuro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peinan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  One of the most important challenges in text generation systems is to produceoutputs that are not only correct but also diverse. Recently, MinimumBayes-Risk (MBR) decoding has gained prominence for generating sentences of thehighest quality among the decoding algorithms. However, existing algorithmsproposed for generating diverse outputs are predominantly based on beam searchor random sampling, thus their output quality is capped by these underlyingmethods. In this paper, we investigate an alternative approach -- we developdiversity-promoting decoding algorithms by enforcing diversity objectives toMBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and$k$-medoids MBR (KMBR), methods to generate a set of sentences with highquality and diversity. We evaluate DMBR and KMBR on a variety of directed textgeneration tasks using encoder-decoder models and a large language model withprompting. The experimental results show that the proposed method achieves abetter trade-off than the diverse beam search and sampling algorithms.",
        "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05055",
        "abstract_url": "http://arxiv.org/abs/2401.05055",
        "authors": [
            {
                "last_name": "Xiang",
                "first_name": "Yawen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Heng"
            },
            {
                "last_name": "Li",
                "first_name": "Chengyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Fangwei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongbo"
            },
            {
                "last_name": "Xie",
                "first_name": "Yongqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Motion deblurring is one of the fundamental problems of computer vision andhas received continuous attention. The variability in blur, both within andacross images, imposes limitations on non-blind deblurring techniques that relyon estimating the blur kernel. As a response, blind motion deblurring hasemerged, aiming to restore clear and detailed images without prior knowledge ofthe blur type, fueled by the advancements in deep learning methodologies.Despite strides in this field, a comprehensive synthesis of recent progress indeep learning-based blind motion deblurring is notably absent. This paper fillsthat gap by providing an exhaustive overview of the role of deep learning inblind motion deblurring, encompassing datasets, evaluation metrics, and methodsdeveloped over the last six years. Specifically, we first introduce the typesof motion blur and the fundamental principles of deblurring. Next, we outlinethe shortcomings of traditional non-blind deblurring algorithms, emphasizingthe advantages of employing deep learning techniques for deblurring tasks.Following this, we categorize and summarize existing blind motion deblurringmethods based on different backbone networks, including convolutional neuralnetworks, generative adversarial networks, recurrent neural networks, andTransformer networks. Subsequently, we elaborate not only on the fundamentalprinciples of these different categories but also provide a comprehensivesummary and comparison of their advantages and limitations. Qualitative andquantitative experimental results conducted on four widely used datasetsfurther compare the performance of SOTA methods. Finally, an analysis ofpresent challenges and future pathways. All collected models, benchmarkdatasets, source code links, and codes for evaluation have been made publiclyavailable at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey",
        "title": "Application of Deep Learning in Blind Motion Deblurring: Current Status  and Future Prospects",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05057",
        "abstract_url": "http://arxiv.org/abs/2401.05057",
        "authors": [
            {
                "last_name": "Oelerich",
                "first_name": "Thies"
            },
            {
                "last_name": "Beck",
                "first_name": "Florian"
            },
            {
                "last_name": "Hartl-Nesic",
                "first_name": "Christian"
            },
            {
                "last_name": "Kugi",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This work presents a novel online model-predictive trajectory planner forrobotic manipulators called BoundMPC. This planner allows the collision-freefollowing of Cartesian reference paths in the end-effector's position andorientation, including via-points, within desired asymmetric bounds of theorthogonal path error. The path parameter synchronizes the position andorientation reference paths. The decomposition of the path error into thetangential direction, describing the path progress, and the orthogonaldirection, which represents the deviation from the path, is well known for theposition from the path-following control in the literature. This paper extendsthis idea to the orientation by utilizing the Lie theory of rotations.Moreover, the orthogonal error plane is further decomposed into basisdirections to define asymmetric Cartesian error bounds easily. Using piecewiselinear position and orientation reference paths with via-points iscomputationally very efficient and allows replanning the pose trajectoriesduring the robot's motion. This feature makes it possible to use this plannerfor dynamically changing environments and varying goals. The flexibility andperformance of BoundMPC are experimentally demonstrated by two scenarios on a7-DoF Kuka LBR iiwa 14 R820 robot. The first scenario shows the transfer of alarger object from a start to a goal pose through a confined space where theobject must be tilted. The second scenario deals with grasping an object from atable where the grasping point changes during the robot's motion, andcollisions with other obstacles in the scene must be avoided.",
        "title": "BoundMPC: Cartesian Trajectory Planning with Error Bounds based on Model  Predictive Control in the Joint Space",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05060",
        "abstract_url": "http://arxiv.org/abs/2401.05060",
        "authors": [
            {
                "last_name": "Costa-juss\u00e0",
                "first_name": "Marta R."
            },
            {
                "last_name": "Meglioli",
                "first_name": "Mariano Coria"
            },
            {
                "last_name": "Andrews",
                "first_name": "Pierre"
            },
            {
                "last_name": "Dale",
                "first_name": "David"
            },
            {
                "last_name": "Hansanti",
                "first_name": "Prangthip"
            },
            {
                "last_name": "Kalbassi",
                "first_name": "Elahe"
            },
            {
                "last_name": "Mourachko",
                "first_name": "Alex"
            },
            {
                "last_name": "Ropers",
                "first_name": "Christophe"
            },
            {
                "last_name": "Wood",
                "first_name": "Carleigh"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "",
            ""
        ],
        "abstract": "  Research in toxicity detection in natural language processing for the speechmodality (audio-based) is quite limited, particularly for languages other thanEnglish. To address these limitations and lay the groundwork for trulymultilingual audio-based toxicity detection, we introduce MuTox, the firsthighly multilingual audio-based dataset with toxicity labels. The datasetcomprises 20,000 audio utterances for English and Spanish, and 4,000 for theother 19 languages. To demonstrate the quality of this dataset, we trained theMuTox audio-based toxicity classifier, which enables zero-shot toxicitydetection across a wide range of languages. This classifier outperformsexisting text-based trainable classifiers by more than 1% AUC, while expandingthe language coverage more than tenfold. When compared to a wordlist-basedclassifier that covers a similar number of languages, MuTox improves precisionand recall by approximately 2.5 times. This significant improvement underscoresthe potential of MuTox in advancing the field of audio-based toxicitydetection.",
        "title": "MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot  Detector",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05064",
        "abstract_url": "http://arxiv.org/abs/2401.05064",
        "authors": [
            {
                "last_name": "Torres",
                "first_name": "Bernardo"
            },
            {
                "last_name": "Lattner",
                "first_name": "Stefan"
            },
            {
                "last_name": "Richard",
                "first_name": "Ga\u00ebl"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            ""
        ],
        "abstract": "  Significant strides have been made in creating voice identity representationsusing speech data. However, the same level of progress has not been achievedfor singing voices. To bridge this gap, we suggest a framework for trainingsinger identity encoders to extract representations suitable for varioussinging-related tasks, such as singing voice similarity and synthesis. Weexplore different self-supervised learning techniques on a large collection ofisolated vocal tracks and apply data augmentations during training to ensurethat the representations are invariant to pitch and content variations. Weevaluate the quality of the resulting representations on singer similarity andidentification tasks across multiple datasets, with a particular emphasis onout-of-domain generalization. Our proposed framework produces high-qualityembeddings that outperform both speaker verification and wav2vec 2.0pre-trained baselines on singing voice while operating at 44.1 kHz. We releaseour code and trained models to facilitate further research on singing voice andrelated areas.",
        "title": "Singer Identity Representation Learning using Self-Supervised Techniques",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05069",
        "abstract_url": "http://arxiv.org/abs/2401.05069",
        "authors": [
            {
                "last_name": "Grzeszczyk",
                "first_name": "Michal K."
            },
            {
                "last_name": "Trzci\u0144ski",
                "first_name": "Tomasz"
            },
            {
                "last_name": "Sitek",
                "first_name": "Arkadiusz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we present a novel, machine-learning approach for constructingMulticlass Interpretable Scoring Systems (MISS) - a fully data-drivenmethodology for generating single, sparse, and user-friendly scoring systemsfor multiclass classification problems. Scoring systems are commonly utilizedas decision support models in healthcare, criminal justice, and other domainswhere interpretability of predictions and ease of use are crucial. Priormethods for data-driven scoring, such as SLIM (Supersparse Linear IntegerModel), were limited to binary classification tasks and extensions tomulticlass domains were primarily accomplished via one-versus-all-typetechniques. The scores produced by our method can be easily transformed intoclass probabilities via the softmax function. We demonstrate techniques fordimensionality reduction and heuristics that enhance the training efficiencyand decrease the optimality gap, a measure that can certify the optimality ofthe model. Our approach has been extensively evaluated on datasets from variousdomains, and the results indicate that it is competitive with other machinelearning models in terms of classification performance metrics and provideswell-calibrated class probabilities.",
        "title": "MISS: Multiclass Interpretable Scoring Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05072",
        "abstract_url": "http://arxiv.org/abs/2401.05072",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yichong"
            },
            {
                "last_name": "Feng",
                "first_name": "Xiaocheng"
            },
            {
                "last_name": "Li",
                "first_name": "Baohang"
            },
            {
                "last_name": "Fu",
                "first_name": "Chengpeng"
            },
            {
                "last_name": "Huo",
                "first_name": "Wenshuai"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            },
            {
                "last_name": "Qin",
                "first_name": "Bing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Although large language models (LLMs) have shown surprising languageunderstanding and generation capabilities, they have yet to gain arevolutionary advancement in the field of machine translation. One potentialcause of the limited performance is the misalignment between thetranslation-specific understanding and general understanding inside LLMs. Toalign the translation-specific understanding to the general one, we propose anovel translation process xIoD (Cross-Lingual Interpretation of Difficultwords), explicitly incorporating the general understanding on the contentincurring inconsistent understanding to guide the translation. Specifically,xIoD performs the cross-lingual interpretation for the difficult-to-translatewords and enhances the translation with the generated interpretations.Furthermore, we reframe the external tools of QE to tackle the challenges ofxIoD in the detection of difficult words and the generation of helpfulinterpretations. We conduct experiments on the self-constructed benchmarkChallengeMT, which includes cases in which multiple SOTA translation systemsconsistently underperform. Experimental results show the effectiveness of ourxIoD, which improves up to +3.85 COMET.",
        "title": "Aligning Translation-Specific Understanding to General Understanding in  Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05073",
        "abstract_url": "http://arxiv.org/abs/2401.05073",
        "authors": [
            {
                "last_name": "Leon",
                "first_name": "Florin"
            },
            {
                "last_name": "Gavrilescu",
                "first_name": "Marius"
            },
            {
                "last_name": "Floria",
                "first_name": "Sabina-Adriana"
            },
            {
                "last_name": "Minea",
                "first_name": "Alina-Adriana"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  This paper proposes a classification framework aimed at identifyingcorrelations between job ad requirements and transversal skill sets, with afocus on predicting the necessary skills for individual job descriptions usinga deep learning model. The approach involves data collection, preprocessing,and labeling using ESCO (European Skills, Competences, and Occupations)taxonomy. Hierarchical classification and multi-label strategies are used forskill identification, while augmentation techniques address data imbalance,enhancing model robustness. A comparison between results obtained withEnglish-specific and multi-language sentence embedding models reveals closeaccuracy. The experimental case studies detail neural network configurations,hyperparameters, and cross-validation results, highlighting the efficacy of thehierarchical approach and the suitability of the multi-language model for thediverse European job market. Thus, a new approach is proposed for thehierarchical classification of transversal skills from job ads.",
        "title": "Hierarchical Classification of Transversal Skills in Job Ads Based on  Sentence Embeddings",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05074",
        "abstract_url": "http://arxiv.org/abs/2401.05074",
        "authors": [
            {
                "last_name": "Wietzke",
                "first_name": "Thore"
            },
            {
                "last_name": "Gall",
                "first_name": "Jan"
            },
            {
                "last_name": "Graichen",
                "first_name": "Knut"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper presents a new approach to predict the occupancy for buildingenergy systems (BES). A Gaussian Process (GP) is used to model the occupancyand is represented as a state space model that is equivalent to the full GP ifKalman filtering and smoothing is used. The combination of GPs and mechanisticmodels is called Latent Force Model (LFM). An LFM-based model predictivecontrol (MPC) concept for BES is presented that benefits from the extrapolationcapability of mechanistic models and the learning ability of GPs to predict theoccupancy within the building. Simulations with EnergyPlus and a comparisonwith real-world data from the Bosch Research Campus in Renningen show that areduced energy demand and thermal discomfort can be obtained with the LFM-basedMPC scheme by accounting for the predicted stochastic occupancy.",
        "title": "Occupancy Prediction for Building Energy Systems with Latent Force  Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05083",
        "abstract_url": "http://arxiv.org/abs/2401.05083",
        "authors": [
            {
                "last_name": "Onuoha",
                "first_name": "Okechi"
            },
            {
                "last_name": "Kurawa",
                "first_name": "Suleiman"
            },
            {
                "last_name": "Tang",
                "first_name": "Zezhi"
            },
            {
                "last_name": "Dong",
                "first_name": "Yi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA",
            ""
        ],
        "abstract": "  This paper considers the distributed leader-follower stress-matrix-basedaffine formation control problem of discrete-time linear multi-agent systemswith static and dynamic leaders. In leader-follower multi-agent formationcontrol, the aim is to drive a set of agents comprising leaders and followersto form any desired geometric pattern and simultaneously execute any requiredmanoeuvre by controlling only a few agents denoted as leaders. Existing worksin literature are mostly limited to the cases where the agents' inter-agentcommunications are either in the continuous-time settings or the sampled-datacases where the leaders are constrained to constant (or zero) velocities oraccelerations. Here, we relax these constraints and study the discrete-timecases where the leaders can have stationary or time-varying velocities. Wepropose control laws in the study of different situations and provide somesufficient conditions to guarantee the overall system stability. Simulationstudy is used to demonstrate the efficacy of our proposed control laws.",
        "title": "Discrete-Time Stress Matrix-Based Formation Control of General Linear  Multi-Agent Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05085",
        "abstract_url": "http://arxiv.org/abs/2401.05085",
        "authors": [
            {
                "last_name": "Aute",
                "first_name": "Shubhada"
            },
            {
                "last_name": "Panolan",
                "first_name": "Fahad"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Minimum sum vertex cover of an $n$-vertex graph $G$ is a bijection $\\phi :V(G) \\to [n]$ that minimizes the cost $\\sum_{\\{u,v\\} \\in E(G)} \\min \\{\\phi(u),\\phi(v) \\}$. Finding a minimum sum vertex cover of a graph (the MSVC problem)is NP-hard. MSVC is studied well in the realm of approximation algorithms. Thebest-known approximation factor in polynomial time for the problem is $16/9$[Bansal, Batra, Farhadi, and Tetali, SODA 2021]. Recently, Stankovic[APPROX/RANDOM 2022] proved that achieving an approximation ratio better than$1.014$ for MSVC is NP-hard, assuming the Unique Games Conjecture. We study theMSVC problem from the perspective of parameterized algorithms. The parameterswe consider are the size of a minimum vertex cover and the size of a minimumclique modulator of the input graph. We obtain the following results.  1. MSVC can be solved in $2^{2^{O(k)}} n^{O(1)}$ time, where $k$ is the sizeof a minimum vertex cover.  2. MSVC can be solved in $f(k)\\cdot n^{O(1)}$ time for some computablefunction $f$, where $k$ is the size of a minimum clique modulator.",
        "title": "Parameterized Algorithms for Minimum Sum Vertex Cover",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05093",
        "abstract_url": "http://arxiv.org/abs/2401.05093",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Lei",
                "first_name": "Jie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With recent advancements in aerospace technology, the volume of unlabeledremote sensing image (RSI) data has increased dramatically. Effectivelyleveraging this data through self-supervised learning (SSL) is vital in thefield of remote sensing. However, current methodologies, particularlycontrastive learning (CL), a leading SSL method, encounter specific challengesin this domain. Firstly, CL often mistakenly identifies geographically adjacentsamples with similar semantic content as negative pairs, leading to confusionduring model training. Secondly, as an instance-level discriminative task, ittends to neglect the essential fine-grained features and complex detailsinherent in unstructured RSIs. To overcome these obstacles, we introduceSwiMDiff, a novel self-supervised pre-training framework designed for RSIs.SwiMDiff employs a scene-wide matching approach that effectively recalibrateslabels to recognize data from the same scene as false negatives. Thisadjustment makes CL more applicable to the nuances of remote sensing.Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Throughthe implementation of pixel-level diffusion constraints, we enhance theencoder's ability to capture both the global semantic information and thefine-grained features of the images more comprehensively. Our proposedframework significantly enriches the information available for downstream tasksin remote sensing. Demonstrating exceptional performance in change detectionand land-cover classification tasks, SwiMDiff proves its substantial utilityand value in the field of remote sensing.",
        "title": "SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion  Constraint for Remote Sensing Image",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05097",
        "abstract_url": "http://arxiv.org/abs/2401.05097",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Junhoo"
            },
            {
                "last_name": "Kim",
                "first_name": "Yearim"
            },
            {
                "last_name": "Lee",
                "first_name": "Hyunho"
            },
            {
                "last_name": "Kwak",
                "first_name": "Nojun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Although meta-learning seems promising performance in the realm of rapidadaptability, it is constrained by fixed cardinality. When faced with tasks ofvarying cardinalities that were unseen during training, the model lacks itsability. In this paper, we address and resolve this challenge by harnessing`label equivalence' emerged from stochastic numeric label assignments duringepisodic task sampling. Questioning what defines ``true\" meta-learning, weintroduce the ``any-way\" learning paradigm, an innovative model trainingapproach that liberates model from fixed cardinality constraints. Surprisingly,this model not only matches but often outperforms traditional fixed-way modelsin terms of performance, convergence speed, and stability. This disruptsestablished notions about domain generalization. Furthermore, we argue that theinherent label equivalence naturally lacks semantic information. To bridge thissemantic information gap arising from label equivalence, we further propose amechanism for infusing semantic class information into the model. This wouldenhance the model's comprehension and functionality. Experiments conducted onrenowned architectures like MAML and ProtoNet affirm the effectiveness of ourmethod.",
        "title": "Any-Way Meta Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05098",
        "abstract_url": "http://arxiv.org/abs/2401.05098",
        "authors": [
            {
                "last_name": "Agouzal",
                "first_name": "Eki"
            },
            {
                "last_name": "Argaud",
                "first_name": "Jean-Philippe"
            },
            {
                "last_name": "Bergmann",
                "first_name": "Michel"
            },
            {
                "last_name": "Fert\u00e9",
                "first_name": "Guilhem"
            },
            {
                "last_name": "Michel-Ponnelle",
                "first_name": "Sylvie"
            },
            {
                "last_name": "Taddei",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We propose a projection-based model order reduction procedure for the ageingof large prestressed concrete structures. Our work is motivated by applicationsin the nuclear industry, particularly in the simulation of containmentbuildings. Such numerical simulations involve a multi-modeling approach: athree-dimensional nonlinear thermo-hydro-visco-elastic rheological model isused for concrete; and prestressing cables are described by a one-dimensionallinear thermo-elastic behavior. A kinematic linkage is performed in order toconnect the concrete nodes and the steel nodes: coincident points in eachmaterial are assumed to have the same displacement. We develop an adaptivealgorithm based on a Proper Orthogonal Decomposition (POD) in time and greedyin parameter to build a reduced order model (ROM). The nonlinearity of theoperator entails that the computational cost of the ROM assembly scales withthe size of the high-fidelity model. We develop an hyper-reduction strategybased on empirical quadrature to bypass this computational bottleneck: ourapproach relies on the construction of a reduced mesh to speed up onlineassembly costs of the ROM. We provide numerical results for a standard sectionof a double-walled containment building using a qualified and broadly-usedindustrial grade finite element solver for structural mechanics(code$\\_$aster).",
        "title": "Projection-based model order reduction for prestressed concrete with an  application to the standard section of a nuclear containment building",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05100",
        "abstract_url": "http://arxiv.org/abs/2401.05100",
        "authors": [
            {
                "last_name": "Moriyasu",
                "first_name": "Ryuta"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Sho"
            },
            {
                "last_name": "Kashima",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Model Predictive Control (MPC) is a versatile approach capable ofaccommodating diverse control requirements, holding significant promise for abroad spectrum of industrial applications. Noteworthy challenges associatedwith MPC include the substantial computational burden and the inherentdifficulty in ensuring system stability. Recently, a rapid computationtechnique has been introduced as a potential solution. This method guides theinput toward convergence with the optimal control problem solution by employingthe primal-dual gradient (PDG) dynamics as a controller. Furthermore, stabilityassurances grounded in dissipativity theory have been established. However,these assurances are applicable solely to continuous-time feedback systems. Asa consequence, when the controller undergoes discretization and is implementedas a sampled-data system, stability cannot be guaranteed. In this paper, wepropose a discrete-time dynamical controller, incorporating specificmodifications to the PDG approach, and present stability conditions relevant tothe resulting sampled-data system. Additionally, we introduce an extensiondesigned to enhance control performance. Numerical examples substantiate thatour proposed method not only enhances control effectiveness but alsoeffectively discerns stability degradation resulting from discretization, anuance often overlooked by conventional methods.",
        "title": "Sampled-Data Primal-Dual Gradient Dynamics in Model Predictive Control",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05111",
        "abstract_url": "http://arxiv.org/abs/2401.05111",
        "authors": [
            {
                "last_name": "Fujita",
                "first_name": "Kenichi"
            },
            {
                "last_name": "Sato",
                "first_name": "Hiroshi"
            },
            {
                "last_name": "Ashihara",
                "first_name": "Takanori"
            },
            {
                "last_name": "Kanagawa",
                "first_name": "Hiroki"
            },
            {
                "last_name": "Delcroix",
                "first_name": "Marc"
            },
            {
                "last_name": "Moriya",
                "first_name": "Takafumi"
            },
            {
                "last_name": "Ijima",
                "first_name": "Yusuke"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "LG",
            ""
        ],
        "abstract": "  The zero-shot text-to-speech (TTS) method, based on speaker embeddingsextracted from reference speech using self-supervised learning (SSL) speechrepresentations, can reproduce speaker characteristics very accurately.However, this approach suffers from degradation in speech synthesis qualitywhen the reference speech contains noise. In this paper, we propose anoise-robust zero-shot TTS method. We incorporated adapters into the SSL model,which we fine-tuned with the TTS model using noisy reference speech. Inaddition, to further improve performance, we adopted a speech enhancement (SE)front-end. With these improvements, our proposed SSL-based zero-shot TTSachieved high-quality speech synthesis with noisy reference speech. Through theobjective and subjective evaluations, we confirmed that the proposed method ishighly robust to noise in reference speech, and effectively works incombination with SE.",
        "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on  self-supervised speech-representation model with adapters",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05112",
        "abstract_url": "http://arxiv.org/abs/2401.05112",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shuxin"
            },
            {
                "last_name": "Rigger",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Extensible Markup Language (XML) is a widely used file format for datastorage and transmission. Many XML processors support XPath, a query languagethat enables the extraction of elements from XML documents. These systems canbe affected by logic bugs, which are bugs that cause the processor to returnincorrect results. In order to tackle such bugs, we propose a new approach,which we realized as a system called XPress. As a test oracle, XPress relies ondifferential testing, which compares the results of multiple systems on thesame test input, and identifies bugs through discrepancies in their outputs. Astest inputs, XPress generates both XML documents and XPath queries. Aiming togenerate meaningful queries that compute non-empty results, XPress selects aso-called targeted node to guide the XPath expression generation process. Usingthe targeted node, XPress generates XPath expressions that reference existingcontext related to the targeted node, such as its tag name and attributes,while also guaranteeing that a predicate evaluates to true before furtherexpanding the query. We tested our approach on six mature XML processors,BaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system.In total, we have found 20 unique bugs in these systems, of which 25 have beenverified by the developers, and 12 of which have been fixed. XPress isefficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fastas naive random generation. We expect that the effectiveness and simplicity ofour approach will help to improve the robustness of many XML processors.",
        "title": "Finding XPath Bugs in XML Document Processors via Differential Testing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05115",
        "abstract_url": "http://arxiv.org/abs/2401.05115",
        "authors": [
            {
                "last_name": "Tsiakas",
                "first_name": "Kostas"
            },
            {
                "last_name": "Murray-Rust",
                "first_name": "Dave"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  This paper aims to develop a semi-formal design space for Human-AIinteractions, by building a set of interaction primitives which specify thecommunication between users and AI systems during their interaction. We showhow these primitives can be combined into a set of interaction patterns whichcan provide an abstract specification for exchanging messages between humansand AI/ML models to carry out purposeful interactions. The motivation behindthis is twofold: firstly, to provide a compact generalisation of existingpractices, that highlights the similarities and differences between systems interms of their interaction behaviours; and secondly, to support the creation ofnew systems, in particular by opening the space of possibilities forinteractions with models. We present a short literature review on frameworks,guidelines and taxonomies related to the design and implementation of HAIinteractions, including human-in-the-loop, explainable AI, as well as hybridintelligence and collaborative learning approaches. From the literature review,we define a vocabulary for describing information exchanges in terms ofproviding and requesting particular model-specific data types. Based on thisvocabulary, a message passing model for interactions between humans and modelsis presented, which we demonstrate can account for existing systems andapproaches. Finally, we build this into design patterns as mid-level constructsthat capture common interactional structures. We discuss how this approach canbe used towards a design space for Human-AI interactions that creates newpossibilities for designs as well as keeping track of implementation issues andconcerns.",
        "title": "Unpacking Human-AI interactions: From interaction primitives to a design  space",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05121",
        "abstract_url": "http://arxiv.org/abs/2401.05121",
        "authors": [
            {
                "last_name": "Fayza",
                "first_name": "Farbin"
            },
            {
                "last_name": "Rao",
                "first_name": "Satyavolu Papa"
            },
            {
                "last_name": "Bunandar",
                "first_name": "Darius"
            },
            {
                "last_name": "Gupta",
                "first_name": "Udit"
            },
            {
                "last_name": "Joshi",
                "first_name": "Ajay"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG"
        ],
        "abstract": "  Photonic integrated circuits are finding use in a variety of applicationsincluding optical transceivers, LIDAR, bio-sensing, photonic quantum computing,and Machine Learning (ML). In particular, with the exponentially increasingsizes of ML models, photonics-based accelerators are getting special attentionas a sustainable solution because they can perform ML inferences with multipleorders of magnitude higher energy efficiency than CMOS-based accelerators.However, recent studies have shown that hardware manufacturing andinfrastructure contribute significantly to the carbon footprint of computingdevices, even surpassing the emissions generated during their use. For example,the manufacturing process accounts for 74% of the total carbon emissions fromApple in 2019. This prompts us to ask -- if we consider both the embodied(manufacturing) and operational carbon cost of photonics, is it indeed a viableavenue for a sustainable future? So, in this paper, we build a carbon footprintmodel for photonic chips and investigate the sustainability of photonics-basedaccelerators by conducting a case study on ADEPT, a photonics-based acceleratorfor deep neural network inference. Our analysis shows that photonics can reduceboth operational and embodied carbon footprints with its high energy efficiencyand at least 4$\\times$ less fabrication carbon cost per unit area than 28 nmCMOS.",
        "title": "Photonics for Sustainable Computing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05125",
        "abstract_url": "http://arxiv.org/abs/2401.05125",
        "authors": [
            {
                "last_name": "Garda",
                "first_name": "Samuele"
            },
            {
                "last_name": "Leser",
                "first_name": "Ulf"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Biomedical entity linking (BEL) is the task of grounding entity mentions to aknowledge base (KB). A popular approach to the task are name-based methods,i.e. those identifying the most appropriate name in the KB for a given mention,either via dense retrieval or autoregressive modeling. However, as thesemethods directly return KB names, they cannot cope with homonyms, i.e.different KB entities sharing the exact same name. This significantly affectstheir performance, especially for KBs where homonyms account for a large amountof entity mentions (e.g. UMLS and NCBI Gene). We therefore present BELHD(Biomedical Entity Linking with Homonym Disambiguation), a new name-basedmethod that copes with this challenge. Specifically, BELHD builds upon theBioSyn (Sung et al.,2020) model introducing two crucial extensions. First, itperforms a preprocessing of the KB in which it expands homonyms with anautomatically chosen disambiguating string, thus enforcing unique linkingdecisions. Second, we introduce candidate sharing, a novel strategy to selectcandidates for contrastive learning that enhances the overall training signal.Experiments with 10 corpora and five entity types show that BELHD improves uponstate-of-the-art approaches, achieving the best results in 6 out 10 corporawith an average improvement of 4.55pp recall@1. Furthermore, the KBpreprocessing is orthogonal to the core prediction model and thus can alsoimprove other methods, which we exemplify for GenBioEL (Yuan et al, 2022), agenerative name-based BEL approach. Code is available at: link added uponpublication.",
        "title": "BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05126",
        "abstract_url": "http://arxiv.org/abs/2401.05126",
        "authors": [
            {
                "last_name": "Nagamori",
                "first_name": "Teru"
            },
            {
                "last_name": "Shiota",
                "first_name": "Sayaka"
            },
            {
                "last_name": "Kiya",
                "first_name": "Hitoshi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We propose a novel method for privacy-preserving deep neural networks (DNNs)with the Vision Transformer (ViT). The method allows us not only to trainmodels and test with visually protected images but to also avoid theperformance degradation caused from the use of encrypted images, whereasconventional methods cannot avoid the influence of image encryption. A domainadaptation method is used to efficiently fine-tune ViT with encrypted images.In experiments, the method is demonstrated to outperform conventional methodsin an image classification task on the CIFAR-10 and ImageNet datasets in termsof classification accuracy.",
        "title": "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving  Vision Transformer",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05131",
        "abstract_url": "http://arxiv.org/abs/2401.05131",
        "authors": [
            {
                "last_name": "Pichon-Pharabod",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SC",
            ""
        ],
        "abstract": "  We provide an algorithm for computing an effective basis of homology ofelliptic surfaces over the complex projective line on which integration ofperiods can be carried out. This allows the heuristic recovery of severalalgebraic invariants of the surface, notably the N\\'eron-Severi lattice, thetranscendental lattice, the Mordell-Weil group and the Mordell-Weil lattice.This algorithm comes with a SageMath implementation.",
        "title": "A semi-numerical algorithm for the homology lattice and periods of  complex elliptic surfaces over the projective line",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05133",
        "abstract_url": "http://arxiv.org/abs/2401.05133",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Siqi"
            },
            {
                "last_name": "Marris",
                "first_name": "Luke"
            },
            {
                "last_name": "Lanctot",
                "first_name": "Marc"
            },
            {
                "last_name": "Piliouras",
                "first_name": "Georgios"
            },
            {
                "last_name": "Leibo",
                "first_name": "Joel Z."
            },
            {
                "last_name": "Heess",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA"
        ],
        "abstract": "  We study computationally efficient methods for finding equilibria in n-playergeneral-sum games, specifically ones that afford complex visuomotor skills. Weshow how existing methods would struggle in this setting, eithercomputationally or in theory. We then introduce NeuPL-JPSRO, a neuralpopulation learning algorithm that benefits from transfer learning of skillsand converges to a Coarse Correlated Equilibrium (CCE) of the game. We showempirical convergence in a suite of OpenSpiel games, validated rigorously byexact game solvers. We then deploy NeuPL-JPSRO to complex domains, where ourapproach enables adaptive coordination in a MuJoCo control domain and skilltransfer in capture-the-flag. Our work shows that equilibrium convergentpopulation learning can be implemented at scale and in generality, paving theway towards solving real-world games between heterogeneous players with mixedmotives.",
        "title": "Neural Population Learning beyond Symmetric Zero-sum Games",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05134",
        "abstract_url": "http://arxiv.org/abs/2401.05134",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Abhisek"
            },
            {
                "last_name": "Bera",
                "first_name": "Shreyangshu"
            },
            {
                "last_name": "Saha",
                "first_name": "Sriparna"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Pushpak"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Samrat"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Over the past few years, the use of the Internet for healthcare-related taskshas grown by leaps and bounds, posing a challenge in effectively managing andprocessing information to ensure its efficient utilization. During moments ofemotional turmoil and psychological challenges, we frequently turn to theinternet as our initial source of support, choosing this over discussing ourfeelings with others due to the associated social stigma. In this paper, wepropose a new task of multi-modal medical concern summary (MMCS) generation,which provides a short and precise summary of patients' major concerns broughtup during the consultation. Nonverbal cues, such as patients' gestures andfacial expressions, aid in accurately identifying patients' concerns. Doctorsalso consider patients' personal information, such as age and gender, in orderto describe the medical condition appropriately. Motivated by the potentialefficacy of patients' personal context and visual gestures, we propose atransformer-based multi-task, multi-modal intent-recognition, and medicalconcern summary generation (IR-MMCSG) system. Furthermore, we propose amultitasking framework for intent recognition and medical concern summarygeneration for doctor-patient consultations. We construct the first multi-modalmedical concern summary generation (MM-MediConSummation) corpus, which includespatient-doctor consultations annotated with medical concern summaries, intents,patient personal information, doctor's recommendations, and keywords. Ourexperiments and analysis demonstrate (a) the significant role of patients'expressions/gestures and their personal information in intent identificationand medical concern summary generation, and (b) the strong correlation betweenintent recognition and patients' medical concern summary generation  The dataset and source code are available at https://github.com/NLP-RL/MMCSG.",
        "title": "Yes, this is what I was looking for! Towards Multi-modal Medical  Consultation Concern Summary Generation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05136",
        "abstract_url": "http://arxiv.org/abs/2401.05136",
        "authors": [
            {
                "last_name": "Tufano",
                "first_name": "Rosalia"
            },
            {
                "last_name": "Dabi\u0107",
                "first_name": "Ozren"
            },
            {
                "last_name": "Mastropaolo",
                "first_name": "Antonio"
            },
            {
                "last_name": "Ciniselli",
                "first_name": "Matteo"
            },
            {
                "last_name": "Bavota",
                "first_name": "Gabriele"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The automation of code review has been tackled by several researchers withthe goal of reducing its cost. The adoption of deep learning in softwareengineering pushed the automation to new boundaries, with techniques imitatingdevelopers in generative tasks, such as commenting on a code change as areviewer would do or addressing a reviewer's comment by modifying code. Theperformance of these techniques is usually assessed through quantitativemetrics, e.g., the percentage of instances in the test set for which correctpredictions are generated, leaving many open questions on the techniques'capabilities. For example, knowing that an approach is able to correctlyaddress a reviewer's comment in 10% of cases is of little value without knowingwhat was asked by the reviewer: What if in all successful cases the code changerequired to address the comment was just the removal of an empty line? In thispaper we aim at characterizing the cases in which three code review automationtechniques tend to succeed or fail in the two above-described tasks. The studyhas a strong qualitative focus, with ~105 man-hours of manual inspectioninvested in manually analyzing correct and wrong predictions generated by thethree techniques, for a total of 2,291 inspected predictions. The output ofthis analysis are two taxonomies reporting, for each of the two tasks, thetypes of code changes on which the experimented techniques tend to succeed orto fail, pointing to areas for future work. A result of our manual analysis wasalso the identification of several issues in the datasets used to train andtest the experimented techniques. Finally, we assess the importance ofresearching in techniques specialized for code review automation by comparingtheir performance with ChatGPT, a general purpose large language model, findingthat ChatGPT struggles in commenting code as a human reviewer would do.",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05137",
        "abstract_url": "http://arxiv.org/abs/2401.05137",
        "authors": [
            {
                "last_name": "Daho",
                "first_name": "Mostafa El Habib"
            },
            {
                "last_name": "Li",
                "first_name": "Yihao"
            },
            {
                "last_name": "Zeghlache",
                "first_name": "Rachid"
            },
            {
                "last_name": "Boit\u00e9",
                "first_name": "Hugo Le"
            },
            {
                "last_name": "Deman",
                "first_name": "Pierre"
            },
            {
                "last_name": "Borderie",
                "first_name": "Laurent"
            },
            {
                "last_name": "Ren",
                "first_name": "Hugang"
            },
            {
                "last_name": "Mannivanan",
                "first_name": "Niranchana"
            },
            {
                "last_name": "Lepicard",
                "first_name": "Capucine"
            },
            {
                "last_name": "Cochener",
                "first_name": "B\u00e9atrice"
            },
            {
                "last_name": "Couturier",
                "first_name": "Aude"
            },
            {
                "last_name": "Tadayoni",
                "first_name": "Ramin"
            },
            {
                "last_name": "Conze",
                "first_name": "Pierre-Henri"
            },
            {
                "last_name": "Lamard",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Quellec",
                "first_name": "Gwenol\u00e9"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leadingcause of blindness worldwide. Traditionally, DR is monitored using Color FundusPhotography (CFP), a widespread 2-D imaging modality. However, DRclassifications based on CFP have poor predictive power, resulting insuboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is arecent 3-D imaging modality offering enhanced structural and functionalinformation (blood flow) with a wider field of view. This paper investigatesautomatic DR severity assessment using 3-D OCTA. A straightforward solution tothis task is a 3-D neural network classifier. However, 3-D architectures havenumerous parameters and typically require many training samples. A lightersolution consists in using 2-D neural network classifiers processing 2-Den-face (or frontal) projections and/or 2-D cross-sectional slices. Such anapproach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-faceflow maps are often used to detect avascular zones and neovascularization, and2) cross-sectional slices are commonly analyzed to detect macular edemas, forinstance. However, arbitrary data reduction or selection might result ininformation loss. Two complementary strategies are thus proposed to optimallysummarize OCTA volumes with 2-D images: 1) a parametric en-face projectionoptimized through deep learning and 2) a cross-sectional slice selectionprocess controlled through gradient-based attribution. The full summarizationand DR classification pipeline is trained from end to end. The automatic 2-Dsummary can be displayed in a viewer or printed in a report to support thedecision. We show that the proposed 2-D summarization and classificationpipeline outperforms direct 3-D classification with the advantage of improvedinterpretability.",
        "title": "DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography  Angiography for Automatic Diabetic Retinopathy Diagnosis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05143",
        "abstract_url": "http://arxiv.org/abs/2401.05143",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose and analyze a general framework called nonlinear preconditionedprimal-dual with projection for solving nonconvex-nonconcave and non-smoothsaddle-point problems. The framework consists of two steps. The first is anonlinear preconditioned map followed by a relaxed projection onto theseparating hyperspace we construct. One key to the method is the selection ofpreconditioned operators, which tailors to the structure of the saddle-pointproblem and is allowed to be nonlinear and asymmetric. The other is theconstruction of separating hyperspace, which guarantees fast convergence. Thisframework paves the way for constructing nonlinear preconditioned primal-dualalgorithms. We show that weak convergence, and so is sublinear convergenceunder the assumption of the convexity of saddle-point problems and linearconvergence under a metric subregularity. We also show that many existingprimal-daul methods, such as the generalized primal-dual algorithm method, arespecial cases of relaxed preconditioned primal-dual with projection.",
        "title": "Nonlinear preconditioned primal-dual method for a class of structured  minimax problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05144",
        "abstract_url": "http://arxiv.org/abs/2401.05144",
        "authors": [
            {
                "last_name": "McKechnie",
                "first_name": "Jack"
            },
            {
                "last_name": "McDonald",
                "first_name": "Graham"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large archival collections, such as email or government documents, must bemanually reviewed to identify any sensitive information before the collectioncan be released publicly. Sensitivity classification has received a lot ofattention in the literature. However, more recently, there has been increasinginterest in developing sensitivity-aware search engines that can provide userswith relevant search results, while ensuring that no sensitive documents arereturned to the user. Sensitivity-aware search would mitigate the need for amanual sensitivity review prior to collections being made available publicly.To develop such systems, there is a need for test collections that containrelevance assessments for a set of information needs as well as ground-truthlabels for a variety of sensitivity categories. The well-known Enron emailcollection contains a classification ground-truth that can be used to representsensitive information, e.g., the Purely Personal and Personal but inProfessional Context categories can be used to represent sensitive personalinformation. However, the existing Enron collection does not contain a set ofinformation needs and relevance assessments. In this work, we present acollection of fifty information needs (topics) with crowdsourced queryformulations (3 per topic) and relevance assessments (11,471 in total) for theEnron collection (mean number of relevant documents per topic = 11, variance =34.7). The developed information needs, queries and relevance judgements areavailable on GitHub and will be available along with the existing Enroncollection through the popular ir_datasets library. Our proposed collectionresults in the first freely available test collection for developingsensitivity-aware search systems.",
        "title": "SARA: A Collection of Sensitivity-Aware Relevance Assessments",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05145",
        "abstract_url": "http://arxiv.org/abs/2401.05145",
        "authors": [
            {
                "last_name": "Beinat",
                "first_name": "Matilda"
            },
            {
                "last_name": "Beinat",
                "first_name": "Julian"
            },
            {
                "last_name": "Shoaib",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Magenti",
                "first_name": "Jorge Gomez"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Projected to impact 1.6 million people in the UK by 2040 and costing{\\pounds}25 billion annually, dementia presents a growing challenge to society.This study, a pioneering effort to predict the translational potential ofdementia research using machine learning, hopes to address the slow translationof fundamental discoveries into practical applications despite dementia'ssignificant societal and economic impact. We used the Dimensions database toextract data from 43,091 UK dementia research publications between the years1990-2023, specifically metadata (authors, publication year etc.), conceptsmentioned in the paper, and the paper abstract. To prepare the data for machinelearning we applied methods such as one hot encoding and/or word embeddings. Wetrained a CatBoost Classifier to predict if a publication will be cited in afuture patent or clinical trial. We trained several model variations. The modelcombining metadata, concept, and abstract embeddings yielded the highestperformance: for patent predictions, an Area Under the Receiver OperatingCharacteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trialpredictions, an AUROC of 0.81 and 75.11% accuracy. The results demonstrate thatintegrating machine learning within current research methodologies can uncoveroverlooked publications, expediting the identification of promising researchand potentially transforming dementia research by predicting real-world impactand guiding translational strategies.",
        "title": "Machine Learning to Promote Translational Research: Predicting Patent  and Clinical Trial Inclusion in Dementia Research",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05146",
        "abstract_url": "http://arxiv.org/abs/2401.05146",
        "authors": [
            {
                "last_name": "Romandini",
                "first_name": "Nicol\u00f2"
            },
            {
                "last_name": "Mora",
                "first_name": "Alessio"
            },
            {
                "last_name": "Mazzocca",
                "first_name": "Carlo"
            },
            {
                "last_name": "Montanari",
                "first_name": "Rebecca"
            },
            {
                "last_name": "Bellavista",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Federated Learning (FL) enables collaborative training of a Machine Learning(ML) model across multiple parties, facilitating the preservation of users' andinstitutions' privacy by keeping data stored locally. Instead of centralizingraw data, FL exchanges locally refined model parameters to build a global modelincrementally. While FL is more compliant with emerging regulations such as theEuropean General Data Protection Regulation (GDPR), ensuring the right to beforgotten in this context - allowing FL participants to remove their datacontributions from the learned model - remains unclear. In addition, it isrecognized that malicious clients may inject backdoors into the global modelthrough updates, e.g. to generate mispredictions on specially crafted dataexamples. Consequently, there is the need for mechanisms that can guaranteeindividuals the possibility to remove their data and erase maliciouscontributions even after aggregation, without compromising the already acquired\"good\" knowledge. This highlights the necessity for novel Federated Unlearning(FU) algorithms, which can efficiently remove specific clients' contributionswithout full model retraining. This survey provides background concepts,empirical evidence, and practical guidelines to design/implement efficient FUschemes. Our study includes a detailed analysis of the metrics for evaluatingunlearning in FL and presents an in-depth literature review categorizingstate-of-the-art FU contributions under a novel taxonomy. Finally, we outlinethe most relevant and still open technical challenges, by identifying the mostpromising research directions in the field.",
        "title": "Federated Unlearning: A Survey on Methods, Design Guidelines, and  Evaluation Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05148",
        "abstract_url": "http://arxiv.org/abs/2401.05148",
        "authors": [
            {
                "last_name": "Gritz",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Hoppe",
                "first_name": "Anett"
            },
            {
                "last_name": "Ewerth",
                "first_name": "Ralph"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Nowadays, learning increasingly involves the usage of search engines and webresources. The related interdisciplinary research field search as learning aimsto understand how people learn on the web. Previous work has investigatedseveral feature classes to predict, for instance, the expected knowledge gainduring web search. Therein, eye-tracking features have not been extensivelystudied so far. In this paper, we extend a previously used reading model from aline-based one to one that can detect reading sequences across multiple lines.We use publicly available study data from a web-based learning task to examinethe relationship between our feature set and the participants' test scores. Ourfindings demonstrate that learners with higher knowledge gain spentsignificantly more time reading, and processing more words in total. We alsofind evidence that faster reading at the expense of more backward regressionsmay be an indicator of better web-based learning. We make our code publiclyavailable at https://github.com/TIBHannover/reading_web_search.",
        "title": "On the Influence of Reading Sequences on Knowledge Gain during Web  Search",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05152",
        "abstract_url": "http://arxiv.org/abs/2401.05152",
        "authors": [
            {
                "last_name": "Fernandez-Cortizas",
                "first_name": "Miguel"
            },
            {
                "last_name": "Bavle",
                "first_name": "Hriday"
            },
            {
                "last_name": "Perez-Saura",
                "first_name": "David"
            },
            {
                "last_name": "Sanchez-Lopez",
                "first_name": "Jose Luis"
            },
            {
                "last_name": "Campoy",
                "first_name": "Pascual"
            },
            {
                "last_name": "Voos",
                "first_name": "Holger"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Collaborative Simultaneous Localization and Mapping (CSLAM) is critical toenable multiple robots to operate in complex environments. Most CSLAMtechniques rely on raw sensor measurement or low-level features such askeyframe descriptors, which can lead to wrong loop closures due to the lack ofdeep understanding of the environment. Moreover, the exchange of thesemeasurements and low-level features among the robots requires the transmissionof a significant amount of data, which limits the scalability of the system. Toovercome these limitations, we present Multi S-Graphs, a decentralized CSLAMsystem that utilizes high-level semantic-relational information embedded in thefour-layered hierarchical and optimizable situational graphs for cooperativemap generation and localization while minimizing the information exchangedbetween the robots. To support this, we present a novel room-based descriptorwhich, along with its connected walls, is used to perform inter-robot loopclosures, addressing the challenges of multi-robot kidnapped probleminitialization. Multiple experiments in simulated and real environmentsvalidate the improvement in accuracy and robustness of the proposed approachwhile reducing the amount of data exchanged between robots compared to otherstate-of-the-art approaches.  Software available within a docker image:https://github.com/snt-arg/multi_s_graphs_docker",
        "title": "Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational  Collaborative SLAM",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05153",
        "abstract_url": "http://arxiv.org/abs/2401.05153",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Yinghui"
            },
            {
                "last_name": "Qu",
                "first_name": "Litao"
            },
            {
                "last_name": "Zhang",
                "first_name": "ShiZhou"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiuwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yanning"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Fusion of a panchromatic (PAN) image and corresponding multispectral (MS)image is also known as pansharpening, which aims to combine abundant spatialdetails of PAN and spectral information of MS. Due to the absence ofhigh-resolution MS images, available deep-learning-based methods usually followthe paradigm of training at reduced resolution and testing at both reduced andfull resolution. When taking original MS and PAN images as inputs, they alwaysobtain sub-optimal results due to the scale variation. In this paper, wepropose to explore the self-supervised representation of pansharpening bydesigning a cross-predictive diffusion model, named CrossDiff. It has two-stagetraining. In the first stage, we introduce a cross-predictive pretext task topre-train the UNet structure based on conditional DDPM, while in the secondstage, the encoders of the UNets are frozen to directly extract spatial andspectral features from PAN and MS, and only the fusion head is trained to adaptfor pansharpening task. Extensive experiments show the effectiveness andsuperiority of the proposed model compared with state-of-the-art supervised andunsupervised methods. Besides, the cross-sensor experiments also verify thegeneralization ability of proposed self-supervised representation learners forother satellite's datasets. We will release our code for reproducibility.",
        "title": "CrossDiff: Exploring Self-Supervised Representation of Pansharpening via  Cross-Predictive Diffusion Model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05154",
        "abstract_url": "http://arxiv.org/abs/2401.05154",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Weichuang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jieru"
            },
            {
                "last_name": "Shen",
                "first_name": "Guan"
            },
            {
                "last_name": "Chen",
                "first_name": "Quan"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Guo",
                "first_name": "Minyi"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "PL"
        ],
        "abstract": "  With the increasing demand for computing capability given limited resourceand power budgets, it is crucial to deploy applications to customizedaccelerators like FPGAs. However, FPGA programming is non-trivial. Althoughexisting high-level synthesis (HLS) tools improve productivity to a certainextent, they are limited in scope and capability to support sufficientFPGA-oriented optimizations. This paper focuses on FPGA-based accelerators andproposes POM, an optimizing framework built on multi-level intermediaterepresentation (MLIR). POM has several features which demonstrate its scope andcapability of performance optimization. First, most HLS tools dependexclusively on a single-level IR to perform all the optimizations, introducingexcessive information into the IR and making debugging an arduous task. Incontrast, POM introduces three layers of IR to perform operations at suitableabstraction levels, streamlining the implementation and debugging process andexhibiting better flexibility, extensibility, and systematicness. Second, POMintegrates the polyhedral model into MLIR, enabling advanced dependenceanalysis and various FPGA-oriented loop transformations. By representing nestedloops with integer sets and maps, loop transformations can be conductedconveniently through manipulations on polyhedral semantics. Finally, to furtherrelieve design effort, POM has a user-friendly programming interface (DSL) thatallows a concise description of computation and includes a rich collection ofscheduling primitives. An automatic design space exploration (DSE) engine isprovided to search for high-performance optimization schemes efficiently andgenerate optimized accelerators automatically. Experimental results show thatPOM achieves a $6.46\\times$ average speedup on typical benchmark suites and a$6.06\\times$ average speedup on real-world applications compared to thestate-of-the-art.",
        "title": "An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator  Generation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05157",
        "abstract_url": "http://arxiv.org/abs/2401.05157",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yitao"
            },
            {
                "last_name": "Li",
                "first_name": "Heng-Chao"
            },
            {
                "last_name": "Liu",
                "first_name": "Nanqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Rui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the conventional change detection (CD) pipeline, two manually registeredand labeled remote sensing datasets serve as the input of the model fortraining and prediction. However, in realistic scenarios, data from differentperiods or sensors could fail to be aligned as a result of various coordinatesystems. Geometric distortion caused by coordinate shifting remains a thornyissue for CD algorithms. In this paper, we propose a reusable self-supervisedframework for bitemporal geometric distortion in CD tasks. The whole frameworkis composed of Pretext Representation Pre-training, Bitemporal Image Alignment,and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, thekey components of the framework can be reused for assistance in the bitemporalimage alignment, while simultaneously enhancing the performance of the CDdecoder. Experimental results in 2 large-scale realistic scenarios demonstratethat our proposed method can alleviate the bitemporal geometric distortion inCD tasks.",
        "title": "Toward distortion-aware change detection in realistic scenarios",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05159",
        "abstract_url": "http://arxiv.org/abs/2401.05159",
        "authors": [
            {
                "last_name": "Farooq",
                "first_name": "Muhammad Ali"
            },
            {
                "last_name": "Yao",
                "first_name": "Wang"
            },
            {
                "last_name": "Schukat",
                "first_name": "Michael"
            },
            {
                "last_name": "Little",
                "first_name": "Mark A"
            },
            {
                "last_name": "Corcoran",
                "first_name": "Peter"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  This study explores the utilization of Dermatoscopic synthetic data generatedthrough stable diffusion models as a strategy for enhancing the robustness ofmachine learning model training. Synthetic data generation plays a pivotal rolein mitigating challenges associated with limited labeled datasets, therebyfacilitating more effective model training. In this context, we aim toincorporate enhanced data transformation techniques by extending the recentsuccess of few-shot learning and a small amount of data representation intext-to-image latent diffusion models. The optimally tuned model is furtherused for rendering high-quality skin lesion synthetic data with diverse andrealistic characteristics, providing a valuable supplement and diversity to theexisting training data. We investigate the impact of incorporating newlygenerated synthetic data into the training pipeline of state-of-art machinelearning models, assessing its effectiveness in enhancing model performance andgeneralization to unseen real-world data. Our experimental results demonstratethe efficacy of the synthetic data generated through stable diffusion modelshelps in improving the robustness and adaptability of end-to-end CNN and visiontransformer models on two different real-world skin lesion datasets.",
        "title": "Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion  Models for Enhanced Skin Disease Classification using ViT and CNN",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05163",
        "abstract_url": "http://arxiv.org/abs/2401.05163",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Yang",
                "first_name": "Dingkang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yue"
            },
            {
                "last_name": "Lei",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lihua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Medical visual question answering (VQA) is a challenging multimodal task,where Vision-Language Pre-training (VLP) models can effectively improve thegeneralization performance. However, most methods in the medical field treatVQA as an answer classification task which is difficult to transfer topractical application scenarios. Additionally, due to the privacy of medicalimages and the expensive annotation process, large-scale medical image-textpairs datasets for pretraining are severely lacking. In this paper, we proposea large-scale MultI-task Self-Supervised learning based framework (MISS) formedical VQA tasks. Unlike existing methods, we treat medical VQA as agenerative task. We unify the text encoder and multimodal encoder and alignimage-text features through multi-task learning. Furthermore, we propose aTransfer-and-Caption method that extends the feature space of single-modalimage datasets using large language models (LLMs), enabling those traditionalmedical vision field task data to be applied to VLP. Experiments show that ourmethod achieves excellent results with fewer multimodal datasets anddemonstrates the advantages of generative VQA models. The code and modelweights will be released upon the paper's acceptance.",
        "title": "MISS: A Generative Pretraining and Finetuning Approach for Med-VQA",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05164",
        "abstract_url": "http://arxiv.org/abs/2401.05164",
        "authors": [
            {
                "last_name": "Tarable",
                "first_name": "Alberto"
            },
            {
                "last_name": "Dossi",
                "first_name": "Laura"
            },
            {
                "last_name": "Virone",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Nordio",
                "first_name": "Alessandro"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Motivated by the challenges of future 6G communications where terahertz (THz)frequencies, intelligent reflective surfaces (IRSs) and ultra-wideband (UWB)signals coexist, we analyse and propose a set of efficient techniques forconfiguring the IRS when the signal bandwidth is a significant fraction of thecentral frequency (up to 50%). To the best of our knowledge this is the firsttime that IRS configuration techniques are analyzed for such huge bandwidths.In our work we take into account for the channel model, the power spectraldensity of the signal reflected by the IRS and the network geometry. Weevaluate the proposed solutions in terms of achievable rate and compare itagainst an upper bound we derived. Our results hint rules for designingIRS-aided communication systems and allow to draw conclusions on the trade-offbetween performance and complexity required for configuring the IRS.",
        "title": "IRS Configuration Techniques for Ultra Wideband Signals and THz  Communications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05165",
        "abstract_url": "http://arxiv.org/abs/2401.05165",
        "authors": [
            {
                "last_name": "Seidl",
                "first_name": "Helmut"
            },
            {
                "last_name": "Erhard",
                "first_name": "Julian"
            },
            {
                "last_name": "Tilscher",
                "first_name": "Sarah"
            },
            {
                "last_name": "Schwarz",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL",
            ""
        ],
        "abstract": "  The weakly relational domain of Octagons offers a decent compromise betweenprecision and efficiency for numerical properties. Here, we are concerned withthe construction of non-numerical relational domains. We provide a generalconstruction of weakly relational domains, which we exemplify with an extensionof constant propagation by disjunctions. Since for the resulting domain of2-disjunctive formulas, satisfiability is NP-complete, we provide a generalconstruction for a further, more abstract weakly relational domain where theabstract operations of restriction and least upper bound can be efficientlyimplemented. In the second step, we consider a relational domain that tracksconjunctions of inequalities between variables, and between variables andconstants for arbitrary partial orders of values. Examples are sub(multi)sets,as well as prefix, substring or scattered substring orderings on strings. Whenthe partial order is a lattice, we provide precise polynomial algorithms forsatisfiability, restriction, and the best abstraction of disjunction.Complementary to the constructions for lattices, we find that, in general,satisfiability of conjunctions is NP-complete. We therefore again providepolynomial abstract versions of restriction, conjunction, and join. By usingour generic constructions, these domains are extended to weakly relationaldomains that additionally track disjunctions. For all our domains, we indicatehow abstract transformers for assignments and guards can be constructed.",
        "title": "Non-Numerical Weakly Relational Domains",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05166",
        "abstract_url": "http://arxiv.org/abs/2401.05166",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Siyang"
            },
            {
                "last_name": "Spitale",
                "first_name": "Micol"
            },
            {
                "last_name": "Luo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Palmero",
                "first_name": "Cristina"
            },
            {
                "last_name": "Barquero",
                "first_name": "German"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hengde"
            },
            {
                "last_name": "Escalera",
                "first_name": "Sergio"
            },
            {
                "last_name": "Valstar",
                "first_name": "Michel"
            },
            {
                "last_name": "Baur",
                "first_name": "Tobias"
            },
            {
                "last_name": "Ringeval",
                "first_name": "Fabien"
            },
            {
                "last_name": "Andre",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Gunes",
                "first_name": "Hatice"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In dyadic interactions, humans communicate their intentions and state of mindusing verbal and non-verbal cues, where multiple different facial reactionsmight be appropriate in response to a specific speaker behaviour. Then, how todevelop a machine learning (ML) model that can automatically generate multipleappropriate, diverse, realistic and synchronised human facial reactions from anpreviously unseen speaker behaviour is a challenging task. Following thesuccessful organisation of the first REACT challenge (REACT 2023), this editionof the challenge (REACT 2024) employs a subset used by the previous challenge,which contains segmented 30-secs dyadic interaction clips originally recordedas part of the NOXI and RECOLA datasets, encouraging participants to developand benchmark Machine Learning (ML) models that can generate multipleappropriate facial reactions (including facial image sequences and theirattributes) given an input conversational partner's stimulus under variousdyadic video conference scenarios. This paper presents: (i) the guidelines ofthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)the performance of the baseline systems on the two proposed sub-challenges:Offline Multiple Appropriate Facial Reaction Generation and Online MultipleAppropriate Facial Reaction Generation, respectively. The challenge baselinecode is publicly available athttps://github.com/reactmultimodalchallenge/baseline_react2024.",
        "title": "REACT 2024: the Second Multiple Appropriate Facial Reaction Generation  Challenge",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05168",
        "abstract_url": "http://arxiv.org/abs/2401.05168",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Nanqing"
            },
            {
                "last_name": "Xu",
                "first_name": "Xun"
            },
            {
                "last_name": "Su",
                "first_name": "Yongyi"
            },
            {
                "last_name": "Liu",
                "first_name": "Chengxin"
            },
            {
                "last_name": "Gong",
                "first_name": "Peiliang"
            },
            {
                "last_name": "Li",
                "first_name": "Heng-Chao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Domain adaptation is crucial in aerial imagery, as the visual representationof these images can significantly vary based on factors such as geographiclocation, time, and weather conditions. Additionally, high-resolution aerialimages often require substantial storage space and may not be readilyaccessible to the public. To address these challenges, we propose a novelSource-Free Object Detection (SFOD) method. Specifically, our approach is builtupon a self-training framework; however, self-training can lead to inaccuratelearning in the absence of labeled training data. To address this issue, wefurther integrate Contrastive Language-Image Pre-training (CLIP) to guide thegeneration of pseudo-labels, termed CLIP-guided Aggregation. By leveragingCLIP's zero-shot classification capability, we use it to aggregate scores withthe original predicted bounding boxes, enabling us to obtain refined scores forthe pseudo-labels. To validate the effectiveness of our method, we constructedtwo new datasets from different domains based on the DIOR dataset, named DIOR-Cand DIOR-Cloudy. Experiments demonstrate that our method outperforms othercomparative algorithms.",
        "title": "CLIP-guided Source-free Object Detection in Aerial Images",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05170",
        "abstract_url": "http://arxiv.org/abs/2401.05170",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Junshuo"
            },
            {
                "last_name": "Huang",
                "first_name": "Yunlong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jianan"
            },
            {
                "last_name": "Xiong",
                "first_name": "Rujing"
            },
            {
                "last_name": "Qiu",
                "first_name": "Robert Caiming"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Device-free human activity recognition plays a pivotal role in wirelesssensing. However, current systems often fail to accommodate signal transmissionthrough walls or necessitate dedicated noise removal algorithms. To overcomethese limitations, we introduce TRTAR: a device-free passive human activityrecognition system integrated with a transmissive reconfigurable intelligentsurface (RIS). TRTAR eliminates the necessity for dedicated devices or noiseremoval algorithms, while specifically addressing signal propagation throughwalls. Unlike existing approaches, TRTAR solely employs a transmissive RIS atthe transmitter or receiver without modifying the inherent hardware structure.Experimental results demonstrate that TRTAR attains an average accuracy of98.13% when signals traverse concrete walls.",
        "title": "TRTAR: Transmissive RIS-assisted Through-the-wall Human Activity  Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05176",
        "abstract_url": "http://arxiv.org/abs/2401.05176",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Zhaokun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziyin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Inspired by the increasing interest in leveraging large language models fortranslation, this paper evaluates the capabilities of large language models(LLMs) represented by ChatGPT in comparison to the mainstream neural machinetranslation (NMT) engines in translating Chinese diplomatic texts into English.Specifically, we examine the translation quality of ChatGPT and NMT engines asmeasured by four automated metrics and human evaluation based on anerror-typology and six analytic rubrics. Our findings show that automatedmetrics yield similar results for ChatGPT under different prompts and NMTsystems, while human annotators tend to assign noticeably higher scores toChatGPT when it is provided an example or contextual information about thetranslation task. Pairwise correlation between automated metrics and dimensionsof human evaluation produces weak and non-significant results, suggesting thedivergence between the two methods of translation quality assessment. Thesefindings provide valuable insights into the potential of ChatGPT as a capablemachine translator, and the influence of prompt engineering on its performance.",
        "title": "Can ChatGPT Rival Neural Machine Translation? A Comparative Study",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05182",
        "abstract_url": "http://arxiv.org/abs/2401.05182",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Pingping"
            },
            {
                "last_name": "Wang",
                "first_name": "Jintao"
            },
            {
                "last_name": "Shao",
                "first_name": "Yulin"
            },
            {
                "last_name": "Ma",
                "first_name": "Shaodan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper presents a new integrated sensing and communication (ISAC)framework, leveraging the recent advancements of reconfigurable distributedantenna and reflecting surface (RDARS). RDARS is a programmable surfacestructure comprising numerous elements, each of which can be flexiblyconfigured to operate either in a reflection mode, resembling a passivereconfigurable intelligent surface (RIS), or in a connected mode, functioningas a remote transmit or receive antenna. Our RDARS-aided ISAC frameworkeffectively mitigates the adverse impact of multiplicative fading when comparedto the passive RIS-aided ISAC, and reduces cost and energy consumption whencompared to the active RIS-aided ISAC. Within our RDARS-aided ISAC framework,we consider a radar output signal-to-noise ratio (SNR) maximization problemunder communication constraints to jointly optimize the active transmitbeamforming matrix of the base station (BS), the reflection and mode selectionmatrices of RDARS, and the receive filter. To tackle the inherent non-convexityand the binary integer optimization introduced by the mode selection in thisoptimization challenge, we propose an efficient iterative algorithm with provedconvergence based on majorization minimization (MM) and penalty-basedmethods.Numerical and simulation results demonstrate the superior performanceof our new framework, and clearly verify substantial distribution, reflectionas well as selection gains obtained by properly configuring the RDARS.",
        "title": "Integrated Sensing and Communication with Reconfigurable Distributed  Antenna and Reflecting Surface: Joint Beamforming and Mode Selection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05190",
        "abstract_url": "http://arxiv.org/abs/2401.05190",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Zijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhaopeng"
            },
            {
                "last_name": "Feng",
                "first_name": "Yang"
            },
            {
                "last_name": "Wang",
                "first_name": "Gaoang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Joey Tianyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Zuozhu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have shown impressive performance in variousreasoning benchmarks with the emergence of Chain-of-Thought (CoT) and itsderivative methods, particularly in tasks involving multi-choice questions(MCQs). However, current works all process data uniformly without consideringthe problem-solving difficulty, which means an excessive focus on simplequestions while insufficient to intricate ones. To address this challenge, weinspired by humans using heuristic strategies to categorize tasks and handlethem individually, propose to apply the Divide and Conquer to LLMs reasoning.First, we divide questions into different subsets based on the statisticalconfidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquerdemanding nuanced process ones with elaborately designed methods, includingPrior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR),as well as their integration variants. Our experiments demonstrate that thisproposed strategy significantly boosts the models' reasoning abilities acrossnine datasets involving arithmetic, commonsense, and logic tasks. For instance,compared to baseline, we make a striking improvement on low confidence subsetsof 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense. Inaddition, through extensive analysis on length of rationale and number ofoptions, we verify that longer reasoning paths in PKR could prevent models fromreferring infer-harmful shortcuts, and also find that removing irrelevantchoices in FCR would substantially avoid models' confusion. The code is at\\url{https://github.com/AiMijie/Divide-and-Conquer}",
        "title": "Divide and Conquer for Large Language Models Reasoning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05191",
        "abstract_url": "http://arxiv.org/abs/2401.05191",
        "authors": [
            {
                "last_name": "Lai",
                "first_name": "Riwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Rui"
            },
            {
                "last_name": "Han",
                "first_name": "Qilong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chi"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Negative sampling is essential for implicit collaborative filtering toprovide proper negative training signals so as to achieve desirableperformance. We experimentally unveil a common limitation of all existingnegative sampling methods that they can only select negative samples of a fixedhardness level, leading to the false positive problem (FPP) and false negativeproblem (FNP). We then propose a new paradigm called adaptive hardness negativesampling (AHNS) and discuss its three key criteria. By adaptively selectingnegative samples with appropriate hardnesses during the training process, AHNScan well mitigate the impacts of FPP and FNP. Next, we present a concreteinstantiation of AHNS called AHNS_{p<0}, and theoretically demonstrate thatAHNS_{p<0} can fit the three criteria of AHNS well and achieve a larger lowerbound of normalized discounted cumulative gain. Besides, we note that existingnegative sampling methods can be regarded as more relaxed cases of AHNS.Finally, we conduct comprehensive experiments, and the results show thatAHNS_{p<0} can consistently and substantially outperform severalstate-of-the-art competitors on multiple datasets.",
        "title": "Adaptive Hardness Negative Sampling for Collaborative Filtering",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05193",
        "abstract_url": "http://arxiv.org/abs/2401.05193",
        "authors": [
            {
                "last_name": "Pacchiano",
                "first_name": "Aldo"
            },
            {
                "last_name": "Lee",
                "first_name": "Jonathan N."
            },
            {
                "last_name": "Brunskill",
                "first_name": "Emma"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We study the problem of experiment planning with function approximation incontextual bandit problems. In settings where there is a significant overheadto deploying adaptive algorithms -- for example, when the execution of the datacollection policies is required to be distributed, or a human in the loop isneeded to implement these policies -- producing in advance a set of policiesfor data collection is paramount. We study the setting where a large dataset ofcontexts but not rewards is available and may be used by the learner to designan effective data collection strategy. Although when rewards are linear thisproblem has been well studied, results are still missing for more complexreward models. In this work we propose two experiment planning strategiescompatible with function approximation. The first is an eluder planning andsampling procedure that can recover optimality guarantees depending on theeluder dimension of the reward function class. For the second, we show that auniform sampler achieves competitive optimality rates in the setting where thenumber of actions is small. We finalize our results introducing a statisticalgap fleshing out the fundamental differences between planning and adaptivelearning and provide results for planning with model selection.",
        "title": "Experiment Planning with Function Approximation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05194",
        "abstract_url": "http://arxiv.org/abs/2401.05194",
        "authors": [
            {
                "last_name": "Caponio",
                "first_name": "Carmine"
            },
            {
                "last_name": "Stano",
                "first_name": "Pietro"
            },
            {
                "last_name": "Carli",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Olivieri",
                "first_name": "Ignazio"
            },
            {
                "last_name": "Ragone",
                "first_name": "Daniele"
            },
            {
                "last_name": "Sorniotti",
                "first_name": "Aldo"
            },
            {
                "last_name": "Montanaro",
                "first_name": "Umberto"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            ""
        ],
        "abstract": "  Mobile robotic systems are becoming increasingly popular. These systems areused in various indoor applications, raging from warehousing and manufacturingto test benches for assessment of advanced control strategies, such asartificial intelligence (AI)-based control solutions, just to name a few.Scaled robotic cars are commonly equipped with a hierarchical controlacthiecture that includes tasks dedicated to vehicle state estimation andcontrol. This paper covers both aspects by proposing (i) a federeted extendedKalman filter (FEKF), and (ii) a novel deep reinforcement learning (DRL) pathtracking controller trained via an expert demonstrator to expedite the learningphase and increase robustess to the simulation-to-reality gap. The paper alsopresents the formulation of a vehicle model along with an effective yet simpleprocedure for identifying tis paramters. The experimentally validated model isused for (i) supporting the design of the FEKF and (ii) serving as a digitaltwin for training the proposed DRL-based path tracking algorithm. Experimentalresults confirm the ability of the FEKF to improve the estimate of the mobilerobot's position. Furthermore, the effectiveness of the DRL path trackingstrateguy is experimentally tested along manoeuvres not considered duringtraining, showing also the ability of the AI-based solution to outpeformmodel-based control strategies and the demonstrator. The comparison withbenchmraking controllers is quantitavely evalueted through a set of keyperformance indicators.",
        "title": "Modelling, Positioning, and Deep Reinforcement Learning Path Tracking  Control of Scaled Robotic Vehicles: Design and Experimental Validation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05196",
        "abstract_url": "http://arxiv.org/abs/2401.05196",
        "authors": [
            {
                "last_name": "Raus",
                "first_name": "Maren"
            },
            {
                "last_name": "Elshiaty",
                "first_name": "Yara"
            },
            {
                "last_name": "Petra",
                "first_name": "Stefania"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  We investigate the problem of minimizing Kullback-Leibler divergence betweena linear model $Ax$ and a positive vector $b$ in different convex domains(positive orthant, $n$-dimensional box, probability simplex). Our focus is onthe SMART method that employs efficient multiplicative updates. We explore theexponentiated gradient method, which can be viewed as a Bregman proximalgradient method and as a Riemannian gradient descent on the parameter manifoldof a corresponding distribution of the exponential family. This dualinterpretation enables us to establish connections and achieve acceleratedSMART iterates while smoothly incorporating constraints. The performance of theproposed acceleration schemes is demonstrated by large-scale numericalexamples.",
        "title": "Accelerated Bregmann divergence optimization with SMART: an information  geometry point of view",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05199",
        "abstract_url": "http://arxiv.org/abs/2401.05199",
        "authors": [
            {
                "last_name": "Taneja",
                "first_name": "Karan"
            },
            {
                "last_name": "Segal",
                "first_name": "Richard"
            },
            {
                "last_name": "Goodwin",
                "first_name": "Richard"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Automatic food recipe generation methods provide a creative tool for chefs toexplore and to create new, and interesting culinary delights. Given the recentsuccess of large language models (LLMs), they have the potential to create newrecipes that can meet individual preferences, dietary constraints, and adapt towhat is in your refrigerator. Existing research on using LLMs to generaterecipes has shown that LLMs can be finetuned to generate realistic-soundingrecipes. However, on close examination, these generated recipes often fail tomeet basic requirements like including chicken as an ingredient in chickendishes. In this paper, we propose RecipeMC, a text generation method usingGPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us todefine reward functions to put soft constraints on text generation and thusimprove the credibility of the generated recipes. Our results show that humanevaluators prefer recipes generated with RecipeMC more often than recipesgenerated with other baseline methods when compared with real recipes.",
        "title": "Monte Carlo Tree Search for Recipe Generation using GPT-2",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05200",
        "abstract_url": "http://arxiv.org/abs/2401.05200",
        "authors": [
            {
                "last_name": "Freire",
                "first_name": "Samuel Kernan"
            },
            {
                "last_name": "Wang",
                "first_name": "Chaofan"
            },
            {
                "last_name": "Foosherian",
                "first_name": "Mina"
            },
            {
                "last_name": "Wellsandt",
                "first_name": "Stefan"
            },
            {
                "last_name": "Ruiz-Arenas",
                "first_name": "Santiago"
            },
            {
                "last_name": "Niforatos",
                "first_name": "Evangelos"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "IR"
        ],
        "abstract": "  Managing knowledge efficiently is crucial for organizational success. Inmanufacturing, operating factories has become increasing knowledge-intensiveputting strain on the factory's capacity to train and support new operators. Inthis paper, we introduce a Large Language Model (LLM)-based system designed touse the extensive knowledge contained in factory documentation. The system aimsto efficiently answer queries from operators and facilitate the sharing of newknowledge. To assess its effectiveness, we conducted an evaluation in a factorysetting. The results of this evaluation demonstrated the system's benefits;namely, in enabling quicker information retrieval and more efficient resolutionof issues. However, the study also highlighted a preference for learning from ahuman expert when such an option is available. Furthermore, we benchmarkedseveral closed and open-sourced LLMs for this system. GPT-4 consistentlyoutperformed its counterparts, with open-source models like StableBeluga2trailing closely, presenting an attractive option given its data privacy andcustomization benefits. Overall, this work offers preliminary insights forfactories considering using LLM-tools for knowledge management.",
        "title": "Knowledge Sharing in Manufacturing using Large Language Models: User  Evaluation and Model Benchmarking",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05202",
        "abstract_url": "http://arxiv.org/abs/2401.05202",
        "authors": [
            {
                "last_name": "Russello",
                "first_name": "Helena"
            },
            {
                "last_name": "van der Tol",
                "first_name": "Rik"
            },
            {
                "last_name": "Holzhauer",
                "first_name": "Menno"
            },
            {
                "last_name": "van Henten",
                "first_name": "Eldert J."
            },
            {
                "last_name": "Kootstra",
                "first_name": "Gert"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This study presents an automated lameness detection system that usesdeep-learning image processing techniques to extract multiple locomotion traitsassociated with lameness. Using the T-LEAP pose estimation model, the motion ofnine keypoints was extracted from videos of walking cows. The videos wererecorded outdoors, with varying illumination conditions, and T-LEAP extracted99.6% of correct keypoints. The trajectories of the keypoints were then used tocompute six locomotion traits: back posture measurement, head bobbing, trackingdistance, stride length, stance duration, and swing duration. The three mostimportant traits were back posture measurement, head bobbing, and trackingdistance. For the ground truth, we showed that a thoughtful merging of thescores of the observers could improve intra-observer reliability and agreement.We showed that including multiple locomotion traits improves the classificationaccuracy from 76.6% with only one trait to 79.9% with the three most importanttraits and to 80.1% with all six locomotion traits.",
        "title": "Video-based Automatic Lameness Detection of Dairy Cows using Pose  Estimation and Multiple Locomotion Traits",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05204",
        "abstract_url": "http://arxiv.org/abs/2401.05204",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yong"
            },
            {
                "last_name": "Luo",
                "first_name": "Senlin"
            },
            {
                "last_name": "Shang",
                "first_name": "Yu-Ming"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The verbalizer, which serves to map label words to class labels, is anessential component of prompt-tuning. In this paper, we present a novelapproach to constructing verbalizers. While existing methods for verbalizerconstruction mainly rely on augmenting and refining sets of synonyms or relatedwords based on class names, this paradigm suffers from a narrow perspective andlack of abstraction, resulting in limited coverage and high bias in thelabel-word space. To address this issue, we propose a label-word constructionprocess that incorporates scenario-specific concepts. Specifically, we extractrich concepts from task-specific scenarios as label-word candidates and thendevelop a novel cascade calibration module to refine the candidates into a setof label words for each class. We evaluate the effectiveness of our proposedapproach through extensive experiments on {five} widely used datasets forzero-shot text classification. The results demonstrate that our methodoutperforms existing methods and achieves state-of-the-art results.",
        "title": "A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts  into a Verbalizer",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05206",
        "abstract_url": "http://arxiv.org/abs/2401.05206",
        "authors": [
            {
                "last_name": "Nordhagen",
                "first_name": "Even Marius"
            },
            {
                "last_name": "Sveinsson",
                "first_name": "Henrik Andersen"
            },
            {
                "last_name": "Malthe-S\u00f8renssen",
                "first_name": "Anders"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This Letter introduces an approach for precisely designing surface frictionproperties using a conditional generative machine learning model, specificallya diffusion denoising probabilistic model (DDPM). We created a dataset ofsynthetic surfaces with frictional properties determined by molecular dynamicssimulations, which trained the DDPM to predict surface structures from desiredfrictional outcomes. Unlike traditional trial-and-error and numericaloptimization methods, our approach directly yields surface designs meetingspecified frictional criteria with high accuracy and efficiency. Thisadvancement in material surface engineering demonstrates the potential ofmachine learning in reducing the iterative nature of surface design processes.Our findings not only provide a new pathway for precise surface propertytailoring but also suggest broader applications in material science wheresurface characteristics are critical.",
        "title": "Tailoring Frictional Properties of Surfaces Using Diffusion Models",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05211",
        "abstract_url": "http://arxiv.org/abs/2401.05211",
        "authors": [
            {
                "last_name": "Stiasny",
                "first_name": "Jochen"
            },
            {
                "last_name": "Chatzivasileiadis",
                "first_name": "Spyros"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  The ability to accurately approximate trajectories of dynamical systemsenables their analysis, prediction, and control. Neural network (NN)-basedapproximations have attracted significant interest due to fast evaluation withgood accuracy over long integration time steps. In contrast to establishednumerical approximation schemes such as Runge-Kutta methods, the estimation ofthe error of the NN-based approximations proves to be difficult. In this work,we propose to use the NN's predictions in a high-order implicit Runge-Kutta(IRK) method. The residuals in the implicit system of equations can be relatedto the NN's prediction error, hence, we can provide an error estimate atseveral points along a trajectory. We find that this error estimate highlycorrelates with the NN's prediction error and that increasing the order of theIRK method improves this estimate. We demonstrate this estimation methodologyfor Physics-Informed Neural Network (PINNs) on the logistic equation as anillustrative example and then apply it to a four-state electric generator modelthat is regularly used in power system modelling.",
        "title": "Error estimation for physics-informed neural networks with implicit  Runge-Kutta methods",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05215",
        "abstract_url": "http://arxiv.org/abs/2401.05215",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Wei"
            },
            {
                "last_name": "Gong",
                "first_name": "Dihong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Financial sentiment analysis refers to classifying financial text contentsinto sentiment categories (e.g. positive, negative, and neutral). In thispaper, we focus on the classification of financial news title, which is achallenging task due to a lack of large amount of training samples. To overcomethis difficulty, we propose to adapt the pretrained large language models(LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from hugeamount of text corpora,have an advantage in text understanding and can beeffectively adapted to domain-specific task while requiring very few amount oftraining samples. In particular, we adapt the open-source Llama2-7B model(2023) with the supervised fine-tuning (SFT) technique [4]. Experimentalevaluation shows that even with the 7B model (which is relatively small forLLMs), our approach significantly outperforms the previous state-of-the-artalgorithms.",
        "title": "Pre-trained Large Language Models for Financial Sentiment Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05217",
        "abstract_url": "http://arxiv.org/abs/2401.05217",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Chenxi"
            },
            {
                "last_name": "Liu",
                "first_name": "Yujia"
            },
            {
                "last_name": "Li",
                "first_name": "Dingquan"
            },
            {
                "last_name": "jiang",
                "first_name": "Tingting"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  No-Reference Image Quality Assessment (NR-IQA) aims to predict image qualityscores consistent with human perception without relying on pristine referenceimages, serving as a crucial component in various visual tasks. Ensuring therobustness of NR-IQA methods is vital for reliable comparisons of differentimage processing techniques and consistent user experiences in recommendations.The attack methods for NR-IQA provide a powerful instrument to test therobustness of NR-IQA. However, current attack methods of NR-IQA heavily rely onthe gradient of the NR-IQA model, leading to limitations when the gradientinformation is unavailable. In this paper, we present a pioneering query-basedblack box attack against NR-IQA methods. We propose the concept of \\emph{scoreboundary} and leverage an adaptive iterative approach with multiple scoreboundaries. Meanwhile, the initial attack directions are also designed toleverage the characteristics of the Human Visual System (HVS). Experiments showour attack method outperforms all compared state-of-the-art methods and is farahead of previous black-box methods. The effective DBCNN model suffers aSpearman rank-order correlation coefficient (SROCC) decline of $0.6972$attacked by our method, revealing the vulnerability of NR-IQA to black-boxattacks. The proposed attack method also provides a potent tool for furtherexploration into NR-IQA robustness.",
        "title": "Exploring Vulnerabilities of No-Reference Image Quality Assessment  Models: A Query-Based Black-Box Method",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05218",
        "abstract_url": "http://arxiv.org/abs/2401.05218",
        "authors": [
            {
                "last_name": "Mey",
                "first_name": "Alexander"
            },
            {
                "last_name": "Castro",
                "first_name": "Rui Manuel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We consider the task of identifying the causal parents of a target variableamong a set of candidate variables from observational data. Our main assumptionis that the candidate variables are observed in different environments whichmay, for example, correspond to different settings of a machine or differenttime intervals in a dynamical process. Under certain assumptions differentenvironments can be regarded as interventions on the observed system. We assumea linear relationship between target and covariates, which can be different ineach environment with the only restriction that the causal structure isinvariant across environments. This is an extension of the ICP($\\textbf{I}$nvariant $\\textbf{C}$ausal $\\textbf{P}$rediction) principle byPeters et al. [2016], who assumed a fixed linear relationship across allenvironments. Within our proposed setting we provide sufficient conditions foridentifiability of the causal parents and introduce a practical method calledLoLICaP ($\\textbf{Lo}$cally $\\textbf{L}$inear $\\textbf{I}$nvariant$\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based on a hypothesis testfor parent identification using a ratio of minimum and maximum statistics. Wethen show in a simplified setting that the statistical power of LoLICaPconverges exponentially fast in the sample size, and finally we analyze thebehavior of LoLICaP experimentally in more general settings.",
        "title": "Invariant Causal Prediction with Locally Linear Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05219",
        "abstract_url": "http://arxiv.org/abs/2401.05219",
        "authors": [
            {
                "last_name": "Karayanni",
                "first_name": "Nader"
            },
            {
                "last_name": "Shahla",
                "first_name": "Robert J."
            },
            {
                "last_name": "Hsiao",
                "first_name": "Chieh-Lien"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            ""
        ],
        "abstract": "  The digital era has seen a marked increase in financial fraud. edge MLemerged as a promising solution for smartphone payment services frauddetection, enabling the deployment of ML models directly on edge devices. Thisapproach enables a more personalized real-time fraud detection. However, asignificant gap in current research is the lack of a robust system formonitoring data distribution shifts in these distributed edge ML applications.Our work bridges this gap by introducing a novel open-source framework designedfor continuous monitoring of data distribution shifts on a network of edgedevices. Our system includes an innovative calculation of theKolmogorov-Smirnov (KS) test over a distributed network of edge devices,enabling efficient and accurate monitoring of users behavior shifts. Wecomprehensively evaluate the proposed framework employing both real-world andsynthetic financial transaction datasets and demonstrate the framework'seffectiveness.",
        "title": "Distributed Monitoring for Data Distribution Shifts in Edge-ML Fraud  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05221",
        "abstract_url": "http://arxiv.org/abs/2401.05221",
        "authors": [
            {
                "last_name": "Lips",
                "first_name": "Johannes"
            },
            {
                "last_name": "DeYoung",
                "first_name": "Stefan"
            },
            {
                "last_name": "Sch\u00f6nsteiner",
                "first_name": "Max"
            },
            {
                "last_name": "Lens",
                "first_name": "Hendrik"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The creation of low-order dynamic models for complex industrial systems iscomplicated by disturbances and limited sensor accuracy. This work presents asystem identification procedure that uses machine learning methods and processknowledge to robustly identify a low-order closed-loop model of a municipalsolid waste (MSW) grate incineration plant. These types of plants are known fortheir strong disturbances coming from fuel composition fluctuations. UsingBayesian optimization, the algorithm ranks and selects inputs from theavailable sensor data and chooses the model structure. This results in accuratemodels with low complexity while avoiding overfitting. The method is appliedand validated using data of an industrial MSW incineration plant. The obtainedmodels give excellent predictions and confidence intervals for the steamcapacity and intermediate quantities such as supply air flow and flue gastemperature. The identified continuous-time models are fully given, and theirstep-response dynamics are discussed. The models can be used to developmodel-based unit control schemes for grate incineration plants. The presentedmethod shows great potential for the identification of over-actuated systems ordisturbed systems with many sensors.",
        "title": "Closed-loop Identification of a MSW Grate Incinerator using Bayesian  Optimization for Selecting Model Inputs and Structure",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05224",
        "abstract_url": "http://arxiv.org/abs/2401.05224",
        "authors": [
            {
                "last_name": "Maniparambil",
                "first_name": "Mayug"
            },
            {
                "last_name": "Akshulakov",
                "first_name": "Raiymbek"
            },
            {
                "last_name": "Djilali",
                "first_name": "Yasser Abdelaziz Dahou"
            },
            {
                "last_name": "Narayan",
                "first_name": "Sanath"
            },
            {
                "last_name": "Seddik",
                "first_name": "Mohamed El Amine"
            },
            {
                "last_name": "Mangalam",
                "first_name": "Karttikeya"
            },
            {
                "last_name": "O'Connor",
                "first_name": "Noel E."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  Aligned text-image encoders such as CLIP have become the de facto model forvision-language tasks. Furthermore, modality-specific encoders achieveimpressive performances in their respective domains. This raises a centralquestion: does an alignment exist between uni-modal vision and languageencoders since they fundamentally represent the same physical world? Analyzingthe latent spaces structure of vision and language models on image-captionbenchmarks using the Centered Kernel Alignment (CKA), we find that therepresentation spaces of unaligned and aligned encoders are semanticallysimilar. In the absence of statistical similarity in aligned encoders likeCLIP, we show that a possible matching of unaligned encoders exists without anytraining. We frame this as a seeded graph-matching problem exploiting thesemantic similarity between graphs and propose two methods - a Fast QuadraticAssignment Problem optimization, and a novel localized CKA metric-basedmatching/retrieval. We demonstrate the effectiveness of this on severaldownstream tasks including cross-lingual, cross-domain caption matching andimage classification.",
        "title": "Do Vision and Language Encoders Represent the World Similarly?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05225",
        "abstract_url": "http://arxiv.org/abs/2401.05225",
        "authors": [
            {
                "last_name": "Mart\u00edn-P\u00e9rez",
                "first_name": "Jorge"
            },
            {
                "last_name": "Lentisco",
                "first_name": "Carlos M."
            },
            {
                "last_name": "Bellido",
                "first_name": "Luis"
            },
            {
                "last_name": "Soto",
                "first_name": "Ignacio"
            },
            {
                "last_name": "Fern\u00e1ndez",
                "first_name": "David"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Tele-operated Driving (ToD) is a challenging use case for mobile networkoperators. Video captured by the built-in vehicle cameras must be streamedmeeting a latency requirement of 5 ms with a 99.999% reliability. Although 5Goffers high bandwidth, ultra-low latencies and high reliability; ToD servicerequirements are violated due to bad channel conditions. Ignoring the channelstate may lead to over-estimate the number of ToD vehicles that can meet theservice requirements, hence comprising the vehicle security. To fill this gap,in this letter we propose TOVAC, an algorithm that guarantees ToD servicerequirements by taking adequate admission control and routing decisions. Thisis achieved by using a channel-based capacity graph that determines the maximumnumber of vehicles that can be tele-operated in any road section. We evaluateTOVAC considering cellular deployments from Turin and show that, unlike a stateof the art solution, TOVAC guarantees the ToD service requirements.",
        "title": "TOVAC: Tele-operated Vehicle Admission Control and Routing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05226",
        "abstract_url": "http://arxiv.org/abs/2401.05226",
        "authors": [
            {
                "last_name": "Barletta",
                "first_name": "Giulio"
            },
            {
                "last_name": "Trezza",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Chiavazzo",
                "first_name": "Eliodoro"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We assume that a sufficiently large database is available, where a physicalproperty of interest and a number of associated ruling primitive variables orobservables are stored. We introduce and test two machine learning approachesto discover possible groups or combinations of primitive variables: The firstapproach is based on regression models whereas the second on classificationmodels. The variable group (here referred to as the new effective goodvariable) can be considered as successfully found, when the physical propertyof interest is characterized by the following effective invariant behaviour: Inthe first method, invariance of the group implies invariance of the property upto a given accuracy; in the other method, upon partition of the physicalproperty values into two or more classes, invariance of the group impliesinvariance of the class. For the sake of illustration, the two methods aresuccessfully applied to two popular empirical correlations describing theconvective heat transfer phenomenon and to the Newton's law of universalgravitation.",
        "title": "Learning effective good variables from physical data",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05232",
        "abstract_url": "http://arxiv.org/abs/2401.05232",
        "authors": [
            {
                "last_name": "Jakab",
                "first_name": "Daniel"
            },
            {
                "last_name": "Grua",
                "first_name": "Eoin Martino"
            },
            {
                "last_name": "Deegan",
                "first_name": "Brian Micheal"
            },
            {
                "last_name": "Scanlan",
                "first_name": "Anthony"
            },
            {
                "last_name": "Van De Ven",
                "first_name": "Pepijn"
            },
            {
                "last_name": "Eising",
                "first_name": "Ciar\u00e1n"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The Modulation Transfer Function (MTF) is an important image quality metrictypically used in the automotive domain. However, despite the fact that opticalquality has an impact on the performance of computer vision in vehicleautomation, for many public datasets, this metric is unknown. Additionally,wide field-of-view (FOV) cameras have become increasingly popular, particularlyfor low-speed vehicle automation applications. To investigate image quality indatasets, this paper proposes an adaptation of the Natural Scenes SpatialFrequency Response (NS-SFR) algorithm to suit cameras with a widefield-of-view.",
        "title": "Measuring Natural Scenes SFR of Automotive Fisheye Cameras",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05233",
        "abstract_url": "http://arxiv.org/abs/2401.05233",
        "authors": [
            {
                "last_name": "Duan",
                "first_name": "Yaqi"
            },
            {
                "last_name": "Wainwright",
                "first_name": "Martin J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT",
            "",
            "",
            ""
        ],
        "abstract": "  We introduce a novel framework for analyzing reinforcement learning (RL) incontinuous state-action spaces, and use it to prove fast rates of convergencein both off-line and on-line settings. Our analysis highlights two keystability properties, relating to how changes in value functions and/orpolicies affect the Bellman operator and occupation measures. We argue thatthese properties are satisfied in many continuous state-action Markov decisionprocesses, and demonstrate how they arise naturally when using linear functionapproximation methods. Our analysis offers fresh perspectives on the roles ofpessimism and optimism in off-line and on-line RL, and highlights theconnection between off-line RL and transfer learning.",
        "title": "Taming \"data-hungry\" reinforcement learning? Stability in continuous  state-action spaces",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05235",
        "abstract_url": "http://arxiv.org/abs/2401.05235",
        "authors": [
            {
                "last_name": "Camur",
                "first_name": "Mustafa Can"
            },
            {
                "last_name": "Vogiatzis",
                "first_name": "Chrysafis"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            ""
        ],
        "abstract": "  Centrality metrics have become a popular concept in network science andoptimization. Over the years, centrality has been used to assign importance andidentify influential elements in various settings, including transportation,infrastructure, biological, and social networks, among others. That said, mostof the literature has focused on nodal versions of centrality. Recently, groupcounterparts of centrality have started attracting scientific and practitionerinterest. The identification of sets of nodes that are influential within anetwork is becoming increasingly more important. This is even more pronouncedwhen these sets of nodes are required to induce a certain motif or structure.In this study, we review group centrality metrics from an operations researchand optimization perspective for the first time. This is particularlyinteresting due to the rapid evolution and development of this area in theoperations research community over the last decade. We first present ahistorical overview of how we have reached this point in the study of groupcentrality. We then discuss the different structures and motifs that appearprominently in the literature, alongside the techniques and methodologies thatare popular. We finally present possible avenues and directions for futurework, mainly in three areas: (i) probabilistic metrics to account forrandomness along with stochastic optimization techniques; (ii) structures andrelaxations that have not been yet studied; and (iii) new emerging applicationsthat can take advantage of group centrality. Our survey offers a concise reviewof group centrality and its intersection with network analysis andoptimization.",
        "title": "A Survey on Optimization Studies of Group Centrality Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05236",
        "abstract_url": "http://arxiv.org/abs/2401.05236",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Ma",
                "first_name": "Wei-Chiu"
            },
            {
                "last_name": "Guan",
                "first_name": "Kaiyu"
            },
            {
                "last_name": "Torralba",
                "first_name": "Antonio"
            },
            {
                "last_name": "Wang",
                "first_name": "Shenlong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Our world is full of identical objects (\\emphe.g., cans of coke, cars of samemodel). These duplicates, when seen together, provide additional and strongcues for us to effectively reason about 3D. Inspired by this observation, weintroduce Structure from Duplicates (SfD), a novel inverse graphics frameworkthat reconstructs geometry, material, and illumination from a single imagecontaining multiple identical objects. SfD begins by identifying multipleinstances of an object within an image, and then jointly estimates the 6DoFpose for all instances.An inverse graphics pipeline is subsequently employed tojointly reason about the shape, material of the object, and the environmentlight, while adhering to the shared geometry and material constraint acrossinstances. Our primary contributions involve utilizing object duplicates as arobust prior for single-image inverse graphics and proposing an in-planerotation-robust Structure from Motion (SfM) formulation for joint 6-DoF objectpose estimation. By leveraging multi-view cues from a single image, SfDgenerates more realistic and detailed 3D reconstructions, significantlyoutperforming existing single image reconstruction models and multi-viewreconstruction approaches with a similar or greater number of observations.",
        "title": "Structure from Duplicates: Neural Inverse Graphics from a Pile of  Objects",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05240",
        "abstract_url": "http://arxiv.org/abs/2401.05240",
        "authors": [
            {
                "last_name": "Luzio",
                "first_name": "Emanuele"
            },
            {
                "last_name": "Ponti",
                "first_name": "Moacir Antonelli"
            },
            {
                "last_name": "Arevalo",
                "first_name": "Christian Ramirez"
            },
            {
                "last_name": "Argerich",
                "first_name": "Luis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Machine learning models typically focus on specific targets like creatingclassifiers, often based on known population feature distributions in abusiness context. However, models calculating individual features adapt overtime to improve precision, introducing the concept of decoupling: shifting frompoint evaluation to data distribution. We use calibration strategies asstrategy for decoupling machine learning (ML) classifiers from score-basedactions within business logic frameworks. To evaluate these strategies, weperform a comparative analysis using a real-world business scenario andmultiple ML models. Our findings highlight the trade-offs and performanceimplications of the approach, offering valuable insights for practitionersseeking to optimize their decoupling efforts. In particular, the Isotonic andBeta calibration methods stand out for scenarios in which there is shiftbetween training and testing data.",
        "title": "Decoupling Decision-Making in Fraud Prevention through Classifier  Calibration for Business Logic Action",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05244",
        "abstract_url": "http://arxiv.org/abs/2401.05244",
        "authors": [
            {
                "last_name": "Thaler",
                "first_name": "Denny"
            },
            {
                "last_name": "Dhulipala",
                "first_name": "Somayajulu L. N."
            },
            {
                "last_name": "Bamer",
                "first_name": "Franz"
            },
            {
                "last_name": "Markert",
                "first_name": "Bernd"
            },
            {
                "last_name": "Shields",
                "first_name": "Michael D."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We present a new Subset Simulation approach using Hamiltonian neuralnetwork-based Monte Carlo sampling for reliability analysis. The proposedstrategy combines the superior sampling of the Hamiltonian Monte Carlo methodwith computationally efficient gradient evaluations using Hamiltonian neuralnetworks. This combination is especially advantageous because the neuralnetwork architecture conserves the Hamiltonian, which defines the acceptancecriteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieveshigh acceptance rates at low computational cost. Our approach estimates smallfailure probabilities using Subset Simulations. However, in low-probabilitysample regions, the gradient evaluation is particularly challenging. Theremarkable accuracy of the proposed strategy is demonstrated on differentreliability problems, and its efficiency is compared to the traditionalHamiltonian Monte Carlo method. We note that this approach can reach itslimitations for gradient estimations in low-probability regions of complex andhigh-dimensional distributions. Thus, we propose techniques to improve gradientprediction in these particular situations and enable accurate estimations ofthe probability of failure. The highlight of this study is the reliabilityanalysis of a system whose parameter distributions must be inferred withBayesian inference problems. In such a case, the Hamiltonian Monte Carlo methodrequires a full model evaluation for each gradient evaluation and, therefore,comes at a very high cost. However, using Hamiltonian neural networks in thisframework replaces the expensive model evaluation, resulting in tremendousimprovements in computational efficiency.",
        "title": "Reliability Analysis of Complex Systems using Subset Simulations with  Hamiltonian Neural Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05247",
        "abstract_url": "http://arxiv.org/abs/2401.05247",
        "authors": [
            {
                "last_name": "Fern\u00e1ndez-C\u00f3rdoba",
                "first_name": "Cristina"
            },
            {
                "last_name": "Torres",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Vela",
                "first_name": "Carlos"
            },
            {
                "last_name": "Villanueva",
                "first_name": "Merc\u00e8"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The Zps-additive codes of length n are subgroups of Zps^n , and can be seenas a generalization of linear codes over Z2, Z4, or more general over Z2s . Inthis paper, we show two methods for computing a parity-check matrix of aZps-additive code from a generator matrix of the code in standard form. We alsocompare the performance of our results implemented in Magma with the currentavailable function in Magma for codes over finite rings in general. A timecomplexity analysis is also shown.",
        "title": "Computing efficiently a parity-check matrix for Zps-additive codes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05249",
        "abstract_url": "http://arxiv.org/abs/2401.05249",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Feng",
                "first_name": "Yansong"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The argument sufficiency assessment task aims to determine if the premises ofa given argument support its conclusion. To tackle this task, existing worksoften train a classifier on data annotated by humans. However, annotating datais laborious, and annotations are often inconsistent due to subjectivecriteria. Motivated by the probability of sufficiency (PS) definition in thecausal literature, we propose CASA, a zero-shot causality-driven argumentsufficiency assessment framework. PS measures how likely introducing thepremise event would lead to the conclusion, when both the premise andconclusion events are absent. To estimate this probability, we propose to uselarge language models (LLMs) to generate contexts that are inconsistent withthe premise and conclusion, and revise them by injecting the premise event.Experiments on two logical fallacy detection datasets demonstrate that CASAaccurately identifies insufficient arguments. We further deploy CASA in awriting assistance application, and find that suggestions generated by CASAenhance the sufficiency of student-written arguments. Code and data areavailable at https://github.com/xxxiaol/CASA.",
        "title": "CASA: Causality-driven Argument Sufficiency Assessment",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05251",
        "abstract_url": "http://arxiv.org/abs/2401.05251",
        "authors": [
            {
                "last_name": "Rudolf",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fl\u00f6gel",
                "first_name": "Daniel"
            },
            {
                "last_name": "Sch\u00fcrmann",
                "first_name": "Tobias"
            },
            {
                "last_name": "S\u00fc\u00df",
                "first_name": "Simon"
            },
            {
                "last_name": "Schwab",
                "first_name": "Stefan"
            },
            {
                "last_name": "Hohmann",
                "first_name": "S\u00f6ren"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Robust and performant controllers are essential for industrial applications.However, deriving controller parameters for complex and nonlinear systems ischallenging and time-consuming. To facilitate automatic controllerparametrization, this work presents a novel approach using deep reinforcementlearning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on thecontrol of parameter-variant systems, a class of systems with complex behaviorwhich depends on the operating conditions. For this system class,gain-scheduling control structures are widely used in applications acrossindustries due to well-known design principles. Facilitating the expensivecontroller parametrization task regarding these control structures, we deployan DRL agent. Based on control system observations, the agent autonomouslydecides how to adapt the controller parameters. We make the adaptation processmore efficient by introducing BSGs to map the controller parameters which maydepend on numerous operating conditions. To preprocess time-series data andextract a fixed-length feature vector, we use a long short-term memory (LSTM)neural networks. Furthermore, this work contributes actor regularizations thatare relevant to real-world environments which differ from training.Accordingly, we apply dropout layer normalization to the actor and criticnetworks of the truncated quantile critic (TQC) algorithm. To show ourapproach's working principle and effectiveness, we train and evaluate the DRLagent on the parametrization task of an industrial control structure withparameter lookup tables.",
        "title": "ReACT: Reinforcement Learning for Controller Parametrization using  B-Spline Geometries",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05252",
        "abstract_url": "http://arxiv.org/abs/2401.05252",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junsong"
            },
            {
                "last_name": "Wu",
                "first_name": "Yue"
            },
            {
                "last_name": "Luo",
                "first_name": "Simian"
            },
            {
                "last_name": "Xie",
                "first_name": "Enze"
            },
            {
                "last_name": "Paul",
                "first_name": "Sayak"
            },
            {
                "last_name": "Luo",
                "first_name": "Ping"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhenguo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This technical report introduces PIXART-{\\delta}, a text-to-image synthesisframework that integrates the Latent Consistency Model (LCM) and ControlNetinto the advanced PIXART-{\\alpha} model. PIXART-{\\alpha} is recognized for itsability to generate high-quality images of 1024px resolution through aremarkably efficient training process. The integration of LCM inPIXART-{\\delta} significantly accelerates the inference speed, enabling theproduction of high-quality images in just 2-4 steps. Notably, PIXART-{\\delta}achieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,marking a 7x improvement over the PIXART-{\\alpha}. Additionally,PIXART-{\\delta} is designed to be efficiently trainable on 32GB V100 GPUswithin a single day. With its 8-bit inference capability (von Platen et al.,2023), PIXART-{\\delta} can synthesize 1024px images within 8GB GPU memoryconstraints, greatly enhancing its usability and accessibility. Furthermore,incorporating a ControlNet-like module enables fine-grained control overtext-to-image diffusion models. We introduce a novel ControlNet-Transformerarchitecture, specifically tailored for Transformers, achieving explicitcontrollability alongside high-quality image generation. As a state-of-the-art,open-source image generation model, PIXART-{\\delta} offers a promisingalternative to the Stable Diffusion family of models, contributingsignificantly to text-to-image synthesis.",
        "title": "PIXART-{\\delta}: Fast and Controllable Image Generation with Latent  Consistency Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05268",
        "abstract_url": "http://arxiv.org/abs/2401.05268",
        "authors": [
            {
                "last_name": "Qiao",
                "first_name": "Shuofei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ningyu"
            },
            {
                "last_name": "Fang",
                "first_name": "Runnan"
            },
            {
                "last_name": "Luo",
                "first_name": "Yujie"
            },
            {
                "last_name": "Zhou",
                "first_name": "Wangchunshu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yuchen Eleanor"
            },
            {
                "last_name": "Lv",
                "first_name": "Chengfei"
            },
            {
                "last_name": "Chen",
                "first_name": "Huajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC",
            "LG",
            "MA"
        ],
        "abstract": "  Language agents have achieved considerable performance on various complextasks. Despite the incessant exploration in this field, existing language agentsystems still struggle with costly, non-reproducible data reliance and face thechallenge of compelling a single model for multiple functions. To this end, weintroduce AutoAct, an automatic agent learning framework that does not rely onlarge-scale annotated data and synthetic trajectories from closed-source models(e.g., GPT-4). Given limited data with a tool library, AutoAct firstautomatically synthesizes planning trajectories without any assistance fromhumans or strong closed-source models. Then, AutoAct leverages adivision-of-labor strategy to automatically differentiate based on the targettask information and synthesized trajectories, producing a sub-agent group tocomplete the task. We conduct comprehensive experiments with different LLMs,which demonstrates that AutoAct yields better or parallel performance comparedto various strong baselines. We even notice that AutoAct, when using theLlama-2-13b model, can achieve performance comparable to that of theGPT-3.5-Turbo agent. Code will be available athttps://github.com/zjunlp/AutoAct.",
        "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05272",
        "abstract_url": "http://arxiv.org/abs/2401.05272",
        "authors": [
            {
                "last_name": "Pueyo",
                "first_name": "Pablo"
            },
            {
                "last_name": "Dendarieta",
                "first_name": "Juan"
            },
            {
                "last_name": "Montijano",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Murillo",
                "first_name": "Ana C."
            },
            {
                "last_name": "Schwager",
                "first_name": "Mac"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We present CineMPC, a complete cinematographic system that autonomouslycontrols a drone to film multiple targets recording user-specified aestheticobjectives. Existing solutions in autonomous cinematography control only thecamera extrinsics, namely its position, and orientation. In contrast, CineMPCis the first solution that includes the camera intrinsic parameters in thecontrol loop, which are essential tools for controlling cinematographic effectslike focus, depth-of-field, and zoom. The system estimates the relative posesbetween the targets and the camera from an RGB-D image and optimizes atrajectory for the extrinsic and intrinsic camera parameters to film theartistic and technical requirements specified by the user. The drone and thecamera are controlled in a nonlinear Model Predicted Control (MPC) loop byre-optimizing the trajectory at each time step in response to currentconditions in the scene. The perception system of CineMPC can track thetargets' position and orientation despite the camera effects. Experiments in aphotorealistic simulation and with a real platform demonstrate the capabilitiesof the system to achieve a full array of cinematographic effects that are notpossible without the control of the intrinsics of the camera. Code for CineMPCis implemented following a modular architecture in ROS and released to thecommunity.",
        "title": "CineMPC: A Fully Autonomous Drone Cinematography System Incorporating  Zoom, Focus, Pose, and Scene Composition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05273",
        "abstract_url": "http://arxiv.org/abs/2401.05273",
        "authors": [
            {
                "last_name": "Pereira",
                "first_name": "Jayr"
            },
            {
                "last_name": "Assumpcao",
                "first_name": "Andre"
            },
            {
                "last_name": "Trecenti",
                "first_name": "Julio"
            },
            {
                "last_name": "Airosa",
                "first_name": "Luiz"
            },
            {
                "last_name": "Lente",
                "first_name": "Caio"
            },
            {
                "last_name": "Cl\u00e9to",
                "first_name": "Jhonatan"
            },
            {
                "last_name": "Dobins",
                "first_name": "Guilherme"
            },
            {
                "last_name": "Nogueira",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Luis"
            },
            {
                "last_name": "Lotufo",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^enciaArtificial), a groundbreaking system designed to integrate Large LanguageModels (LLMs) into the operational framework of Brazilian Federal Court ofAccounts (TCU). The system automates various stages of case analysis, includingbasic information extraction, admissibility examination, Periculum in mora andFumus boni iuris analyses, and recommendations generation. Through a series ofexperiments, we demonstrate INACIA's potential in extracting relevantinformation from case documents, evaluating its legal plausibility, andgenerating judicial recommendations. Utilizing a validation dataset alongsideLLMs, our evaluation methodology presents an innovative approach to assessingsystem performance, correlating highly with human judgment. The resultshighlight INACIA's proficiency in handling complex legal tasks, indicating itssuitability for augmenting efficiency and judicial fairness within legalsystems. The paper also discusses potential enhancements and futureapplications, positioning INACIA as a model for worldwide AI integration inlegal domains.",
        "title": "INACIA: Integrating Large Language Models in Brazilian Audit Courts:  Opportunities and Challenges",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05286",
        "abstract_url": "http://arxiv.org/abs/2401.05286",
        "authors": [
            {
                "last_name": "Cavicchioni",
                "first_name": "Giulia"
            },
            {
                "last_name": "Guerrini",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Meneghetti",
                "first_name": "Alessio"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Locally recoverable codes deal with the task of reconstructing a lost symbolby relying on a portion of the remaining coordinates smaller than aninformation set. We consider the case of codes over finite chain rings,generalizing known results and bounds for codes over fields. In particular, wepropose a new family of locally recoverable codes by extending a constructionproposed in 2014 by Tamo and Barg, and we discuss its optimality.",
        "title": "A class of locally recoverable codes over finite chain rings",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05290",
        "abstract_url": "http://arxiv.org/abs/2401.05290",
        "authors": [
            {
                "last_name": "Hauser",
                "first_name": "Kris"
            },
            {
                "last_name": "Watson",
                "first_name": "Eleanor"
            },
            {
                "last_name": "Bae",
                "first_name": "Joonbum"
            },
            {
                "last_name": "Bankston",
                "first_name": "Josh"
            },
            {
                "last_name": "Behnke",
                "first_name": "Sven"
            },
            {
                "last_name": "Borgia",
                "first_name": "Bill"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            },
            {
                "last_name": "Dafarra",
                "first_name": "Stefano"
            },
            {
                "last_name": "van Erp",
                "first_name": "Jan B. F."
            },
            {
                "last_name": "Ferris",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fishel",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Hoffman",
                "first_name": "Guy"
            },
            {
                "last_name": "Ivaldi",
                "first_name": "Serena"
            },
            {
                "last_name": "Kanehiro",
                "first_name": "Fumio"
            },
            {
                "last_name": "Kheddar",
                "first_name": "Abderrahmane"
            },
            {
                "last_name": "Lannuzel",
                "first_name": "Gaelle"
            },
            {
                "last_name": "Morie",
                "first_name": "Jacqueline Ford"
            },
            {
                "last_name": "Naughton",
                "first_name": "Patrick"
            },
            {
                "last_name": "NGuyen",
                "first_name": "Steve"
            },
            {
                "last_name": "Oh",
                "first_name": "Paul"
            },
            {
                "last_name": "Padir",
                "first_name": "Taskin"
            },
            {
                "last_name": "Pippine",
                "first_name": "Jim"
            },
            {
                "last_name": "Park",
                "first_name": "Jaeheung"
            },
            {
                "last_name": "Pucci",
                "first_name": "Daniele"
            },
            {
                "last_name": "Vaz",
                "first_name": "Jean"
            },
            {
                "last_name": "Whitney",
                "first_name": "Peter"
            },
            {
                "last_name": "Wu",
                "first_name": "Peggy"
            },
            {
                "last_name": "Locke",
                "first_name": "David"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  The ANA Avatar XPRIZE was a four-year competition to develop a robotic\"avatar\" system to allow a human operator to sense, communicate, and act in aremote environment as though physically present. The competition featured aunique requirement that judges would operate the avatars after less than onehour of training on the human-machine interfaces, and avatar systems werejudged on both objective and subjective scoring metrics. This paper presents aunified summary and analysis of the competition from technical, judging, andorganizational perspectives. We study the use of telerobotics technologies andinnovations pursued by the competing teams in their avatar systems, andcorrelate the use of these technologies with judges' task performance andsubjective survey ratings. It also summarizes perspectives from team leads,judges, and organizers about the competition's execution and impact to informthe future development of telerobotics and telepresence.",
        "title": "Analysis and Perspectives on the ANA Avatar XPRIZE Competition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05293",
        "abstract_url": "http://arxiv.org/abs/2401.05293",
        "authors": [
            {
                "last_name": "Alldieck",
                "first_name": "Thiemo"
            },
            {
                "last_name": "Kolotouros",
                "first_name": "Nikos"
            },
            {
                "last_name": "Sminchisescu",
                "first_name": "Cristian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Score Distillation Sampling (SDS) is a recent but already widely popularmethod that relies on an image diffusion model to control optimization problemsusing text prompts. In this paper, we conduct an in-depth analysis of the SDSloss function, identify an inherent problem with its formulation, and propose asurprisingly easy but effective fix. Specifically, we decompose the loss intodifferent factors and isolate the component responsible for noisy gradients. Inthe original formulation, high text guidance is used to account for the noise,leading to unwanted side effects. Instead, we train a shallow network mimickingthe timestep-dependent denoising deficiency of the image diffusion model inorder to effectively factor it out. We demonstrate the versatility and theeffectiveness of our novel loss formulation through several qualitative andquantitative experiments, including optimization-based image synthesis andediting, zero-shot image translation network training, and text-to-3Dsynthesis.",
        "title": "Score Distillation Sampling with Learned Manifold Corrective",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05294",
        "abstract_url": "http://arxiv.org/abs/2401.05294",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Mathai",
                "first_name": "Tejas Sudharshan"
            },
            {
                "last_name": "Liu",
                "first_name": "Jianfei"
            },
            {
                "last_name": "Parnell",
                "first_name": "Christopher"
            },
            {
                "last_name": "Summers",
                "first_name": "Ronald M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Purpose: Body composition measurements from routine abdominal CT can yieldpersonalized risk assessments for asymptomatic and diseased patients. Inparticular, attenuation and volume measures of muscle and fat are associatedwith important clinical outcomes, such as cardiovascular events, fractures, anddeath. This study evaluates the reliability of an Internal tool for thesegmentation of muscle and fat (subcutaneous and visceral) as compared to thewell-established public TotalSegmentator tool.  Methods: We assessed the tools across 900 CT series from the publiclyavailable SAROS dataset, focusing on muscle, subcutaneous fat, and visceralfat. The Dice score was employed to assess accuracy in subcutaneous fat andmuscle segmentation. Due to the lack of ground truth segmentations for visceralfat, Cohen's Kappa was utilized to assess segmentation agreement between thetools.  Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) forsubcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentationrespectively. A Wilcoxon signed-rank test revealed that our results werestatistically different with p<0.01. For visceral fat, the Cohen's kappa scoreof 0.856 indicated near-perfect agreement between the two tools. Our internaltool also showed very strong correlations for muscle volume (R^2=0.99), muscleattenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderatecorrelation for subcutaneous fat attenuation (R^2=0.45).  Conclusion: Our findings indicated that our Internal tool outperformedTotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen'sKappa score for visceral fat suggests a reliable level of agreement between thetwo tools. These results demonstrate the potential of our tool in advancing theaccuracy of body composition analysis.",
        "title": "Enhanced Muscle and Fat Segmentation for CT-Based Body Composition  Analysis: A Comparative Study",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05295",
        "abstract_url": "http://arxiv.org/abs/2401.05295",
        "authors": [
            {
                "last_name": "Regad\u00edo",
                "first_name": "Alberto"
            },
            {
                "last_name": "Esteban",
                "first_name": "Luis"
            },
            {
                "last_name": "S\u00e1nchez-Prieto",
                "first_name": "Sebasti\u00e1n"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  To address the possible lack or total absence of pulses from particledetectors during the development of its associate electronics, we propose amodel that can generate them without losing the features of the real ones. Thismodel is based on artificial neural networks, namely Generative AdversarialNetworks (GAN). We describe the proposed network architecture, its trainingmethodology and the approach to train the GAN with real pulses from ascintillator receiving radiation from sources of ${}^{137}$Cs and ${}^{22}$Na.The Generator was installed in a Xilinx's System-On-Chip (SoC). We show how thenetwork is capable of generating pulses with the same shape as the real onesthat even match the data distributions in the original pulse-height histogramdata.",
        "title": "Synthesis of pulses from particle detectors with a Generative  Adversarial Network (GAN)",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05300",
        "abstract_url": "http://arxiv.org/abs/2401.05300",
        "authors": [
            {
                "last_name": "Thrush",
                "first_name": "Tristan"
            },
            {
                "last_name": "Moore",
                "first_name": "Jared"
            },
            {
                "last_name": "Monares",
                "first_name": "Miguel"
            },
            {
                "last_name": "Potts",
                "first_name": "Christopher"
            },
            {
                "last_name": "Kiela",
                "first_name": "Douwe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Statements involving metalinguistic self-reference (\"This paper has sixsections.\") are prevalent in many domains. Can large language models (LLMs)handle such language? In this paper, we present \"I am a Strange Dataset\", a newdataset for addressing this question. There are two subtasks: generation andverification. In generation, models continue statements like \"The penultimateword in this sentence is\" (where a correct continuation is \"is\"). Inverification, models judge the truth of statements like \"The penultimate wordin this sentence is sentence.\" (false). We also provide minimally differentmetalinguistic non-self-reference examples to complement the main dataset byprobing for whether models can handle metalinguistic language at all. Thedataset is hand-crafted by experts and validated by non-expert annotators. Wetest a variety of open-source LLMs (7B to 70B parameters) as well asclosed-source LLMs through APIs. All models perform close to chance across bothsubtasks and even on the non-self-referential metalinguistic control data,though we find some steady improvement with model scale. GPT 4 is the onlymodel to consistently do significantly better than chance, and it is still onlyin the 60% range, while our untrained human annotators score well in the 89-93%range. The dataset and evaluation toolkit are available athttps://github.com/TristanThrush/i-am-a-strange-dataset.",
        "title": "I am a Strange Dataset: Metalinguistic Tests for Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05302",
        "abstract_url": "http://arxiv.org/abs/2401.05302",
        "authors": [
            {
                "last_name": "Verma",
                "first_name": "Mudit"
            },
            {
                "last_name": "Bhambri",
                "first_name": "Siddhant"
            },
            {
                "last_name": "Kambhampati",
                "first_name": "Subbarao"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "HC"
        ],
        "abstract": "  Large Language Models have shown exceptional generative abilities in variousnatural language and generation tasks. However, possible anthropomorphizationand leniency towards failure cases have propelled discussions on emergentabilities of Large Language Models especially on Theory of Mind (ToM) abilitiesin Large Language Models. While several false-belief tests exists to verify theability to infer and maintain mental models of another entity, we study aspecial application of ToM abilities that has higher stakes and possiblyirreversible consequences : Human Robot Interaction. In this work, we explorethe task of Perceived Behavior Recognition, where a robot employs a LargeLanguage Model (LLM) to assess the robot's generated behavior in a mannersimilar to human observer. We focus on four behavior types, namely -explicable, legible, predictable, and obfuscatory behavior which have beenextensively used to synthesize interpretable robot behaviors. The LLMs goal is,therefore to be a human proxy to the agent, and to answer how a certain agentbehavior would be perceived by the human in the loop, for example \"Given arobot's behavior X, would the human observer find it explicable?\". We conduct ahuman subject study to verify that the users are able to correctly answer sucha question in the curated situations (robot setting and plan) across fivedomains. A first analysis of the belief test yields extremely positive resultsinflating ones expectations of LLMs possessing ToM abilities. We then proposeand perform a suite of perturbation tests which breaks this illusion, i.e.Inconsistent Belief, Uninformative Context and Conviction Test. We concludethat, the high score of LLMs on vanilla prompts showcases its potential use inHRI settings, however to possess ToM demands invariance to trivial orirrelevant perturbations in the context which LLMs lack.",
        "title": "Theory of Mind abilities of Large Language Models in Human-Robot  Interaction : An Illusion?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05304",
        "abstract_url": "http://arxiv.org/abs/2401.05304",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Jessica"
            },
            {
                "last_name": "Flanigan",
                "first_name": "Bailey"
            },
            {
                "last_name": "Haghtalab",
                "first_name": "Nika"
            },
            {
                "last_name": "Jagadeesan",
                "first_name": "Meena"
            },
            {
                "last_name": "Podimata",
                "first_name": "Chara"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  A common explanation for negative user impacts of content recommender systemsis misalignment between the platform's objective and user welfare. In thiswork, we show that misalignment in the platform's objective is not the onlypotential cause of unintended impacts on users: even when the platform'sobjective is fully aligned with user welfare, the platform's learning algorithmcan induce negative downstream impacts on users. The source of these userimpacts is that different pieces of content may generate observable userreactions (feedback information) at different rates; these feedback rates maycorrelate with content properties, such as controversiality or demographicsimilarity of the creator, that affect the user experience. Since differencesin feedback rates can impact how often the learning algorithm engages withdifferent content, the learning algorithm may inadvertently promote contentwith certain such properties. Using the multi-armed bandit framework withprobabilistic feedback, we examine the relationship between feedback rates anda learning algorithm's engagement with individual arms for different no-regretalgorithms. We prove that no-regret algorithms can exhibit a wide range ofdependencies: if the feedback rate of an arm increases, some no-regretalgorithms engage with the arm more, some no-regret algorithms engage with thearm less, and other no-regret algorithms engage with the arm approximately thesame number of times. From a platform design perspective, our results highlightthe importance of looking beyond regret when measuring an algorithm'sperformance, and assessing the nature of a learning algorithm's engagement withdifferent types of content as well as their resulting downstream impacts.",
        "title": "Can Probabilistic Feedback Drive User Impacts in Online Platforms?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05308",
        "abstract_url": "http://arxiv.org/abs/2401.05308",
        "authors": [
            {
                "last_name": "Farajzadeh",
                "first_name": "Amin"
            },
            {
                "last_name": "Yadav",
                "first_name": "Animesh"
            },
            {
                "last_name": "Yanikomeroglu",
                "first_name": "Halim"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CV",
            "LG"
        ],
        "abstract": "  The deployment of federated learning (FL) within vertical heterogeneousnetworks, such as those enabled by high-altitude platform station (HAPS),offers the opportunity to engage a wide array of clients, each endowed withdistinct communication and computational capabilities. This diversity not onlyenhances the training accuracy of FL models but also hastens their convergence.Yet, applying FL in these expansive networks presents notable challenges,particularly the significant non-IIDness in client data distributions. Suchdata heterogeneity often results in slower convergence rates and reducedeffectiveness in model training performance. Our study introduces a clientselection strategy tailored to address this issue, leveraging user networktraffic behaviour. This strategy involves the prediction and classification ofclients based on their network usage patterns while prioritizing user privacy.By strategically selecting clients whose data exhibit similar patterns forparticipation in FL training, our approach fosters a more uniform andrepresentative data distribution across the network. Our simulationsdemonstrate that this targeted client selection methodology significantlyreduces the training loss of FL models in HAPS networks, thereby effectivelytackling a crucial challenge in implementing large-scale FL systems.",
        "title": "Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL  Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05314",
        "abstract_url": "http://arxiv.org/abs/2401.05314",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Kevin"
            },
            {
                "last_name": "Liu",
                "first_name": "Chonghua"
            },
            {
                "last_name": "Chan",
                "first_name": "David M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "CV",
            "SD"
        ],
        "abstract": "  The Internet's wealth of content, with up to 60% published in English,starkly contrasts the global population, where only 18.8% are English speakers,and just 5.1% consider it their native language, leading to disparities inonline information access. Unfortunately, automated processes for dubbing ofvideo - replacing the audio track of a video with a translated alternative -remains a complex and challenging task due to pipelines, necessitating precisetiming, facial movement synchronization, and prosody matching. While end-to-enddubbing offers a solution, data scarcity continues to impede the progress ofboth end-to-end and pipeline-based methods. In this work, we introduceAnim-400K, a comprehensive dataset of over 425K aligned animated video segmentsin Japanese and English supporting various video-related tasks, includingautomated dubbing, simultaneous translation, guided video summarization, andgenre/theme/style classification. Our dataset is made publicly available forresearch purposes at https://github.com/davidmchan/Anim400K.",
        "title": "ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of  Video",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05318",
        "abstract_url": "http://arxiv.org/abs/2401.05318",
        "authors": [
            {
                "last_name": "Piazza",
                "first_name": "Cristina"
            },
            {
                "last_name": "Della Santina",
                "first_name": "Cosimo"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            },
            {
                "last_name": "Grioli",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Bicchi",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robot feet are crucial for maintaining dynamic stability and propelling thebody during walking, especially on uneven terrains. Traditionally, robot feetwere mostly designed as flat and stiff pieces of metal, which meets itslimitations when the robot is required to step on irregular grounds, e.g.stones. While one could think that adding compliance under such feet wouldsolve the problem, this is not the case. To address this problem, we introducedthe SoftFoot, an adaptive foot design that can enhance walking performance overirregular grounds. The proposed design is completely passive and varies itsshape and stiffness based on the exerted forces, through a system of pulley,tendons, and springs opportunely placed in the structure. This paper outlinesthe motivation behind the SoftFoot and describes the theoretical model whichled to its final design. The proposed system has been experimentally tested andcompared with two analogous conventional feet, a rigid one and a compliant one,with similar footprints and soles. The experimental validation focuses on theanalysis of the standing performance, measured in terms of the equivalentsupport surface extension and the compensatory ankle angle, and the rejectionof impulsive forces, which is important in events such as stepping onunforeseen obstacles. Results show that the SoftFoot has the largest equivalentsupport surface when standing on obstacles, and absorbs impulsive loads in away almost as good as a compliant foot.",
        "title": "Analytical Model and Experimental Testing of the SoftFoot: an Adaptive  Robot Foot for Walking over Obstacles and Irregular Terrains",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05319",
        "abstract_url": "http://arxiv.org/abs/2401.05319",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xueyu"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Sun",
                "first_name": "Jiankai"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  Large language models (LLMs) have made significant progress in codegeneration tasks, but their performance in tackling programming problems withcomplex data structures and algorithms remains suboptimal. To address thisissue, we propose an in-context learning approach that guides LLMs to debug byusing a \"print debugging\" method, which involves inserting print statements totrace and analysing logs for fixing the bug. We collect a Leetcode problemdataset and evaluate our method using the Leetcode online judging system.Experiments with GPT-4 demonstrate the effectiveness of our approach,outperforming rubber duck debugging in easy and medium-level Leetcode problemsby 1.5% and 17.9%.",
        "title": "Leveraging Print Debugging to Improve Code Generation in Large Language  Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05321",
        "abstract_url": "http://arxiv.org/abs/2401.05321",
        "authors": [
            {
                "last_name": "Beame",
                "first_name": "Paul"
            },
            {
                "last_name": "Kornerup",
                "first_name": "Niels"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "",
            "",
            ""
        ],
        "abstract": "  We consider the time and space required for quantum computers to solve a widevariety of problems involving matrices, many of which have only been analyzedclassically in prior work. Our main results show that for a range of linearalgebra problems -- including matrix-vector product, matrix inversion, matrixmultiplication and powering -- existing classical time-space tradeoffs, severalof which are tight for every space bound, also apply to quantum algorithms. Forexample, for almost all matrices $A$, including the discrete Fourier transform(DFT) matrix, we prove that quantum circuits with at most $T$ input queries and$S$ qubits of memory require $T=\\Omega(n^2/S)$ to compute matrix-vector product$Ax$ for $x \\in \\{0,1\\}^n$. We similarly prove that matrix multiplication for$n\\times n$ binary matrices requires $T=\\Omega(n^3 / \\sqrt{S})$. Because manyof our lower bounds match deterministic algorithms with the same time and spacecomplexity, we show that quantum computers cannot provide any asymptoticadvantage for these problems with any space bound. We obtain matching lowerbounds for the stronger notion of quantum cumulative memory complexity -- thesum of the space per layer of a circuit.  We also consider Boolean (i.e. AND-OR) matrix multiplication andmatrix-vector products, improving the previous quantum time-space tradeofflower bounds for $n\\times n$ Boolean matrix multiplication to$T=\\Omega(n^{2.5}/S^{1/3})$ from $T=\\Omega(n^{2.5}/S^{1/2})$.  Our improved lower bound for Boolean matrix multiplication is based on a newcoloring argument that extracts more from the strong direct product theoremused in prior work. Our tight lower bounds for linear algebra problems requireadding a new bucketing method to the recording-query technique of Zhandry thatlets us apply classical arguments to upper bound the success probability ofquantum circuits.",
        "title": "Quantum Time-Space Tradeoffs for Matrix Problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05322",
        "abstract_url": "http://arxiv.org/abs/2401.05322",
        "authors": [
            {
                "last_name": "Schmidt",
                "first_name": "Carolin"
            },
            {
                "last_name": "Tygesen",
                "first_name": "Mathias"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Filipe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Urban mobility is on the cusp of transformation with the emergence of shared,connected, and cooperative automated vehicles. Yet, for them to be accepted bycustomers, trust in their punctuality is vital. Many pilot initiatives operatewithout a fixed schedule, thus enhancing the importance of reliable arrivaltime (AT) predictions. This study presents an AT prediction system forautonomous shuttles, utilizing separate models for dwell and running timepredictions, validated on real-world data from five cities. Alongsideestablished methods such as XGBoost, we explore the benefits of integratingspatial data using graph neural networks (GNN). To accurately handle the caseof a shuttle bypassing a stop, we propose a hierarchical model combining arandom forest classifier and a GNN. The results for the final AT prediction arepromising, showing low errors even when predicting several stops ahead. Yet, nosingle model emerges as universally superior, and we provide insights into thecharacteristics of pilot sites that influence the model selection process.Finally, we identify dwell time prediction as the key determinant in overall ATprediction accuracy when autonomous shuttles are deployed in low-traffic areasor under regulatory speed limits. This research provides insights into thecurrent state of autonomous public transport prediction models and paves theway for more data-informed decision-making as the field advances.",
        "title": "Arrival Time Prediction for Autonomous Shuttle Services in the Real  World: Evidence from Five Cities",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05329",
        "abstract_url": "http://arxiv.org/abs/2401.05329",
        "authors": [
            {
                "last_name": "Sen",
                "first_name": "Argha"
            },
            {
                "last_name": "Pal",
                "first_name": "Bhupendra"
            },
            {
                "last_name": "Achari",
                "first_name": "Seemant"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Sandip"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In the landscape of next-generation cellular networks, a projected surge ofover 12 billion subscriptions foreshadows a considerable upswing in thenetwork's overall energy consumption. The proliferation of User Equipment (UE)drives this energy demand, urging 5G deployments to seek more energy-efficientmethodologies. In this work, we propose SmartMME, as a pivotal solution aimedat optimizing Base Station (BS) energy usage. By harnessing and analyzingcritical network states-such as UE connections, data traffic at individual UEs,and other pertinent metrics-our methodology intelligently orchestrates the BS'spower states, making informed decisions on when to activate or deactivate theBS. This meticulous approach significantly curtails the network's overallenergy consumption. In a bid to validate its efficiency, we seamlesslyintegrated our module into Network Simulator-3 (ns-3), conducting extensivetesting to demonstrate its prowess in effectively managing and reducing netenergy consumption. As advocates of collaborative progress, we've opted toopen-source this module, inviting the engagement and feedback of the widerresearch community on GitHub.",
        "title": "\\textit{SmartMME}: Implementation of Base Station Switching Off Strategy  in ns-3",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05334",
        "abstract_url": "http://arxiv.org/abs/2401.05334",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhaoxi"
            },
            {
                "last_name": "Moon",
                "first_name": "Gyeongsik"
            },
            {
                "last_name": "Guo",
                "first_name": "Kaiwen"
            },
            {
                "last_name": "Cao",
                "first_name": "Chen"
            },
            {
                "last_name": "Pidhorskyi",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Simon",
                "first_name": "Tomas"
            },
            {
                "last_name": "Joshi",
                "first_name": "Rohan"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Yichen"
            },
            {
                "last_name": "Pires",
                "first_name": "Bernardo"
            },
            {
                "last_name": "Wen",
                "first_name": "He"
            },
            {
                "last_name": "Evans",
                "first_name": "Lucas"
            },
            {
                "last_name": "Peng",
                "first_name": "Bo"
            },
            {
                "last_name": "Buffalini",
                "first_name": "Julia"
            },
            {
                "last_name": "Trimble",
                "first_name": "Autumn"
            },
            {
                "last_name": "McPhail",
                "first_name": "Kevyn"
            },
            {
                "last_name": "Schoeller",
                "first_name": "Melissa"
            },
            {
                "last_name": "Yu",
                "first_name": "Shoou-I"
            },
            {
                "last_name": "Romero",
                "first_name": "Javier"
            },
            {
                "last_name": "Zollh\u00f6fer",
                "first_name": "Michael"
            },
            {
                "last_name": "Sheikh",
                "first_name": "Yaser"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Saito",
                "first_name": "Shunsuke"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Existing photorealistic relightable hand models require extensiveidentity-specific observations in different views, poses, and illuminations,and face challenges in generalizing to natural illuminations and novelidentities. To bridge this gap, we present URHand, the first universalrelightable hand model that generalizes across viewpoints, poses,illuminations, and identities. Our model allows few-shot personalization usingimages captured with a mobile phone, and is ready to be photorealisticallyrendered under novel illuminations. To simplify the personalization processwhile retaining photorealism, we build a powerful universal relightable priorbased on neural relighting from multi-view images of hands captured in a lightstage with hundreds of identities. The key challenge is scaling thecross-identity training while maintaining personalized fidelity and sharpdetails without compromising generalization under natural illuminations. Tothis end, we propose a spatially varying linear lighting model as the neuralrenderer that takes physics-inspired shading as input feature. By removingnon-linear activations and bias, our specifically designed lighting modelexplicitly keeps the linearity of light transport. This enables single-stagetraining from light-stage data while generalizing to real-time rendering underarbitrary continuous illuminations across diverse identities. In addition, weintroduce the joint learning of a physically based model and our neuralrelighting model, which further improves fidelity and generalization. Extensiveexperiments show that our approach achieves superior performance over existingmethods in terms of both quality and generalizability. We also demonstratequick personalization of URHand from a short phone scan of an unseen identity.",
        "title": "URHand: Universal Relightable Hands",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05335",
        "abstract_url": "http://arxiv.org/abs/2401.05335",
        "authors": [
            {
                "last_name": "Shahbazi",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Claessens",
                "first_name": "Liesbeth"
            },
            {
                "last_name": "Niemeyer",
                "first_name": "Michael"
            },
            {
                "last_name": "Collins",
                "first_name": "Edo"
            },
            {
                "last_name": "Tonioni",
                "first_name": "Alessio"
            },
            {
                "last_name": "Van Gool",
                "first_name": "Luc"
            },
            {
                "last_name": "Tombari",
                "first_name": "Federico"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "LG"
        ],
        "abstract": "  We introduce InseRF, a novel method for generative object insertion in theNeRF reconstructions of 3D scenes. Based on a user-provided textual descriptionand a 2D bounding box in a reference viewpoint, InseRF generates new objects in3D scenes. Recently, methods for 3D scene editing have been profoundlytransformed, owing to the use of strong priors of text-to-image diffusionmodels in 3D generative modeling. Existing methods are mostly effective inediting 3D scenes via style and appearance changes or removing existingobjects. Generating new objects, however, remains a challenge for such methods,which we address in this study. Specifically, we propose grounding the 3Dobject insertion to a 2D object insertion in a reference view of the scene. The2D edit is then lifted to 3D using a single-view object reconstruction method.The reconstructed object is then inserted into the scene, guided by the priorsof monocular depth estimation methods. We evaluate our method on various 3Dscenes and provide an in-depth analysis of the proposed components. Ourexperiments with generative insertion of objects in several 3D scenes indicatethe effectiveness of our method compared to the existing methods. InseRF iscapable of controllable and 3D-consistent object insertion without requiringexplicit 3D information as input. Please visit our project page athttps://mohamad-shahbazi.github.io/inserf.",
        "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05336",
        "abstract_url": "http://arxiv.org/abs/2401.05336",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Ronglai"
            },
            {
                "last_name": "Wei",
                "first_name": "Fangyun"
            },
            {
                "last_name": "Mak",
                "first_name": "Brian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The objective of sign language recognition is to bridge the communication gapbetween the deaf and the hearing. Numerous previous works train their modelsusing the well-established connectionist temporal classification (CTC) loss.During the inference stage, the CTC-based models typically take the entire signvideo as input to make predictions. This type of inference scheme is referredto as offline recognition. In contrast, while mature speech recognition systemscan efficiently recognize spoken words on the fly, sign language recognitionstill falls short due to the lack of practical online solutions. In this work,we take the first step towards filling this gap. Our approach comprises threephases: 1) developing a sign language dictionary encompassing all glossespresent in a target sign language dataset; 2) training an isolated signlanguage recognition model on augmented signs using both conventionalclassification loss and our novel saliency loss; 3) employing a sliding windowapproach on the input sign sequence and feeding each sign clip to thewell-optimized model for online recognition. Furthermore, our onlinerecognition model can be extended to boost the performance of any offlinemodel, and to support online translation by appending a gloss-to-text networkonto the recognition model. By integrating our online framework with thepreviously best-performing offline model, TwoStream-SLR, we achieve newstate-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T,and CSL-Daily. Code and models will be available athttps://github.com/FangyunWei/SLRT",
        "title": "Towards Online Sign Language Recognition and Translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05337",
        "abstract_url": "http://arxiv.org/abs/2401.05337",
        "authors": [
            {
                "last_name": "Renucci",
                "first_name": "Pierre"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This study presents an unsupervised machine learning approach for optimizingProfit and Loss (PnL) in quantitative finance. Our algorithm, akin to anunsupervised variant of linear regression, maximizes the Sharpe Ratio of PnLgenerated from signals constructed linearly from exogenous variables. Themethodology employs a linear relationship between exogenous variables and thetrading signal, with the objective of maximizing the Sharpe Ratio throughparameter optimization. Empirical application on an ETF representing U.S.Treasury bonds demonstrates the model's effectiveness, supported byregularization techniques to mitigate overfitting. The study concludes withpotential avenues for further development, including generalized time steps andenhanced corrective terms.",
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to  Optimize PnL with Linear Signals",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05338",
        "abstract_url": "http://arxiv.org/abs/2401.05338",
        "authors": [
            {
                "last_name": "Shao",
                "first_name": "Daqian"
            },
            {
                "last_name": "Fesser",
                "first_name": "Lukas"
            },
            {
                "last_name": "Kwiatkowska",
                "first_name": "Marta"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Robustness certification, which aims to formally certify the predictions ofneural networks against adversarial inputs, has become an integral part ofimportant tool for safety-critical applications. Despite considerable progress,existing certification methods are limited to elementary architectures, such asconvolutional networks, recurrent networks and recently Transformers, onbenchmark datasets such as MNIST. In this paper, we focus on the robustnesscertification of scene text recognition (STR), which is a complex andextensively deployed image-based sequence prediction problem. We tackle threetypes of STR model architectures, including the standard STR pipelines and theVision Transformer. We propose STR-Cert, the first certification method for STRmodels, by significantly extending the DeepPoly polyhedral verificationframework via deriving novel polyhedral bounds and algorithms for key STR modelcomponents. Finally, we certify and compare STR models on six datasets,demonstrating the efficiency and scalability of robustness certification,particularly for the Vision Transformer.",
        "title": "STR-Cert: Robustness Certification for Deep Text Recognition on Deep  Learning Pipelines and Vision Transformers",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05339",
        "abstract_url": "http://arxiv.org/abs/2401.05339",
        "authors": [
            {
                "last_name": "Chong",
                "first_name": "Toby"
            },
            {
                "last_name": "Chadwick",
                "first_name": "Alina"
            },
            {
                "last_name": "Shen",
                "first_name": "I-chao"
            },
            {
                "last_name": "Xie",
                "first_name": "Haoran"
            },
            {
                "last_name": "Igarashi",
                "first_name": "Takeo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  In this paper, we present a cosmetic-specific skin image dataset. It consistsof skin images from $45$ patches ($5$ skin patches each from $9$ participants)of size $8mm^*8mm$ under three cosmetic products (i.e., foundation, blusher,and highlighter). We designed a novel capturing device inspired by Light Stage.Using the device, we captured over $600$ images of each skin patch underdiverse lighting conditions in $30$ seconds. We repeated the process for thesame skin patch under three cosmetic products. Finally, we demonstrate theviability of the dataset with an image-to-image translation-based pipeline forcosmetic rendering and compared our data-driven approach to an existingcosmetic rendering method.",
        "title": "MicroGlam: Microscopic Skin Image Dataset with Cosmetics",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05341",
        "abstract_url": "http://arxiv.org/abs/2401.05341",
        "authors": [
            {
                "last_name": "Vogt",
                "first_name": "Yannick"
            },
            {
                "last_name": "Naouar",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Kalweit",
                "first_name": "Maria"
            },
            {
                "last_name": "Miething",
                "first_name": "Christoph Cornelius"
            },
            {
                "last_name": "Duyster",
                "first_name": "Justus"
            },
            {
                "last_name": "Mertelsmann",
                "first_name": "Roland"
            },
            {
                "last_name": "Kalweit",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Boedecker",
                "first_name": "Joschka"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The field of antibody-based therapeutics has grown significantly in recentyears, with targeted antibodies emerging as a potentially effective approach topersonalized therapies. Such therapies could be particularly beneficial forcomplex, highly individual diseases such as cancer. However, progress in thisfield is often constrained by the extensive search space of amino acidsequences that form the foundation of antibody design. In this study, weintroduce a novel reinforcement learning method specifically tailored toaddress the unique challenges of this domain. We demonstrate that our methodcan learn the design of high-affinity antibodies against multiple targets insilico, utilizing either online interaction or offline datasets. To the best ofour knowledge, our approach is the first of its kind and outperforms existingmethods on all tested antigens in the Absolut! database.",
        "title": "Stable Online and Offline Reinforcement Learning for Antibody CDRH3  Design",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05342",
        "abstract_url": "http://arxiv.org/abs/2401.05342",
        "authors": [
            {
                "last_name": "Burg",
                "first_name": "Max F."
            },
            {
                "last_name": "Zenkel",
                "first_name": "Thomas"
            },
            {
                "last_name": "Vystr\u010dilov\u00e1",
                "first_name": "Michaela"
            },
            {
                "last_name": "Oesterle",
                "first_name": "Jonathan"
            },
            {
                "last_name": "H\u00f6fling",
                "first_name": "Larissa"
            },
            {
                "last_name": "Willeke",
                "first_name": "Konstantin F."
            },
            {
                "last_name": "Lause",
                "first_name": "Jan"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Sarah"
            },
            {
                "last_name": "Fahey",
                "first_name": "Paul G."
            },
            {
                "last_name": "Ding",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Restivo",
                "first_name": "Kelli"
            },
            {
                "last_name": "Sridhar",
                "first_name": "Shashwat"
            },
            {
                "last_name": "Gollisch",
                "first_name": "Tim"
            },
            {
                "last_name": "Berens",
                "first_name": "Philipp"
            },
            {
                "last_name": "Tolias",
                "first_name": "Andreas S."
            },
            {
                "last_name": "Euler",
                "first_name": "Thomas"
            },
            {
                "last_name": "Bethge",
                "first_name": "Matthias"
            },
            {
                "last_name": "Ecker",
                "first_name": "Alexander S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Identifying cell types and understanding their functional properties iscrucial for unraveling the mechanisms underlying perception and cognition. Inthe retina, functional types can be identified by carefully selected stimuli,but this requires expert domain knowledge and biases the procedure towardspreviously known cell types. In the visual cortex, it is still unknown whatfunctional types exist and how to identify them. Thus, for unbiasedidentification of the functional cell types in retina and visual cortex, newapproaches are needed. Here we propose an optimization-based clusteringapproach using deep predictive models to obtain functional clusters of neuronsusing Most Discriminative Stimuli (MDS). Our approach alternates betweenstimulus optimization with cluster reassignment akin to anexpectation-maximization algorithm. The algorithm recovers functional clustersin mouse retina, marmoset retina and macaque visual area V4. This demonstratesthat our approach can successfully find discriminative stimuli across species,stages of the visual system and recording techniques. The resulting mostdiscriminative stimuli can be used to assign functional cell types fast and onthe fly, without the need to train complex predictive models or show a largenatural scene dataset, paving the way for experiments that were previouslylimited by experimental time. Crucially, MDS are interpretable: they visualizethe distinctive stimulus patterns that most unambiguously identify a specifictype of neuron. We will make our code available online upon publication.",
        "title": "Most discriminative stimuli for functional cell type identification",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05345",
        "abstract_url": "http://arxiv.org/abs/2401.05345",
        "authors": [
            {
                "last_name": "Durvasula",
                "first_name": "Sankeerth"
            },
            {
                "last_name": "Zhao",
                "first_name": "Adrian"
            },
            {
                "last_name": "Chen",
                "first_name": "Fan"
            },
            {
                "last_name": "Liang",
                "first_name": "Ruofan"
            },
            {
                "last_name": "Sanjaya",
                "first_name": "Pawan Kumar"
            },
            {
                "last_name": "Vijaykumar",
                "first_name": "Nandita"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "PF"
        ],
        "abstract": "  Differentiable rendering is a technique used in an important emerging classof visual computing applications that involves representing a 3D scene as amodel that is trained from 2D images using gradient descent. Recent works (e.g.3D Gaussian Splatting) use a rasterization pipeline to enable rendering highquality photo-realistic imagery at high speeds from these learned 3D models.These methods have been demonstrated to be very promising, providingstate-of-art quality for many important tasks. However, training a model torepresent a scene is still a time-consuming task even when using powerful GPUs.In this work, we observe that the gradient computation phase during training isa significant bottleneck on GPUs due to the large number of atomic operationsthat need to be processed. These atomic operations overwhelm atomic units inthe L2 partitions causing stalls. To address this challenge, we leverage theobservations that during the gradient computation: (1) for most warps, allthreads atomically update the same memory locations; and (2) warps generatevarying amounts of atomic traffic (since some threads may be inactive). Wepropose DISTWAR, a software-approach to accelerate atomic operations based ontwo key ideas: First, we enable warp-level reduction of threads at the SMsub-cores using registers to leverage the locality in intra-warp atomicupdates. Second, we distribute the atomic computation between the warp-levelreduction at the SM and the L2 atomic units to increase the throughput ofatomic computation. Warps with many threads performing atomic updates to thesame memory locations are scheduled at the SM, and the rest using L2 atomicunits. We implement DISTWAR using existing warp-level primitives. We evaluateDISTWAR on widely used raster-based differentiable rendering workloads. Wedemonstrate significant speedups of 2.44x on average (up to 5.7x).",
        "title": "DISTWAR: Fast Differentiable Rendering on Raster-based Rendering  Pipelines",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05346",
        "abstract_url": "http://arxiv.org/abs/2401.05346",
        "authors": [
            {
                "last_name": "Attapu",
                "first_name": "Amitha"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Robots can be very useful to automate tasks and reduce the human effortrequired. But for the robot to know, how to perform tasks, we need to give it aclear set of steps to follow. It is nearly impossible to provide a robot withinstructions for every possible task. Therefore we have a Universal Functionalobject-oriented network (FOON) which was created and expanded and has a lot ofexisting recipe information [1]. But certain tasks are complicated for robotsto perform and similarly, some tasks are complicated for humans to perform.Therefore weights have been added to functional units to represent the chanceof successful execution of the motion by the robot [2]. Given a set of kitchenitems and a goal node, using Universal FOON, a robot must be able to determineif the required items are present in the kitchen, and if yes, get the steps toconvert the required kitchen items to the goal node. Now through this paper, weuse two algorithms (IDS and GBFS) to retrieve a task tree (if possible) for agoal node and a given set of kitchen items. The following would be thedifferent parts of the paper: Section II FOON creation, where we will discussthe different terminologies related to FOON and visualization of FOON. InSection III Methodology we discuss the IDS and GBFS search algorithms and thetwo different heuristics implemented and used in GBFS. In Section IVExperiment/Discussion, we compare the performance of different algorithms. Inthe final section V, we specify the references of the papers that have beencited.",
        "title": "Task tree retrieval from FOON using search algorithms",
        "date": "2023-12-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05350",
        "abstract_url": "http://arxiv.org/abs/2401.05350",
        "authors": [
            {
                "last_name": "Aydin",
                "first_name": "Mehmet Emin"
            },
            {
                "last_name": "Durgut",
                "first_name": "Rafet"
            },
            {
                "last_name": "Rakib",
                "first_name": "Abdur"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "LG"
        ],
        "abstract": "  Optimisation problems, particularly combinatorial optimisation problems, aredifficult to solve due to their complexity and hardness. Such problems havebeen successfully solved by evolutionary and swarm intelligence algorithms,especially in binary format. However, the approximation may suffer due to thethe issues in balance between exploration and exploitation activities (EvE),which remain as the major challenge in this context. Although the complementaryusage of multiple operators is becoming more popular for managing EvE withadaptive operator selection schemes, a bespoke adaptive selection system isstill an important topic in research. Reinforcement Learning (RL) has recentlybeen proposed as a way to customise and shape up a highly effective adaptiveselection system. However, it is still challenging to handle the problem interms of scalability. This paper proposes and assesses a RL-based novelapproach to help develop a generalised framework for gaining, processing, andutilising the experiences for both the immediate and future use. Theexperimental results support the proposed approach with a certain level ofsuccess.",
        "title": "Adaptive operator selection utilising generalised experience",
        "date": "2023-12-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05351",
        "abstract_url": "http://arxiv.org/abs/2401.05351",
        "authors": [
            {
                "last_name": "Runge",
                "first_name": "Frederic"
            },
            {
                "last_name": "Franke",
                "first_name": "J\u00f6rg K. H."
            },
            {
                "last_name": "Fertmann",
                "first_name": "Daniel"
            },
            {
                "last_name": "Hutter",
                "first_name": "Frank"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Accurate RNA secondary structure prediction is vital for understandingcellular regulation and disease mechanisms. Deep learning (DL) methods havesurpassed traditional algorithms by predicting complex features likepseudoknots and multi-interacting base pairs. However, traditional distancemeasures can hardly deal with such tertiary interactions and the currently usedevaluation measures (F1 score, MCC) have limitations. We propose theWeisfeiler-Lehman graph kernel (WL) as an alternative metric. Embracinggraph-based metrics like WL enables fair and accurate evaluation of RNAstructure prediction algorithms. Further, WL provides informative guidance, asdemonstrated in an RNA design experiment.",
        "title": "Rethinking Performance Measures of RNA Secondary Structure Problems",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05352",
        "abstract_url": "http://arxiv.org/abs/2401.05352",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyun"
            },
            {
                "last_name": "Meinel",
                "first_name": "Christoph"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Generalized Class Discovery (GCD) plays a pivotal role in discerning bothknown and unknown categories from unlabeled datasets by harnessing the insightsderived from a labeled set comprising recognized classes. A significantlimitation in prevailing GCD methods is their presumption of an equitablydistributed category occurrence in unlabeled data. Contrary to this assumption,visual classes in natural environments typically exhibit a long-taileddistribution, with known or prevalent categories surfacing more frequently thantheir rarer counterparts. Our research endeavors to bridge this disconnect byfocusing on the long-tailed Generalized Category Discovery (Long-tailed GCD)paradigm, which echoes the innate imbalances of real-world unlabeled datasets.In response to the unique challenges posed by Long-tailed GCD, we present arobust methodology anchored in two strategic regularizations: (i) a reweightingmechanism that bolsters the prominence of less-represented, tail-endcategories, and (ii) a class prior constraint that aligns with the anticipatedclass distribution. Comprehensive experiments reveal that our proposed methodsurpasses previous state-of-the-art GCD methods by achieving an improvement ofapproximately 6 - 9% on ImageNet100 and competitive performance on CIFAR100.",
        "title": "Generalized Categories Discovery for Long-tailed Recognition",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05353",
        "abstract_url": "http://arxiv.org/abs/2401.05353",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyun"
            },
            {
                "last_name": "Dai",
                "first_name": "Ben"
            },
            {
                "last_name": "Simsek",
                "first_name": "Furkan"
            },
            {
                "last_name": "Meinel",
                "first_name": "Christoph"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Generalized class discovery (GCD) aims to infer known and unknown categoriesin an unlabeled dataset leveraging prior knowledge of a labeled set comprisingknown classes. Existing research implicitly/explicitly assumes that thefrequency of occurrence for each category, whether known or unknown, isapproximately the same in the unlabeled data. However, in nature, we are morelikely to encounter known/common classes than unknown/uncommon ones, accordingto the long-tailed property of visual classes. Therefore, we present achallenging and practical problem, Imbalanced Generalized Category Discovery(ImbaGCD), where the distribution of unlabeled data is imbalanced, with knownclasses being more frequent than unknown ones. To address these issues, wepropose ImbaGCD, A novel optimal transport-based expectation maximizationframework that accomplishes generalized category discovery by aligning themarginal class prior distribution. ImbaGCD also incorporates a systematicmechanism for estimating the imbalanced class prior distribution under the GCDsetup. Our comprehensive experiments reveal that ImbaGCD surpasses previousstate-of-the-art GCD methods by achieving an improvement of approximately 2 -4% on CIFAR-100 and 15 - 19% on ImageNet-100, indicating its superioreffectiveness in solving the Imbalanced GCD problem.",
        "title": "ImbaGCD: Imbalanced Generalized Category Discovery",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05354",
        "abstract_url": "http://arxiv.org/abs/2401.05354",
        "authors": [
            {
                "last_name": "Namura",
                "first_name": "Norihisa"
            },
            {
                "last_name": "Nakao",
                "first_name": "Hiroya"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  We propose a method for optimizing mutual coupling functions to achieve fastand global synchronization between a pair of weakly coupled limit-cycleoscillators. Our method is based on phase reduction that provides a conciselow-dimensional representation of the synchronization dynamics of mutuallycoupled oscillators, including the case where the coupling depends on past timeseries of the oscillators. We first describe a method for a pair of identicaloscillators and then generalize it to the case of slightly nonidenticaloscillators. The coupling function is designed in two optimization steps forthe functional form and amplitude, where the amplitude is numerically optimizedto minimize the average convergence time under a constraint on the total power.We perform numerical simulations of the synchronization dynamics with theoptimized coupling functions using the FitzHugh-Nagumo and R\\\"{o}ssleroscillators as examples. We show that the coupling function optimized by theproposed method can achieve global synchronization more efficiently than theprevious methods.",
        "title": "Optimal coupling functions for fast and global synchronization of weakly  coupled limit-cycle oscillators",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05355",
        "abstract_url": "http://arxiv.org/abs/2401.05355",
        "authors": [
            {
                "last_name": "Mih",
                "first_name": "Atah Nuh"
            },
            {
                "last_name": "Cao",
                "first_name": "Hung"
            },
            {
                "last_name": "Kawnine",
                "first_name": "Asfia"
            },
            {
                "last_name": "Wachowicz",
                "first_name": "Monica"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Resource constraints have restricted several EdgeAI applications to machinelearning inference approaches, where models are trained on the cloud anddeployed to the edge device. This poses challenges such as bandwidth, latency,and privacy associated with storing data off-site for model building. Trainingon the edge device can overcome these challenges by eliminating the need totransfer data to another device for storage and model development. On-devicetraining also provides robustness to data variations as models can be retrainedon newly acquired data to improve performance. We, therefore, propose alightweight EdgeAI architecture modified from Xception, for on-device trainingin a resource-constraint edge environment. We evaluate our model on a PCBdefect detection task and compare its performance against existing lightweightmodels - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of ourexperiment show that our model has a remarkable performance with a testaccuracy of 73.45% without pre-training. This is comparable to the testaccuracy of non-pre-trained MobileViT-XXS (75.40%) and much better than othernon-pre-trained models (MobileNetV2 - 50.05%, EfficientNetV2B0 - 54.30%). Thetest accuracy of our model without pre-training is comparable to pre-trainedMobileNetV2 model - 75.45% and better than pre-trained EfficientNetV2B0 model -58.10%. In terms of memory efficiency, our model performs better thanEfficientNetV2B0 and MobileViT-XXS. We find that the resource efficiency ofmachine learning models does not solely depend on the number of parameters butalso depends on architectural considerations. Our method can be applied toother resource-constraint applications while maintaining significantperformance.",
        "title": "Developing a Resource-Constraint EdgeAI model for Surface Defect  Detection",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05357",
        "abstract_url": "http://arxiv.org/abs/2401.05357",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Zheyu"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaobo Sharon"
            },
            {
                "last_name": "Shi",
                "first_name": "Yiyu"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  Architectures that incorporate Computing-in-Memory (CiM) using emergingnon-volatile memory (NVM) devices have become strong contenders for deep neuralnetwork (DNN) acceleration due to their impressive energy efficiency. Yet, asignificant challenge arises when using these emerging devices: they can showsubstantial variations during the weight-mapping process. This can severelyimpact DNN accuracy if not mitigated. A widely accepted remedy for imperfectweight mapping is the iterative write-verify approach, which involves verifyingconductance values and adjusting devices if needed. In all existingpublications, this procedure is applied to every individual device, resultingin a significant programming time overhead. In our research, we illustrate thatonly a small fraction of weights need this write-verify treatment for thecorresponding devices and the DNN accuracy can be preserved, yielding a notableprogramming acceleration. Building on this, we introduce USWIM, a novel methodbased on the second derivative. It leverages a single iteration of forward andbackpropagation to pinpoint the weights demanding write-verify. Throughextensive tests on diverse DNN designs and datasets, USWIM manifests up to a10x programming acceleration against the traditional exhaustive write-verifymethod, all while maintaining a similar accuracy level. Furthermore, comparedto our earlier SWIM technique, USWIM excels, showing a 7x speedup when dealingwith devices exhibiting non-uniform variations.",
        "title": "U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural  Accelerators",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05358",
        "abstract_url": "http://arxiv.org/abs/2401.05358",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhong",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Zheng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The development of U.S. Army and NATO data link systems is introduced first,and then the development trend of future intelligent data link is summarizedinto integration, generalization, multifunctionality and high security. Aunit-level combat system architecture based on the global combat cloud, whichis capable of realizing the flexible scheduling of global combat resources andmaximizing the overall combat effectiveness, is proposed. Intelligent data linkis an important part of this solution, providing strong information support forfuture urban unit-level warfare.",
        "title": "Future Intelligent Data link and Unit-Level Combat System Based on  Global Combat Cloud",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05362",
        "abstract_url": "http://arxiv.org/abs/2401.05362",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Ziqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Liyuan"
            },
            {
                "last_name": "Ding",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xingxing"
            },
            {
                "last_name": "Zhong",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Ai",
                "first_name": "Jianyong"
            },
            {
                "last_name": "Li",
                "first_name": "Jianmin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In real-world applications, an object detector often encounters objectinstances from new classes and needs to accommodate them effectively. Previouswork formulated this critical problem as incremental object detection (IOD),which assumes the object instances of new classes to be fully annotated inincremental data. However, as supervisory signals are usually rare andexpensive, the supervised IOD may not be practical for implementation. In thiswork, we consider a more realistic setting named semi-supervised IOD (SSIOD),where the object detector needs to learn new classes incrementally from a fewlabelled data and massive unlabelled data without catastrophic forgetting ofold classes. A commonly-used strategy for supervised IOD is to encourage thecurrent model (as a student) to mimic the behavior of the old model (as ateacher), but it generally fails in SSIOD because a dominant number of objectinstances from old and new classes are coexisting and unlabelled, with theteacher only recognizing a fraction of them. Observing that learning only theclasses of interest tends to preclude detection of other classes, we propose tobridge the coexistence of unlabelled classes by constructing two teacher modelsrespectively for old and new classes, and using the concatenation of theirpredictions to instruct the student. This approach is referred to asDualTeacher, which can serve as a strong baseline for SSIOD with limitedresource overhead and no extra hyperparameters. We build various benchmarks forSSIOD and perform extensive experiments to demonstrate the superiority of ourapproach (e.g., the performance lead is up to 18.28 AP on MS-COCO). Our code isavailable at \\url{https://github.com/chuxiuhong/DualTeacher}.",
        "title": "DualTeacher: Bridging Coexistence of Unlabelled Classes for  Semi-supervised Incremental Object Detection",
        "date": "2023-12-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05363",
        "abstract_url": "http://arxiv.org/abs/2401.05363",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiquan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Sha"
            },
            {
                "last_name": "Jiang",
                "first_name": "Haiteng"
            },
            {
                "last_name": "Li",
                "first_name": "Shijian"
            },
            {
                "last_name": "Li",
                "first_name": "Tao"
            },
            {
                "last_name": "Pan",
                "first_name": "Gang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Automatic sleep staging is essential for sleep assessment and disorderdiagnosis. Most existing methods depend on one specific dataset and are limitedto be generalized to other unseen datasets, for which the training data andtesting data are from the same dataset. In this paper, we introduce domaingeneralization into automatic sleep staging and propose the task ofgeneralizable sleep staging which aims to improve the model generalizationability to unseen datasets. Inspired by existing domain generalization methods,we adopt the feature alignment idea and propose a framework called SleepDG tosolve it. Considering both of local salient features and sequential featuresare important for sleep staging, we propose a Multi-level Feature Alignmentcombining epoch-level and sequence-level feature alignment to learndomain-invariant feature representations. Specifically, we design anEpoch-level Feature Alignment to align the feature distribution of each singlesleep epoch among different domains, and a Sequence-level Feature Alignment tominimize the discrepancy of sequential features among different domains.SleepDG is validated on five public datasets, achieving the state-of-the-artperformance.",
        "title": "Generalizable Sleep Staging via Multi-level Domain Alignment",
        "date": "2023-12-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05365",
        "abstract_url": "http://arxiv.org/abs/2401.05365",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Rapetti",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Grieco",
                "first_name": "Riccardo"
            },
            {
                "last_name": "Draicchio",
                "first_name": "Francesco"
            },
            {
                "last_name": "Pucci",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This paper proposes a framework that combines online human state estimation,action recognition and motion prediction to enable early assessment andprevention of worker biomechanical risk during lifting tasks. The frameworkleverages the NIOSH index to perform online risk assessment, thus fittingreal-time applications. In particular, the human state is retrieved via inversekinematics/dynamics algorithms from wearable sensor data. Human actionrecognition and motion prediction are achieved by implementing an LSTM-basedGuided Mixture of Experts architecture, which is trained offline and inferredonline. With the recognized actions, a single lifting activity is divided intoa series of continuous movements and the Revised NIOSH Lifting Equation can beapplied for risk assessment. Moreover, the predicted motions enableanticipation of future risks. A haptic actuator, embedded in the wearablesystem, can alert the subject of potential risk, acting as an active preventiondevice. The performance of the proposed framework is validated by executingreal lifting tasks, while the subject is equipped with the iFeel wearablesystem.",
        "title": "Online Action Recognition for Human Risk Prediction with Anticipated  Haptic Alert via Wearables",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05367",
        "abstract_url": "http://arxiv.org/abs/2401.05367",
        "authors": [
            {
                "last_name": "Aqajari",
                "first_name": "Seyed Amir Hossein"
            },
            {
                "last_name": "Labbaf",
                "first_name": "Sina"
            },
            {
                "last_name": "Tran",
                "first_name": "Phuc Hoang"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Brenda"
            },
            {
                "last_name": "Mehrabadi",
                "first_name": "Milad Asgari"
            },
            {
                "last_name": "Levorato",
                "first_name": "Marco"
            },
            {
                "last_name": "Dutt",
                "first_name": "Nikil"
            },
            {
                "last_name": "Rahmani",
                "first_name": "Amir M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Daily monitoring of stress is a critical component of maintaining optimalphysical and mental health. Physiological signals and contextual informationhave recently emerged as promising indicators for detecting instances ofheightened stress. Nonetheless, developing a real-time monitoring system thatutilizes both physiological and contextual data to anticipate stress levels ineveryday settings while also gathering stress labels from participantsrepresents a significant challenge. We present a monitoring system thatobjectively tracks daily stress levels by utilizing both physiological andcontextual data in a daily-life environment. Additionally, we have integrated asmart labeling approach to optimize the ecological momentary assessment (EMA)collection, which is required for building machine learning models for stressdetection. We propose a three-tier Internet-of-Things-based system architectureto address the challenges. We utilized a cross-validation technique toaccurately estimate the performance of our stress models. We achieved theF1-score of 70\\% with a Random Forest classifier using both PPG and contextualdata, which is considered an acceptable score in models built for everydaysettings. Whereas using PPG data alone, the highest F1-score achieved isapproximately 56\\%, emphasizing the significance of incorporating both PPG andcontextual data in stress detection tasks.",
        "title": "Context-Aware Stress Monitoring using Wearable and Mobile Technologies  in Everyday Settings",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05369",
        "abstract_url": "http://arxiv.org/abs/2401.05369",
        "authors": [
            {
                "last_name": "Gandhi",
                "first_name": "Govind"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG",
            ""
        ],
        "abstract": "  Growing interest in modelling complex systems from brains to societies tocities using networks has led to increased efforts to describe generativeprocesses that explain those networks. Recent successes in machine learninghave prompted the usage of evolutionary computation, especially geneticprogramming to evolve computer programs that effectively forage amultidimensional search space to iteratively find better solutions that explainnetwork structure. Symbolic regression contributes to these approaches byreplicating network morphologies using both structure and processes, all whilenot relying on the scientists intuition or expertise. It distinguishes itselfby introducing a novel formulation of a network generator and a parameter-freefitness function to evaluate the generated network and is found to consistentlyretrieve synthetically generated growth processes as well as simple,interpretable rules for a range of empirical networks. We extend this approachby modifying generator semantics to create and retrieve rules for time-varyingnetworks. Lexicon to study networks created dynamically in multiple stages isintroduced. The framework was improved using methods from the geneticprogramming toolkit (recombination) and computational improvements (usingheuristic distance measures) and used to test the consistency and robustness ofthe upgrades to the semantics using synthetically generated networks. Usingrecombination was found to improve retrieval rate and fitness of the solutions.The framework was then used on three empirical datasets - subway networks ofmajor cities, regions of street networks and semantic co-occurrence networks ofliterature in Artificial Intelligence to illustrate the possibility ofobtaining interpretable, decentralised growth processes from complex networks.",
        "title": "Symbolic Regression of Dynamic Network Models",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05370",
        "abstract_url": "http://arxiv.org/abs/2401.05370",
        "authors": [
            {
                "last_name": "Ghorbani",
                "first_name": "Mahdi"
            },
            {
                "last_name": "Gendelev",
                "first_name": "Leo"
            },
            {
                "last_name": "Beroza",
                "first_name": "Paul"
            },
            {
                "last_name": "Keiser",
                "first_name": "Michael J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In this work, we introduce AutoFragDiff, a fragment-based autoregressivediffusion model for generating 3D molecular structures conditioned on targetprotein structures. We employ geometric vector perceptrons to predict atomtypes and spatial coordinates of new molecular fragments conditioned onmolecular scaffolds and protein pockets. Our approach improves the localgeometry of the resulting 3D molecules while maintaining high predicted bindingaffinity to protein targets. The model can also perform scaffold extension fromuser-provided starting molecular scaffold.",
        "title": "Autoregressive fragment-based diffusion for pocket-aware ligand design",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05373",
        "abstract_url": "http://arxiv.org/abs/2401.05373",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Nan"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengzhu"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhenghan"
            },
            {
                "last_name": "De Masi",
                "first_name": "Giulia"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            },
            {
                "last_name": "Xiong",
                "first_name": "Huan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "LG"
        ],
        "abstract": "  The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks(GNNs) is gradually attracting attention due to the low power consumption andhigh efficiency in processing the non-Euclidean data represented by graphs.However, as a common problem, dynamic graph representation learning faceschallenges such as high complexity and large memory overheads. Current workoften uses SNNs instead of Recurrent Neural Networks (RNNs) by using binaryfeatures instead of continuous ones for efficient training, which wouldoverlooks graph structure information and leads to the loss of details duringpropagation. Additionally, optimizing dynamic spiking models typically requirespropagation of information across time steps, which increases memoryrequirements. To address these challenges, we present a framework named\\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph\\underline{N}eural Networks (\\method{}). To mitigate the information lossproblem, \\method{} propagates early-layer information directly to the lastlayer for information compensation. To accommodate the memory requirements, weapply the implicit differentiation on the equilibrium state, which does notrely on the exact reverse of the forward computation. While traditionalimplicit differentiation methods are usually used for static situations,\\method{} extends it to the dynamic graph setting. Extensive experiments onthree large-scale real-world dynamic graph datasets validate the effectivenessof \\method{} on dynamic node classification tasks with lower computationalcosts.",
        "title": "Dynamic Spiking Graph Neural Networks",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05375",
        "abstract_url": "http://arxiv.org/abs/2401.05375",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Taining"
            },
            {
                "last_name": "Goldstein",
                "first_name": "Adam"
            },
            {
                "last_name": "Levin",
                "first_name": "Michael"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "DS",
            "MA"
        ],
        "abstract": "  The emerging field of Diverse Intelligence seeks to identify, formalize, andunderstand commonalities in behavioral competencies across a wide range ofimplementations. Especially interesting are simple systems that provideunexpected examples of memory, decision-making, or problem-solving insubstrates that at first glance do not appear to be complex enough to implementsuch capabilities. We seek to develop tools to help understand the minimalrequirements for such capabilities, and to learn to recognize and predict basalforms of intelligence in unconventional substrates. Here, we apply novelanalyses to the behavior of classical sorting algorithms, short pieces of codewhich have been studied for many decades. To study these sorting algorithms asa model of biological morphogenesis and its competencies, we break twoformerly-ubiquitous assumptions: top-down control (instead, showing how eachelement within a array of numbers can exert minimal agency and implementsorting policies from the bottom up), and fully reliable hardware (instead,allowing some of the elements to be \"damaged\" and fail to execute thealgorithm). We quantitatively characterize sorting activity as the traversal ofa problem space, showing that arrays of autonomous elements sort themselvesmore reliably and robustly than traditional implementations in the presence oferrors. Moreover, we find the ability to temporarily reduce progress in orderto navigate around a defect, and unexpected clustering behavior among theelements in chimeric arrays whose elements follow one of two differentalgorithms. The discovery of emergent problem-solving capacities in simple,familiar algorithms contributes a new perspective to the field of DiverseIntelligence, showing how basal forms of intelligence can emerge in simplesystems without being explicitly encoded in their underlying mechanics.",
        "title": "Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting  arrays reveal unexpected competencies in a minimal model of basal  intelligence",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05376",
        "abstract_url": "http://arxiv.org/abs/2401.05376",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chunzhuo"
            },
            {
                "last_name": "Kumar",
                "first_name": "T. Sunil"
            },
            {
                "last_name": "De Raedt",
                "first_name": "Walter"
            },
            {
                "last_name": "Camps",
                "first_name": "Guido"
            },
            {
                "last_name": "Hallez",
                "first_name": "Hans"
            },
            {
                "last_name": "Vanrumste",
                "first_name": "Bart"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC"
        ],
        "abstract": "  Eating speed is an important indicator that has been widely scrutinized innutritional studies. The relationship between eating speed and severalintake-related problems such as obesity, diabetes, and oral health has receivedincreased attention from researchers. However, existing studies mainly useself-reported questionnaires to obtain participants' eating speed, where theychoose options from slow, medium, and fast. Such a non-quantitative method ishighly subjective and coarse in individual level. In this study, we propose anovel approach to measure eating speed in free-living environmentsautomatically and objectively using wrist-worn inertial measurement unit (IMU)sensors. Specifically, a temporal convolutional network combined with amulti-head attention module (TCN-MHA) is developed to detect bites (includingeating and drinking gestures) from free-living IMU data. The predicted bitesequences are then clustered to eating episodes. Eating speed is calculated byusing the time taken to finish the eating episode to divide the number ofbites. To validate the proposed approach on eating speed measurement, a 7-foldcross validation is applied to the self-collected fine-annotated full-day-I(FD-I) dataset, and a hold-out experiment is conducted on the full-day-II(FD-II) dataset. The two datasets are collected from 61 participants infree-living environments with a total duration of 513 h, which are publiclyavailable. Experimental results shows that the proposed approach achieves amean absolute percentage error (MAPE) of 0.110 and 0.146 in the FD-I and FD-IIdatasets, respectively, showcasing the feasibility of automated eating speedmeasurement. To the best of our knowledge, this is the first studyinvestigating automated eating speed measurement.",
        "title": "Eating Speed Measurement Using Wrist-Worn IMU Sensors in Free-Living  Environments",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05377",
        "abstract_url": "http://arxiv.org/abs/2401.05377",
        "authors": [
            {
                "last_name": "Capraro",
                "first_name": "Valerio"
            },
            {
                "last_name": "Lentsch",
                "first_name": "Austin"
            },
            {
                "last_name": "Acemoglu",
                "first_name": "Daron"
            },
            {
                "last_name": "Akgun",
                "first_name": "Selin"
            },
            {
                "last_name": "Akhmedova",
                "first_name": "Aisel"
            },
            {
                "last_name": "Bilancini",
                "first_name": "Ennio"
            },
            {
                "last_name": "Bonnefon",
                "first_name": "Jean-Fran\u00e7ois"
            },
            {
                "last_name": "Bra\u00f1as-Garza",
                "first_name": "Pablo"
            },
            {
                "last_name": "Butera",
                "first_name": "Luigi"
            },
            {
                "last_name": "Douglas",
                "first_name": "Karen M."
            },
            {
                "last_name": "Everett",
                "first_name": "Jim A. C."
            },
            {
                "last_name": "Gigerenzer",
                "first_name": "Gerd"
            },
            {
                "last_name": "Greenhow",
                "first_name": "Christine"
            },
            {
                "last_name": "Hashimoto",
                "first_name": "Daniel A."
            },
            {
                "last_name": "Holt-Lunstad",
                "first_name": "Julianne"
            },
            {
                "last_name": "Jetten",
                "first_name": "Jolanda"
            },
            {
                "last_name": "Johnson",
                "first_name": "Simon"
            },
            {
                "last_name": "Longoni",
                "first_name": "Chiara"
            },
            {
                "last_name": "Lunn",
                "first_name": "Pete"
            },
            {
                "last_name": "Natale",
                "first_name": "Simone"
            },
            {
                "last_name": "Rahwan",
                "first_name": "Iyad"
            },
            {
                "last_name": "Selwyn",
                "first_name": "Neil"
            },
            {
                "last_name": "Singh",
                "first_name": "Vivek"
            },
            {
                "last_name": "Suri",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Sutcliffe",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Tomlinson",
                "first_name": "Joe"
            },
            {
                "last_name": "van der Linden",
                "first_name": "Sander"
            },
            {
                "last_name": "Van Lange",
                "first_name": "Paul A. M."
            },
            {
                "last_name": "Wall",
                "first_name": "Friederike"
            },
            {
                "last_name": "Van Bavel",
                "first_name": "Jay J."
            },
            {
                "last_name": "Viale",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Generative artificial intelligence, including chatbots like ChatGPT, has thepotential to both exacerbate and ameliorate existing socioeconomicinequalities. In this article, we provide a state-of-the-art interdisciplinaryoverview of the probable impacts of generative AI on four critical domains:work, education, health, and information. Our goal is to warn about howgenerative AI could worsen existing inequalities while illuminating directionsfor using AI to resolve pervasive social problems. Generative AI in theworkplace can boost productivity and create new jobs, but the benefits willlikely be distributed unevenly. In education, it offers personalized learningbut may widen the digital divide. In healthcare, it improves diagnostics andaccessibility but could deepen pre-existing inequalities. For information, itdemocratizes content creation and access but also dramatically expands theproduction and proliferation of misinformation. Each section covers a specifictopic, evaluates existing research, identifies critical gaps, and recommendsresearch directions. We conclude with a section highlighting the role ofpolicymaking to maximize generative AI's potential to reduce inequalities whilemitigating its harmful effects. We discuss strengths and weaknesses of existingpolicy frameworks in the European Union, the United States, and the UnitedKingdom, observing that each fails to fully confront the socioeconomicchallenges we have identified. We contend that these policies should promoteshared prosperity through the advancement of generative AI. We suggest severalconcrete policies to encourage further research and debate. This articleemphasizes the need for interdisciplinary collaborations to understand andaddress the complex challenges of generative AI.",
        "title": "The impact of generative artificial intelligence on socioeconomic  inequalities and policy making",
        "date": "2023-12-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05378",
        "abstract_url": "http://arxiv.org/abs/2401.05378",
        "authors": [
            {
                "last_name": "Alam",
                "first_name": "Ridwan"
            },
            {
                "last_name": "Aguirre",
                "first_name": "Aaron"
            },
            {
                "last_name": "Stultz",
                "first_name": "Collin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  For a number of antiarrhythmics, drug loading requires a 3 dayhospitalization with monitoring for QT prolongation. Automated QT monitoringwith wearable ECG monitors would facilitate out-of-hospital care. We develop adeep learning model that infers QT intervals from ECG lead-I - the lead mostoften acquired from ambulatory ECG monitors - and to use this model to detectclinically meaningful QT-prolongation episodes during Dofetilide drug loading.Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at theMassachusetts General Hospital, we develop a deep learning model, QTNet, thatinfers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patientsare used to train the model and an internal-test set containing 633 thousandECGs from 135 thousand patients was used for testing. QTNet is furtherevaluated on an external-validation set containing 3.1 million ECGs from 667thousand patients at another institution. QTNet was used to detectDofetilide-induced QT prolongation in a publicly available database(ECGRDVQ-dataset) containing ECGs from subjects enrolled in a clinical trialevaluating the effects of antiarrhythmic drugs. QTNet achieves mean absoluteerrors of 12.63ms (internal-test) and 12.30ms (external-validation) forestimating absolute QT intervals. The associated Pearson correlationcoefficients are 0.91 (internal-test) and 0.92 (external-validation). For theECGRDVQ-dataset, QTNet detects Dofetilide-induced QTc prolongation with 87%sensitivity and 77% specificity. The negative predictive value of the model isgreater than 95% when the pre-test probability of drug-induced QTc prolongationis below 25%. Drug-induced QT prolongation risk can be tracked from ECG lead-Iusing deep learning.",
        "title": "Detecting QT prolongation From a Single-lead ECG With Deep Learning",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05379",
        "abstract_url": "http://arxiv.org/abs/2401.05379",
        "authors": [
            {
                "last_name": "Hashemi",
                "first_name": "Amirreza"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This study presents a comprehensive evaluation of tools available on theHuggingFace platform for two pivotal applications in artificial intelligence:image segmentation and voice conversion. The primary objective was to identifythe top three tools within each category and subsequently install and configurethese tools on Linux systems. We leveraged the power of pre-trainedsegmentation models such as SAM and DETR Model with ResNet-50 backbone forimage segmentation, and the so-vits-svc-fork model for voice conversion. Thispaper delves into the methodologies and challenges encountered during theimplementation process, and showcases the successful combination of videosegmentation and voice conversion in a unified project named AutoVisual FusionSuite.",
        "title": "AutoVisual Fusion Suite: A Comprehensive Evaluation of Image  Segmentation and Voice Conversion Tools on HuggingFace Platform",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05380",
        "abstract_url": "http://arxiv.org/abs/2401.05380",
        "authors": [
            {
                "last_name": "Dyoub",
                "first_name": "Abeer"
            },
            {
                "last_name": "Letteri",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  In this study, we investigated the application of bio-inspired optimizationalgorithms, including Genetic Algorithm, Particle Swarm Optimization, and WhaleOptimization Algorithm, for feature selection in chronic disease prediction.The primary goal was to enhance the predictive accuracy of models streamlinedata dimensionality, and make predictions more interpretable and actionable.  The research encompassed a comparative analysis of the three bio-inspiredfeature selection approaches across diverse chronic diseases, includingdiabetes, cancer, kidney, and cardiovascular diseases. Performance metrics suchas accuracy, precision, recall, and f1 score are used to assess theeffectiveness of the algorithms in reducing the number of features needed foraccurate classification.  The results in general demonstrate that the bio-inspired optimizationalgorithms are effective in reducing the number of features required foraccurate classification. However, there have been variations in the performanceof the algorithms on different datasets.  The study highlights the importance of data pre-processing and cleaning inensuring the reliability and effectiveness of the analysis.  This study contributes to the advancement of predictive analytics in therealm of chronic diseases. The potential impact of this work extends to earlyintervention, precision medicine, and improved patient outcomes, providing newavenues for the delivery of healthcare services tailored to individual needs.The findings underscore the potential benefits of using bio-inspiredoptimization algorithms for feature selection in chronic disease prediction,offering valuable insights for improving healthcare outcomes.",
        "title": "Dataset Optimization for Chronic Disease Prediction with Bio-Inspired  Feature Selection",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05381",
        "abstract_url": "http://arxiv.org/abs/2401.05381",
        "authors": [
            {
                "last_name": "Petralia",
                "first_name": "Adrien"
            },
            {
                "last_name": "Charpentier",
                "first_name": "Philippe"
            },
            {
                "last_name": "Palpanas",
                "first_name": "Themis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Over the past decade, millions of smart meters have been installed byelectricity suppliers worldwide, allowing them to collect a large amount ofelectricity consumption data, albeit sampled at a low frequency (one pointevery 30min). One of the important challenges these suppliers face is how toutilize these data to detect the presence/absence of different appliances inthe customers' households. This valuable information can help them providepersonalized offers and recommendations to help customers towards the energytransition. Appliance detection can be cast as a time series classificationproblem. However, the large amount of data combined with the long and variablelength of the consumption series pose challenges when training a classifier. Inthis paper, we propose ADF, a framework that uses subsequences of a clientconsumption series to detect the presence/absence of appliances. We alsointroduce TransApp, a Transformer-based time series classifier that is firstpretrained in a self-supervised way to enhance its performance on appliancedetection tasks. We test our approach on two real datasets, including apublicly available one. The experimental results with two large real datasetsshow that the proposed approach outperforms current solutions, includingstate-of-the-art time series classifiers applied to appliance detection. Thispaper appeared in VLDB 2024.",
        "title": "ADF & TransApp: A Transformer-Based Framework for Appliance Detection  Using Smart Meter Consumption Series",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05382",
        "abstract_url": "http://arxiv.org/abs/2401.05382",
        "authors": [
            {
                "last_name": "Ghasemi",
                "first_name": "Zahra"
            },
            {
                "last_name": "Neumann",
                "first_name": "Frank"
            },
            {
                "last_name": "Zanin",
                "first_name": "Max"
            },
            {
                "last_name": "Karageorgos",
                "first_name": "John"
            },
            {
                "last_name": "Chen",
                "first_name": "Lei"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Semi-autogenous grinding (SAG) mills play a pivotal role in the grindingcircuit of mineral processing plants. Accurate prediction of SAG millthroughput as a crucial performance metric is of utmost importance. Whileempirical models have been developed in previous studies for SAG millthroughput prediction, the potential of applying machine learning (ML)techniques for this purpose remains underexplored. Unlike empirical modelling,which relies on expensive and time-consuming experimental data, ML techniquescan utilize data collected during regular operations. Genetic programming (GP)is one of ML techniques that offers the advantage of providing a transparentequation for precise mill throughput prediction. This study explores theapplication of GP to predict SAG mill throughput and introduces five new GPvariants to enhance prediction performance. These variants extract multipleequations, each accurately predicting mill throughput for specific clusters oftraining data. These equations are then employed to predict mill throughput fortest data using various approaches. To assess the effect of distance measureson the new GP variants, four different distance measures are employed.Comparative analysis reveals that the new GP variants achieve an averageimprovement of 12.49% in prediction accuracy. Further investigation of distancemeasures indicates that the Euclidean distance measure yields the most accurateresults for the majority of data splits. Additionally, the most precise new GPvariant considers all equations and incorporates both the number of data pointsin each data cluster and the distance to clusters when calculating the finalprediction. The developed GP variants in this study present a precise,transparent, and cost-effective approach for modelling SAG mill throughput inmineral processing plants.",
        "title": "An improved genetic programming for predicting semi autogenous grinding  mill throughput",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05384",
        "abstract_url": "http://arxiv.org/abs/2401.05384",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Nuo"
            },
            {
                "last_name": "Li",
                "first_name": "Hongguang"
            },
            {
                "last_name": "Wang",
                "first_name": "Baoyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper investigates the performance of Large Language Models (LLMs) andTool-augmented LLMs in tackling complex mathematical reasoning tasks. Weintroduce IMP-TIP: Improving Math Reasoning with Tool-augmented InterleafPrompting, a framework that combines the strengths of both LLMs andTool-augmented LLMs. IMP-TIP follows the ``From Good to Great\" concept,collecting multiple potential solutions from both LLMs and their Tool-Augmentedcounterparts for the same math problem, and then selecting or re-generating themost accurate answer after cross-checking these solutions via tool-augmentedinterleaf prompting. The framework incorporates two key aspects: self-promptand tool-augmented interleaf prompting (TIP). The former allows LLMs toautonomously refine and improve an initial prompt related to tool usage, whilethe latter enables LLMs to derive the final answer by dynamically analyzing theproblem, cross-checking potential solutions, and revising previous reasoninghints in an interleaved manner. Experimental analysis shows that IMP-TIPachieves enhanced mathematical capabilities and outperforms traditional LLMsand tool-augmented LLMs in accuracy and reasoning diversity on math reasoningtasks. For instance, IMP-TIP can improve Tool-augmented ChatGPT on GSM8K-Hardfrom 56.0% to 65.2%.",
        "title": "From Good to Great: Improving Math Reasoning with Tool-Augmented  Interleaf Prompting",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05385",
        "abstract_url": "http://arxiv.org/abs/2401.05385",
        "authors": [
            {
                "last_name": "Oswald",
                "first_name": "Christian"
            },
            {
                "last_name": "Toth",
                "first_name": "Mate"
            },
            {
                "last_name": "Meissner",
                "first_name": "Paul"
            },
            {
                "last_name": "Pernkopf",
                "first_name": "Franz"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  In automotive applications, frequency modulated continuous wave (FMCW) radaris an established technology to determine the distance, velocity and angle ofobjects in the vicinity of the vehicle. The quality of predictions might beseriously impaired if mutual interference between radar sensors occurs.Previous work processes data from the entire receiver array in parallel toincrease interference mitigation quality using neural networks (NNs). However,these architectures do not generalize well across different angles of arrival(AoAs) of interferences and objects. In this paper we introduce fullyconvolutional neural network (CNN) with rank-three convolutions which is ableto transfer learned patterns between different AoAs. Our proposed architectureoutperforms previous work while having higher robustness and a lower number oftrainable parameters. We evaluate our network on a diverse data set anddemonstrate its angle equivariance.",
        "title": "Angle-Equivariant Convolutional Neural Networks for Interference  Mitigation in Automotive Radar",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05386",
        "abstract_url": "http://arxiv.org/abs/2401.05386",
        "authors": [
            {
                "last_name": "Colot",
                "first_name": "Martin"
            },
            {
                "last_name": "Simar",
                "first_name": "C\u00e9dric"
            },
            {
                "last_name": "Petieau",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Ana Maria Cebolla"
            },
            {
                "last_name": "Cheron",
                "first_name": "Guy"
            },
            {
                "last_name": "Bontempi",
                "first_name": "Gianluca"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            "LG"
        ],
        "abstract": "  Electromyograms (EMG)-based hand gesture recognition systems are a promisingtechnology for human/machine interfaces. However, one of their main limitationsis the long calibration time that is typically required to handle new users.The paper discusses and analyses the challenge of cross-subject generalizationthanks to an original dataset containing the EMG signals of 14 human subjectsduring hand gestures. The experimental results show that, though an accurategeneralization based on pooling multiple subjects is hardly achievable, it ispossible to improve the cross-subject estimation by identifying a robustlow-dimensional subspace for multiple subjects and aligning it to a targetsubject. A visualization of the subspace enables us to provide insights for theimprovement of cross-subject generalization with EMG signals.",
        "title": "EMG subspace alignment and visualization for cross-subject hand gesture  classification",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05388",
        "abstract_url": "http://arxiv.org/abs/2401.05388",
        "authors": [
            {
                "last_name": "Cardoso",
                "first_name": "Gabriel V."
            },
            {
                "last_name": "Bedin",
                "first_name": "Lisa"
            },
            {
                "last_name": "Duchateau",
                "first_name": "Josselin"
            },
            {
                "last_name": "Dubois",
                "first_name": "R\u00e9mi"
            },
            {
                "last_name": "Moulines",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  In this work, we propose a denoising diffusion generative model (DDGM)trained with healthy electrocardiogram (ECG) data that focuses on ECGmorphology and inter-lead dependence. Our results show that this innovativegenerative model can successfully generate realistic ECG signals. Furthermore,we explore the application of recent breakthroughs in solving linear inverseBayesian problems using DDGM. This approach enables the development of severalimportant clinical tools. These include the calculation of corrected QTintervals (QTc), effective noise suppression of ECG signals, recovery ofmissing ECG leads, and identification of anomalous readings, enablingsignificant advances in cardiac health monitoring and diagnosis.",
        "title": "Bayesian ECG reconstruction using denoising diffusion generative models",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05390",
        "abstract_url": "http://arxiv.org/abs/2401.05390",
        "authors": [
            {
                "last_name": "Troncoso-Pastoriza",
                "first_name": "Francisco"
            },
            {
                "last_name": "Egu\u00eda-Oller",
                "first_name": "Pablo"
            },
            {
                "last_name": "D\u00edaz-Redondo",
                "first_name": "Rebeca P."
            },
            {
                "last_name": "Granada-\u00c1lvarez",
                "first_name": "Enrique"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper we introduce a method that supports the detection,identification and localization of lamps in a building, with the main goal ofautomatically feeding its energy model by means of Building InformationModeling (BIM) methods. The proposed method, thus, provides useful informationto apply energy-saving strategies to reduce energy consumption in the buildingsector through the correct management of the lighting infrastructure. Based onthe unique geometry and brightness of lamps and the use of only greyscaleimages, our methodology is able to obtain accurate results despite its lowcomputational needs, resulting in near-real-time processing. The main noveltyis that the focus of the candidate search is not over the entire image butinstead only on a limited region that summarizes the specific characteristicsof the lamp. The information obtained from our approach was used on the GreenBuilding XML Schema to illustrate the automatic generation of BIM data from theresults of the algorithm.",
        "title": "Generation of BIM data based on the automatic detection, identification  and localization of lamps in buildings",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05391",
        "abstract_url": "http://arxiv.org/abs/2401.05391",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hui"
            },
            {
                "last_name": "Gan",
                "first_name": "Yi"
            },
            {
                "last_name": "Yuan",
                "first_name": "Feng"
            },
            {
                "last_name": "Ma",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Yutao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoli"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinghui"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            ""
        ],
        "abstract": "  Transformer based Large Language Models (LLMs) have been widely used in manyfields, and the efficiency of LLM inference becomes hot topic in realapplications. However, LLMs are usually complicatedly designed in modelstructure with massive operations and perform inference in the auto-regressivemode, making it a challenging task to design a system with high efficiency.  In this paper, we propose an efficient LLM inference solution with lowlatency and high throughput. Firstly, we simplify the LLM decoder layer byfusing data movement and element-wise operations to reduce the memory accessfrequency and lower system latency. We also propose a segment KV cache policyto keep key/value of the request and response tokens in separate physicalmemory for effective device memory management, helping enlarge the runtimebatch size and improve system throughput. A customizedScaled-Dot-Product-Attention kernel is designed to match our fusion policybased on the segment KV cache solution. We implement our LLM inference solutionon Intel GPU and publish it publicly. Compared with the standard HuggingFaceimplementation, the proposed solution achieves up to 7x lower token latency and27x higher throughput for some popular LLMs on Intel GPU.",
        "title": "Efficient LLM inference solution on Intel GPU",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05392",
        "abstract_url": "http://arxiv.org/abs/2401.05392",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Vikas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Noise is inevitably common in digital images, leading to visual imagedeterioration. Therefore, a suitable filtering method is required to lessen thenoise while preserving the image features (edges, corners, etc.). This paperpresents the efficient type-2 fuzzy weighted mean filter with an adaptivethreshold to remove the SAP noise. The present filter has two primary steps:The first stage categorizes images as lightly, medium, and heavily corruptedbased on an adaptive threshold by comparing the M-ALD of processed pixels withthe upper and lower MF of the type-2 fuzzy identifier. The second stageeliminates corrupted pixels by computing the appropriate weight using GMF withthe mean and variance of the uncorrupted pixels in the filter window.Simulation results vividly show that the obtained denoised images preserveimage features, i.e., edges, corners, and other sharp structures, compared withdifferent filtering methods.",
        "title": "AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted  with Salt-and-Pepper",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05394",
        "abstract_url": "http://arxiv.org/abs/2401.05394",
        "authors": [
            {
                "last_name": "de Vazelhes",
                "first_name": "William"
            },
            {
                "last_name": "Mukhoty",
                "first_name": "Bhaskar"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xiao-Tong"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  Sparse recovery is ubiquitous in machine learning and signal processing. Dueto the NP-hard nature of sparse recovery, existing methods are known to suffereither from restrictive (or even unknown) applicability conditions, or highcomputational cost. Recently, iterative regularization methods have emerged asa promising fast approach because they can achieve sparse recovery in one passthrough early stopping, rather than the tedious grid-search used in thetraditional methods. However, most of those iterative methods are based on the$\\ell_1$ norm which requires restrictive applicability conditions and couldfail in many cases. Therefore, achieving sparse recovery with iterativeregularization methods under a wider range of conditions has yet to be furtherexplored. To address this issue, we propose a novel iterative regularizationalgorithm, IRKSN, based on the $k$-support norm regularizer rather than the$\\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, andcompare them with traditional conditions for recovery with $\\ell_1$ normregularizers. Additionally, we give an early stopping bound on the model errorof IRKSN with explicit constants, achieving the standard linear rate for sparserecovery. Finally, we illustrate the applicability of our algorithm on severalexperiments, including a support recovery experiment with a correlated designmatrix.",
        "title": "Iterative Regularization with k-Support Norm: an Important Complement to  Sparse Recovery",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05395",
        "abstract_url": "http://arxiv.org/abs/2401.05395",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Ruixin"
            },
            {
                "last_name": "Chen",
                "first_name": "Bowei"
            },
            {
                "last_name": "Wilson",
                "first_name": "James M."
            },
            {
                "last_name": "Yan",
                "first_name": "Zhi"
            },
            {
                "last_name": "Huang",
                "first_name": "Yufei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CY",
            "LG"
        ],
        "abstract": "  The automotive industry plays a critical role in the global economy, andparticularly important is the expanding Chinese automobile market due to itsimmense scale and influence. However, existing automotive sector datasets arelimited in their coverage, failing to adequately consider the growing demandfor more and diverse variables. This paper aims to bridge this data gap byintroducing a comprehensive dataset spanning the years from 2016 to 2022,encompassing sales data, online reviews, and a wealth of information related tothe Chinese automotive industry. This dataset serves as a valuable resource,significantly expanding the available data. Its impact extends to variousdimensions, including improving forecasting accuracy, expanding the scope ofbusiness applications, informing policy development and regulation, andadvancing academic research within the automotive sector. To illustrate thedataset's potential applications in both business and academic contexts, wepresent two application examples. Our developed dataset enhances ourunderstanding of the Chinese automotive market and offers a valuable tool forresearchers, policymakers, and industry stakeholders worldwide.",
        "title": "SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive  market",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05396",
        "abstract_url": "http://arxiv.org/abs/2401.05396",
        "authors": [
            {
                "last_name": "\u00c1lvarez-Tu\u00f1\u00f3n",
                "first_name": "Olaya"
            },
            {
                "last_name": "Brodskiy",
                "first_name": "Yury"
            },
            {
                "last_name": "Kayacan",
                "first_name": "Erdal"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper overviews different pose representations and metric functions invisual odometry (VO) networks. The performance of VO networks heavily relies onhow their architecture encodes the information. The choice of poserepresentation and loss function significantly impacts network convergence andgeneralization. We investigate these factors in the VO network DeepVO byimplementing loss functions based on Euler, quaternion, and chordal distanceand analyzing their influence on performance. The results of this study provideinsights into how loss functions affect the designing of efficient and accurateVO networks for camera motion estimation. The experiments illustrate that adistance that complies with the mathematical requirements of a metric, such asthe chordal distance, provides better generalization and faster convergence.The code for the experiments can be found athttps://github.com/remaro-network/Loss_VO_right",
        "title": "Loss it right: Euclidean and Riemannian Metrics in Learning-based Visual  Odometry",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05397",
        "abstract_url": "http://arxiv.org/abs/2401.05397",
        "authors": [
            {
                "last_name": "Marto",
                "first_name": "Sim\u00e3o da Gra\u00e7a"
            },
            {
                "last_name": "Vasile",
                "first_name": "Massimiliano"
            },
            {
                "last_name": "Campbell",
                "first_name": "Andrew"
            },
            {
                "last_name": "Murray",
                "first_name": "Paul"
            },
            {
                "last_name": "Marshall",
                "first_name": "Stephen"
            },
            {
                "last_name": "Savitski",
                "first_name": "Vasili"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Spectral lightcurves consisting of time series single-pixel spectralmeasurements of spacecraft are used to infer the spacecraft's attitude androtation. Two methods are used. One based on numerical optimisation of aregularised least squares cost function, and another based on machine learningwith a neural network model. The aim is to work with minimal information, thusno prior is available on the attitude nor on the inertia tensor. Thetheoretical and practical aspects of this task are investigated, and themethodology is tested on synthetic data. Results are shown based on syntheticdata.",
        "title": "Hyperspectral Lightcurve Inversion for Attitude Determination",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05398",
        "abstract_url": "http://arxiv.org/abs/2401.05398",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenwen"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  GeoAI, or geospatial artificial intelligence, is an exciting new area thatleverages artificial intelligence (AI), geospatial big data, and massivecomputing power to solve problems with high automation and intelligence. Thispaper reviews the progress of AI in social science research, highlightingimportant advancements in using GeoAI to fill critical data and knowledge gaps.It also discusses the importance of breaking down data silos, acceleratingconvergence among GeoAI research methods, as well as moving GeoAI beyondgeospatial benefits.",
        "title": "GeoAI in Social Science",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05399",
        "abstract_url": "http://arxiv.org/abs/2401.05399",
        "authors": [
            {
                "last_name": "Oli",
                "first_name": "Priti"
            },
            {
                "last_name": "Banjade",
                "first_name": "Rabin"
            },
            {
                "last_name": "Chapagain",
                "first_name": "Jeevan"
            },
            {
                "last_name": "Rus",
                "first_name": "Vasile"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "",
            "CL"
        ],
        "abstract": "  Assessing student's answers and in particular natural language answers is acrucial challenge in the field of education. Advances in machine learning,including transformer-based models such as Large Language Models(LLMs), haveled to significant progress in various natural language tasks. Nevertheless,amidst the growing trend of evaluating LLMs across diverse tasks, evaluatingLLMs in the realm of automated answer assesment has not received muchattention. To address this gap, we explore the potential of using LLMs forautomated assessment of student's short and open-ended answer. Particularly, weuse LLMs to compare students' explanations with expert explanations in thecontext of line-by-line explanations of computer programs.  For comparison purposes, we assess both Large Language Models (LLMs) andencoder-based Semantic Textual Similarity (STS) models in the context ofassessing the correctness of students' explanation of computer code. Ourfindings indicate that LLMs, when prompted in few-shot and chain-of-thoughtsetting perform comparable to fine-tuned encoder-based models in evaluatingstudents' short answers in programming domain.",
        "title": "Automated Assessment of Students' Code Comprehension using LLMs",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05400",
        "abstract_url": "http://arxiv.org/abs/2401.05400",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Gyeong-Geon"
            },
            {
                "last_name": "Mun",
                "first_name": "Seonyeong"
            },
            {
                "last_name": "Shin",
                "first_name": "Myeong-Kyeong"
            },
            {
                "last_name": "Zhai",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  This research aims to demonstrate that AI can function not only as a tool forlearning, but also as an intelligent agent with which humans can engage incollaborative learning (CL) to change epistemic practices in scienceclassrooms. We adopted a design and development research approach, followingthe Analysis, Design, Development, Implementation and Evaluation (ADDIE) model,to prototype a tangible instructional system called Collaborative Learning withAI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learnersjoin an AI speaker to form a small group, where humans and AI are considered aspeers participating in the Jigsaw learning process. The development was carriedout using the NUGU AI speaker platform. The CLAIS system was successfullyimplemented in a Science Education course session with 15 pre-serviceelementary science teachers. The participants evaluated the CLAIS systemthrough mixed methods surveys as teachers, learners, peers, and users.Quantitative data showed that the participants' Intelligent-Technological,Pedagogical, And Content Knowledge was significantly increased after the CLAISsession, the perception of the CLAIS learning experience was positive, the peerassessment on AI speakers and human peers was different, and the userexperience was ambivalent. Qualitative data showed that the participantsanticipated future changes in the epistemic process in science classrooms,while acknowledging technical issues such as speech recognition performance andresponse latency. This study highlights the potential of Human-AI Collaborationfor knowledge co-construction in authentic classroom settings and exemplify howAI could shape the future landscape of epistemic practices in the classroom.",
        "title": "Collaborative Learning with Artificial Intelligence Speakers (CLAIS):  Pre-Service Elementary Science Teachers' Responses to the Prototype",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05401",
        "abstract_url": "http://arxiv.org/abs/2401.05401",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xisheng"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Song",
                "first_name": "Pinhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingjun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The inherent characteristics and light fluctuations of water bodies give riseto the huge difference between different layers and regions in underwaterenvironments. When the test set is collected in a different marine area fromthe training set, the issue of domain shift emerges, significantly compromisingthe model's ability to generalize. The Domain Adversarial Learning (DAL)training strategy has been previously utilized to tackle such challenges.However, DAL heavily depends on manually one-hot domain labels, which impliesno difference among the samples in the same domain. Such an assumption resultsin the instability of DAL. This paper introduces the concept of DomainSimilarity-Perceived Label Assignment (DSP). The domain label for each image isregarded as its similarity to the specified domains. Through domain-specificdata augmentation techniques, we achieved state-of-the-art results on theunderwater cross-domain object detection benchmark S-UODAC2020. Furthermore, wevalidated the effectiveness of our method in the Cityscapes dataset.",
        "title": "Domain Similarity-Perceived Label Assignment for Domain Generalized  Underwater Object Detection",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05402",
        "abstract_url": "http://arxiv.org/abs/2401.05402",
        "authors": [
            {
                "last_name": "Klipfel",
                "first_name": "Astrid"
            },
            {
                "last_name": "Fregier",
                "first_name": "Ya\u00ebl"
            },
            {
                "last_name": "Sayede",
                "first_name": "Adlane"
            },
            {
                "last_name": "Bouraoui",
                "first_name": "Zied"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Discovering crystal structures with specific chemical properties has becomean increasingly important focus in material science. However, current modelsare limited in their ability to generate new crystal lattices, as they onlyconsider atomic positions or chemical composition. To address this issue, wepropose a probabilistic diffusion model that utilizes a geometricallyequivariant GNN to consider atomic positions and crystal lattices jointly. Toevaluate the effectiveness of our model, we introduce a new generation metricinspired by Frechet Inception Distance, but based on GNN energy predictionrather than InceptionV3 used in computer vision. In addition to commonly usedmetrics like validity, which assesses the plausibility of a structure, this newmetric offers a more comprehensive evaluation of our model's capabilities. Ourexperiments on existing benchmarks show the significance of our diffusionmodel. We also show that our method can effectively learn meaningfulrepresentations.",
        "title": "Vector Field Oriented Diffusion Model for Crystal Material Generation",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05403",
        "abstract_url": "http://arxiv.org/abs/2401.05403",
        "authors": [
            {
                "last_name": "Honghu",
                "first_name": "Yi"
            },
            {
                "last_name": "Ting",
                "first_name": "Liu"
            },
            {
                "last_name": "Gongjin",
                "first_name": "Lan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  Artificial Intelligence (AI) technologies have been applied in variousdomains, including early childhood education (ECE). Integration of AIeducational technology is a recent significant trend in ECE. Currently, thereare more and more studies of AI in ECE. To date, there is a lack of surveyarticles that discuss the studies of AI in ECE. In this paper, we provide anup-to-date and in-depth overview of the key AI technologies in ECE thatprovides a historical perspective, summarizes the representative works,outlines open questions, discusses the trends and challenges through a detailedbibliometric analysis, and provides insightful recommendations for futureresearch. We mainly discuss the studies that apply AI-based robots and AItechnologies to ECE, including improving the social interaction of childrenwith an autism spectrum disorder. This paper significantly contributes toprovide an up-to-date and in-depth survey that is suitable as introductorymaterial for beginners to AI in ECE, as well as supplementary material foradvanced users.",
        "title": "The Key Artificial Intelligence Technologies in Early Childhood  Education: A Review",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05405",
        "abstract_url": "http://arxiv.org/abs/2401.05405",
        "authors": [
            {
                "last_name": "Del Pup",
                "first_name": "Federico"
            },
            {
                "last_name": "Zanola",
                "first_name": "Andrea"
            },
            {
                "last_name": "Tshimanga",
                "first_name": "Louis Fabrice"
            },
            {
                "last_name": "Mazzon",
                "first_name": "Paolo Emilio"
            },
            {
                "last_name": "Atzori",
                "first_name": "Manfredo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  SelfEEG is an open-source Python library developed to assist researchers inconducting Self-Supervised Learning (SSL) experiments on electroencephalography(EEG) data. Its primary objective is to offer a user-friendly but highlycustomizable environment, enabling users to efficiently design and executeself-supervised learning tasks on EEG data.  SelfEEG covers all the stages of a typical SSL pipeline, ranging from dataimport to model design and training. It includes modules specifically designedto: split data at various granularity levels (e.g., session-, subject-, ordataset-based splits); effectively manage data stored with differentconfigurations (e.g., file extensions, data types) during mini-batchconstruction; provide a wide range of standard deep learning models, dataaugmentations and SSL baseline methods applied to EEG data.  Most of the functionalities offered by selfEEG can be executed both on GPUsand CPUs, expanding its usability beyond the self-supervised learning area.Additionally, these functionalities can be employed for the analysis of otherbiomedical signals often coupled with EEGs, such as electromyography orelectrocardiography data.  These features make selfEEG a versatile deep learning tool for biomedicalapplications and a useful resource in SSL, one of the currently most activefields of Artificial Intelligence.",
        "title": "SelfEEG: A Python library for Self-Supervised Learning in  Electroencephalography",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05406",
        "abstract_url": "http://arxiv.org/abs/2401.05406",
        "authors": [
            {
                "last_name": "Rosen",
                "first_name": "Daniel"
            },
            {
                "last_name": "Rochez",
                "first_name": "Illa"
            },
            {
                "last_name": "McIrvin",
                "first_name": "Caleb"
            },
            {
                "last_name": "Lee",
                "first_name": "Joshua"
            },
            {
                "last_name": "D'Alessandro",
                "first_name": "Kevin"
            },
            {
                "last_name": "Wiecek",
                "first_name": "Max"
            },
            {
                "last_name": "Hoang",
                "first_name": "Nhan"
            },
            {
                "last_name": "Saffarini",
                "first_name": "Ramzy"
            },
            {
                "last_name": "Philips",
                "first_name": "Sam"
            },
            {
                "last_name": "Jones",
                "first_name": "Vanessa"
            },
            {
                "last_name": "Ivey",
                "first_name": "Will"
            },
            {
                "last_name": "Harris-Smart",
                "first_name": "Zavier"
            },
            {
                "last_name": "Harris-Smart",
                "first_name": "Zavion"
            },
            {
                "last_name": "Chin",
                "first_name": "Zayden"
            },
            {
                "last_name": "Johnson",
                "first_name": "Amos"
            },
            {
                "last_name": "Jones",
                "first_name": "Alyse M."
            },
            {
                "last_name": "Headley",
                "first_name": "William C."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            "NI"
        ],
        "abstract": "  Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widelyapplicable technology in the next generation of wireless communication systems,particularly 6G and next-gen military communications. Given this, our researchis focused on developing a tool to promote the development of RFRL techniquesthat leverage spectrum sensing. In particular, the tool was designed to addresstwo cognitive radio applications, specifically dynamic spectrum access andjamming. In order to train and test reinforcement learning (RL) algorithms forthese applications, a simulation environment is necessary to simulate theconditions that an agent will encounter within the Radio Frequency (RF)spectrum. In this paper, such an environment has been developed, hereinreferred to as the RFRL Gym. Through the RFRL Gym, users can design their ownscenarios to model what an RL agent may encounter within the RF spectrum aswell as experiment with different spectrum sensing techniques. Additionally,the RFRL Gym is a subclass of OpenAI gym, enabling the use of third-party ML/RLLibraries. We plan to open-source this codebase to enable other researchers toutilize the RFRL Gym to test their own scenarios and RL algorithms, ultimatelyleading to the advancement of RL research in the wireless communicationsdomain. This paper describes in further detail the components of the Gym,results from example scenarios, and plans for future additions.  Index Terms-machine learning, reinforcement learning, wirelesscommunications, dynamic spectrum access, OpenAI gym",
        "title": "RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio  Applications",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05407",
        "abstract_url": "http://arxiv.org/abs/2401.05407",
        "authors": [
            {
                "last_name": "Koffi",
                "first_name": "Tresor Y."
            },
            {
                "last_name": "Mourchid",
                "first_name": "Youssef"
            },
            {
                "last_name": "Hindawi",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Dupuis",
                "first_name": "Yohan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Falls among individuals, especially the elderly population, can lead toserious injuries and complications. Detecting impact moments within a fallevent is crucial for providing timely assistance and minimizing the negativeconsequences. In this work, we aim to address this challenge by applyingthorough preprocessing techniques to the multisensor dataset, the goal is toeliminate noise and improve data quality. Furthermore, we employ a featureselection process to identify the most relevant features derived from themultisensor UP-FALL dataset, which in turn will enhance the performance andefficiency of machine learning models. We then evaluate the efficiency ofvarious machine learning models in detecting the impact moment using theresulting data information from multiple sensors. Through extensiveexperimentation, we assess the accuracy of our approach using variousevaluation metrics. Our results achieve high accuracy rates in impactdetection, showcasing the power of leveraging multisensor data for falldetection tasks. This highlights the potential of our approach to enhance falldetection systems and improve the overall safety and well-being of individualsat risk of falls.",
        "title": "Machine Learning and Feature Ranking for Impact Fall Detection Event  Using Multisensor Data",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05408",
        "abstract_url": "http://arxiv.org/abs/2401.05408",
        "authors": [
            {
                "last_name": "Grzeszczyk",
                "first_name": "Michal K."
            },
            {
                "last_name": "Lisowska",
                "first_name": "Anna"
            },
            {
                "last_name": "Sitek",
                "first_name": "Arkadiusz"
            },
            {
                "last_name": "Lisowska",
                "first_name": "Aneta"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Automatic detection and tracking of emotional states has the potential forhelping individuals with various mental health conditions. While previousstudies have captured physiological signals using wearable devices inlaboratory settings, providing valuable insights into the relationship betweenphysiological responses and mental states, the transfer of these findings toreal-life scenarios is still in its nascent stages. Our research aims to bridgethe gap between laboratory-based studies and real-life settings by leveragingconsumer-grade wearables and self-report measures. We conducted a preliminarystudy involving 15 healthy participants to assess the efficacy of wearables incapturing user valence in real-world settings. In this paper, we present theinitial analysis of the collected data, focusing primarily on the results ofvalence classification. Our findings demonstrate promising results indistinguishing between high and low positive valence, achieving an F1 score of0.65. This research opens up avenues for future research in the field of mobilemental health interventions.",
        "title": "Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True  Feelings?",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05409",
        "abstract_url": "http://arxiv.org/abs/2401.05409",
        "authors": [
            {
                "last_name": "Maiwald",
                "first_name": "Aaron"
            },
            {
                "last_name": "Ackermann",
                "first_name": "Leon"
            },
            {
                "last_name": "Kalcher",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Wu",
                "first_name": "Daniel J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Alternative data representations are powerful tools that augment theperformance of downstream models. However, there is an abundance of suchrepresentations within the machine learning toolbox, and the field lacks acomparative understanding of the suitability of each representation method.  In this paper, we propose artifact detection and classification within EEGdata as a testbed for profiling image-based data representations of time seriesdata. We then evaluate eleven popular deep learning architectures on each ofsix commonly-used representation methods.  We find that, while the choice of representation entails a choice within thetradeoff between bias and variance, certain representations are practicallymore effective in highlighting features which increase the signal-to-noiseratio of the data. We present our results on EEG data, and open-source ourtesting framework to enable future comparative analyses in this vein.",
        "title": "Image-based Data Representations of Time Series: A Comparative Analysis  in EEG Artifact Detection",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05410",
        "abstract_url": "http://arxiv.org/abs/2401.05410",
        "authors": [
            {
                "last_name": "Laham",
                "first_name": "Saria Al"
            },
            {
                "last_name": "Baghi",
                "first_name": "Bobak H."
            },
            {
                "last_name": "Lajoie",
                "first_name": "Pierre-Yves"
            },
            {
                "last_name": "Feriani",
                "first_name": "Amal"
            },
            {
                "last_name": "Herath",
                "first_name": "Sachini"
            },
            {
                "last_name": "Liu",
                "first_name": "Steve"
            },
            {
                "last_name": "Dudek",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CY",
            "LG",
            "NI"
        ],
        "abstract": "  We present a human state estimation framework that allows us to estimate thelocation, and even the activities, of people in an indoor environment withoutthe requirement that they carry a specific devices with them. To achieve this\"device free\" localization we use a small number of low-cost Ultra-Wide Band(UWB) sensors distributed across the environment of interest. To achieve highquality estimation from the UWB signals merely reflected of people in theenvironment, we exploit a deep network that can learn to make inferences. Thehardware setup consists of commercial off-the-shelf (COTS) single antenna UWBmodules for sensing, paired with Raspberry PI units for computationalprocessing and data transfer. We make use of the channel impulse response (CIR)measurements from the UWB sensors to estimate the human state - comprised oflocation and activity - in a given area. Additionally, we can also estimate thenumber of humans that occupy this region of interest. In our approach, first,we pre-process the CIR data which involves meticulous aggregation ofmeasurements and extraction of key statistics. Afterwards, we leverage aconvolutional deep neural network to map the CIRs into precise locationestimates with sub-30 cm accuracy. Similarly, we achieve accurate humanactivity recognition and occupancy counting results. We show that we canquickly fine-tune our model for new out-of-distribution users, a process thatrequires only a few minutes of data and a few epochs of training. Our resultsshow that UWB is a promising solution for adaptable smart-home localization andactivity recognition problems.",
        "title": "Device-Free Human State Estimation using UWB Multi-Static Radios",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05411",
        "abstract_url": "http://arxiv.org/abs/2401.05411",
        "authors": [
            {
                "last_name": "Ben-Moshe",
                "first_name": "Noam"
            },
            {
                "last_name": "Tsutsui",
                "first_name": "Kenta"
            },
            {
                "last_name": "Biton",
                "first_name": "Shany"
            },
            {
                "last_name": "S\u00f6rnmo",
                "first_name": "Leif"
            },
            {
                "last_name": "Behar",
                "first_name": "Joachim A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Introduction: Deep learning models for detecting episodes of atrialfibrillation (AF) using rhythm information in long-term, ambulatory ECGrecordings have shown high performance. However, the rhythm-based approach doesnot take advantage of the morphological information conveyed by the differentECG waveforms, particularly the f-waves. As a result, the performance of suchmodels may be inherently limited. Methods: To address this limitation, we havedeveloped a deep learning model, named RawECGNet, to detect episodes of AF andatrial flutter (AFl) using the raw, single-lead ECG. We compare thegeneralization performance of RawECGNet on two external data sets that accountfor distribution shifts in geography, ethnicity, and lead position. RawECGNetis further benchmarked against a state-of-the-art deep learning model, namedArNet2, which utilizes rhythm information as input. Results: Using RawECGNet,the results for the different leads in the external test sets in terms of theF1 score were 0.91--0.94 in RBDB and 0.93 in SHDB, compared to 0.89--0.91 inRBDB and 0.91 in SHDB for ArNet2. The results highlight RawECGNet as ahigh-performance, generalizable algorithm for detection of AF and AFl episodes,exploiting information on both rhythm and morphology.",
        "title": "RawECGNet: Deep Learning Generalization for Atrial Fibrillation  Detection from the Raw ECG",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05412",
        "abstract_url": "http://arxiv.org/abs/2401.05412",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xueyuan"
            },
            {
                "last_name": "Yao",
                "first_name": "Chao"
            },
            {
                "last_name": "Ban",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            ""
        ],
        "abstract": "  Leveraging wearable devices for motion reconstruction has emerged as aneconomical and viable technique. Certain methodologies employ sparse InertialMeasurement Units (IMUs) on the human body and harness data-driven strategiesto model human poses. However, the reconstruction of motion based solely onsparse IMUs data is inherently fraught with ambiguity, a consequence ofnumerous identical IMU readings corresponding to different poses. In thispaper, we explore the spatial importance of multiple sensors, supervised bytext that describes specific actions. Specifically, uncertainty is introducedto derive weighted features for each IMU. We also design a HierarchicalTemporal Transformer (HTT) and apply contrastive learning to achieve precisetemporal and feature alignment of sensor data with textual semantics.Experimental results demonstrate our proposed approach achieves significantimprovements in multiple metrics compared to existing methods. Notably, withtextual supervision, our method not only differentiates between ambiguousactions such as sitting and standing but also produces more precise and naturalmotion.",
        "title": "Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted  with Textual Semantics",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05414",
        "abstract_url": "http://arxiv.org/abs/2401.05414",
        "authors": [
            {
                "last_name": "Dong",
                "first_name": "Xinshuai"
            },
            {
                "last_name": "Dai",
                "first_name": "Haoyue"
            },
            {
                "last_name": "Fan",
                "first_name": "Yewen"
            },
            {
                "last_name": "Jin",
                "first_name": "Songyao"
            },
            {
                "last_name": "Rajendran",
                "first_name": "Sathyamoorthy"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kun"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Financial data is generally time series in essence and thus suffers fromthree fundamental issues: the mismatch in time resolution, the time-varyingproperty of the distribution - nonstationarity, and causal factors that areimportant but unknown/unobserved. In this paper, we follow a causal perspectiveto systematically look into these three demons in finance. Specifically, wereexamine these issues in the context of causality, which gives rise to a noveland inspiring understanding of how the issues can be addressed. Following thisperspective, we provide systematic solutions to these problems, which hopefullywould serve as a foundation for future research in the area.",
        "title": "On the Three Demons in Causality in Finance: Time Resolution,  Nonstationarity, and Latent Factors",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05415",
        "abstract_url": "http://arxiv.org/abs/2401.05415",
        "authors": [
            {
                "last_name": "Bano",
                "first_name": "Muneera"
            },
            {
                "last_name": "Chaudhri",
                "first_name": "Zahid"
            },
            {
                "last_name": "Zowghi",
                "first_name": "Didar"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  As Artificial Intelligence (AI) transforms the domain of diplomacy in the21st century, this research addresses the pressing need to evaluate thedualistic nature of these advancements, unpacking both the challenges they poseand the opportunities they offer. It has been almost a year since the launch ofChatGPT by OpenAI that revolutionised various work domains with itscapabilities. The scope of application of these capabilities to diplomacy isyet to be fully explored or understood. Our research objective is tosystematically examine the current discourse on Digital and AI Diplomacy, thusinforming the development of a comprehensive framework for the role ofGenerative AI in modern diplomatic practices. Through the systematic analysisof 230 scholarly articles, we identified a spectrum of opportunities andchallenges, culminating in a strategic framework that captures the multifacetedconcepts for integration of Generative AI, setting a course for future researchand innovation in diplomacy.",
        "title": "The Role of Generative AI in Global Diplomatic Practices: A Strategic  Framework",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05416",
        "abstract_url": "http://arxiv.org/abs/2401.05416",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yifeng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  As attitude and motion sensing components, inertial sensors are widely usedin various portable devices. But the severe errors of inertial sensors restraintheir function, especially the trajectory recovery and semantic recognition. Asa mainstream signal processing method, wavelet is hailed as the mathematicalmicroscope of signal due to the plentiful and diverse wavelet basis functions.However, complicated noise types and application scenarios of inertial sensorsmake selecting wavelet basis perplexing. To this end, we propose a waveletdynamic selection network (WDSNet), which intelligently selects the appropriatewavelet basis for variable inertial signals. In addition, existing deeplearning architectures excel at extracting features from input data but neglectto learn the characteristics of target categories, which is essential toenhance the category awareness capability, thereby improving the selection ofwavelet basis. Therefore, we propose a category representation mechanism (CRM),which enables the network to extract and represent category features withoutincreasing trainable parameters. Furthermore, CRM transforms the common fullyconnected network into category representations, which provide closersupervision to the feature extractor than the far and trivial one-hotclassification labels. We call this process of imposing interpretability on anetwork and using it to supervise the feature extractor the feature supervisionmechanism, and its effectiveness is demonstrated experimentally andtheoretically in this paper. The enhanced inertial signal can performimpracticable tasks with regard to the original signal, such as trajectoryreconstruction. Both quantitative and visual results show that WDSNetoutperforms the existing methods. Remarkably, WDSNet, as a weakly-supervisedmethod, achieves the state-of-the-art performance of all the comparedfully-supervised methods.",
        "title": "Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05418",
        "abstract_url": "http://arxiv.org/abs/2401.05418",
        "authors": [
            {
                "last_name": "Haidri",
                "first_name": "Salman"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "IR",
            "LG"
        ],
        "abstract": "  The advent of compact, handheld devices has given us a pool of trackedmovement data that could be used to infer trends and patterns that can be madeto use. With this flooding of various trajectory data of animals, humans,vehicles, etc., the idea of ANALYTiC originated, using active learning to infersemantic annotations from the trajectories by learning from sets of labeleddata. This study explores the application of dimensionality reduction anddecision boundaries in combination with the already present active learning,highlighting patterns and clusters in data. We test these features with threedifferent trajectory datasets with objective of exploiting the the alreadylabeled data and enhance their interpretability. Our experimental analysisexemplifies the potential of these combined methodologies in improving theefficiency and accuracy of trajectory labeling. This study serves as astepping-stone towards the broader integration of machine learning and visualmethods in context of movement data analysis.",
        "title": "ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction  in Machine Learning",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05420",
        "abstract_url": "http://arxiv.org/abs/2401.05420",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Debamita"
            },
            {
                "last_name": "Hanawal",
                "first_name": "Manjesh Kumar"
            },
            {
                "last_name": "Zlatanova",
                "first_name": "Nikola"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Holographic Metasurface Transceivers (HMTs) are emerging as cost-effectivesubstitutes to large antenna arrays for beamforming in Millimeter and TeraHertzwave communication. However, to achieve desired channel gains throughbeamforming in HMT, phase-shifts of a large number of elements need to beappropriately set, which is challenging. Also, these optimal phase-shiftsdepend on the location of the receivers, which could be unknown. In this work,we develop a learning algorithm using a {\\it fixed-budget multi-armed banditframework} to beamform and maximize received signal strength at the receiverfor far-field regions. Our algorithm, named \\Algo exploits the parametric formof channel gains of the beams, which can be expressed in terms of two {\\itphase-shifting parameters}. Even after parameterization, the problem is stillchallenging as phase-shifting parameters take continuous values. To overcomethis, {\\it\\HB} works with the discrete values of phase-shifting parameters andexploits their unimodal relations with channel gains to learn the optimalvalues faster. We upper bound the probability of {\\it\\HB} incorrectlyidentifying the (discrete) optimal phase-shift parameters in terms of thenumber of pilots used in learning. We show that this probability decaysexponentially with the number of pilot signals. We demonstrate that {\\it\\HB}outperforms state-of-the-art algorithms through extensive simulations.",
        "title": "HoloBeam: Learning Optimal Beamforming in Far-Field Holographic  Metasurface Transceivers",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05421",
        "abstract_url": "http://arxiv.org/abs/2401.05421",
        "authors": [
            {
                "last_name": "Al-Lawati",
                "first_name": "Ali"
            },
            {
                "last_name": "Eshra",
                "first_name": "Elsayed"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  Trajectory generation is an important concern in pedestrian, vehicle, andwildlife movement studies. Generated trajectories help enrich the trainingcorpus in relation to deep learning applications, and may be used to facilitatesimulation tasks. This is especially significant in the wildlife domain, wherethe cost of obtaining additional real data can be prohibitively expensive,time-consuming, and bear ethical considerations. In this paper, we introduceWildGEN: a conceptual framework that addresses this challenge by employing aVariational Auto-encoders (VAEs) based method for the acquisition of movementcharacteristics exhibited by wild geese over a long horizon using a sparse setof truth samples. A subsequent post-processing step of the generatedtrajectories is performed based on smoothing filters to reduce excessivewandering. Our evaluation is conducted through visual inspection and thecomputation of the Hausdorff distance between the generated and realtrajectories. In addition, we utilize the Pearson Correlation Coefficient as away to measure how realistic the trajectories are based on the similarity ofclusters evaluated on the generated and real trajectories.",
        "title": "WildGEN: Long-horizon Trajectory Generation for Wildlife",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05422",
        "abstract_url": "http://arxiv.org/abs/2401.05422",
        "authors": [
            {
                "last_name": "M",
                "first_name": "Karthik R"
            },
            {
                "last_name": "Hegde",
                "first_name": "Dhiraj Nagaraja"
            },
            {
                "last_name": "Sarajlic",
                "first_name": "Muris"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Beam management (BM) protocols are critical for establishing and maintainingconnectivity between network radio nodes and User Equipments (UEs). InDistributed Multiple Input Multiple Output systems (D-MIMO), a number of accesspoints (APs), coordinated by a central processing unit (CPU), serves a numberof UEs. At mmWave frequencies, the problem of finding the best AP and beam toserve the UEs is challenging due to a large number of beams that need to besounded with Downlink (DL) reference signals. The objective of this paper is toinvestigate whether the best AP/beam can be reliably inferred from soundingonly a small subset of beams and leveraging AI/ML for inference of bestbeam/AP. We use Random Forest (RF), MissForest (MF) and conditional GenerativeAdversarial Networks (c-GAN) for demonstrating the performance benefits ofinference.",
        "title": "Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave  Distributed Multiple Input Multiple Output (D-MIMO) systems",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05424",
        "abstract_url": "http://arxiv.org/abs/2401.05424",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Djemili",
                "first_name": "Karim"
            },
            {
                "last_name": "Elezi",
                "first_name": "Denis"
            },
            {
                "last_name": "Shalman",
                "first_name": "Aaneel"
            },
            {
                "last_name": "P\u00e9rez-Ortiz",
                "first_name": "Mar\u00eda"
            },
            {
                "last_name": "Yilmaz",
                "first_name": "Emine"
            },
            {
                "last_name": "Shawe-Taylor",
                "first_name": "John"
            },
            {
                "last_name": "Bulathwela",
                "first_name": "Sahan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "IR",
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  With the advancement and utility of Artificial Intelligence (AI),personalising education to a global population could be a cornerstone of neweducational systems in the future. This work presents the PEEKC dataset and theTrueLearn Python library, which contains a dataset and a series of onlinelearner state models that are essential to facilitate research on learnerengagement modelling.TrueLearn family of models was designed following the\"open learner\" concept, using humanly-intuitive user representations. Thisfamily of scalable, online models also help end-users visualise the learnermodels, which may in the future facilitate user interaction with theirmodels/recommenders. The extensive documentation and coding examples make thelibrary highly accessible to both machine learning developers and educationaldata mining and learning analytics practitioners. The experiments show theutility of both the dataset and the library with predictive performancesignificantly exceeding comparative baseline models. The dataset contains alarge amount of AI-related educational videos, which are of interest forbuilding and validating AI-specific educational recommenders.",
        "title": "A Toolbox for Modelling Engagement with Educational Videos",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05425",
        "abstract_url": "http://arxiv.org/abs/2401.05425",
        "authors": [
            {
                "last_name": "Aziz",
                "first_name": "Abdul"
            },
            {
                "last_name": "Pham",
                "first_name": "Nhat"
            },
            {
                "last_name": "Vora",
                "first_name": "Neel"
            },
            {
                "last_name": "Reynolds",
                "first_name": "Cody"
            },
            {
                "last_name": "Lehnen",
                "first_name": "Jaime"
            },
            {
                "last_name": "Venkatesh",
                "first_name": "Pooja"
            },
            {
                "last_name": "Yao",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Harvey",
                "first_name": "Jay"
            },
            {
                "last_name": "Vu",
                "first_name": "Tam"
            },
            {
                "last_name": "Ding",
                "first_name": "Kan"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Phuc"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Epilepsy is one of the most common neurological diseases globally, affectingaround 50 million people worldwide. Fortunately, up to 70 percent of peoplewith epilepsy could live seizure-free if properly diagnosed and treated, and areliable technique to monitor the onset of seizures could improve the qualityof life of patients who are constantly facing the fear of random seizureattacks. The scalp-based EEG test, despite being the gold standard fordiagnosing epilepsy, is costly, necessitates hospitalization, demands skilledprofessionals for operation, and is discomforting for users. In this paper, wepropose EarSD, a novel lightweight, unobtrusive, and socially acceptableear-worn system to detect epileptic seizure onsets by measuring thephysiological signals from behind the user's ears. EarSD includes an integratedcustom-built sensing, computing, and communication PCB to collect and amplifythe signals of interest, remove the noises caused by motion artifacts andenvironmental impacts, and stream the data wirelessly to the computer or mobilephone nearby, where data are uploaded to the host computer for furtherprocessing. We conducted both in-lab and in-hospital experiments with epilepticseizure patients who were hospitalized for seizure studies. The preliminaryresults confirm that EarSD can detect seizures with up to 95.3 percent accuracyby just using classical machine learning algorithms.",
        "title": "An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic  Seizure Detection",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05426",
        "abstract_url": "http://arxiv.org/abs/2401.05426",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zimin"
            },
            {
                "last_name": "Gei\u00dfler",
                "first_name": "Daniel"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Suh",
                "first_name": "Sungho"
            },
            {
                "last_name": "Lukowicz",
                "first_name": "Paul"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Recent advancements in Artificial Neural Networks have significantly improvedhuman activity recognition using multiple time-series sensors. While employingnumerous sensors with high-frequency sampling rates usually improves theresults, it often leads to data inefficiency and unnecessary expansion of theANN, posing a challenge for their practical deployment on edge devices.Addressing these issues, our work introduces a pragmatic framework fordata-efficient utilization in HAR tasks, considering the optimization of bothsensor modalities and sampling rate simultaneously. Central to our approach arethe designed trainable parameters, termed 'Weight Scores,' which assess thesignificance of each sensor modality and sampling rate during the trainingphase. These scores guide the sensor modalities and sampling rate selection.The pruning method allows users to make a trade-off between computationalbudgets and performance by selecting the sensor modalities and sampling ratesaccording to the weight score ranking. We tested our framework's effectivenessin optimizing sensor modality and sampling rate selection using three publicHAR benchmark datasets. The results show that the sensor and sampling ratecombination selected via CoSS achieves similar classification performance toconfigurations using the highest sampling rate with all sensors but at areduced hardware cost.",
        "title": "CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in  Human Activity Recognition",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05427",
        "abstract_url": "http://arxiv.org/abs/2401.05427",
        "authors": [
            {
                "last_name": "van Putten",
                "first_name": "Maurice H. P. M."
            },
            {
                "last_name": "Wilson",
                "first_name": "Leighton"
            },
            {
                "last_name": "Lavely",
                "first_name": "Adam W."
            },
            {
                "last_name": "Hair",
                "first_name": "Mark"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "DC",
            "DS"
        ],
        "abstract": "  Searches for signals at low signal-to-noise ratios frequently involve theFast Fourier Transform (FFT). For high-throughput searches, we here considerFFT on the homogeneous mesh of Processing Elements (PEs) of a wafer-scaleengine (WSE). To minimize memory overhead in the inherently non-local FFTalgorithm, we introduce a new synchronous slide operation ({\\em Slide})exploiting the fast interconnect between adjacent PEs. Feasibility ofcompute-limited performance is demonstrated in linear scaling of Slideexecution times with varying array size in preliminary benchmarks on the CS-2WSE. The proposed implementation appears opportune to accelerate and open thefull discovery potential of FFT-based signal processing in multi-messengerastronomy.",
        "title": "Slide FFT on a homogeneous mesh in wafer-scale computing",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05430",
        "abstract_url": "http://arxiv.org/abs/2401.05430",
        "authors": [
            {
                "last_name": "You",
                "first_name": "Zinuo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Pengju"
            },
            {
                "last_name": "Zheng",
                "first_name": "Jin"
            },
            {
                "last_name": "Cartlidge",
                "first_name": "John"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "NE"
        ],
        "abstract": "  Stock trend classification remains a fundamental yet challenging task, owingto the intricate time-evolving dynamics between and within stocks. To tacklethese two challenges, we propose a graph-based representation learning approachaimed at predicting the future movements of multiple stocks. Initially, wemodel the complex time-varying relationships between stocks by generatingdynamic multi-relational stock graphs. This is achieved through a novel edgegeneration algorithm that leverages information entropy and signal energy toquantify the intensity and directionality of inter-stock relations on eachtrading day. Then, we further refine these initial graphs through a stochasticmulti-relational diffusion process, adaptively learning task-optimal edges.Subsequently, we implement a decoupled representation learning scheme withparallel retention to obtain the final graph representation. This strategybetter captures the unique temporal features within individual stocks whilealso capturing the overall structure of the stock graph. Comprehensiveexperiments conducted on real-world datasets from two US markets (NASDAQ andNYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate theeffectiveness of our method. Our approach consistently outperformsstate-of-the-art baselines in forecasting next trading day stock trends acrossthree test periods spanning seven years. Datasets and code have been released(https://github.com/pixelhero98/MGDPR).",
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention  for Stock Trends Classification",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05431",
        "abstract_url": "http://arxiv.org/abs/2401.05431",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Luyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Cong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhai",
                "first_name": "Shengfang"
            },
            {
                "last_name": "Fang",
                "first_name": "Yuejian"
            },
            {
                "last_name": "Shen",
                "first_name": "Qingni"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhonghai"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Representation learning frameworks in unlabeled time series have beenproposed for medical signal processing. Despite the numerous excellentprogresses have been made in previous works, we observe the representationextracted for the time series still does not generalize well. In this paper, wepresent a Time series (medical signal) Representation Learning framework viaSpectrogram (TRLS) to get more informative representations. We transform theinput time-domain medical signals into spectrograms and design a time-frequencyencoder named Time Frequency RNN (TFRNN) to capture more robust multi-scalerepresentations from the augmented spectrograms. Our TRLS takes spectrogram asinput with two types of different data augmentations and maximizes thesimilarity between positive ones, which effectively circumvents the problem ofdesigning negative samples. Our evaluation of four real-world medical signaldatasets focusing on medical signal classification shows that TRLS is superiorto the existing frameworks.",
        "title": "TRLS: A Time Series Representation Learning Framework via Spectrogram  for Medical Signal Processing",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05432",
        "abstract_url": "http://arxiv.org/abs/2401.05432",
        "authors": [
            {
                "last_name": "Hossain",
                "first_name": "Khondoker Murad"
            },
            {
                "last_name": "Oates",
                "first_name": "Tim"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CR"
        ],
        "abstract": "  As deep neural networks and the datasets used to train them get larger, thedefault approach to integrating them into research and commercial projects isto download a pre-trained model and fine tune it. But these models can haveuncertain provenance, opening up the possibility that they embed hiddenmalicious behavior such as trojans or backdoors, where small changes to aninput (triggers) can cause the model to produce incorrect outputs (e.g., tomisclassify). This paper introduces a novel approach to backdoor detection thatuses two tensor decomposition methods applied to network activations. This hasa number of advantages relative to existing detection methods, including theability to analyze multiple models at the same time, working across a widevariety of network architectures, making no assumptions about the nature oftriggers used to alter network behavior, and being computationally efficient.We provide a detailed description of the detection pipeline along with resultson models trained on the MNIST digit dataset, CIFAR-10 dataset, and twodifficult datasets from NIST's TrojAI competition. These results show that ourmethod detects backdoored networks more accurately and efficiently than currentstate-of-the-art methods.",
        "title": "TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep  Neural Networks",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05433",
        "abstract_url": "http://arxiv.org/abs/2401.05433",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Che",
                "first_name": "Chang"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Liu",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The objective of this study is to improve automated feedback tools designedfor English Language Learners (ELLs) through the utilization of data sciencetechniques encompassing machine learning, natural language processing, andeducational data analytics. Automated essay scoring (AES) research has madestrides in evaluating written essays, but it often overlooks the specific needsof English Language Learners (ELLs) in language development. This studyexplores the application of BERT-related techniques to enhance the assessmentof ELLs' writing proficiency within AES.  To address the specific needs of ELLs, we propose the use of DeBERTa, astate-of-the-art neural language model, for improving automated feedback tools.DeBERTa, pretrained on large text corpora using self-supervised learning,learns universal language representations adaptable to various natural languageunderstanding tasks. The model incorporates several innovative techniques,including adversarial training through Adversarial Weights Perturbation (AWP)and Metric-specific AttentionPooling (6 kinds of AP) for each label in thecompetition.  The primary focus of this research is to investigate the impact ofhyperparameters, particularly the adversarial learning rate, on the performanceof the model. By fine-tuning the hyperparameter tuning process, including theinfluence of 6AP and AWP, the resulting models can provide more accurateevaluations of language proficiency and support tailored learning tasks forELLs. This work has the potential to significantly benefit ELLs by improvingtheir English language proficiency and facilitating their educational journey.",
        "title": "Enhancing Essay Scoring with Adversarial Weights Perturbation and  Metric-specific AttentionPooling",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05434",
        "abstract_url": "http://arxiv.org/abs/2401.05434",
        "authors": [
            {
                "last_name": "Akan",
                "first_name": "Taymaz"
            },
            {
                "last_name": "Alp",
                "first_name": "Sait"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Mohammad Alfrad Nobel"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat.There are various types of arrhythmias that can originate from different areasof the heart, resulting in either a rapid, slow, or irregular heartbeat. Anelectrocardiogram (ECG) is a vital diagnostic tool used to detect heartirregularities and abnormalities, allowing experts to analyze the heart'selectrical signals to identify intricate patterns and deviations from the norm.Over the past few decades, numerous studies have been conducted to developautomated methods for classifying heartbeats based on ECG data. In recentyears, deep learning has demonstrated exceptional capabilities in tacklingvarious medical challenges, particularly with transformers as a modelarchitecture for sequence processing. By leveraging the transformers, wedeveloped the ECGformer model for the classification of various arrhythmiaspresent in electrocardiogram data. We assessed the suggested approach using theMIT-BIH and PTB datasets. ECG heartbeat arrhythmia classification results showthat the proposed method is highly effective.",
        "title": "ECGformer: Leveraging transformer for ECG heartbeat arrhythmia  classification",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05436",
        "abstract_url": "http://arxiv.org/abs/2401.05436",
        "authors": [
            {
                "last_name": "Jameel",
                "first_name": "Abu Shafin Mohammad Mahdee"
            },
            {
                "last_name": "Malhotra",
                "first_name": "Akshay"
            },
            {
                "last_name": "Gamal",
                "first_name": "Aly El"
            },
            {
                "last_name": "Hamidi-Rad",
                "first_name": "Shahab"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "NI"
        ],
        "abstract": "  In this paper, we propose a deep-learning-based channel estimation scheme inan orthogonal frequency division multiplexing (OFDM) system. Our proposedmethod, named Single Slot Recurrence Along Frequency Network (SisRafNet), isbased on a novel study of recurrent models for exploiting sequential behaviorof channels across frequencies. Utilizing the fact that wireless channels havea high degree of correlation across frequencies, we employ recurrent neuralnetwork techniques within a single OFDM slot, thus overcoming the latency andmemory constraints typically associated with recurrence based methods. Theproposed SisRafNet delivers superior estimation performance compared toexisting deep-learning-based channel estimation techniques and the performancehas been validated on a wide range of 3rd Generation Partnership Project (3GPP)compliant channel scenarios at multiple signal-to-noise ratios.",
        "title": "Deep OFDM Channel Estimation: Capturing Frequency Recurrence",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05437",
        "abstract_url": "http://arxiv.org/abs/2401.05437",
        "authors": [
            {
                "last_name": "Jungo",
                "first_name": "Janosch"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yutong"
            },
            {
                "last_name": "Gashi",
                "first_name": "Shkurta"
            },
            {
                "last_name": "Holz",
                "first_name": "Christian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Wearable devices continuously collect sensor data and use it to infer anindividual's behavior, such as sleep, physical activity, and emotions. Despitethe significant interest and advancements in this field, modeling multimodalsensor data in real-world environments is still challenging due to low dataquality and limited data annotations. In this work, we investigaterepresentation learning for imputing missing wearable data and compare it withstate-of-the-art statistical approaches. We investigate the performance of thetransformer model on 10 physiological and behavioral signals with differentmasking ratios. Our results show that transformers outperform baselines formissing data imputation of signals that change more frequently, but not formonotonic signals. We further investigate the impact of imputation strategiesand masking rations on downstream classification tasks. Our study providesinsights for the design and development of masking-based self-supervisedlearning tasks and advocates the adoption of hybrid-based imputation strategiesto address the challenge of missing data in wearable devices.",
        "title": "Representation Learning for Wearable-Based Applications in the Case of  Missing Data",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05438",
        "abstract_url": "http://arxiv.org/abs/2401.05438",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Jiayi"
            },
            {
                "last_name": "Hnatyshyn",
                "first_name": "Rostyslav"
            },
            {
                "last_name": "Santos",
                "first_name": "Ebrar A. D."
            },
            {
                "last_name": "Maciejewski",
                "first_name": "Ross"
            },
            {
                "last_name": "Isenberg",
                "first_name": "Tobias"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  We examine visual representations of data that make use of combinations ofboth 2D and 3D data mappings. Combining 2D and 3D representations is a commontechnique that allows viewers to understand multiple facets of the data withwhich they are interacting. While 3D representations focus on the spatialcharacter of the data or the dedicated 3D data mapping, 2D representationsoften show abstract data properties and take advantage of the unique benefitsof mapping to a plane. Many systems have used unique combinations of both typesof data mappings effectively. Yet there are no systematic reviews of themethods in linking 2D and 3D representations. We systematically survey therelationships between 2D and 3D visual representations in major visualizationpublications -- IEEE VIS, IEEE TVCG, and EuroVis -- from 2012 to 2022. Weclosely examined 105 papers where 2D and 3D representations are connectedvisually, interactively, or through animation. These approaches are designedbased on their visual environment, the relationships between their visualrepresentations, and their possible layouts. Through our analysis, we introducea design space as well as provide design guidelines for effectively linking 2Dand 3D visual representations.",
        "title": "A Survey of Designs for Combined 2D+3D Visual Representations",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05439",
        "abstract_url": "http://arxiv.org/abs/2401.05439",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Biao"
            },
            {
                "last_name": "Heitor",
                "first_name": "Ana"
            },
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaohui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  The emergence of neural networks constrained by physical governing equationshas sparked a new trend in deep learning research, which is known asPhysics-Informed Neural Networks (PINNs). However, solving high-dimensionalproblems with PINNs is still a substantial challenge, the space complexitybrings difficulty to solving large multidirectional problems. In this paper, anovel PINN framework to quickly predict several three-dimensional Terzaghiconsolidation cases under different conditions is proposed. Meanwhile, the lossfunctions for different cases are introduced, and their differences inthree-dimensional consolidation problems are highlighted. The tuning strategiesfor the PINNs framework for three-dimensional consolidation problems areintroduced. Then, the performance of PINNs is tested and compared withtraditional numerical methods adopted in forward problems, and the coefficientsof consolidation and the impact of noisy data in inverse problems areidentified. Finally, the results are summarized and presented fromthree-dimensional simulations of PINNs, which show an accuracy rate of over 99%compared with ground truth for both forward and inverse problems. These resultsare desirable with good accuracy and can be used for soil settlementprediction, which demonstrates that the proposed PINNs framework can learn thethree-dimensional consolidation PDE well.  Keywords: Three-dimensional Terzaghi consolidation; Physics-informed neuralnetworks (PINNs); Forward problems; Inverse problems; soil settlement",
        "title": "Physics-informed Deep Learning to Solve Three-dimensional Terzaghi  Consolidation Equation: Forward and Inverse Problems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05440",
        "abstract_url": "http://arxiv.org/abs/2401.05440",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Qian"
            },
            {
                "last_name": "Hao",
                "first_name": "Yanling"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanwei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            "LG"
        ],
        "abstract": "  WiFi human sensing is highly regarded for its low-cost and privacy advantagesin recognizing human activities. However, its effectiveness is largely confinedto controlled, single-user, line-of-sight settings, limited by data collectioncomplexities and the scarcity of labeled datasets. Traditional cross-modalmethods, aimed at mitigating these limitations by enabling self-supervisedlearning without labeled data, struggle to extract meaningful features fromamplitude-phase combinations. In response, we introduce AutoSen, an innovativeautomatic WiFi sensing solution that departs from conventional approaches.AutoSen establishes a direct link between amplitude and phase through automatedcross-modal autoencoder learning. This autoencoder efficiently extractsvaluable features from unlabeled CSI data, encompassing amplitude and phaseinformation while eliminating their respective unique noises. These featuresare then leveraged for specific tasks using few-shot learning techniques.AutoSen's performance is rigorously evaluated on a publicly accessiblebenchmark dataset, demonstrating its exceptional capabilities in automatic WiFisensing through the extraction of comprehensive cross-modal features.",
        "title": "Autosen: improving automatic wifi human sensing through cross-modal  autoencoder",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05441",
        "abstract_url": "http://arxiv.org/abs/2401.05441",
        "authors": [
            {
                "last_name": "Mehrban",
                "first_name": "Ali"
            },
            {
                "last_name": "Ahadian",
                "first_name": "Pegah"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "CR",
            "LG"
        ],
        "abstract": "  This paper describes an architecture for predicting the price ofcryptocurrencies for the next seven days using the Adaptive Network Based FuzzyInference System (ANFIS). Historical data of cryptocurrencies and indexes thatare considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D),and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teachthe data are hybrid and backpropagation algorithms, as well as grid partition,subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, whichare used in data clustering. The architectural performance designed in thispaper has been compared with different inputs and neural network models interms of statistical evaluation criteria. Finally, the proposed method canpredict the price of digital currencies in a short time.",
        "title": "An adaptive network-based approach for advanced forecasting of  cryptocurrency values",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05442",
        "abstract_url": "http://arxiv.org/abs/2401.05442",
        "authors": [
            {
                "last_name": "Kuba",
                "first_name": "Jakub Grudzien"
            },
            {
                "last_name": "Uehara",
                "first_name": "Masatoshi"
            },
            {
                "last_name": "Abbeel",
                "first_name": "Pieter"
            },
            {
                "last_name": "Levine",
                "first_name": "Sergey"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While machine learning models are typically trained to solve predictionproblems, we might often want to use them for optimization problems. Forexample, given a dataset of proteins and their corresponding fluorescencelevels, we might want to optimize for a new protein with the highest possiblefluorescence. This kind of data-driven optimization (DDO) presents a range ofchallenges beyond those in standard prediction problems, since we need modelsthat successfully predict the performance of new designs that are better thanthe best designs seen in the training set. It is not clear theoretically whenexisting approaches can even perform better than the naive approach that simplyselects the best design in the dataset. In this paper, we study how structurecan enable sample-efficient data-driven optimization. To formalize the notionof structure, we introduce functional graphical models (FGMs) and showtheoretically how they can provide for principled data-driven optimization bydecomposing the original high-dimensional optimization problem into smallersub-problems. This allows us to derive much more practical regret bounds forDDO, and the result implies that DDO with FGMs can achieve nearly optimaldesigns in situations where naive approaches fail due to insufficient coverageof the offline data. We further present a data-driven optimization algorithmthat inferes the FGM structure itself, either over the original input variablesor a latent variable representation of the inputs.",
        "title": "Functional Graphical Models: Structure Enables Offline Data-Driven  Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05443",
        "abstract_url": "http://arxiv.org/abs/2401.05443",
        "authors": [
            {
                "last_name": "Fakih",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Dharmaji",
                "first_name": "Rahul"
            },
            {
                "last_name": "Moghaddas",
                "first_name": "Yasamin"
            },
            {
                "last_name": "Araya",
                "first_name": "Gustavo Quiros"
            },
            {
                "last_name": "Ogundare",
                "first_name": "Oluwatosin"
            },
            {
                "last_name": "Faruque",
                "first_name": "Mohammad Abdullah Al"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "CL",
            "PL",
            "",
            "",
            ""
        ],
        "abstract": "  Although Large Language Models (LLMs) have established pre-dominance inautomated code generation, they are not devoid of shortcomings. The pertinentissues primarily relate to the absence of execution guarantees for generatedcode, a lack of explainability, and suboptimal support for essential but nicheprogramming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail toproduce valid programs for Industrial Control Systems (ICS) operated byProgrammable Logic Controllers (PLCs). We propose LLM4PLC, a user-guidediterative pipeline leveraging user feedback and external verification toolsincluding grammar checkers, compilers and SMV verifiers to guide the LLM'sgeneration. We further enhance the generation potential of LLM by employingPrompt Engineering and model fine-tuning through the creation and usage ofLoRAs. We validate this system using a FischerTechnik Manufacturing TestBed(MFTB), illustrating how LLMs can evolve from generating structurally flawedcode to producing verifiably correct programs for industrial applications. Werun a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned CodeLlama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. Theproposed pipeline improved the generation success rate from 47% to 72%, and theSurvey-of-Experts code quality from 2.25/10 to 7.75/10. To promote openresearch, we share the complete experimental setup, the LLM Fine-TuningWeights, and the video demonstrations of the different programs on ourdedicated webpage.",
        "title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of  PLCs in Industrial Control Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05444",
        "abstract_url": "http://arxiv.org/abs/2401.05444",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Ding"
            },
            {
                "last_name": "Peng",
                "first_name": "Peixi"
            },
            {
                "last_name": "Huang",
                "first_name": "Tiejun"
            },
            {
                "last_name": "Tian",
                "first_name": "Yonghong"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "LG"
        ],
        "abstract": "  With the help of special neuromorphic hardware, spiking neural networks(SNNs) are expected to realize artificial intelligence (AI) with less energyconsumption. It provides a promising energy-efficient way for realistic controltasks by combining SNNs with deep reinforcement learning (DRL). In this paper,we focus on the task where the agent needs to learn multi-dimensionaldeterministic policies to control, which is very common in real scenarios.Recently, the surrogate gradient method has been utilized for trainingmulti-layer SNNs, which allows SNNs to achieve comparable performance with thecorresponding deep networks in this task. Most existing spike-based RL methodstake the firing rate as the output of SNNs, and convert it to representcontinuous action space (i.e., the deterministic policy) through afully-connected (FC) layer. However, the decimal characteristic of the firingrate brings the floating-point matrix operations to the FC layer, making thewhole SNN unable to deploy on the neuromorphic hardware directly. To develop afully spiking actor network without any floating-point matrix operations, wedraw inspiration from the non-spiking interneurons found in insects and employthe membrane voltage of the non-spiking neurons to represent the action. Beforethe non-spiking neurons, multiple population neurons are introduced to decodedifferent dimensions of actions. Since each population is used to decode adimension of action, we argue that the neurons in each population should beconnected in time domain and space domain. Hence, the intra-layer connectionsare used in output populations to enhance the representation capacity. Finally,we propose a fully spiking actor network with intra-layer connections(ILC-SAN).",
        "title": "Fully Spiking Actor Network with Intra-layer Connections for  Reinforcement Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05446",
        "abstract_url": "http://arxiv.org/abs/2401.05446",
        "authors": [
            {
                "last_name": "Weng",
                "first_name": "Weining"
            },
            {
                "last_name": "Gu",
                "first_name": "Yang"
            },
            {
                "last_name": "Guo",
                "first_name": "Shuai"
            },
            {
                "last_name": "Ma",
                "first_name": "Yuan"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhaohua"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Yiqiang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Electroencephalogram (EEG) is a non-invasive technique to recordbioelectrical signals. Integrating supervised deep learning techniques with EEGsignals has recently facilitated automatic analysis across diverse EEG-basedtasks. However, the label issues of EEG signals have constrained thedevelopment of EEG-based deep models. Obtaining EEG annotations is difficultthat requires domain experts to guide collection and labeling, and thevariability of EEG signals among different subjects causes significant labelshifts. To solve the above challenges, self-supervised learning (SSL) has beenproposed to extract representations from unlabeled samples throughwell-designed pretext tasks. This paper concentrates on integrating SSLframeworks with temporal EEG signals to achieve efficient representation andproposes a systematic review of the SSL for EEG signals. In this paper, 1) weintroduce the concept and theory of self-supervised learning and typical SSLframeworks. 2) We provide a comprehensive review of SSL for EEG analysis,including taxonomy, methodology, and technique details of the existingEEG-based SSL frameworks, and discuss the difference between these methods. 3)We investigate the adaptation of the SSL approach to various downstream tasks,including the task description and related benchmark datasets. 4) Finally, wediscuss the potential directions for future SSL-EEG research.",
        "title": "Self-supervised Learning for Electroencephalogram: A Systematic Survey",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05447",
        "abstract_url": "http://arxiv.org/abs/2401.05447",
        "authors": [
            {
                "last_name": "Lefort",
                "first_name": "Baptiste"
            },
            {
                "last_name": "Benhamou",
                "first_name": "Eric"
            },
            {
                "last_name": "Ohana",
                "first_name": "Jean-Jacques"
            },
            {
                "last_name": "Saltiel",
                "first_name": "David"
            },
            {
                "last_name": "Guez",
                "first_name": "Beatrice"
            },
            {
                "last_name": "Challet",
                "first_name": "Damien"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to2023, reposted on large financial media, to determine how global news headlinesmay affect stock market movements using ChatGPT and a two-stage promptapproach. We document a statistically significant positive correlation betweenthe sentiment score and future equity market returns over short to medium term,which reverts to a negative correlation over longer horizons. Validation ofthis correlation pattern across multiple equity markets indicates itsrobustness across equity regions and resilience to non-linearity, evidenced bycomparison of Pearson and Spearman correlations. Finally, we provide anestimate of the optimal horizon that strikes a balance between reactivity tonew information and correlation.",
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market  Wraps?",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05450",
        "abstract_url": "http://arxiv.org/abs/2401.05450",
        "authors": [
            {
                "last_name": "Mandran",
                "first_name": "Nadine"
            },
            {
                "last_name": "Prior",
                "first_name": "Estelle"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Eric"
            },
            {
                "last_name": "Vermeulen",
                "first_name": "Mathieu"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  One of the main difficulties remains the collaboration between the variousexperts involved in designing the Learning Games (LG). Our literature reviewfocuses on the pitfalls and principles that have been identified by variousauthors in learning games design. Based on this review, a prototype wasdesigned to support the LG design process and to study more precisely thecollaboration between actors (teachers, researchers, game designers, dataanalyst and computer scientist). Indeed, according to the state of the art, theskills and knowledge involved in design are difficult to integrate. It has beentested in a real-world scenario for designing learning games to teachalgorithmic. Through participant observation in thirty-three workshopsinvolving nine experts, we were able to identify recurring pitfalls as weapplied the recommendations in the literature. The analysis of these workshopsled to propose eight principles aimed at facilitating the collaboration betweenthe learning games design process and re-evaluating research on its.",
        "title": "Reorienting Learning Game Design in Design-Based Research: a Case Study",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05451",
        "abstract_url": "http://arxiv.org/abs/2401.05451",
        "authors": [
            {
                "last_name": "Joshy",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Assessing and comparing player skill in online multiplayer gamingenvironments is essential for fair matchmaking and player engagement.Traditional ranking models like Elo and Glicko-2, designed for two-playergames, are insufficient for the complexity of multi-player, asymmetricteam-based matches. To address this gap, the OpenSkill library offers a suiteof sophisticated, fast, and adaptable models tailored for such dynamics.Drawing from Bayesian inference methods, OpenSkill provides a more accuraterepresentation of individual player contributions and speeds up the computationof ranks. This paper introduces the OpenSkill library, featuring a Pythonimplementation of the Plackett-Luce model among others, highlighting itsperformance advantages and predictive accuracy against proprietary systems likeTrueSkill. OpenSkill is a valuable tool for game developers and researchers,ensuring a responsive and fair gaming experience by efficiently adjustingplayer rankings based on game outcomes. The library's support for time decayand diligent documentation further aid in its practical application, making ita robust solution for the nuanced world of multiplayer ranking systems. Thispaper also acknowledges areas for future enhancement, such as partial play andcontribution weighting, emphasizing the library's ongoing development to meetthe evolving needs of online gaming communities.",
        "title": "OpenSkill: A faster asymmetric multi-team, multiplayer rating system",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05452",
        "abstract_url": "http://arxiv.org/abs/2401.05452",
        "authors": [
            {
                "last_name": "Tahir",
                "first_name": "Muhammad Ahmad"
            },
            {
                "last_name": "Mehmood",
                "first_name": "Ahsan"
            },
            {
                "last_name": "Rahman",
                "first_name": "Muhammad Mahboob Ur"
            },
            {
                "last_name": "Nawaz",
                "first_name": "Muhammad Wasim"
            },
            {
                "last_name": "Riaz",
                "first_name": "Kashif"
            },
            {
                "last_name": "Abbasi",
                "first_name": "Qammer H."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "LG"
        ],
        "abstract": "  We propose two novel purpose-built deep learning (DL) models for synthesis ofthe arterial blood pressure (ABP) waveform in a cuff-less manner, using asingle-site photoplethysmography (PPG) signal. We utilize the public UCIdataset on cuff-less blood pressure (CLBP) estimation to train and evaluate ourDL models. Firstly, we implement a transformer model that incorporatespositional encoding, multi-head attention, layer normalization, and dropouttechniques, and synthesizes the ABP waveform with a mean absolute error (MAE)of 14. Secondly, we implement a frequency-domain (FD) learning approach wherewe first obtain the discrete cosine transform (DCT) coefficients of the PPG andABP signals corresponding to two cardiac cycles, and then learn alinear/non-linear (L/NL) regression between them. We learn that the FD L/NLregression model outperforms the transformer model by achieving an MAE of 11.87and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP),respectively. Our FD L/NL regression model also fulfills the AAMI criterion ofutilizing data from more than 85 subjects, and achieves grade B by the BHScriterion.",
        "title": "Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site  PPG using Transformer & Frequency-domain Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05453",
        "abstract_url": "http://arxiv.org/abs/2401.05453",
        "authors": [
            {
                "last_name": "Anderberg",
                "first_name": "Alastair"
            },
            {
                "last_name": "Bailey",
                "first_name": "James"
            },
            {
                "last_name": "Campello",
                "first_name": "Ricardo J. G. B."
            },
            {
                "last_name": "Houle",
                "first_name": "Michael E."
            },
            {
                "last_name": "Marques",
                "first_name": "Henrique O."
            },
            {
                "last_name": "Radovanovi\u0107",
                "first_name": "Milo\u0161"
            },
            {
                "last_name": "Zimek",
                "first_name": "Arthur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We present a nonparametric method for outlier detection that takes fullaccount of local variations in intrinsic dimensionality within the dataset.Using the theory of Local Intrinsic Dimensionality (LID), our'dimensionality-aware' outlier detection method, DAO, is derived as anestimator of an asymptotic local expected density ratio involving the querypoint and a close neighbor drawn at random. The dimensionality-aware behaviorof DAO is due to its use of local estimation of LID values in atheoretically-justified way. Through comprehensive experimentation on more than800 synthetic and real datasets, we show that DAO significantly outperformsthree popular and important benchmark outlier detection methods: Local OutlierFactor (LOF), Simplified LOF, and kNN.",
        "title": "Dimensionality-Aware Outlier Detection: Theoretical and Experimental  Analysis",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05458",
        "abstract_url": "http://arxiv.org/abs/2401.05458",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dongyu"
            },
            {
                "last_name": "Hu",
                "first_name": "Ruofan"
            },
            {
                "last_name": "Rundensteiner",
                "first_name": "Elke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Deep neural networks (DNNs) have advanced many machine learning tasks, buttheir performance is often harmed by noisy labels in real-world data.Addressing this, we introduce CoLafier, a novel approach that uses LocalIntrinsic Dimensionality (LID) for learning with noisy labels. CoLafierconsists of two subnets: LID-dis and LID-gen. LID-dis is a specializedclassifier. Trained with our uniquely crafted scheme, LID-dis consumes both asample's features and its label to predict the label - which allows it toproduce an enhanced internal representation. We observe that LID scorescomputed from this representation effectively distinguish between correct andincorrect labels across various noise scenarios. In contrast to LID-dis,LID-gen, functioning as a regular classifier, operates solely on the sample'sfeatures. During training, CoLafier utilizes two augmented views per instanceto feed both subnets. CoLafier considers the LID scores from the two views asproduced by LID-dis to assign weights in an adapted loss function for bothsubnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.LID-dis then processes these pseudo-labels along with two views to derive LIDscores. Finally, these LID scores along with the differences in predictionsfrom the two subnets guide the label update decisions. This dual-view anddual-subnet approach enhances the overall reliability of the framework. Uponcompletion of the training, we deploy the LID-gen subnet of CoLafier as thefinal classification model. CoLafier demonstrates improved prediction accuracy,surpassing existing methods, particularly under severe label noise. For moredetails, see the code at https://github.com/zdy93/CoLafier.",
        "title": "CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic  Dimensionality Guidance",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05459",
        "abstract_url": "http://arxiv.org/abs/2401.05459",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanchun"
            },
            {
                "last_name": "Wen",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Weijun"
            },
            {
                "last_name": "Li",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yizhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Guohong"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenxing"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Yi"
            },
            {
                "last_name": "Kong",
                "first_name": "Rui"
            },
            {
                "last_name": "Wang",
                "first_name": "Yile"
            },
            {
                "last_name": "Geng",
                "first_name": "Hanfei"
            },
            {
                "last_name": "Luan",
                "first_name": "Jian"
            },
            {
                "last_name": "Jin",
                "first_name": "Xuefeng"
            },
            {
                "last_name": "Ye",
                "first_name": "Zilong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Guanjing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Mengwei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhijun"
            },
            {
                "last_name": "Li",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ya-Qin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunxin"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "SE"
        ],
        "abstract": "  Since the advent of personal computing devices, intelligent personalassistants (IPAs) have been one of the key technologies that researchers andengineers have focused on, aiming to help users efficiently obtain informationand execute tasks, and provide users with more intelligent, convenient, andrich interaction experiences. With the development of smartphones and IoT,computing and sensing devices have become ubiquitous, greatly expanding theboundaries of IPAs. However, due to the lack of capabilities such as userintent understanding, task planning, tool using, and personal data managementetc., existing IPAs still have limited practicality and scalability. Recently,the emergence of foundation models, represented by large language models(LLMs), brings new opportunities for the development of IPAs. With the powerfulsemantic understanding and reasoning capabilities, LLM can enable intelligentagents to solve complex problems autonomously. In this paper, we focus onPersonal LLM Agents, which are LLM-based agents that are deeply integrated withpersonal data and personal devices and used for personal assistance. Weenvision that Personal LLM Agents will become a major software paradigm forend-users in the upcoming era. To realize this vision, we take the first stepto discuss several important questions about Personal LLM Agents, includingtheir architecture, capability, efficiency and security. We start bysummarizing the key components and design choices in the architecture ofPersonal LLM Agents, followed by an in-depth analysis of the opinions collectedfrom domain experts. Next, we discuss several key challenges to achieveintelligent, efficient and secure Personal LLM Agents, followed by acomprehensive survey of representative solutions to address these challenges.",
        "title": "Personal LLM Agents: Insights and Survey about the Capability,  Efficiency and Security",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05461",
        "abstract_url": "http://arxiv.org/abs/2401.05461",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Zhanliang"
            },
            {
                "last_name": "Xiong",
                "first_name": "Nuoye"
            },
            {
                "last_name": "Li",
                "first_name": "Hongsheng"
            },
            {
                "last_name": "Shen",
                "first_name": "Peiyi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "LG"
        ],
        "abstract": "  Despite neural networks (NN) have been widely applied in various fields andgenerally outperforms humans, they still lack interpretability to a certainextent, and humans are unable to intuitively understand the decision logic ofNN. This also hinders the knowledge interaction between humans and NN,preventing humans from getting involved to give direct guidance when NN'sdecisions go wrong. While recent research in explainable AI has achievedinterpretability of NN from various perspectives, it has not yet providedeffective methods for knowledge exchange between humans and NN. To address thisproblem, we constructed a two-way interaction interface that uses structuredrepresentations of visual concepts and their relationships as the \"language\"for knowledge exchange between humans and NN. Specifically, NN provideintuitive reasoning explanations to humans based on the class-specificstructural concepts graph (C-SCG). On the other hand, humans can modify thebiases present in the C-SCG through their prior knowledge and reasoningability, and thus provide direct knowledge guidance to NN through thisinterface. Through experimental validation, based on this interactioninterface, NN can provide humans with easily understandable explanations of thereasoning process. Furthermore, human involvement and prior knowledge candirectly and effectively contribute to enhancing the performance of NN.",
        "title": "The two-way knowledge interaction interface between humans and neural  networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05462",
        "abstract_url": "http://arxiv.org/abs/2401.05462",
        "authors": [
            {
                "last_name": "Ge",
                "first_name": "Mouzhi"
            },
            {
                "last_name": "Rossi",
                "first_name": "Bruno"
            },
            {
                "last_name": "Chren",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Blanco",
                "first_name": "Jos\u00e9 Miguel"
            }
        ],
        "primary_category": "OH",
        "categories": [
            "OH"
        ],
        "abstract": "  Since the energy domain is in a transformative shift towards sustainability,the integration of new technologies and smart systems into traditional powergrids has emerged. As an effective approach, Petri Nets (PN) have been appliedto model and analyze the complex dynamics in Smart Grid (SG) environments.However, we are currently missing an overview of types of PNs applied todifferent areas and problems related to SGs. Therefore, this paper proposesfour fundamental research questions related to the application areas of PNs inSGs, PNs types, aspects modelled by PNs in the identified areas, and thevalidation methods in the evaluation. The answers to the research questions arederived from a comprehensive and interdisciplinary literature analysis. Theresults capture a valuable overview of PNs applications in the global energylandscape and can offer indications for future research directions.",
        "title": "Petri Nets for Smart Grids: The Story So Far",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05465",
        "abstract_url": "http://arxiv.org/abs/2401.05465",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lin"
            },
            {
                "last_name": "Xu",
                "first_name": "Linghan"
            },
            {
                "last_name": "Motamed",
                "first_name": "Saman"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Shayok"
            },
            {
                "last_name": "De la Torre",
                "first_name": "Fernando"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unsupervised domain adaptation (UDA) for image classification has maderemarkable progress in transferring classification knowledge from a labeledsource domain to an unlabeled target domain, thanks to effective domainalignment techniques. Recently, in order to further improve performance on atarget domain, many Single-Target Active Domain Adaptation (ST-ADA) methodshave been proposed to identify and annotate the salient and exemplar targetsamples. However, it requires one model to be trained and deployed for eachtarget domain and the domain label associated with each test sample. Thislargely restricts its application in the ubiquitous scenarios with multipletarget domains. Therefore, we propose a Multi-Target Active Domain Adaptation(MT-ADA) framework for image classification, named D3GU, to simultaneouslyalign different domains and actively select samples from them for annotation.This is the first research effort in this field to our best knowledge. D3GUapplies Decomposed Domain Discrimination (D3) during training to achieve bothsource-target and target-target domain alignments. Then during active sampling,a Gradient Utility (GU) score is designed to weight every unlabeled targetimage by its contribution towards classification and domain alignment tasks,and is further combined with KMeans clustering to form GU-KMeans for diverseimage sampling. Extensive experiments on three benchmark datasets, Office31,OfficeHome, and DomainNet, have been conducted to validate consistentlysuperior performance of D3GU for MT-ADA.",
        "title": "D3GU: Multi-Target Active Domain Adaptation via Enhancing Domain  Alignment",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05467",
        "abstract_url": "http://arxiv.org/abs/2401.05467",
        "authors": [
            {
                "last_name": "Taneja",
                "first_name": "Karan"
            },
            {
                "last_name": "Goel",
                "first_name": "Ashok"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The recent advances in large language models (LLMs) have led to the creationof many modular AI agents. These agents employ LLMs as zero-shot learners toperform sub-tasks in order to solve complex tasks set forth by human users. Wepropose an approach to enhance the robustness and performance of modular AIagents that utilize LLMs as zero-shot learners. Our iterative machine teachingmethod offers an efficient way to teach AI agents over time with limited humanfeedback, addressing the limit posed by the quality of zero-shot learning. Weadvocate leveraging the data traces from initial deployments and outputs orannotations from the zero-shot learners to train smaller and task-specificsubstitute models which can reduce both the monetary costs and environmentalimpact. Our machine teaching process avails human expertise to correct exampleswith a high likelihood of misannotations. Results on three tasks, common toconversational AI agents, show that close-to-oracle performance can be achievedwith supervision on 20-70% of the dataset depending upon the complexity of thetask and performance of zero-shot learners.",
        "title": "Machine Teaching for Building Modular AI Agents based on Zero-shot  Learners",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05468",
        "abstract_url": "http://arxiv.org/abs/2401.05468",
        "authors": [
            {
                "last_name": "Zanardini",
                "first_name": "Damiano"
            },
            {
                "last_name": "Serrano",
                "first_name": "Emilio"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  This paper introduces a new problem in the field of graph mining and socialnetwork analysis called new node prediction. More technically, the task can becategorized as zero-shot out-of-graph all-links prediction. This challengingproblem aims to predict all links from a new, isolated, and unobserved nodethat was previously disconnected from the graph. Unlike classic approaches tolink prediction (including few-shot out-of-graph link prediction), this problempresents two key differences: (1) the new node has no existing links from whichto extract patterns for new predictions; and (2) the goal is to predict notjust one, but all the links of this new node, or at least a significant part ofthem. Experiments demonstrate that an architecture based on Deep Graph NeuralNetworks can learn to solve this challenging problem in a bibliographiccitation network.",
        "title": "Introducing New Node Prediction in Graph Mining: Predicting All Links  from Isolated Nodes with Graph Neural Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05469",
        "abstract_url": "http://arxiv.org/abs/2401.05469",
        "authors": [
            {
                "last_name": "Kazemi",
                "first_name": "Kianoosh"
            },
            {
                "last_name": "Azimi",
                "first_name": "Iman"
            },
            {
                "last_name": "Liljeberg",
                "first_name": "Pasi"
            },
            {
                "last_name": "Rahmani",
                "first_name": "Amir M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Respiratory rate (RR) serves as an indicator of various medical conditions,such as cardiovascular diseases and sleep disorders. These RR estimationmethods were mostly designed for finger-based PPG collected from subjects instationary situations (e.g., in hospitals). In contrast to finger-based PPGsignals, wrist-based PPG are more susceptible to noise, particularly in theirlow frequency range, which includes respiratory information. Therefore, theexisting methods struggle to accurately extract RR when PPG data are collectedfrom wrist area under free-living conditions. The increasing popularity ofsmartwatches, equipped with various sensors including PPG, has prompted theneed for a robust RR estimation method. In this paper, we propose aconvolutional neural network-based approach to extract RR from PPG,accelerometer, and gyroscope signals captured via smartwatches. Our method,including a dilated residual inception module and 1D convolutions, extract thetemporal information from the signals, enabling RR estimation. Our method istrained and tested using data collected from 36 subjects under free-livingconditions for one day using Samsung Gear Sport watches. For evaluation, wecompare the proposed method with four state-of-the-art RR estimation methods.The RR estimates are compared with RR references obtained from a chest-banddevice. The results show that our method outperforms the existing methods withthe Mean-Absolute-Error and Root-Mean-Square-Error of 1.85 and 2.34, while thebest results obtained by the other methods are 2.41 and 3.29, respectively.Moreover, compared to the other methods, the absolute error distribution of ourmethod was narrow (with the lowest median), indicating a higher level ofagreement between the estimated and reference RR values.",
        "title": "Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05470",
        "abstract_url": "http://arxiv.org/abs/2401.05470",
        "authors": [
            {
                "last_name": "Estopinan",
                "first_name": "Joaquim"
            },
            {
                "last_name": "Bonnet",
                "first_name": "Pierre"
            },
            {
                "last_name": "Servajean",
                "first_name": "Maximilien"
            },
            {
                "last_name": "Munoz",
                "first_name": "Fran\u00e7ois"
            },
            {
                "last_name": "Joly",
                "first_name": "Alexis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  The post-2020 global biodiversity framework needs ambitious, research-basedtargets. Estimating the accelerated extinction risk due to climate change iscritical. The International Union for Conservation of Nature (IUCN) measuresthe extinction risk of species. Automatic methods have been developed toprovide information on the IUCN status of under-assessed taxa. However, thesecompensatory methods are based on current species characteristics, mainlygeographical, which precludes their use in future projections. Here, weevaluate a novel method for classifying the IUCN status of species benefitingfrom the generalisation power of species distribution models based on deeplearning. Our method matches state-of-the-art classification performance whilerelying on flexible SDM-based features that capture species' environmentalpreferences. Cross-validation yields average accuracies of 0.61 for statusclassification and 0.78 for binary classification. Climate change will reshapefuture species distributions. Under the species-environment equilibriumhypothesis, SDM projections approximate plausible future outcomes. Two extremesof species dispersal capacity are considered: unlimited or null. The projectedspecies distributions are translated into features feeding our IUCNclassification method. Finally, trends in threatened species are analysed overtime and i) by continent and as a function of average ii) latitude or iii)altitude. The proportion of threatened species is increasing globally, withcritical rates in Africa, Asia and South America. Furthermore, the proportionof threatened species is predicted to peak around the two Tropics, at theEquator, in the lowlands and at altitudes of 800-1,500 m.",
        "title": "Modelling Species Distributions with Deep Learning to Predict Plant  Extinction Risk and Assess Climate Change Impacts",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05476",
        "abstract_url": "http://arxiv.org/abs/2401.05476",
        "authors": [
            {
                "last_name": "Kapsalis",
                "first_name": "Timo"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "GR"
        ],
        "abstract": "  This paper introduces CADgpt, an innovative plugin integrating NaturalLanguage Processing (NLP) with Rhino3D for enhancing 3D modelling incomputer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgptsimplifies the CAD interface, enabling users, particularly beginners, toperform complex 3D modelling tasks through intuitive natural language commands.This approach significantly reduces the learning curve associated withtraditional CAD software, fostering a more inclusive and engaging educationalenvironment. The paper discusses CADgpt's technical architecture, including itsintegration within Rhino3D and the adaptation of GPT-4 capabilities for CADtasks. It presents case studies demonstrating CADgpt's efficacy in variousdesign scenarios, highlighting its potential to democratise design education bymaking sophisticated design tools accessible to a broader range of students.The discussion further explores CADgpt's implications for pedagogy andcurriculum development, emphasising its role in enhancing creative explorationand conceptual thinking in design education.  Keywords: Natural Language Processing, Computer-Aided Design, 3D Modelling,Design Automation, Design Education, Architectural Education",
        "title": "CADgpt: Harnessing Natural Language Processing for 3D Modelling to  Enhance Computer-Aided Design Workflows",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05477",
        "abstract_url": "http://arxiv.org/abs/2401.05477",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yiran"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haibin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yexu"
            },
            {
                "last_name": "Riedel",
                "first_name": "Till"
            },
            {
                "last_name": "Beigl",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In recent years, deep learning has emerged as a potent tool across amultitude of domains, leading to a surge in research pertaining to itsapplication in the wearable human activity recognition (WHAR) domain. Despitethe rapid development, concerns have been raised about the lack ofstandardization and consistency in the procedures used for experimental modeltraining, which may affect the reproducibility and reliability of researchresults. In this paper, we provide an exhaustive review of contemporary deeplearning research in the field of WHAR and collate information pertaining tothe training procedure employed in various studies. Our findings suggest that amajor trend is the lack of detail provided by model training protocols.Besides, to gain a clearer understanding of the impact of missing descriptions,we utilize a control variables approach to assess the impact of key tunablecomponents (e.g., optimization techniques and early stopping criteria) on theinter-subject generalization capabilities of HAR models. With insights from theanalyses, we define a novel integrated training procedure tailored to the WHARmodel. Empirical results derived using five well-known \\ac{whar} benchmarkdatasets and three classical HAR model architectures demonstrate theeffectiveness of our proposed methodology: in particular, there is asignificant improvement in macro F1 leave one subject out cross-validationperformance.",
        "title": "Standardizing Your Training Process for Human Activity Recognition  Models: A Comprehensive Review in the Tunable Factors",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05478",
        "abstract_url": "http://arxiv.org/abs/2401.05478",
        "authors": [
            {
                "last_name": "Stephens",
                "first_name": "Anna"
            },
            {
                "last_name": "Santos",
                "first_name": "Francisco"
            },
            {
                "last_name": "Tan",
                "first_name": "Pang-Ning"
            },
            {
                "last_name": "Esfahanian",
                "first_name": "Abdol-Hossein"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            "LG"
        ],
        "abstract": "  Graph neural networks (GNN) are a powerful tool for combining imaging andnon-imaging medical information for node classification tasks. Cross-networknode classification extends GNN techniques to account for domain drift,allowing for node classification on an unlabeled target network. In this paperwe present OTGCN, a powerful, novel approach to cross-network nodeclassification. This approach leans on concepts from graph convolutionalnetworks to harness insights from graph data structures while simultaneouslyapplying strategies rooted in optimal transport to correct for the domain driftthat can occur between samples from different data collection sites. Thisblended approach provides a practical solution for scenarios with many distinctforms of data collected across different locations and equipment. Wedemonstrate the effectiveness of this approach at classifying Autism SpectrumDisorder subjects using a blend of imaging and non-imaging data.",
        "title": "Population Graph Cross-Network Node Classification for Autism Detection  Across Sample Groups",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05479",
        "abstract_url": "http://arxiv.org/abs/2401.05479",
        "authors": [
            {
                "last_name": "Miniak-G\u00f3recka",
                "first_name": "Alicja"
            },
            {
                "last_name": "Podlaski",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Gwizda\u0142\u0142a",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The problem of data clustering is one of the most important in data analysis.It can be problematic when dealing with experimental data characterized bymeasurement uncertainties and errors. Our paper proposes a recursive scheme forclustering data obtained in geographical (climatological) experiments. Thediscussion of results obtained by k-means and SOM methods with the developedrecursive procedure is presented. We show that the clustering using the newapproach gives more acceptable results when compared to experts assessments.",
        "title": "The recursive scheme of clustering",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05480",
        "abstract_url": "http://arxiv.org/abs/2401.05480",
        "authors": [
            {
                "last_name": "Zavanelli",
                "first_name": "Nathan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            ""
        ],
        "abstract": "  This paper summarizes and presents PulsatioMech: an open-source MATLABtoolbox for seismocardiography (SCG) signal processing. The toolbox may befound here: https://github.com/nzavanelli/SCG_master_toolbox PulsatioMech iscurrently under development as a common tool to promote new studies anddiscoveries in the use of cardiac mechanical signal for wearable healthmonitoring. This toolbox is designed to assist users in analyzing SCG signalswithout the need to devote significant effort into signal processing and codingtasks. Simultaneously, it provides a uniform basis to assess thereproducibility of works based on this toolbox, including those cited here[1-6]. The referenced works contain a great deal more detail regarding thespecific algorithms implemented here, whereas this paper will present a shortoverview of the PulsatioMech Toolbox.",
        "title": "PulsatioMech: An Open-Source MATLAB Toolbox for Seismocardiography  Signal Processing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05481",
        "abstract_url": "http://arxiv.org/abs/2401.05481",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Siddharth"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  The segmentation of medical images is important for the improvement andcreation of healthcare systems, particularly for early disease detection andtreatment planning. In recent years, the use of convolutional neural networks(CNNs) and other state-of-the-art methods has greatly advanced medical imagesegmentation. However, CNNs have been found to struggle with learninglong-range dependencies and capturing global context due to the limitations ofconvolution operations. In this paper, we explore the use of transformers andCNNs for medical image segmentation and propose a hybrid architecture thatcombines the ability of transformers to capture global dependencies with theability of CNNs to capture low-level spatial details. We compare variousarchitectures and configurations and conduct multiple experiments to evaluatetheir effectiveness.",
        "title": "Transformer-CNN Fused Architecture for Enhanced Skin Lesion Segmentation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05501",
        "abstract_url": "http://arxiv.org/abs/2401.05501",
        "authors": [
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Carley",
                "first_name": "Kathleen M."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  As digitalization increases, countries employ digital diplomacy, harnessingdigital resources to project their desired image. Digital diplomacy alsoencompasses the interactivity of digital platforms, providing a trove of publicopinion that diplomatic agents can collect. Social media bots activelyparticipate in political events through influencing political communication andpurporting coordinated narratives to influence human behavior. This articleprovides a methodology towards identifying three types of bots: General Bots,News Bots and Bridging Bots, then further identify these classes of bots onTwitter during a diplomatic incident involving the United States and China.Using a series of computational methods, this article examines the impact ofbots on the topics disseminated, the influence and the use of informationmaneuvers of bots within the social communication network. Among others, ourresults observe that all three types of bots are present across the twocountries; bots geotagged to the US are generally concerned with the balloonlocation while those geotagged to China discussed topics related to escalatingtensions; and perform different extent of positive narrative and networkinformation maneuvers.",
        "title": "Deflating the Chinese Balloon: Types of Twitter Bots in US-China balloon  incident",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05502",
        "abstract_url": "http://arxiv.org/abs/2401.05502",
        "authors": [
            {
                "last_name": "Thejaswi",
                "first_name": "Suhas"
            },
            {
                "last_name": "Gadekar",
                "first_name": "Ameet"
            },
            {
                "last_name": "Ordozgoiti",
                "first_name": "Bruno"
            },
            {
                "last_name": "Gionis",
                "first_name": "Aristides"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "",
            "CC",
            "LG"
        ],
        "abstract": "  In this work, we study diversity-aware clustering problems where the datapoints are associated with multiple attributes resulting in intersectinggroups. A clustering solution need to ensure that a minimum number of clustercenters are chosen from each group while simultaneously minimizing theclustering objective, which can be either $k$-median, $k$-means or$k$-supplier. We present parameterized approximation algorithms withapproximation ratios $1+ \\frac{2}{e}$, $1+\\frac{8}{e}$ and $3$ fordiversity-aware $k$-median, diversity-aware $k$-means and diversity-aware$k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETHand FPT $\\neq$ W[2]. For fair $k$-median and fair $k$-means with disjointfaicility groups, we present parameterized approximation algorithm withapproximation ratios $1+\\frac{2}{e}$ and $1+\\frac{8}{e}$, respectively. Forfair $k$-supplier with disjoint facility groups, we present a polynomial-timeapproximation algorithm with factor $3$, improving the previous best knownapproximation ratio of factor $5$.",
        "title": "Diversity-aware clustering: Computational Complexity and Approximation  Algorithms",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05507",
        "abstract_url": "http://arxiv.org/abs/2401.05507",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xueyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Wei",
                "first_name": "Shuang"
            },
            {
                "last_name": "Chai",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Wang",
                "first_name": "Guoyin"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuwu"
            },
            {
                "last_name": "Su",
                "first_name": "Jing"
            },
            {
                "last_name": "Xu",
                "first_name": "Jingjing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jianbo"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Yang",
                "first_name": "Yang"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In this paper, we introduce \"InfiAgent-DABench\", the first benchmarkspecifically designed to evaluate LLM-based agents in data analysis tasks. Thisbenchmark contains DAEval, a dataset consisting of 311 data analysis questionsderived from 55 CSV files, and an agent framework to evaluate LLMs as dataanalysis agents. We adopt a format-prompting technique, ensuring questions tobe closed-form that can be automatically evaluated. Our extensive benchmarkingof 23 state-of-the-art LLMs uncovers the current challenges encountered in dataanalysis tasks. In addition, we have developed DAAgent, a specialized agenttrained on instruction-tuning datasets. Evaluation datasets and toolkits forInfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.",
        "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05509",
        "abstract_url": "http://arxiv.org/abs/2401.05509",
        "authors": [
            {
                "last_name": "Injadat",
                "first_name": "MohammadNoor"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The continued growth in the deployment of Internet-of-Things (IoT) deviceshas been fueled by the increased connectivity demand, particularly inindustrial environments. However, this has led to an increase in the number ofnetwork related attacks due to the increased number of potential attacksurfaces. Industrial IoT (IIoT) devices are prone to various network relatedattacks that can have severe consequences on the manufacturing process as wellas on the safety of the workers in the manufacturing plant. One promisingsolution that has emerged in recent years for attack detection is Machinelearning (ML). More specifically, ensemble learning models have shown greatpromise in improving the performance of the underlying ML models. Accordingly,this paper proposes a framework based on the combined use of BayesianOptimization-Gaussian Process (BO-GP) with an ensemble tree-based learningmodel to improve the performance of intrusion and attack detection in IIoTenvironments. The proposed framework's performance is evaluated using theWindows 10 dataset collected by the Cyber Range and IoT labs at University ofNew South Wales. Experimental results illustrate the improvement in detectionaccuracy, precision, and F-score when compared to standard tree and ensembletree models.",
        "title": "Optimized Ensemble Model Towards Secured Industrial IoT Devices",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05511",
        "abstract_url": "http://arxiv.org/abs/2401.05511",
        "authors": [
            {
                "last_name": "Rogha",
                "first_name": "Milad"
            },
            {
                "last_name": "Sah",
                "first_name": "Subham"
            },
            {
                "last_name": "Karduni",
                "first_name": "Alireza"
            },
            {
                "last_name": "Markant",
                "first_name": "Douglas"
            },
            {
                "last_name": "Dou",
                "first_name": "Wenwen"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  News articles containing data visualizations play an important role ininforming the public on issues ranging from public health to politics. Recentresearch on the persuasive appeal of data visualizations suggests that priorattitudes can be notoriously difficult to change. Inspired by an NYT article,we designed two experiments to evaluate the impact of elicitation andcontrasting narratives on attitude change, recall, and engagement. Wehypothesized that eliciting prior beliefs leads to more elaborative thinkingthat ultimately results in higher attitude change, better recall, andengagement. Our findings revealed that visual elicitation leads to higherengagement in terms of feelings of surprise. While there is an overall attitudechange across all experiment conditions, we did not observe a significanteffect of belief elicitation on attitude change. With regard to recall error,while participants in the draw trend elicitation exhibited significantly lowerrecall error than participants in the categorize trend condition, we found nosignificant difference in recall error when comparing elicitation conditions tono elicitation. In a follow-up study, we added contrasting narratives with thepurpose of making the main visualization (communicating data on the focalissue) appear strikingly different. Compared to the results of study 1, wefound that contrasting narratives improved engagement in terms of surprise andinterest but interestingly resulted in higher recall error and no significantchange in attitude. We discuss the effects of elicitation and contrastingnarratives in the context of topic involvement and the strengths of temporaltrends encoded in the data visualization.",
        "title": "The Impact of Elicitation and Contrasting Narratives on Engagement,  Recall and Attitude Change with News Articles Containing Data Visualization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05516",
        "abstract_url": "http://arxiv.org/abs/2401.05516",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "GeonU"
            },
            {
                "last_name": "Youwang",
                "first_name": "Kim"
            },
            {
                "last_name": "Oh",
                "first_name": "Tae-Hyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "GR"
        ],
        "abstract": "  We present FPRF, a feed-forward photorealistic style transfer method forlarge-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes witharbitrary, multiple style reference images without additional optimizationwhile preserving multi-view appearance consistency. Prior arts required tediousper-style/-scene optimization and were limited to small-scale 3D scenes. FPRFefficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3Dneural radiance field, which inherits AdaIN's feed-forward stylizationmachinery, supporting arbitrary style reference images. Furthermore, FPRFsupports multi-reference stylization with the semantic correspondence matchingand local AdaIN, which adds diverse user control for 3D scene styles. FPRF alsopreserves multi-view consistency by applying semantic matching and styletransfer processes directly onto queried features in 3D space. In experiments,we demonstrate that FPRF achieves favorable photorealistic quality 3D scenestylization for large-scale scenes with diverse reference images. Project page:https://kim-geonu.github.io/FPRF/",
        "title": "FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D  Neural Radiance Fields",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05518",
        "abstract_url": "http://arxiv.org/abs/2401.05518",
        "authors": [
            {
                "last_name": "Panferov",
                "first_name": "Andrei"
            },
            {
                "last_name": "Demidovich",
                "first_name": "Yury"
            },
            {
                "last_name": "Rammal",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Richt\u00e1rik",
                "first_name": "Peter"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DC",
            ""
        ],
        "abstract": "  Quantization (Alistarh et al., 2017) is an important (stochastic) compressiontechnique that reduces the volume of transmitted bits during each communicationround in distributed model training. Suresh et al. (2022) introduce correlatedquantizers and show their advantages over independent counterparts by analyzingdistributed SGD communication complexity. We analyze the forefront distributednon-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing theproposed correlated quantizers and show that it outperforms the original MARINAand distributed SGD of Suresh et al. (2022) with regard to the communicationcomplexity. We significantly refine the original analysis of MARINA without anyadditional assumptions using the weighted Hessian variance (Tyurin et al.,2022), and then we expand the theoretical framework of MARINA to accommodate asubstantially broader range of potentially correlated and biased compressors,thus dilating the applicability of the method beyond the conventionalindependent unbiased compressor setup. Extensive experimental resultscorroborate our theoretical findings.",
        "title": "Correlated Quantization for Faster Nonconvex Distributed Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05520",
        "abstract_url": "http://arxiv.org/abs/2401.05520",
        "authors": [
            {
                "last_name": "Amadeus",
                "first_name": "Marcellus"
            },
            {
                "last_name": "Casta\u00f1eda",
                "first_name": "William Alberto Cruz"
            },
            {
                "last_name": "Zanella",
                "first_name": "Andr\u00e9 Felipe"
            },
            {
                "last_name": "Mahlow",
                "first_name": "Felipe Rodrigues Perche"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL"
        ],
        "abstract": "  Generative AI has become pervasive in society, witnessing significantadvancements in various domains. Particularly in the realm of Text-to-Image(TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilitiesin generating visual content based on textual prompts. This paper addresses thepotential of LDMs in representing local cultural concepts, historical figures,and endangered species. In this study, we use the cultural heritage of RioGrande do Sul (RS), Brazil, as an illustrative case. Our objective is tocontribute to the broader understanding of how generative models can help tocapture and preserve the cultural and historical identity of regions. The paperoutlines the methodology, including subject selection, dataset creation, andthe fine-tuning process. The results showcase the images generated, alongsidethe challenges and feasibility of each concept. In conclusion, this work showsthe power of these models to represent and preserve unique aspects of diverseregions and communities.",
        "title": "From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\\'ucho  Heritage",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05521",
        "abstract_url": "http://arxiv.org/abs/2401.05521",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Danjie"
            },
            {
                "last_name": "Yang",
                "first_name": "Simon X."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            ""
        ],
        "abstract": "  The paper presents an innovative approach (CBNNTAP) that addresses thecomplexities and challenges introduced by ocean currents when optimizing targetassignment and motion planning for a multi-unmanned underwater vehicle (UUV)system. The core of the proposed algorithm involves the integration of severalkey components. Firstly, it incorporates a bio-inspired neural network-based(BINN) approach which predicts the most efficient paths for individual UUVswhile simultaneously ensuring collision avoidance among the vehicles. Secondly,an efficient target assignment component is integrated by considering the pathdistances determined by the BINN algorithm. In addition, a critical innovationwithin the CBNNTAP algorithm is its capacity to address the disruptive effectsof ocean currents, where an adjustment component is seamlessly integrated tocounteract the deviations caused by these currents, which enhances the accuracyof both motion planning and target assignment for the UUVs. The effectivenessof the CBNNTAP algorithm is demonstrated through comprehensive simulationresults and the outcomes underscore the superiority of the developed algorithmin nullifying the effects of static and dynamic ocean currents in 2D and 3Dscenarios.",
        "title": "Current Effect-eliminated Optimal Target Assignment and Motion Planning  for a Multi-UUV System",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05523",
        "abstract_url": "http://arxiv.org/abs/2401.05523",
        "authors": [
            {
                "last_name": "Levit",
                "first_name": "Vadim E."
            },
            {
                "last_name": "Mandrescu",
                "first_name": "Eugen"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            "",
            ""
        ],
        "abstract": "  The graph G=(V,E) is called Konig-Egervary if the sum of its independencenumber and its matching number equals its order. Let RV(G) denote the number ofvertices v such that G-v is Konig-Egervary, and let RE(G) denote the number ofedges e such that G-e is Konig-Egervary. Clearly, RV(G) = |V| and RE(G) = |E|for bipartite graphs. Unlike the bipartiteness, the property of being aKonig-Egervary graph is not hereditary. In this paper, we present an equalityexpressing RV(G) in terms of some graph parameters, and a tight inequalitybounding RE(G) in terms of the same parameters, when G is Konig-Egervary.",
        "title": "On the Number of Vertices/Edges whose Deletion Preserves the  Konig-Egervary Property",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05525",
        "abstract_url": "http://arxiv.org/abs/2401.05525",
        "authors": [
            {
                "last_name": "Dinh",
                "first_name": "Lam"
            },
            {
                "last_name": "Quang",
                "first_name": "Pham Tran Anh"
            },
            {
                "last_name": "Leguay",
                "first_name": "J\u00e9r\u00e9mie"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG"
        ],
        "abstract": "  Deep Reinforcement Learning (DRL) algorithms have recently made significantstrides in improving network performance. Nonetheless, their practical use isstill limited in the absence of safe exploration and safe decision-making. Inthe context of commercial solutions, reliable and safe-to-operate systems areof paramount importance. Taking this problem into account, we propose a safelearning-based load balancing algorithm for Software Defined-Wide Area Network(SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined witha Control Barrier Function (CBF). It safely projects unsafe actions intofeasible ones during both training and testing, and it guides learning towardssafe policies. We successfully implemented the solution on GPU to acceleratetraining by approximately 110x times and achieve model updates for on-policymethods within a few seconds, making the solution practical. We show that ourapproach delivers near-optimal Quality-of-Service (QoS performance in terms ofend-to-end delay while respecting safety requirements related to link capacityconstraints. We also demonstrated that on-policy learning based on ProximalPolicy Optimization (PPO) performs better than off-policy learning with DeepDeterministic Policy Gradient (DDPG) when both are combined with a CBF for safeload balancing.",
        "title": "Towards Safe Load Balancing based on Control Barrier Functions and Deep  Reinforcement Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05529",
        "abstract_url": "http://arxiv.org/abs/2401.05529",
        "authors": [
            {
                "last_name": "Di",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Bingchang"
            },
            {
                "last_name": "Gao",
                "first_name": "Yiyi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  This paper presents a novel fuzzing framework, called MicroFuzz, specificallydesigned for Microservices. Mocking-Assisted Seed Execution, DistributedTracing, Seed Refresh and Pipeline Parallelism approaches are adopted toaddress the environmental complexities and dynamics of Microservices andimprove the efficiency of fuzzing. MicroFuzz has been successfully implementedand deployed in Ant Group, a prominent FinTech company. Its performance hasbeen evaluated in three distinct industrial scenarios: normalized fuzzing,iteration testing, and taint verification.Throughout five months of operation,MicroFuzz has diligently analyzed a substantial codebase, consisting of 261Apps with over 74.6 million lines of code (LOC). The framework's effectivenessis evident in its detection of 5,718 potential quality or security risks, with1,764 of them confirmed and fixed as actual security threats by softwarespecialists. Moreover, MicroFuzz significantly increased program coverage by12.24% and detected program behavior by 38.42% in the iteration testing.",
        "title": "MicroFuzz: An Efficient Fuzzing Framework for Microservices",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05530",
        "abstract_url": "http://arxiv.org/abs/2401.05530",
        "authors": [
            {
                "last_name": "Salgado",
                "first_name": "Erik Isai Valle"
            },
            {
                "last_name": "Li",
                "first_name": "Chen"
            },
            {
                "last_name": "Han",
                "first_name": "Yaqi"
            },
            {
                "last_name": "Shi",
                "first_name": "Linchao"
            },
            {
                "last_name": "Li",
                "first_name": "Xinghui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Ensemble methods exploit the availability of a given number of classifiers ordetectors trained in single or multiple source domains and tasks to addressmachine learning problems such as domain adaptation or multi-source transferlearning. Existing research measures the domain distance between the sourcesand the target dataset, trains multiple networks on the same data withdifferent samples per class, or combines predictions from models trained undervaried hyperparameters and settings. Their solutions enhanced the performanceon small or tail categories but hurt the rest. To this end, we propose amodified consensus focus for semi-supervised and long-tailed object detection.We introduce a voting system based on source confidence that spots thecontribution of each model in a consensus, lets the user choose the relevanceof each class in the target label space so that it relaxes minority boundingboxes suppression, and combines multiple models' results without discarding thepoisonous networks. Our tests on synthetic driving datasets retrieved higherconfidence and more accurate bounding boxes than the NMS, soft-NMS, and WBF.",
        "title": "Consensus Focus for Object Detection and minority classes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05531",
        "abstract_url": "http://arxiv.org/abs/2401.05531",
        "authors": [
            {
                "last_name": "Fischer",
                "first_name": "John"
            },
            {
                "last_name": "Orescanin",
                "first_name": "Marko"
            },
            {
                "last_name": "Eckstrand",
                "first_name": "Eric"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD",
            ""
        ],
        "abstract": "  Transfer learning (TL) is an increasingly popular approach to training deeplearning (DL) models that leverages the knowledge gained by training afoundation model on diverse, large-scale datasets for use on downstream taskswhere less domain- or task-specific data is available. The literature is richwith TL techniques and applications; however, the bulk of the research makesuse of deterministic DL models which are often uncalibrated and lack theability to communicate a measure of epistemic (model) uncertainty inprediction. Unlike their deterministic counterparts, Bayesian DL (BDL) modelsare often well-calibrated, provide access to epistemic uncertainty for aprediction, and are capable of achieving competitive predictive performance. Inthis study, we propose variational inference pre-trained audio neural networks(VI-PANNs). VI-PANNs are a variational inference variant of the popularResNet-54 architecture which are pre-trained on AudioSet, a large-scale audioevent detection dataset. We evaluate the quality of the resulting uncertaintywhen transferring knowledge from VI-PANNs to other downstream acousticclassification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets. Wedemonstrate, for the first time, that it is possible to transfer calibrateduncertainty information along with knowledge from upstream tasks to enhance amodel's capability to perform downstream tasks.",
        "title": "VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational  Inference for Improved Generalization in Audio Pattern Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05533",
        "abstract_url": "http://arxiv.org/abs/2401.05533",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Ningfeng"
            },
            {
                "last_name": "Ren",
                "first_name": "Jing"
            },
            {
                "last_name": "Sorkine-Hornung",
                "first_name": "Olga"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  We formalize Italian smocking, an intricate embroidery technique that gathersflat fabric into pleats along meandering lines of stitches, resulting in pleatsthat fold and gather where the stitching veers. In contrast to Englishsmocking, characterized by colorful stitches decorating uniformly shapedpleats, and Canadian smocking, which uses localized knots to form voluminouspleats, Italian smocking permits the fabric to move freely along the stitchedthreads following curved paths, resulting in complex and unpredictable pleatswith highly diverse, irregular structures, achieved simply by pulling on thethreads. We introduce a novel method for digital previewing of Italian smockingresults, given the thread stitching path as input. Our method uses acoarse-grained mass-spring system to simulate the interaction between thethreads and the fabric. This configuration guides the fine-level fabricdeformation through an adaptation of the state-of-the-art simulator, C-IPC. Ourmethod models the general problem of fabric-thread interaction and can bereadily adapted to preview Canadian smocking as well. We compare our results tobaseline approaches and physical fabrications to demonstrate the accuracy ofour method.",
        "title": "Computational Smocking through Fabric-Thread Interaction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05535",
        "abstract_url": "http://arxiv.org/abs/2401.05535",
        "authors": [
            {
                "last_name": "Dorador",
                "first_name": "Albert"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  Decades after their inception, random forests continue to providestate-of-the-art accuracy in a variety of learning problems, outperforming inthis respect alternative machine learning algorithms such as decision trees oreven neural networks. However, being an ensemble method, the one aspect whererandom forests tend to severely underperform decision trees isinterpretability. In the present work, we propose a post-hoc approach that aimsto have the best of both worlds: the accuracy of random forests and theinterpretability of decision trees. To this end, we present two forest-pruningmethods to find an optimal sub-forest within a given random forest, and then,when applicable, combine the selected trees into one. Our first method relieson constrained exhaustive search, while our second method is based on anadaptation of the LASSO methodology. Extensive experiments over synthetic andreal world datasets show that, in the majority of scenarios, at least one ofthe two methods proposed is more accurate than the original random forest,while just using a small fraction of the trees, aiding result interpretability.Compared to current state-of-the-art forestpruning methods, namely sequentialforward selection and (a variation of) sequential backward selection, ourmethods tend to outperform both of them, whether in terms of accuracy, numberof trees employed, or both.",
        "title": "Improving the Accuracy and Interpretability of Random Forests via Forest  Pruning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05538",
        "abstract_url": "http://arxiv.org/abs/2401.05538",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Le Ngu"
            },
            {
                "last_name": "Casado",
                "first_name": "Constantino \u00c1lvarez"
            },
            {
                "last_name": "Ca\u00f1ellas",
                "first_name": "Manuel Lage"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Anirban"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Nhi"
            },
            {
                "last_name": "Jayagopi",
                "first_name": "Dinesh Babu"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Miguel Bordallo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Radio frequency (RF) signals have facilitated the development of non-contacthuman monitoring tasks, such as vital signs measurement, activity recognition,and user identification. In some specific scenarios, an RF signal analysisframework may prioritize the performance of one task over that of others. Inresponse to this requirement, we employ a multi-objective optimization approachinspired by biological principles to select discriminative features thatenhance the accuracy of breathing patterns recognition while simultaneouslyimpeding the identification of individual users. This approach is validatedusing a novel vital signs dataset consisting of 50 subjects engaged in fourdistinct breathing patterns. Our findings indicate a remarkable result: asubstantial divergence in accuracy between breathing recognition and useridentification. As a complementary viewpoint, we present a contrariwise resultto maximize user identification accuracy and minimize the system's capacity forbreathing activity recognition.",
        "title": "Multi-objective Feature Selection in Remote Health Monitoring  Applications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05544",
        "abstract_url": "http://arxiv.org/abs/2401.05544",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yong"
            },
            {
                "last_name": "Luo",
                "first_name": "Senlin"
            },
            {
                "last_name": "Shang",
                "first_name": "Yu-Ming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yifei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengjun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Researchers have explored the potential of utilizing pre-trained languagemodels, such as CodeBERT, to improve source code-related tasks. Previousstudies have mainly relied on CodeBERT's text embedding capability and the`[CLS]' sentence embedding information as semantic representations forfine-tuning downstream source code-related tasks. However, these methodsrequire additional neural network layers to extract effective features,resulting in higher computational costs. Furthermore, existing approaches havenot leveraged the rich knowledge contained in both source code and relatedtext, which can lead to lower accuracy. This paper presents a novel approach,CodePrompt, which utilizes rich knowledge recalled from a pre-trained model byprompt learning and an attention mechanism to improve source code-relatedclassification tasks. Our approach initially motivates the language model withprompt information to retrieve abundant knowledge associated with the input asrepresentative features, thus avoiding the need for additional neural networklayers and reducing computational costs. Subsequently, we employ an attentionmechanism to aggregate multiple layers of related knowledge for each task asfinal features to boost their accuracy. We conducted extensive experiments onfour downstream source code-related tasks to evaluate our approach and ourresults demonstrate that CodePrompt achieves new state-of-the-art performanceon the accuracy metric while also exhibiting computation cost-savingcapabilities.",
        "title": "CodePrompt: Improving Source Code-Related Classification with Knowledge  Features through Prompt Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05548",
        "abstract_url": "http://arxiv.org/abs/2401.05548",
        "authors": [
            {
                "last_name": "Machetti",
                "first_name": "Simone"
            },
            {
                "last_name": "Schiavone",
                "first_name": "Pasquale Davide"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Thomas Christoph"
            },
            {
                "last_name": "Pe\u00f3n-Quir\u00f3s",
                "first_name": "Miguel"
            },
            {
                "last_name": "Atienza",
                "first_name": "David"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The field of edge computing has witnessed remarkable growth owing to theincreasing demand for real-time processing of data in applications. However,challenges persist due to limitations in performance and power consumption. Toovercome these challenges, heterogeneous architectures have emerged thatcombine host processors with specialized accelerators tailored to specificapplications, leading to improved performance and reduced power consumption.However, most of the existing platforms lack the necessary configurability andextendability options for integrating custom accelerators. To overcome theselimitations, we introduce in this paper the eXtendible HeterogeneousEnergy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designedto natively support the integration of ultra-low-power edge accelerators. Itprovides customization options to match specific application requirements byexploring various core types, bus topologies, addressing modes, memory sizes,and peripherals. Moreover, the platform prioritizes energy efficiency byimplementing low-power strategies, such as clock-gating and power-gating. Wedemonstrate the real-world applicability of X-HEEP by providing an integrationexample tailored for healthcare applications that includes a coarse-grainedreconfigurable array (CGRA) and in-memory computing (IMC) accelerators. Theresulting design, called HEEPocrates, has been implemented both in fieldprogrammable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon withTSMC 65 nm low-power CMOS technology. We run a set of healthcare applicationsand measure their energy consumption to demonstrate the alignment of our chipwith other state-of-the-art microcontrollers commonly adopted in this domain.Moreover, we showcase the energy benefit of 4.9 x gained by exploiting theintegrated CGRA accelerator, compared to running on the host CPU.",
        "title": "X-HEEP: An Open-Source, Configurable and Extendible RISC-V  Microcontroller for the Exploration of Ultra-Low-Power Edge Accelerators",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05551",
        "abstract_url": "http://arxiv.org/abs/2401.05551",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Changye"
            },
            {
                "last_name": "Xu",
                "first_name": "Weizhe"
            },
            {
                "last_name": "Cohen",
                "first_name": "Trevor"
            },
            {
                "last_name": "Pakhomov",
                "first_name": "Serguei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  \\textbf{Objectives}: We aimed to investigate how errors from automatic speechrecognition (ASR) systems affect dementia classification accuracy, specificallyin the ``Cookie Theft'' picture description task. We aimed to assess whetherimperfect ASR-generated transcripts could provide valuable information fordistinguishing between language samples from cognitively healthy individualsand those with Alzheimer's disease (AD).  \\textbf{Methods}: We conducted experiments using various ASR models, refiningtheir transcripts with post-editing techniques. Both these imperfect ASRtranscripts and manually transcribed ones were used as inputs for thedownstream dementia classification. We conducted comprehensive error analysisto compare model performance and assess ASR-generated transcript effectivenessin dementia classification.  \\textbf{Results}: Imperfect ASR-generated transcripts surprisinglyoutperformed manual transcription for distinguishing between individuals withAD and those without in the ``Cookie Theft'' task. These ASR-based modelssurpassed the previous state-of-the-art approach, indicating that ASR errorsmay contain valuable cues related to dementia. The synergy between ASR andclassification models improved overall accuracy in dementia classification.  \\textbf{Conclusion}: Imperfect ASR transcripts effectively capture linguisticanomalies linked to dementia, improving accuracy in classification tasks. Thissynergy between ASR and classification models underscores ASR's potential as avaluable tool in assessing cognitive impairment and related clinicalapplications.",
        "title": "Useful Blunders: Can Automated Speech Recognition Errors Improve  Downstream Dementia Classification?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05553",
        "abstract_url": "http://arxiv.org/abs/2401.05553",
        "authors": [
            {
                "last_name": "Pareschi",
                "first_name": "Lorenzo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Probably one of the most striking examples of the close connections betweenglobal optimization processes and statistical physics is the simulatedannealing method, inspired by the famous Monte Carlo algorithm devised byMetropolis et al. in the middle of the last century. In this paper we show howthe tools of linear kinetic theory allow to describe this gradient-freealgorithm from the perspective of statistical physics and how convergence tothe global minimum can be related to classical entropy inequalities. Thisanalysis highlight the strong link between linear Boltzmann equations andstochastic optimization methods governed by Markov processes. Thanks to thisformalism we can establish the connections between the simulated annealingprocess and the corresponding mean-field Langevin dynamics characterized by astochastic gradient descent approach. Generalizations to other selectionstrategies in simulated annealing that avoid the acceptance-rejection dynamicare also provided.",
        "title": "Optimization by linear kinetic equations and mean-field Langevin  dynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05554",
        "abstract_url": "http://arxiv.org/abs/2401.05554",
        "authors": [
            {
                "last_name": "Lo",
                "first_name": "John"
            },
            {
                "last_name": "Parslew",
                "first_name": "Ben"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Previous design methodologies for spring-driven jumping robots focused onjump height optimization for specific tasks. In doing so, numerous designs havebeen proposed including using nonlinear spring-linkages to increase the elasticenergy storage and jump height. However, these systems can never achieve theirtheoretical maximum jump height due to taking off before the spring energy isfully released, resulting in an incomplete transfer of stored elastic energy togravitational potential energy. This paper presents low-order models aimed atcharacterising the energy conversion during the acceleration phase of jumping.It also proposes practical solutions for increasing the energy efficiency ofjumping robots. A dynamic analysis is conducted on a multibody system comprisedof rotational links, which is experimentally validated using a physicaldemonstrator. The analysis reveals that inefficient energy conversion isattributed to inertial effects caused by rotational and unsprung masses. Sincethese masses cannot be entirely eliminated from a physical linkage, a practicalapproach to improving energy efficiency involves structural redesign to reducestructural mass and moments of inertia while maintaining compliance withstructural strength and stiffness requirements.",
        "title": "Characterising the take-off dynamics and energy efficiency in  spring-driven jumping robots",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05558",
        "abstract_url": "http://arxiv.org/abs/2401.05558",
        "authors": [
            {
                "last_name": "Asinowski",
                "first_name": "Andrei"
            },
            {
                "last_name": "Banderier",
                "first_name": "Cyril"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "CG",
            "FL",
            ""
        ],
        "abstract": "  We enumerate several classes of pattern-avoiding rectangulations. Weestablish bijective links with pattern-avoiding permutations, prove that theirgenerating functions are algebraic, and confirm several conjectures by Merinoand M\\\"utze. We also analyze a new class of rectangulations, called whirls,using a generating tree.",
        "title": "From geometry to generating functions: rectangulations and permutations",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05561",
        "abstract_url": "http://arxiv.org/abs/2401.05561",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Lichao"
            },
            {
                "last_name": "Huang",
                "first_name": "Yue"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Wu",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qihui"
            },
            {
                "last_name": "Gao",
                "first_name": "Chujie"
            },
            {
                "last_name": "Huang",
                "first_name": "Yixin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yixuan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiner"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengliang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yixin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yijue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhikun"
            },
            {
                "last_name": "Kailkhura",
                "first_name": "Bhavya"
            },
            {
                "last_name": "Xiong",
                "first_name": "Caiming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            },
            {
                "last_name": "Xiao",
                "first_name": "Chaowei"
            },
            {
                "last_name": "Li",
                "first_name": "Chunyuan"
            },
            {
                "last_name": "Xing",
                "first_name": "Eric"
            },
            {
                "last_name": "Huang",
                "first_name": "Furong"
            },
            {
                "last_name": "Liu",
                "first_name": "Hao"
            },
            {
                "last_name": "Ji",
                "first_name": "Heng"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Huan"
            },
            {
                "last_name": "Yao",
                "first_name": "Huaxiu"
            },
            {
                "last_name": "Kellis",
                "first_name": "Manolis"
            },
            {
                "last_name": "Zitnik",
                "first_name": "Marinka"
            },
            {
                "last_name": "Jiang",
                "first_name": "Meng"
            },
            {
                "last_name": "Bansal",
                "first_name": "Mohit"
            },
            {
                "last_name": "Zou",
                "first_name": "James"
            },
            {
                "last_name": "Pei",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jian"
            },
            {
                "last_name": "Gao",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Han",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jieyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiliang"
            },
            {
                "last_name": "Wang",
                "first_name": "Jindong"
            },
            {
                "last_name": "Mitchell",
                "first_name": "John"
            },
            {
                "last_name": "Shu",
                "first_name": "Kai"
            },
            {
                "last_name": "Xu",
                "first_name": "Kaidi"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            },
            {
                "last_name": "He",
                "first_name": "Lifang"
            },
            {
                "last_name": "Huang",
                "first_name": "Lifu"
            },
            {
                "last_name": "Backes",
                "first_name": "Michael"
            },
            {
                "last_name": "Gong",
                "first_name": "Neil Zhenqiang"
            },
            {
                "last_name": "Yu",
                "first_name": "Philip S."
            },
            {
                "last_name": "Chen",
                "first_name": "Pin-Yu"
            },
            {
                "last_name": "Gu",
                "first_name": "Quanquan"
            },
            {
                "last_name": "Xu",
                "first_name": "Ran"
            },
            {
                "last_name": "Ying",
                "first_name": "Rex"
            },
            {
                "last_name": "Ji",
                "first_name": "Shuiwang"
            },
            {
                "last_name": "Jana",
                "first_name": "Suman"
            },
            {
                "last_name": "Chen",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianming"
            },
            {
                "last_name": "Zhou",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Willian"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiangliang"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Xie",
                "first_name": "Xing"
            },
            {
                "last_name": "Chen",
                "first_name": "Xun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Yan"
            },
            {
                "last_name": "Ye",
                "first_name": "Yanfang"
            },
            {
                "last_name": "Cao",
                "first_name": "Yinzhi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs), exemplified by ChatGPT, have gainedconsiderable attention for their excellent natural language processingcapabilities. Nonetheless, these LLMs present many challenges, particularly inthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMsemerges as an important topic. This paper introduces TrustLLM, a comprehensivestudy of trustworthiness in LLMs, including principles for different dimensionsof trustworthiness, established benchmark, evaluation, and analysis oftrustworthiness for mainstream LLMs, and discussion of open challenges andfuture directions. Specifically, we first propose a set of principles fortrustworthy LLMs that span eight different dimensions. Based on theseprinciples, we further establish a benchmark across six dimensions includingtruthfulness, safety, fairness, robustness, privacy, and machine ethics. Wethen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting ofover 30 datasets. Our findings firstly show that in general trustworthiness andutility (i.e., functional effectiveness) are positively related. Secondly, ourobservations reveal that proprietary LLMs generally outperform most open-sourcecounterparts in terms of trustworthiness, raising concerns about the potentialrisks of widely accessible open-source LLMs. However, a few open-source LLMscome very close to proprietary ones. Thirdly, it is important to note that someLLMs may be overly calibrated towards exhibiting trustworthiness, to the extentthat they compromise their utility by mistakenly treating benign prompts asharmful and consequently not responding. Finally, we emphasize the importanceof ensuring transparency not only in the models themselves but also in thetechnologies that underpin trustworthiness. Knowing the specific trustworthytechnologies that have been employed is crucial for analyzing theireffectiveness.",
        "title": "TrustLLM: Trustworthiness in Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05562",
        "abstract_url": "http://arxiv.org/abs/2401.05562",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhangchen"
            },
            {
                "last_name": "Jiang",
                "first_name": "Fengqing"
            },
            {
                "last_name": "Niu",
                "first_name": "Luyao"
            },
            {
                "last_name": "Jia",
                "first_name": "Jinyuan"
            },
            {
                "last_name": "Poovendran",
                "first_name": "Radha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "DC"
        ],
        "abstract": "  Federated learning (FL) enables multiple participants to train a globalmachine learning model without sharing their private training data.Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminatingthe server that aggregates local models from participants and then updates theglobal model. However, P2P FL is vulnerable to (i) honest-but-curiousparticipants whose objective is to infer private training data of otherparticipants, and (ii) Byzantine participants who can transmit arbitrarilymanipulated local models to corrupt the learning process. P2P FL schemes thatsimultaneously guarantee Byzantine resilience and preserve privacy have beenless studied. In this paper, we develop Brave, a protocol that ensuresByzantine Resilience And privacy-preserving property for P2P FL in the presenceof both types of adversaries. We show that Brave preserves privacy byestablishing that any honest-but-curious adversary cannot infer otherparticipants' private data by observing their models. We further prove thatBrave is Byzantine-resilient, which guarantees that all benign participantsconverge to an identical model that deviates from a global model trainedwithout Byzantine adversaries by a bounded distance. We evaluate Brave againstthree state-of-the-art adversaries on a P2P FL for image classification taskson benchmark datasets CIFAR10 and MNIST. Our results show that the global modellearned with Brave in the presence of adversaries achieves comparableclassification accuracy to a global model trained in the absence of anyadversary.",
        "title": "Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated  Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05563",
        "abstract_url": "http://arxiv.org/abs/2401.05563",
        "authors": [
            {
                "last_name": "Dwarakanath",
                "first_name": "Kshama"
            },
            {
                "last_name": "Vyetrenko",
                "first_name": "Svitlana"
            },
            {
                "last_name": "Oyebode",
                "first_name": "Toks"
            },
            {
                "last_name": "Balch",
                "first_name": "Tucker"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Is transparency always beneficial in complex systems such as traffic networksand stock markets? How is transparency defined in multi-agent systems, and whatis its optimal degree at which social welfare is highest? We take anagent-based view to define transparency (or its lacking) as delay in agentobservability of environment states, and utilize simulations to analyze theimpact of delay on social welfare. To model the adaptation of agent strategieswith varying delays, we model agents as learners maximizing the same objectivesunder different delays in a simulated environment. Focusing on two agent types- constrained and unconstrained, we use multi-agent reinforcement learning toevaluate the impact of delay on agent outcomes and social welfare. Empiricaldemonstration of our framework in simulated financial markets shows opposingtrends in outcomes of the constrained and unconstrained agents with delay, withan optimal partial transparency regime at which social welfare is maximal.",
        "title": "Transparency as Delayed Observability in Multi-Agent Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05566",
        "abstract_url": "http://arxiv.org/abs/2401.05566",
        "authors": [
            {
                "last_name": "Hubinger",
                "first_name": "Evan"
            },
            {
                "last_name": "Denison",
                "first_name": "Carson"
            },
            {
                "last_name": "Mu",
                "first_name": "Jesse"
            },
            {
                "last_name": "Lambert",
                "first_name": "Mike"
            },
            {
                "last_name": "Tong",
                "first_name": "Meg"
            },
            {
                "last_name": "MacDiarmid",
                "first_name": "Monte"
            },
            {
                "last_name": "Lanham",
                "first_name": "Tamera"
            },
            {
                "last_name": "Ziegler",
                "first_name": "Daniel M."
            },
            {
                "last_name": "Maxwell",
                "first_name": "Tim"
            },
            {
                "last_name": "Cheng",
                "first_name": "Newton"
            },
            {
                "last_name": "Jermyn",
                "first_name": "Adam"
            },
            {
                "last_name": "Askell",
                "first_name": "Amanda"
            },
            {
                "last_name": "Radhakrishnan",
                "first_name": "Ansh"
            },
            {
                "last_name": "Anil",
                "first_name": "Cem"
            },
            {
                "last_name": "Duvenaud",
                "first_name": "David"
            },
            {
                "last_name": "Ganguli",
                "first_name": "Deep"
            },
            {
                "last_name": "Barez",
                "first_name": "Fazl"
            },
            {
                "last_name": "Clark",
                "first_name": "Jack"
            },
            {
                "last_name": "Ndousse",
                "first_name": "Kamal"
            },
            {
                "last_name": "Sachan",
                "first_name": "Kshitij"
            },
            {
                "last_name": "Sellitto",
                "first_name": "Michael"
            },
            {
                "last_name": "Sharma",
                "first_name": "Mrinank"
            },
            {
                "last_name": "DasSarma",
                "first_name": "Nova"
            },
            {
                "last_name": "Grosse",
                "first_name": "Roger"
            },
            {
                "last_name": "Kravec",
                "first_name": "Shauna"
            },
            {
                "last_name": "Bai",
                "first_name": "Yuntao"
            },
            {
                "last_name": "Witten",
                "first_name": "Zachary"
            },
            {
                "last_name": "Favaro",
                "first_name": "Marina"
            },
            {
                "last_name": "Brauner",
                "first_name": "Jan"
            },
            {
                "last_name": "Karnofsky",
                "first_name": "Holden"
            },
            {
                "last_name": "Christiano",
                "first_name": "Paul"
            },
            {
                "last_name": "Bowman",
                "first_name": "Samuel R."
            },
            {
                "last_name": "Graham",
                "first_name": "Logan"
            },
            {
                "last_name": "Kaplan",
                "first_name": "Jared"
            },
            {
                "last_name": "Mindermann",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Greenblatt",
                "first_name": "Ryan"
            },
            {
                "last_name": "Shlegeris",
                "first_name": "Buck"
            },
            {
                "last_name": "Schiefer",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Perez",
                "first_name": "Ethan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "CL",
            "LG",
            "SE"
        ],
        "abstract": "  Humans are capable of strategically deceptive behavior: behaving helpfully inmost situations, but then behaving very differently in order to pursuealternative objectives when given the opportunity. If an AI system learned sucha deceptive strategy, could we detect it and remove it using currentstate-of-the-art safety training techniques? To study this question, weconstruct proof-of-concept examples of deceptive behavior in large languagemodels (LLMs). For example, we train models that write secure code when theprompt states that the year is 2023, but insert exploitable code when thestated year is 2024. We find that such backdoored behavior can be madepersistent, so that it is not removed by standard safety training techniques,including supervised fine-tuning, reinforcement learning, and adversarialtraining (eliciting unsafe behavior and then training to remove it). Thebackdoored behavior is most persistent in the largest models and in modelstrained to produce chain-of-thought reasoning about deceiving the trainingprocess, with the persistence remaining even when the chain-of-thought isdistilled away. Furthermore, rather than removing backdoors, we find thatadversarial training can teach models to better recognize their backdoortriggers, effectively hiding the unsafe behavior. Our results suggest that,once a model exhibits deceptive behavior, standard techniques could fail toremove such deception and create a false impression of safety.",
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety  Training",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05568",
        "abstract_url": "http://arxiv.org/abs/2401.05568",
        "authors": [
            {
                "last_name": "Vandermause",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Johansson",
                "first_name": "Anders"
            },
            {
                "last_name": "Miao",
                "first_name": "Yucong"
            },
            {
                "last_name": "Vlassak",
                "first_name": "Joost J."
            },
            {
                "last_name": "Kozinsky",
                "first_name": "Boris"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range ofbiomedical and engineering devices, but direct molecular dynamics simulationsof the martensitic B19' -> B2 phase transition driving its shape-memorybehavior are rare and have relied on classical force fields with limitedaccuracy. Here, we train four machine-learned force fields for equiatomic NiTibased on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trainedon the fly during NPT molecular dynamics, with DFT calculations and modelupdates performed automatically whenever the uncertainty of a local energyprediction exceeds a chosen threshold. The models achieve accuracies of 1-2meV/atom during training and are shown to closely track DFT predictions of B2and B19' elastic constants and phonon frequencies. Surprisingly, in large-scalemolecular dynamics simulations, only the SCAN model predicts a reversible B19'-> B2 phase transition, with the LDA, PBE, and PBEsol models predicting areversible transition to a previously uncharacterized low-volume phase, whichwe hypothesize to be a new stable high-pressure phase. We examine the structureof the new phase and estimate its stability on the temperature-pressure phasediagram. This work establishes an automated active learning protocol forstudying displacive transformations, reveals important differences between DFTfunctionals that can only be detected in large-scale simulations, provides anaccurate force field for NiTi, and identifies a new phase.",
        "title": "Phase discovery with active learning: Application to structural phase  transitions in equiatomic NiTi",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05569",
        "abstract_url": "http://arxiv.org/abs/2401.05569",
        "authors": [
            {
                "last_name": "Ozen",
                "first_name": "Irfan"
            },
            {
                "last_name": "Subramani",
                "first_name": "Karthika"
            },
            {
                "last_name": "Vadrevu",
                "first_name": "Phani"
            },
            {
                "last_name": "Perdisci",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Social engineering (SE) aims at deceiving users into performing actions thatmay compromise their security and privacy. These threats exploit weaknesses inhuman's decision making processes by using tactics such as pretext, baiting,impersonation, etc. On the web, SE attacks include attack classes such asscareware, tech support scams, survey scams, sweepstakes, etc., which canresult in sensitive data leaks, malware infections, and monetary loss. Forinstance, US consumers lose billions of dollars annually due to various SEattacks. Unfortunately, generic social engineering attacks remain understudied,compared to other important threats, such as software vulnerabilities andexploitation, network intrusions, malicious software, and phishing. The fewexisting technical studies that focus on social engineering are limited inscope and mostly focus on measurements rather than developing a genericdefense. To fill this gap, we present SEShield, a framework for in-browserdetection of social engineering attacks. SEShield consists of three maincomponents: (i) a custom security crawler, called SECrawler, that is dedicatedto scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet,a deep learning-based image classifier trained on data collected by SECrawlerthat aims to detect the often glaring visual traits of SE attack pages; and(iii) SEGuard, a proof-of-concept extension that embeds SENet into the webbrowser and enables real-time SE attack detection. We perform an extensiveevaluation of our system and show that SENet is able to detect new instances ofSE attacks with a detection rate of up to 99.6% at 1% false positive, thusproviding an effective first defense against SE attacks on the web.",
        "title": "SENet: Visual Detection of Online Social Engineering Attack Campaigns",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05570",
        "abstract_url": "http://arxiv.org/abs/2401.05570",
        "authors": [
            {
                "last_name": "Van Vorst",
                "first_name": "Kevin"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Self-supervised learning has become a popular way to pretrain a deep learningmodel and then transfer it to perform downstream tasks. However, most of thesemethods are developed on large-scale image datasets that contain naturalobjects with clear textures, outlines, and distinct color contrasts. It remainsuncertain whether these methods are equally effective for medical imaging,where the regions of interest often blend subtly and indistinctly with thesurrounding tissues. In this study, we propose an alternative method that usescontralateral mammograms to train a neural network to encode similar embeddingswhen a pair contains both normal images and different embeddings when a paircontains normal and abnormal images. Our approach leverages the naturalsymmetry of human body as weak labels to learn to distinguish abnormal lesionsfrom background tissues in a fully unsupervised manner. Our findings suggestthat it's feasible by incorporating soft labels derived from the Euclideandistances between the embeddings of the image pairs into the Siamese networkloss. Our method demonstrates superior performance in mammogram patchclassification compared to existing self-supervised learning methods. Thisapproach not only leverages a vast amount of image data effectively but alsominimizes reliance on costly labels, a significant advantage particularly inthe field of medical imaging.",
        "title": "Siamese Networks with Soft Labels for Unsupervised Lesion Detection and  Patch Pretraining on Screening Mammograms",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05571",
        "abstract_url": "http://arxiv.org/abs/2401.05571",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hanrui"
            },
            {
                "last_name": "Gu",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Zirui"
            },
            {
                "last_name": "Pan",
                "first_name": "David Z."
            },
            {
                "last_name": "Chong",
                "first_name": "Frederic T."
            },
            {
                "last_name": "Han",
                "first_name": "Song"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhangyang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "AR",
            "LG"
        ],
        "abstract": "  Parameterized Quantum Circuits (PQC) have obtained increasing popularitythanks to their great potential for near-term Noisy Intermediate-Scale Quantum(NISQ) computers. Achieving quantum advantages usually requires a large numberof qubits and quantum circuits with enough capacity. However, limited coherencetime and massive quantum noises severely constrain the size of quantum circuitsthat can be executed reliably on real machines. To address these two painpoints, we propose QuantumSEA, an in-time sparse exploration for noise-adaptivequantum circuits, aiming to achieve two key objectives: (1) implicit circuitscapacity during training - by dynamically exploring the circuit's sparseconnectivity and sticking a fixed small number of quantum gates throughout thetraining which satisfies the coherence time and enjoy light noises, enablingfeasible executions on real quantum devices; (2) noise robustness - by jointlyoptimizing the topology and parameters of quantum circuits under real devicenoise models. In each update step of sparsity, we leverage the moving averageof historical gradients to grow necessary gates and utilize salience-basedpruning to eliminate insignificant gates. Extensive experiments are conductedwith 7 Quantum Machine Learning (QML) and Variational Quantum Eigensolver (VQE)benchmarks on 6 simulated or real quantum computers, where QuantumSEAconsistently surpasses noise-aware search, human-designed, and randomlygenerated quantum circuit baselines by a clear performance margin. For example,even in the most challenging on-chip training regime, our method establishesstate-of-the-art results with only half the number of quantum gates and ~2xtime saving of circuit executions. Codes are available athttps://github.com/VITA-Group/QuantumSEA.",
        "title": "QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum  Circuits",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05572",
        "abstract_url": "http://arxiv.org/abs/2401.05572",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Qin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "MA",
            "RO"
        ],
        "abstract": "  Innate values describe agents' intrinsic motivations, which reflect theirinherent interests and preferences to pursue goals and drive them to developdiverse skills satisfying their various needs. The essence of reinforcementlearning (RL) is learning from interaction based on reward-driven (such asutilities) behaviors, much like natural agents. It is an excellent model todescribe the innate-values-driven (IV) behaviors of AI agents. Especially inmulti-agent systems (MAS), building the awareness of AI agents to balance thegroup utilities and system costs and satisfy group members' needs in theircooperation is a crucial problem for individuals learning to support theircommunity and integrate human society in the long term. This paper proposes ahierarchical compound intrinsic value reinforcement learning model --innate-values-driven reinforcement learning termed IVRL to describe the complexbehaviors of multi-agent interaction in their cooperation. We implement theIVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment andcompare the cooperative performance within three characteristics of innatevalue agents (Coward, Neutral, and Reckless) through three benchmarkmulti-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate thatby organizing individual various needs rationally, the group can achieve betterperformance with lower costs effectively.",
        "title": "Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent  Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05577",
        "abstract_url": "http://arxiv.org/abs/2401.05577",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Chenbin"
            },
            {
                "last_name": "Yaman",
                "first_name": "Burhaneddin"
            },
            {
                "last_name": "Nesti",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Mallik",
                "first_name": "Abhirup"
            },
            {
                "last_name": "Allievi",
                "first_name": "Alessandro G"
            },
            {
                "last_name": "Velipasalar",
                "first_name": "Senem"
            },
            {
                "last_name": "Ren",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Autonomous driving is a complex and challenging task that aims at safe motionplanning through scene understanding and reasoning. While vision-onlyautonomous driving methods have recently achieved notable performance, throughenhanced scene understanding, several key issues, including lack of reasoning,low generalization performance and long-tail scenarios, still need to beaddressed. In this paper, we present VLP, a novel Vision-Language-Planningframework that exploits language models to bridge the gap between linguisticunderstanding and autonomous driving. VLP enhances autonomous driving systemsby strengthening both the source memory foundation and the self-driving car'scontextual understanding. VLP achieves state-of-the-art end-to-end planningperformance on the challenging NuScenes dataset by achieving 35.9\\% and 60.5\\%reduction in terms of average L2 error and collision rates, respectively,compared to the previous best method. Moreover, VLP shows improved performancein challenging long-tail scenarios and strong generalization capabilities whenfaced with new urban environments.",
        "title": "VLP: Vision Language Planning for Autonomous Driving",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05578",
        "abstract_url": "http://arxiv.org/abs/2401.05578",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Zang",
                "first_name": "Zhenya"
            },
            {
                "last_name": "Li",
                "first_name": "Xingda"
            },
            {
                "last_name": "Li",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a rapid and precise analytical approach for analyzing cerebralblood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with theapplication of the Extreme Learning Machine (ELM). Our evaluation of ELM andexisting algorithms involves a comprehensive set of metrics. We assess thesealgorithms using synthetic datasets for both semi-infinite and multi-layermodels. The results demonstrate that ELM consistently achieves higher fidelityacross various noise levels and optical parameters, showcasing robustgeneralization ability and outperforming iterative fitting algorithms. Througha comparison with a computationally efficient neural network, ELM attainscomparable accuracy with reduced training and inference times. Notably, theabsence of a back-propagation process in ELM during training results insignificantly faster training speeds compared to existing neural networkapproaches. This proposed strategy holds promise for edge computingapplications with online training capabilities.",
        "title": "Fast Cerebral Blood Flow Analysis via Extreme Learning Machine",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05579",
        "abstract_url": "http://arxiv.org/abs/2401.05579",
        "authors": [
            {
                "last_name": "Raihan",
                "first_name": "Ahmed Shoyeb"
            },
            {
                "last_name": "Khosravi",
                "first_name": "Hamed"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Tanveer Hossain"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Imtiaz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry,offering benefits like intricate design, minimal waste, rapid prototyping,material versatility, and customized solutions. However, its full industryadoption faces hurdles, particularly in achieving consistent product quality. Acrucial aspect for MAM's success is understanding the relationship betweenprocess parameters and melt pool characteristics. Integrating ArtificialIntelligence (AI) into MAM is essential. Traditional machine learning (ML)methods, while effective, depend on large datasets to capture complexrelationships, a significant challenge in MAM due to the extensive time andresources required for dataset creation. Our study introduces a novelsurprise-guided sequential learning framework, SurpriseAF-BO, signaling asignificant shift in MAM. This framework uses an iterative, adaptive learningprocess, modeling the dynamics between process parameters and melt poolcharacteristics with limited data, a key benefit in MAM's cyber manufacturingcontext. Compared to traditional ML models, our sequential learning methodshows enhanced predictive accuracy for melt pool dimensions. Further improvingour approach, we integrated a Conditional Tabular Generative AdversarialNetwork (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This producessynthetic data resembling real experimental data, improving learningeffectiveness. This enhancement boosts predictive precision without requiringadditional physical experiments. Our study demonstrates the power of advanceddata-driven techniques in cyber manufacturing and the substantial impact ofsequential AI and ML, particularly in overcoming MAM's traditional challenges.",
        "title": "An Augmented Surprise-guided Sequential Learning Framework for  Predicting the Melt Pool Geometry",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05580",
        "abstract_url": "http://arxiv.org/abs/2401.05580",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Li",
                "first_name": "Xingda"
            },
            {
                "last_name": "Li",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Diffuse correlation spectroscopy (DCS) is an emerging noninvasive techniquethat measures the tissue blood flow, by using near-infrared coherentpoint-source illumination to detect spectral changes. While machine learninghas demonstrated significant potential for measuring blood flow index (BFi), anopen question concerning the success of this approach pertains to itsrobustness in scenarios involving deviations between datasets with varyingSignal-to-Noise Ratios (SNRs) originating from diverse clinical applicationsand various setups. This study proposes a transfer learning approach, aims toassess the influence of SNRs on the generalization ability of learned features,and demonstrate the robustness for transfer learning. A synthetic dataset withvarying levels of added noise is utilized to simulate different SNRs. Theproposed network takes a 1x64 autocorrelation curve as input and generates BFiand the correlation parameter beta. The proposed model demonstrates excellentperformance across different SNRs, exhibiting enhanced fitting accuracy,particularly for low SNR datasets when compared with other fitting methods.This highlights its potential for clinical diagnosis and treatment acrossvarious scenarios under different clinical setups.",
        "title": "Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A  Transfer Learning Approach with Noise Robustness Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05583",
        "abstract_url": "http://arxiv.org/abs/2401.05583",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chaoyang"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Peiye"
            },
            {
                "last_name": "Siarohin",
                "first_name": "Aliaksandr"
            },
            {
                "last_name": "Cao",
                "first_name": "Junli"
            },
            {
                "last_name": "Qian",
                "first_name": "Guocheng"
            },
            {
                "last_name": "Lee",
                "first_name": "Hsin-Ying"
            },
            {
                "last_name": "Tulyakov",
                "first_name": "Sergey"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Dynamic novel view synthesis aims to capture the temporal evolution of visualcontent within videos. Existing methods struggle to distinguishing betweenmotion and structure, particularly in scenarios where camera poses are eitherunknown or constrained compared to object motion. Furthermore, with informationsolely from reference images, it is extremely challenging to hallucinate unseenregions that are occluded or partially observed in the given videos. To addressthese issues, we first finetune a pretrained RGB-D diffusion model on the videoframes using a customization technique. Subsequently, we distill the knowledgefrom the finetuned model to a 4D representations encompassing both dynamic andstatic Neural Radiance Fields (NeRF) components. The proposed pipeline achievesgeometric consistency while preserving the scene identity. We perform thoroughexperiments to evaluate the efficacy of the proposed method qualitatively andquantitatively. Our results demonstrate the robustness and utility of ourapproach in challenging cases, further advancing dynamic novel view synthesis.",
        "title": "Diffusion Priors for Dynamic View Synthesis from Monocular Videos",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05584",
        "abstract_url": "http://arxiv.org/abs/2401.05584",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Edison"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Maruf"
            },
            {
                "last_name": "Sun",
                "first_name": "Yue"
            },
            {
                "last_name": "Mahendru",
                "first_name": "Rahul"
            },
            {
                "last_name": "Yang",
                "first_name": "Rui"
            },
            {
                "last_name": "Cook",
                "first_name": "Harrison"
            },
            {
                "last_name": "Leeuwenburg",
                "first_name": "Tennessee"
            },
            {
                "last_name": "Evans",
                "first_name": "Ben"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recently, the FourCastNet Neural Earth System Model (NESM) has shownimpressive results on predicting various atmospheric variables, trained on theERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memorycomplexity in sequence length compared to quadratic complexity in vanillatransformers, training FourCastNet on ERA5 from scratch still requires largeamount of compute resources, which is expensive or even inaccessible to mostresearchers. In this work, we will show improved methods that can trainFourCastNet using only 1% of the compute required by the baseline, whilemaintaining model performance or par or even better than the baseline.",
        "title": "FourCastNeXt: Improving FourCastNet Training with Limited Compute",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05585",
        "abstract_url": "http://arxiv.org/abs/2401.05585",
        "authors": [
            {
                "last_name": "Kirigin",
                "first_name": "Tajana Ban"
            },
            {
                "last_name": "Comer",
                "first_name": "Jesse"
            },
            {
                "last_name": "Kanovich",
                "first_name": "Max"
            },
            {
                "last_name": "Scedrov",
                "first_name": "Andre"
            },
            {
                "last_name": "Talcott",
                "first_name": "Carolyn"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Most research on formal system design has focused on optimizing variousmeasures of efficiency. However, insufficient attention has been given to thedesign of systems optimizing resilience, the ability of systems to adapt tounexpected changes or adversarial disruptions. In our prior work, we formalizedthe intuitive notion of resilience as a property of cyber-physical systems byusing a multiset rewriting language with explicit time. In the present work, westudy the computational complexity of a formalization of time-boundedresilience problems for the class of progressing timed systems (PTS), where,intuitively, only a finite number of actions can be carried out in a boundedtime period. We show that, in the time-bounded model with n (potentiallyadversarially chosen) updates, the corresponding time-bounded resilienceproblem is complete for the $\\Sigma^P_{2n+1}$ class of the polynomialhierarchy, PH. To support the formal models and complexity results, we performautomated experiments for time-bounded verification using the rewriting logictool Maude.",
        "title": "Technical Report: Time-Bounded Resilience",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05587",
        "abstract_url": "http://arxiv.org/abs/2401.05587",
        "authors": [
            {
                "last_name": "Sullivan",
                "first_name": "Dakota"
            },
            {
                "last_name": "White",
                "first_name": "Nathan Thomas"
            },
            {
                "last_name": "Schoen",
                "first_name": "Andrew"
            },
            {
                "last_name": "Mutlu",
                "first_name": "Bilge"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robots are ubiquitous in small-to-large-scale manufacturers. Whilecollaborative robots (cobots) have significant potential in these settings dueto their flexibility and ease of use, proper integration is critical to realizetheir full potential. Specifically, cobots need to be integrated in ways thatutilize their strengths, improve manufacturing performance, and facilitate usein concert with human workers. Effective integration requires carefulconsideration and the knowledge of roboticists, manufacturing engineers, andbusiness administrators. We propose an approach involving the stages ofplanning, analysis, development, and presentation, to inform manufacturersabout cobot integration within their facilities prior to the integrationprocess. We contextualize our approach in a case study with an SME collaboratorand discuss insights learned.",
        "title": "Making Informed Decisions: Supporting Cobot Integration Considering  Business and Worker Preferences",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05593",
        "abstract_url": "http://arxiv.org/abs/2401.05593",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Adrian Xuan Wei"
            },
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Griffin",
                "first_name": "Conor"
            },
            {
                "last_name": "Kyger",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Baghernezhad",
                "first_name": "Faraz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present Reverse Projection, a novel projective texture mapping techniquefor painting a decal directly to the texture of a 3D object. Designed to beused in games, this technique works in real-time. By using projectiontechniques that are computed in local space textures and outward-looking, usersusing low-end android devices to high-end gaming desktops are able to enjoy thepersonalization of their assets. We believe our proposed pipeline is a step inimproving the speed and versatility of model painting.",
        "title": "Reverse Projection: Real-Time Local Space Texture Mapping",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05594",
        "abstract_url": "http://arxiv.org/abs/2401.05594",
        "authors": [
            {
                "last_name": "Mallick",
                "first_name": "Prakash"
            },
            {
                "last_name": "Dayoub",
                "first_name": "Feras"
            },
            {
                "last_name": "Sherrah",
                "first_name": "Jamie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper addresses the significant challenge in open-set object detection(OSOD): the tendency of state-of-the-art detectors to erroneously classifyunknown objects as known categories with high confidence. We present a novelapproach that effectively identifies unknown objects by distinguishing betweenhigh and low-density regions in latent space. Our method builds upon theOpen-Det (OD) framework, introducing two new elements to the loss function.These elements enhance the known embedding space's clustering and expand theunknown space's low-density regions. The first addition is the ClassWasserstein Anchor (CWA), a new function that refines the classificationboundaries. The second is a spectral normalisation step, improving therobustness of the model. Together, these augmentations to the existingContrastive Feature Learner (CFL) and Unknown Probability Learner (UPL) lossfunctions significantly improve OSOD performance. Our proposed OpenDet-CWA(OD-CWA) method demonstrates: a) a reduction in open-set errors byapproximately 17%-22%, b) an enhancement in novelty detection capability by1.5%-16%, and c) a decrease in the wilderness index by 2%-20% across variousopen-set scenarios. These results represent a substantial advancement in thefield, showcasing the potential of our approach in managing the complexities ofopen-set object detection.",
        "title": "Wasserstein Distance-based Expansion of Low-Density Latent Regions for  Unknown Class Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05596",
        "abstract_url": "http://arxiv.org/abs/2401.05596",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Shilong"
            },
            {
                "last_name": "Tian",
                "first_name": "Zhiliang"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Huang",
                "first_name": "Zhen"
            },
            {
                "last_name": "Wen",
                "first_name": "Zhihua"
            },
            {
                "last_name": "Li",
                "first_name": "Dongsheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Low-resource languages (LRLs) face challenges in supervised neural machinetranslation due to limited parallel data, prompting research into unsupervisedmethods. Unsupervised neural machine translation (UNMT) methods, includingback-translation, transfer learning, and pivot-based translation, offerpractical solutions for LRL translation, but they are hindered by issues likesynthetic data noise, language bias, and error propagation, which canpotentially be mitigated by Large Language Models (LLMs). LLMs have advancedNMT with in-context learning (ICL) and supervised fine-tuning methods, butinsufficient training data results in poor performance in LRLs. We argue thatLLMs can mitigate the linguistic noise with auxiliary languages to improvetranslations in LRLs. In this paper, we propose Probability-driven Meta-graphPrompter (POMP), a novel approach employing a dynamic, sampling-based graph ofmultiple auxiliary languages to enhance LLMs' translation capabilities forLRLs. POMP involves constructing a directed acyclic meta-graph for each sourcelanguage, from which we dynamically sample multiple paths to prompt LLMs tomitigate the linguistic noise and improve translations during training. We usethe BLEURT metric to evaluate the translations and back-propagate rewards,estimated by scores, to update the probabilities of auxiliary languages in thepaths. Our experiments show significant improvements in the translation qualityof three LRLs, demonstrating the effectiveness of our approach.",
        "title": "POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource  Unsupervised Neural Machine Translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05602",
        "abstract_url": "http://arxiv.org/abs/2401.05602",
        "authors": [
            {
                "last_name": "Remedios",
                "first_name": "Lucas W."
            },
            {
                "last_name": "Bao",
                "first_name": "Shunxing"
            },
            {
                "last_name": "Remedios",
                "first_name": "Samuel W."
            },
            {
                "last_name": "Lee",
                "first_name": "Ho Hin"
            },
            {
                "last_name": "Cai",
                "first_name": "Leon Y."
            },
            {
                "last_name": "Li",
                "first_name": "Thomas"
            },
            {
                "last_name": "Deng",
                "first_name": "Ruining"
            },
            {
                "last_name": "Cui",
                "first_name": "Can"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Liu",
                "first_name": "Qi"
            },
            {
                "last_name": "Lau",
                "first_name": "Ken S."
            },
            {
                "last_name": "Roland",
                "first_name": "Joseph T."
            },
            {
                "last_name": "Washington",
                "first_name": "Mary K."
            },
            {
                "last_name": "Coburn",
                "first_name": "Lori A."
            },
            {
                "last_name": "Wilson",
                "first_name": "Keith T."
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            },
            {
                "last_name": "Landman",
                "first_name": "Bennett A."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Understanding the way cells communicate, co-locate, and interrelate isessential to understanding human physiology. Hematoxylin and eosin (H&E)staining is ubiquitously available both for clinical studies and research. TheColon Nucleus Identification and Classification (CoNIC) Challenge has recentlyinnovated on robust artificial intelligence labeling of six cell types on H&Estains of the colon. However, this is a very small fraction of the number ofpotential cell classification types. Specifically, the CoNIC Challenge isunable to classify epithelial subtypes (progenitor, endocrine, goblet),lymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes(fibroblasts, stromal). In this paper, we propose to use inter-modalitylearning to label previously un-labelable cell types on virtual H&E. Weleveraged multiplexed immunofluorescence (MxIF) histology imaging to identify14 subclasses of cell types. We performed style transfer to synthesize virtualH&E from MxIF and transferred the higher density labels from MxIF to thesevirtual H&E images. We then evaluated the efficacy of learning in thisapproach. We identified helper T and progenitor nuclei with positive predictivevalues of $0.34 \\pm 0.15$ (prevalence $0.03 \\pm 0.01$) and $0.47 \\pm 0.1$(prevalence $0.07 \\pm 0.02$) respectively on virtual H&E. This approachrepresents a promising step towards automating annotation in digital pathology.",
        "title": "Nucleus subtype classification using inter-modality learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05603",
        "abstract_url": "http://arxiv.org/abs/2401.05603",
        "authors": [
            {
                "last_name": "Jhaver",
                "first_name": "Shagun"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  Personal moderation tools on social media platforms allow users to controltheir feeds by configuring the acceptable toxicity thresholds for their feedcontent or muting inappropriate accounts. This research examines how theend-user configuration of these tools is shaped by four critical psychosocialfactors - fear of missing out (FoMO), social media addiction, subjective norms,and trust in moderation systems. Findings from a nationally representativesample of 1,061 participants show that FoMO and social media addiction makeFacebook users more vulnerable to content-based harms by reducing theirlikelihood of adopting personal moderation tools to hide inappropriate posts.In contrast, descriptive and injunctive norms positively influence the use ofthese tools. Further, trust in Facebook's moderation systems also significantlyaffects users' engagement with personal moderation. This analysis highlightsqualitatively different pathways through which FoMO and social media addictionmake affected users disproportionately unsafe and offers design and policysolutions to address this challenge.",
        "title": "Exploring How FoMO, Social Media Addiction, and Subjective Norms  Influence Personal Moderation Configurations",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05604",
        "abstract_url": "http://arxiv.org/abs/2401.05604",
        "authors": [
            {
                "last_name": "Gritsevskiy",
                "first_name": "Andrew"
            },
            {
                "last_name": "Panickssery",
                "first_name": "Arjun"
            },
            {
                "last_name": "Kirtland",
                "first_name": "Aaron"
            },
            {
                "last_name": "Kauffman",
                "first_name": "Derik"
            },
            {
                "last_name": "Gundlach",
                "first_name": "Hans"
            },
            {
                "last_name": "Gritsevskaya",
                "first_name": "Irina"
            },
            {
                "last_name": "Cavanagh",
                "first_name": "Joe"
            },
            {
                "last_name": "Chiang",
                "first_name": "Jonathan"
            },
            {
                "last_name": "La Roux",
                "first_name": "Lydia"
            },
            {
                "last_name": "Hung",
                "first_name": "Michelle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "CV",
            "CY"
        ],
        "abstract": "  We propose a new benchmark evaluating the performance of multimodal largelanguage models on rebus puzzles. The dataset covers 333 original examples ofimage-based wordplay, cluing 13 categories such as movies, composers, majorcities, and food. To achieve good performance on the benchmark of identifyingthe clued word or phrase, models must combine image recognition and stringmanipulation with hypothesis testing, multi-step reasoning, and anunderstanding of human cognition, making for a complex, multimodal evaluationof capabilities. We find that proprietary models such as GPT-4V and Gemini Prosignificantly outperform all other tested models. However, even the best modelhas a final accuracy of just 24%, highlighting the need for substantialimprovements in reasoning. Further, models rarely understand all parts of apuzzle, and are almost always incapable of retroactively explaining the correctanswer. Our benchmark can therefore be used to identify major shortcomings inthe knowledge and reasoning of multimodal large language models.",
        "title": "REBUS: A Robust Evaluation Benchmark of Understanding Symbols",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05605",
        "abstract_url": "http://arxiv.org/abs/2401.05605",
        "authors": [
            {
                "last_name": "Kalajdzievski",
                "first_name": "Damjan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            ""
        ],
        "abstract": "  We study and quantify the problem of forgetting when fine-tuning pre-trainedlarge language models (LLMs) on a downstream task. We find thatparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters(LoRA), still suffer from catastrophic forgetting. In particular, we identify astrong inverse linear relationship between the fine-tuning performance and theamount of forgetting when fine-tuning LLMs with LoRA. We further obtain precisescaling laws that show forgetting increases as a shifted power law in thenumber of parameters fine-tuned and the number of update steps. We also examinethe impact of forgetting on knowledge, reasoning, and the safety guardrailstrained into Llama 2 7B chat. Our study suggests that forgetting cannot beavoided through early stopping or by varying the number of parametersfine-tuned. We believe this opens up an important safety-critical direction forfuture research to evaluate and develop fine-tuning schemes which mitigateforgetting",
        "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05609",
        "abstract_url": "http://arxiv.org/abs/2401.05609",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenxiong"
            },
            {
                "last_name": "Huang",
                "first_name": "Qikun"
            },
            {
                "last_name": "Chen",
                "first_name": "Suiyin"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  This paper introduces a cable finite element model based on an accuratedescription of the tension field for the static nonlinear analysis of cablestructures. The proposed cable element is developed using the geometricallyexact beam model that adequately considers the effects of large displacements.By neglecting flexural stiffness and shear deformation, the formulation of thecable finite element for scenarios involving given unstrained length andundetermined unstrained length is respectively presented. Additionally, theimplementations of solutions based on complete tangent matrix and elementinternal iteration are introduced. Numerical examples are conducted to validatethe accuracy of the presented formulation for cable analysis under variousconditions and to demonstrate the computational efficiency of the proposedelement and solution method. The results indicate that the proposed cablefinite element not only exhibits extremely high accuracy but also effectivelyaddresses the problem of determining the cable state with an unknown unstrainedlength, demonstrating the wide applicability of the proposed element. Throughthe utilization of an iteration algorithm with arc-length control and theintroduction of additional control conditions, the proposed cable finiteelement can be further utilized to solve complex practical engineeringproblems.",
        "title": "A cable finite element formulation based on exact tension field for  static nonlinear analysis of cable structures",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05610",
        "abstract_url": "http://arxiv.org/abs/2401.05610",
        "authors": [
            {
                "last_name": "Dax",
                "first_name": "Victoria M."
            },
            {
                "last_name": "Li",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Leahy",
                "first_name": "Kevin"
            },
            {
                "last_name": "Kochenderfer",
                "first_name": "Mykel J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Graph-structured data is ubiquitous throughout natural and social sciences,and Graph Neural Networks (GNNs) have recently been shown to be effective atsolving prediction and inference problems on graph data. In this paper, wepropose and demonstrate that GNNs can be applied to solve CombinatorialOptimization (CO) problems. CO concerns optimizing a function over a discretesolution space that is often intractably large. To learn to solve CO problems,we formulate the optimization process as a sequential decision making problem,where the return is related to how close the candidate solution is tooptimality. We use a GNN to learn a policy to iteratively build increasinglypromising candidate solutions. We present preliminary evidence that GNNstrained through Q-Learning can solve CO problems with performance approachingstate-of-the-art heuristic-based solvers, using only a fraction of theparameters and training time.",
        "title": "Graph Q-Learning for Combinatorial Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05611",
        "abstract_url": "http://arxiv.org/abs/2401.05611",
        "authors": [
            {
                "last_name": "D\u00f6cker",
                "first_name": "Janosch"
            },
            {
                "last_name": "Linz",
                "first_name": "Simone"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CC",
            "DS"
        ],
        "abstract": "  Recently, there has been a growing interest in the relationships betweenunrooted and rooted phylogenetic networks. In this context, a natural questionto ask is if an unrooted phylogenetic network U can be oriented as a rootedphylogenetic network such that the latter satisfies certain structuralproperties. In a recent preprint, Bulteau et al. claim that it is computationalhard to decide if U has a funneled (resp. funneled tree-child) orientation, forwhen the internal vertices of U have degree at most 5. Unfortunately, the proofof their funneled tree-child result appears to be incorrect. In this paper, wepresent a corrected proof and show that hardness remains for other popularclasses of rooted phylogenetic networks such as funneled normal and funneledreticulation-visible. Additionally, our results hold regardless of whether U isrooted at an existing vertex or by subdividing an edge with the root.",
        "title": "On the existence of funneled orientations for classes of unrooted  phylogenetic networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05612",
        "abstract_url": "http://arxiv.org/abs/2401.05612",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Shiye"
            },
            {
                "last_name": "Liu",
                "first_name": "Anqi"
            },
            {
                "last_name": "Huang",
                "first_name": "Chien-Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Appropriate reliance is critical to achieving synergistic human-AIcollaboration. For instance, when users over-rely on AI assistance, theirhuman-AI team performance is bounded by the model's capability. This workstudies how the presentation of model uncertainty may steer users'decision-making toward fostering appropriate reliance. Our results demonstratethat showing the calibrated model uncertainty alone is inadequate. Rather,calibrating model uncertainty and presenting it in a frequency format allowusers to adjust their reliance accordingly and help reduce the effect ofconfirmation bias on their decisions. Furthermore, the critical nature of ourskin cancer screening task skews participants' judgment, causing their relianceto vary depending on their initial decision. Additionally, step-wise multipleregression analyses revealed how user demographics such as age and familiaritywith probability and statistics influence human-AI collaborativedecision-making. We discuss the potential for model uncertainty presentation,initial user decision, and user demographics to be incorporated in designingpersonalized AI aids for appropriate reliance.",
        "title": "Designing for Appropriate Reliance:Designing for Appropriate Reliance:  The Roles of AI Uncertainty Presentation, Initial User Decision, and User  Demographics in AI-Assisted Decision-Making",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05614",
        "abstract_url": "http://arxiv.org/abs/2401.05614",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Lian"
            },
            {
                "last_name": "Pun",
                "first_name": "Chi-Man"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "MM",
            ""
        ],
        "abstract": "  Due to the successful application of deep learning, audio spoofing detectionhas made significant progress. Spoofed audio with speech synthesis or voiceconversion can be well detected by many countermeasures. However, an automaticspeaker verification system is still vulnerable to spoofing attacks such asreplay or Deep-Fake audio. Deep-Fake audio means that the spoofed utterancesare generated using text-to-speech (TTS) and voice conversion (VC) algorithms.Here, we propose a novel framework based on hybrid features with theself-attention mechanism. It is expected that hybrid features can be used toget more discrimination capacity. Firstly, instead of only one type ofconventional feature, deep learning features and Mel-spectrogram features willbe extracted by two parallel paths: convolution neural networks and ashort-time Fourier transform (STFT) followed by Mel-frequency. Secondly,features will be concatenated by a max-pooling layer. Thirdly, there is aSelf-attention mechanism for focusing on essential elements. Finally, ResNetand a linear layer are built to get the results. Experimental results revealthat the hybrid features, compared with conventional features, can cover moredetails of an utterance. We achieve the best Equal Error Rate (EER) of 9.67\\%in the physical access (PA) scenario and 8.94\\% in the Deep fake task on theASVspoof 2021 dataset. Compared with the best baseline system, the proposedapproach improves by 74.60\\% and 60.05\\%, respectively.",
        "title": "Self-Attention and Hybrid Features for Replay and Deep-Fake Audio  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05618",
        "abstract_url": "http://arxiv.org/abs/2401.05618",
        "authors": [
            {
                "last_name": "Renze",
                "first_name": "Matthew"
            },
            {
                "last_name": "Guven",
                "first_name": "Erhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. Wecompared standard CoT and CCoT prompts to see how conciseness impacts responselength and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reducedaverage response length by 48.70% for both GPT-3.5 and GPT-4 while having anegligible impact on problem-solving performance. However, on math problems,GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leadsto an average per-token cost reduction of 22.67%. These results have practicalimplications for AI systems engineers using LLMs to solve real-world problemswith CoT prompt-engineering techniques. In addition, these results provide moregeneral insight for AI researchers studying the emergent behavior ofstep-by-step reasoning in LLMs.",
        "title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large  Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05624",
        "abstract_url": "http://arxiv.org/abs/2401.05624",
        "authors": [
            {
                "last_name": "Tissaoui",
                "first_name": "Yassine"
            },
            {
                "last_name": "Kelly",
                "first_name": "James F."
            },
            {
                "last_name": "Marras",
                "first_name": "Simone"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Mitigating the impact of waves leaving a numerical domain has been apersistent challenge in numerical modeling. Reducing wave reflection at thedomain boundary is crucial for accurate simulations. Absorbing layers, whilecommon, often incur significant computational costs. This paper introduces anefficient application of a Legendre-Laguerre basis for absorbing layers fortwo-dimensional non-linear compressible Euler equations. The method couples aspectral-element bounded domain with a semi-infinite region, employing a tensorproduct of Lagrange and scaled Laguerre basis functions. The semi-infiniteregion serves as an absorbing layer for our simulations. In comparison toexisting methods with similar absorbing layer extensions, our approach, apioneering application to the Euler equations, demonstrates substantialcomputational savings. The study marks the first application of semi-infiniteelements to mitigate wave reflection in the solution of the Euler equations,particularly in nonhydrostatic atmospheric modeling. A comprehensive set oftests demonstrates the method's versatility for general systems of conservationlaws, with a focus on its effectiveness in damping vertically propagatinggravity waves in a linear hydrostatic mountain simulation a benchmark foratmospheric models. Across all tests, our model consistently exhibits notableperformance improvements compared to a traditional Rayleigh damping approach.",
        "title": "Efficient Spectral Element Method for the Euler Equations on Unbounded  Domains in Multiple Dimensions",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05625",
        "abstract_url": "http://arxiv.org/abs/2401.05625",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Juni"
            },
            {
                "last_name": "Dong",
                "first_name": "Zhikang"
            },
            {
                "last_name": "Polak",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a novel method that combines differential geometry, kernelssmoothing, and spectral analysis to quantify facial muscle activity from widelyaccessible video recordings, such as those captured on personal smartphones.Our approach emphasizes practicality and accessibility. It has significantpotential for applications in national security and plastic surgery.Additionally, it offers remote diagnosis and monitoring for medical conditionssuch as stroke, Bell's palsy, and acoustic neuroma. Moreover, it is adept atdetecting and classifying emotions, from the overt to the subtle. The proposedface muscle analysis technique is an explainable alternative to deep learningmethods and a non-invasive substitute to facial electromyography (fEMG).",
        "title": "Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle  Dynamics in Videos",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05626",
        "abstract_url": "http://arxiv.org/abs/2401.05626",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Yizhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baoheng"
            },
            {
                "last_name": "Ding",
                "first_name": "Yuhao"
            },
            {
                "last_name": "So",
                "first_name": "Hayden Kwok-Hay"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Event-based vision represents a paradigm shift in how vision information iscaptured and processed. By only responding to dynamic intensity changes in thescene, event-based sensing produces far less data than conventional frame-basedcameras, promising to springboard a new generation of high-speed, low-powermachines for edge intelligence. However, processing such dynamically sparseinput originated from event cameras efficiently in real time, particularly withcomplex deep neural networks (DNN), remains a formidable challenge. Existingsolutions that employ GPUs and other frame-based DNN accelerators oftenstruggle to efficiently process the dynamically sparse event data, missing theopportunities to improve processing efficiency with sparse data. To addressthis, we propose ESDA, a composable dynamic sparse dataflow architecture thatallows customized DNN accelerators to be constructed rapidly on FPGAs forevent-based vision tasks. ESDA is a modular system that is composed of a set ofparametrizable modules for each network layer type. These modules share auniform sparse token-feature interface and can be connected easily to composean all-on-chip dataflow accelerator on FPGA for each network model. To fullyexploit the intrinsic sparsity in event data, ESDA incorporates the use ofsubmanifold sparse convolutions that largely enhance the activation sparsitythroughout the layers while simplifying hardware implementation. Finally, anetwork architecture and hardware implementation co-optimizing framework thatallows tradeoffs between accuracy and performance is also presented.Experimental results demonstrate that when compared with existing GPU andhardware-accelerated solutions, ESDA achieves substantial speedup andimprovement in energy efficiency across different applications, and it allowsmuch wider design space for real-world deployments.",
        "title": "A Composable Dynamic Sparse Dataflow Architecture for Efficient  Event-based Vision Processing on FPGA",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05627",
        "abstract_url": "http://arxiv.org/abs/2401.05627",
        "authors": [
            {
                "last_name": "Henzinger",
                "first_name": "Monika"
            },
            {
                "last_name": "Li",
                "first_name": "Jason"
            },
            {
                "last_name": "Rao",
                "first_name": "Satish"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In 1996, Karger [Kar96] gave a startling randomized algorithm that finds aminimum-cut in a (weighted) graph in time $O(m\\log^3n)$ which he termednear-linear time meaning linear (in the size of the input) times apolylogarthmic factor. In this paper, we give the first deterministic algorithmwhich runs in near-linear time for weighted graphs.  Previously, the breakthrough results of Kawarabayashi and Thorup [KT19] gavea near-linear time algorithm for simple graphs. The main technique here is aclustering procedure that perfectly preserves minimum cuts. Recently, Li [Li21]gave an $m^{1+o(1)}$ deterministic minimum-cut algorithm for weighted graphs;this form of running time has been termed \"almost-linear''. Li usesalmost-linear time deterministic expander decompositions which do not perfectlypreserve minimum cuts, but he can use these clusterings to, in a sense,\"derandomize'' the methods of Karger.  In terms of techniques, we provide a structural theorem that says thereexists a sparse clustering that preserves minimum cuts in a weighted graph with$o(1)$ error. In addition, we construct it deterministically in near lineartime. This was done exactly for simple graphs in [KT19, HRW20] and withpolylogarithmic error for weighted graphs in [Li21]. Extending the techniquesin [KT19, HRW20] to weighted graphs presents significant challenges, andmoreover, the algorithm can only polylogarithmically approximately preserveminimum cuts. A remaining challenge is to reduce thepolylogarithmic-approximate clusterings to $1+o(1/\\log n)$-approximate so thatthey can be applied recursively as in [Li21] over $O(\\log n)$ many levels. Thisis an additional challenge that requires building on properties oftree-packings in the presence of a wide range of edge weights to, for example,find sources for local flow computations which identify minimum cuts that crossclusters.",
        "title": "Deterministic Near-Linear Time Minimum Cut in Weighted Graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05628",
        "abstract_url": "http://arxiv.org/abs/2401.05628",
        "authors": [
            {
                "last_name": "Elkin",
                "first_name": "Michael"
            },
            {
                "last_name": "Trehan",
                "first_name": "Chhaya"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "",
            "",
            ""
        ],
        "abstract": "  Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a set $S \\subseteq V$,$|S| = n^{\\sigma}$ (for some $0 < \\sigma \\le 1$) of designated sources, the $S\\times V$-direcahability problem is to compute for every $s \\in S$, the set ofall the vertices reachable from $s$ in $G$. Known naive algorithms for thisproblem either run a BFS/DFS separately from every source, and as a resultrequire $O(m \\cdot n^{\\sigma})$ time, or compute the transitive closure of $G$in $\\tilde O(n^{\\omega})$ time, where $\\omega < 2.371552\\ldots$ is the matrixmultiplication exponent. Hence, the current state-of-the-art bound for theproblem on graphs with $m = \\Theta(n^{\\mu})$ edges in $\\tilde O(n^{\\min \\{\\mu +\\sigma, \\omega \\}})$. Our first contribution is an algorithm with running time$\\tilde O(n^{1 + \\tiny{\\frac{2}{3}} \\omega(\\sigma)})$ for this problem, where$\\omega(\\sigma)$ is the rectangular matrix multiplication exponent. Usingcurrent state-of-the-art estimates on $\\omega(\\sigma)$, our exponent is betterthan $\\min \\{2 + \\sigma, \\omega \\}$ for $\\tilde \\sigma \\le \\sigma \\le 0.53$,where $1/3 < \\tilde \\sigma < 0.3336$ is a universal constant.  Our second contribution is a sequence of algorithms $\\mathcal A_0, \\mathcalA_1, \\mathcal A_2, \\ldots$ for the $S \\times V$-direachability problem. Weargue that under a certain assumption that we introduce, for every $\\tilde\\sigma \\le \\sigma < 1$, there exists a sufficiently large index $k = k(\\sigma)$so that $\\mathcal A_k$ improves upon the current state-of-the-art bounds for $S\\times V$-direachability with $|S| = n^{\\sigma}$, in the densest regime $\\mu=2$. We show that to prove this assumption, it is sufficient to devise analgorithm that computes a rectangular max-min matrix product roughly asefficiently as ordinary $(+, \\cdot)$ matrix product.  Our algorithms heavily exploit recent constructions of directed shortcuts byKogan and Parter.",
        "title": "Faster Multi-Source Directed Reachability via Shortcuts and Matrix  Multiplication",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05629",
        "abstract_url": "http://arxiv.org/abs/2401.05629",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shaoru"
            },
            {
                "last_name": "Fazlyab",
                "first_name": "Mahyar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Control Barrier Functions (CBFs) provide an elegant framework for designingsafety filters for nonlinear control systems by constraining their trajectoriesto an invariant subset of a prespecified safe set. However, the task of findinga CBF that concurrently maximizes the volume of the resulting control invariantset while accommodating complex safety constraints, particularly in highrelative degree systems with actuation constraints, continues to pose asubstantial challenge. In this work, we propose a novel self-supervisedlearning framework that holistically addresses these hurdles. Given a Booleancomposition of multiple state constraints that define the safe set, ourapproach starts with building a single continuously differentiable functionwhose 0-superlevel set provides an inner approximation of the safe set. We thenuse this function together with a smooth neural network to parameterize the CBFcandidate. Finally, we design a training loss function based on aHamilton-Jacobi partial differential equation to train the CBF while enlargingthe volume of the induced control invariant set. We demonstrate theeffectiveness of our approach via numerical experiments.",
        "title": "Learning Performance-Oriented Control Barrier Functions Under Complex  Safety Constraints and Limited Actuation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05631",
        "abstract_url": "http://arxiv.org/abs/2401.05631",
        "authors": [
            {
                "last_name": "Rosenberg",
                "first_name": "Karl Toby"
            },
            {
                "last_name": "Kazi",
                "first_name": "Rubaiat Habib"
            },
            {
                "last_name": "Wei",
                "first_name": "Li-Yi"
            },
            {
                "last_name": "Xia",
                "first_name": "Haijun"
            },
            {
                "last_name": "Perlin",
                "first_name": "Ken"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL",
            "GR",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  We introduce an interactive approach, DrawTalking, in which the user buildsinteractive worlds by sketching and speaking. It emphasizes user control andflexibility, and gives programming-like capability without code. We implementedit on the iPad. An open-ended study shows the mechanics resonate and areapplicable to many creative-exploratory use cases. We hope to inspire andinform research in future natural user-centered interfaces.",
        "title": "DrawTalking: Building Interactive Worlds by Sketching and Speaking",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05632",
        "abstract_url": "http://arxiv.org/abs/2401.05632",
        "authors": [
            {
                "last_name": "Joshi",
                "first_name": "Aditya"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            },
            {
                "last_name": "Kanojia",
                "first_name": "Diptesh"
            },
            {
                "last_name": "Li",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Haolan"
            },
            {
                "last_name": "Haffari",
                "first_name": "Gholamreza"
            },
            {
                "last_name": "Dippold",
                "first_name": "Doris"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  State-of-the-art natural language processing (NLP) models are trained onmassive training corpora, and report a superlative performance on evaluationdatasets. This survey delves into an important attribute of these datasets: thedialect of a language. Motivated by the performance degradation of NLP modelsfor dialectic datasets and its implications for the equity of languagetechnologies, we survey past research in NLP for dialects in terms of datasets,and approaches. We describe a wide range of NLP tasks in terms of twocategories: natural language understanding (NLU) (for tasks such as dialectclassification, sentiment analysis, parsing, and NLU benchmarks) and naturallanguage generation (NLG) (for summarisation, machine translation, and dialoguesystems). The survey is also broad in its coverage of languages which includeEnglish, Arabic, German among others. We observe that past work in NLPconcerning dialects goes deeper than mere dialect classification, and . Thisincludes early approaches that used sentence transduction that lead to therecent approaches that integrate hypernetworks into LoRA. We expect that thissurvey will be useful to NLP researchers interested in building equitablelanguage technologies by rethinking LLM benchmarks and model architectures.",
        "title": "Natural Language Processing for Dialects of a Language: A Survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05633",
        "abstract_url": "http://arxiv.org/abs/2401.05633",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Gang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junpeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recent progress in single-image super-resolution (SISR) has achievedremarkable performance, yet the computational costs of these methods remain achallenge for deployment on resource-constrained devices. Especially fortransformer-based methods, the self-attention mechanism in such models bringsgreat breakthroughs while incurring substantial computational costs. To tacklethis issue, we introduce the Convolutional Transformer layer (ConvFormer) andthe ConvFormer-based Super-Resolution network (CFSR), which offer an effectiveand efficient solution for lightweight image super-resolution tasks. In detail,CFSR leverages the large kernel convolution as the feature mixer to replace theself-attention module, efficiently modeling long-range dependencies andextensive receptive fields with a slight computational cost. Furthermore, wepropose an edge-preserving feed-forward network, simplified as EFN, to obtainlocal feature aggregation and simultaneously preserve more high-frequencyinformation. Extensive experiments demonstrate that CFSR can achieve anadvanced trade-off between computational cost and performance when compared toexisting lightweight SR methods. Compared to state-of-the-art methods, e.g.ShuffleMixer, the proposed CFSR achieves 0.39 dB gains on Urban100 dataset forx2 SR task while containing 26% and 31% fewer parameters and FLOPs,respectively. Code and pre-trained models are available athttps://github.com/Aitical/CFSR.",
        "title": "Transforming Image Super-Resolution: A ConvFormer-based Efficient  Approach",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05638",
        "abstract_url": "http://arxiv.org/abs/2401.05638",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Changtai"
            },
            {
                "last_name": "Han",
                "first_name": "Xu"
            },
            {
                "last_name": "Yao",
                "first_name": "Chao"
            },
            {
                "last_name": "Ban",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate and efficient extraction of microstructures in microscopic images ofmaterials plays a critical role in the exploration of structure-propertyrelationships and the optimization of process parameters. Deep learning-basedimage segmentation techniques that rely on manual annotation are time-consumingand labor-intensive and hardly meet the demand for model transferability andgeneralization. Segment Anything Model (SAM), a large visual model withpowerful deep feature representation and zero-shot generalization capabilities,has provided new solutions for image segmentation. However, directly applyingSAM to segmenting microstructures in microscopic images of materials withouthuman annotation cannot achieve the expected results, as the difficulty ofadapting its native prompt engineering to the dense and dispersedcharacteristics of key microstructures in materials microscopy images. In thispaper, we propose MatSAM, a general and efficient microstructure extractionsolution based on SAM. A new point-based prompts generation strategy isdesigned, grounded on the distribution and shape of materials microstructures.It generates prompts for different microscopic images, fuses the prompts of theregion of interest (ROI) key points and grid key points, and integratespost-processing methods for quantitative characterization of materialsmicrostructures. For common microstructures including grain boundary and phase,MatSAM achieves superior segmentation performance to conventional methods andis even preferable to supervised learning methods evaluated on 18 materialsmicrostructures imaged by the optical microscope (OM) and scanning electronmicroscope (SEM). We believe that MatSAM can significantly reduce the cost ofquantitative characterization of materials microstructures and accelerate thedesign of new materials.",
        "title": "MatSAM: Efficient Materials Microstructure Extraction via Visual Large  Model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05639",
        "abstract_url": "http://arxiv.org/abs/2401.05639",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Yahui"
            },
            {
                "last_name": "Cheng",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper addresses the full-state prescribed performance-based consensusproblem for double-integrator multi-agent systems with jointly connectedtopologies. To improve the transient performance, a distributed prescribedperformance control protocol consisting of the transformed relative positionand the transformed relative velocity is proposed, where the communicationtopology satisfies the jointly connected assumption. Different from theexisting literatures, two independent transient performance specificationsimposed on relative positions and relative velocities can be guaranteedsimultaneously. A numerical example is ultimately used to validate theeffectiveness of proposed protocol.",
        "title": "Full-State Prescribed Performance-Based Consensus of Double-Integrator  Multi-Agent Systems with Jointly Connected Topologies",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05641",
        "abstract_url": "http://arxiv.org/abs/2401.05641",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Tiejin"
            },
            {
                "last_name": "Dai",
                "first_name": "Qinrun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yueqi"
            },
            {
                "last_name": "Wei",
                "first_name": "Hua"
            },
            {
                "last_name": "Zeng",
                "first_name": "Qingkai"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS",
            "CR",
            "LG"
        ],
        "abstract": "  Compartmentalization effectively prevents initial corruption from turninginto a successful attack. This paper presents O2C, a pioneering system designedto enforce OS kernel compartmentalization on the fly. It not only providesimmediate remediation for sudden threats but also maintains consistent systemavailability through the enforcement process.  O2C is empowered by the newest advancements of the eBPF ecosystem whichallows to instrument eBPF programs that perform enforcement actions into thekernel at runtime. O2C takes the lead in embedding a machine learning modelinto eBPF programs, addressing unique challenges in on-the-flycompartmentalization. Our comprehensive evaluation shows that O2C effectivelyconfines damage within the compartment. Further, we validate that decision treeis optimally suited for O2C owing to its advantages in processing tabular data,its explainable nature, and its compliance with the eBPF ecosystem. Last butnot least, O2C is lightweight, showing negligible overhead and excellentsacalability system-wide.",
        "title": "When eBPF Meets Machine Learning: On-the-fly OS Kernel  Compartmentalization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05642",
        "abstract_url": "http://arxiv.org/abs/2401.05642",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Zheng"
            },
            {
                "last_name": "Mathur",
                "first_name": "Umang"
            },
            {
                "last_name": "Pavlogiannis",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Dynamic data race detection has emerged as a key technique for ensuringreliability of concurrent software in practice. However, dynamic approaches canoften miss data races owing to nondeterminism in the thread scheduler.Predictive race detection techniques cater to this shortcoming by inferringalternate executions that may expose data races without re-executing theunderlying program. More formally, the dynamic data race prediction problemasks, given a trace \\sigma of an execution of a concurrent program, can \\sigmabe correctly reordered to expose a data race? Existing state-of-the arttechniques for data race prediction either do not scale to executions arisingfrom real world concurrent software, or only expose a limited class of dataraces, such as those that can be exposed without reversing the order ofsynchronization operations.  In general, exposing data races by reasoning about synchronization reversalsis an intractable problem. In this work, we identify a class of data races,called Optimistic Sync(hronization)-Reversal races that can be detected in atractable manner and often include non-trivial data races that cannot beexposed by prior tractable techniques. We also propose a sound algorithm OSRfor detecting all optimistic sync-reversal data races in overall quadratictime, and show that the algorithm is optimal by establishing a matching lowerbound. Our experiments demonstrate the effectiveness of OSR on our extensivesuite of benchmarks, OSR reports the largest number of data races, and scaleswell to large execution traces.",
        "title": "Optimistic Prediction of Synchronization-Reversal Data Races",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05646",
        "abstract_url": "http://arxiv.org/abs/2401.05646",
        "authors": [
            {
                "last_name": "Peng",
                "first_name": "Chunlei"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Decheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Nannan"
            },
            {
                "last_name": "Hu",
                "first_name": "Ruimin"
            },
            {
                "last_name": "Gao",
                "first_name": "Xinbo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cloth-changing person re-identification (CC-ReID) aims to match persons whochange clothes over long periods. The key challenge in CC-ReID is to extractclothing-independent features, such as face, hairstyle, body shape, and gait.Current research mainly focuses on modeling body shape using multi-modalbiological features (such as silhouettes and sketches). However, it does notfully leverage the personal description information hidden in the original RGBimage. Considering that there are certain attribute descriptions which remainunchanged after the changing of cloth, we propose a Masked AttributeDescription Embedding (MADE) method that unifies personal visual appearance andattribute description for CC-ReID. Specifically, handling variableclothing-sensitive information, such as color and type, is challenging foreffective modeling. To address this, we mask the clothing and color informationin the personal attribute description extracted through an attribute detectionmodel. The masked attribute description is then connected and embedded intoTransformer blocks at various levels, fusing it with the low-level tohigh-level features of the image. This approach compels the model to discardclothing information. Experiments are conducted on several CC-ReID benchmarks,including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADEeffectively utilizes attribute description, enhancing cloth-changing personre-identification performance, and compares favorably with state-of-the-artmethods. The code is available at https://github.com/moon-wh/MADE.",
        "title": "Masked Attribute Description Embedding for Cloth-Changing Person  Re-identification",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05648",
        "abstract_url": "http://arxiv.org/abs/2401.05648",
        "authors": [
            {
                "last_name": "Curbelo",
                "first_name": "Israel R."
            },
            {
                "last_name": "Malko",
                "first_name": "Hannah R."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DS",
            ""
        ],
        "abstract": "  We consider the on-line coloring problem restricted to proper interval graphswith known interval representation. Chrobak and \\'{S}lusarek (1981) showed thatthe greedy $\\textrm{First-Fit}$ algorithm has a strict competitive ratio of$2$. It remains open whether there is an on-line algorithm that performs betterthan $\\textrm{First-Fit}$. Piotr (2008) showed that if the representation isnot known, there is no better on-line algorithm. Epstein and Levy (2005) showedthat no on-line algorithm has a strict competitive ratio less than $1.5$ when aunit-interval representation is known, which was later improved to$1.\\overline{3}$. In this paper, we show that there is no on-line algorithmwith strict competitive ratio less than $1.75$ by presenting a strategy thatcan force any on-line algorithm to use $7$ colors on a proper interval graph$G$ with chromatic number $\\chi(G)\\leq 4$ and known interval representation.",
        "title": "On the on-line coloring of proper interval graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05650",
        "abstract_url": "http://arxiv.org/abs/2401.05650",
        "authors": [
            {
                "last_name": "Jaradat",
                "first_name": "Israa"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haiqi"
            },
            {
                "last_name": "Li",
                "first_name": "Chengkai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Cherry-picking refers to the deliberate selection of evidence or facts thatfavor a particular viewpoint while ignoring or distorting evidence thatsupports an opposing perspective. Manually identifying instances ofcherry-picked statements in news stories can be challenging, particularly whenthe opposing viewpoint's story is absent. This study introduces Cherry, aninnovative approach for automatically detecting cherry-picked statements innews articles by finding missing important statements in the target news story.Cherry utilizes the analysis of news coverage from multiple sources to identifyinstances of cherry-picking. Our approach relies on language models thatconsider contextual information from other news sources to classify statementsbased on their importance to the event covered in the target news story.Furthermore, this research introduces a novel dataset specifically designed forcherry-picking detection, which was used to train and evaluate the performanceof the models. Our best performing model achieves an F-1 score of about %89 indetecting important statements when tested on unseen set of news stories.Moreover, results show the importance incorporating external knowledge fromalternative unbiased narratives when assessing a statement's importance.",
        "title": "On Detecting Cherry-picking in News Coverage Using Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05653",
        "abstract_url": "http://arxiv.org/abs/2401.05653",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Sean"
            },
            {
                "last_name": "Musunuru",
                "first_name": "Sriya"
            },
            {
                "last_name": "Zong",
                "first_name": "Baoshi"
            },
            {
                "last_name": "Thornton",
                "first_name": "Brooks"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper explores the application of Shapley Value Regression in dissectingmarketing performance at channel-partner level, complementing channel-levelMarketing Mix Modeling (MMM). Utilizing real-world data from the financialservices industry, we demonstrate the practicality of Shapley Value Regressionin evaluating individual partner contributions. Although structured in-fieldtesting along with cooperative game theory is most accurate, it can often behighly complex and expensive to conduct. Shapley Value Regression is thus amore feasible approach to disentangle the influence of each marketing partnerwithin a marketing channel. We also propose a simple method to derive adjustedcoefficients of Shapley Value Regression and compares it with alternativeapproaches.",
        "title": "Quantifying Marketing Performance at Channel-Partner Level by Using  Marketing Mix Modeling (MMM) and Shapley Value Regression",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05654",
        "abstract_url": "http://arxiv.org/abs/2401.05654",
        "authors": [
            {
                "last_name": "Tu",
                "first_name": "Tao"
            },
            {
                "last_name": "Palepu",
                "first_name": "Anil"
            },
            {
                "last_name": "Schaekermann",
                "first_name": "Mike"
            },
            {
                "last_name": "Saab",
                "first_name": "Khaled"
            },
            {
                "last_name": "Freyberg",
                "first_name": "Jan"
            },
            {
                "last_name": "Tanno",
                "first_name": "Ryutaro"
            },
            {
                "last_name": "Wang",
                "first_name": "Amy"
            },
            {
                "last_name": "Li",
                "first_name": "Brenna"
            },
            {
                "last_name": "Amin",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Tomasev",
                "first_name": "Nenad"
            },
            {
                "last_name": "Azizi",
                "first_name": "Shekoofeh"
            },
            {
                "last_name": "Singhal",
                "first_name": "Karan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yong"
            },
            {
                "last_name": "Hou",
                "first_name": "Le"
            },
            {
                "last_name": "Webson",
                "first_name": "Albert"
            },
            {
                "last_name": "Kulkarni",
                "first_name": "Kavita"
            },
            {
                "last_name": "Mahdavi",
                "first_name": "S Sara"
            },
            {
                "last_name": "Semturs",
                "first_name": "Christopher"
            },
            {
                "last_name": "Gottweis",
                "first_name": "Juraj"
            },
            {
                "last_name": "Barral",
                "first_name": "Joelle"
            },
            {
                "last_name": "Chou",
                "first_name": "Katherine"
            },
            {
                "last_name": "Corrado",
                "first_name": "Greg S"
            },
            {
                "last_name": "Matias",
                "first_name": "Yossi"
            },
            {
                "last_name": "Karthikesalingam",
                "first_name": "Alan"
            },
            {
                "last_name": "Natarajan",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "LG"
        ],
        "abstract": "  At the heart of medicine lies the physician-patient dialogue, where skillfulhistory-taking paves the way for accurate diagnosis, effective management, andenduring trust. Artificial Intelligence (AI) systems capable of diagnosticdialogue could increase accessibility, consistency, and quality of care.However, approximating clinicians' expertise is an outstanding grand challenge.Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a LargeLanguage Model (LLM) based AI system optimized for diagnostic dialogue.  AMIE uses a novel self-play based simulated environment with automatedfeedback mechanisms for scaling learning across diverse disease conditions,specialties, and contexts. We designed a framework for evaluatingclinically-meaningful axes of performance including history-taking, diagnosticaccuracy, management reasoning, communication skills, and empathy. We comparedAMIE's performance to that of primary care physicians (PCPs) in a randomized,double-blind crossover study of text-based consultations with validated patientactors in the style of an Objective Structured Clinical Examination (OSCE). Thestudy included 149 case scenarios from clinical providers in Canada, the UK,and India, 20 PCPs for comparison with AMIE, and evaluations by specialistphysicians and patient actors. AMIE demonstrated greater diagnostic accuracyand superior performance on 28 of 32 axes according to specialist physiciansand 24 of 26 axes according to patient actors. Our research has severallimitations and should be interpreted with appropriate caution. Clinicians werelimited to unfamiliar synchronous text-chat which permits large-scaleLLM-patient interactions but is not representative of usual clinical practice.While further research is required before AMIE could be translated toreal-world settings, the results represent a milestone towards conversationaldiagnostic AI.",
        "title": "Towards Conversational Diagnostic AI",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05655",
        "abstract_url": "http://arxiv.org/abs/2401.05655",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Kaixun"
            },
            {
                "last_name": "Rakovi\u0107",
                "first_name": "Mladen"
            },
            {
                "last_name": "Li",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Guan",
                "first_name": "Quanlong"
            },
            {
                "last_name": "Ga\u0161evi\u0107",
                "first_name": "Dragan"
            },
            {
                "last_name": "Chen",
                "first_name": "Guanliang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Automatic Essay Scoring (AES) is a well-established educational pursuit thatemploys machine learning to evaluate student-authored essays. While much efforthas been made in this area, current research primarily focuses on either (i)boosting the predictive accuracy of an AES model for a specific prompt (i.e.,developing prompt-specific models), which often heavily relies on the use ofthe labeled data from the same target prompt; or (ii) assessing theapplicability of AES models developed on non-target prompts to the intendedtarget prompt (i.e., developing the AES models in a cross-prompt setting).Given the inherent bias in machine learning and its potential impact onmarginalized groups, it is imperative to investigate whether such bias existsin current AES methods and, if identified, how it intervenes with an AESmodel's accuracy and generalizability. Thus, our study aimed to uncover theintricate relationship between an AES model's accuracy, fairness, andgeneralizability, contributing practical insights for developing effective AESmodels in real-world education. To this end, we meticulously selected nineprominent AES methods and evaluated their performance using seven metrics on anopen-sourced dataset, which contains over 25,000 essays and various demographicinformation about students such as gender, English language learner status, andeconomic status. Through extensive evaluations, we demonstrated that: (1)prompt-specific models tend to outperform their cross-prompt counterparts interms of predictive accuracy; (2) prompt-specific models frequently exhibit agreater bias towards students of different economic statuses compared tocross-prompt models; (3) in the pursuit of generalizability, traditionalmachine learning models coupled with carefully engineered features hold greaterpotential for achieving both high accuracy and fairness than complex neuralnetwork models.",
        "title": "Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive  Investigation of Accuracy, Fairness, and Generalizability",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05657",
        "abstract_url": "http://arxiv.org/abs/2401.05657",
        "authors": [
            {
                "last_name": "Holliday",
                "first_name": "Wesley H."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GT",
            "MA",
            "",
            ""
        ],
        "abstract": "  In the context of social choice theory with ordinal preferences, we say thatthe defensible set is the set of alternatives $x$ such that for any alternative$y$, if $y$ beats $x$ in a head-to-head majority comparison, then there is analternative $z$ that beats $y$ in a head-to-head majority comparison by amargin at least as large as the margin by which $y$ beat $x$. We show that anyordinal voting method satisfying two well-known axioms from votingtheory--positive involvement and the Condorcet winner criterion--refines thedefensible set. Using this lemma, we prove an impossibility theorem: there isno such voting method that also satisfies the Condorcet loser criterion,resolvability, and a common invariance property for Condorcet methods, namelythat the choice of winners depends only on the relative sizes of majoritymargins.",
        "title": "The defensible set and a new impossibility theorem in voting",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05659",
        "abstract_url": "http://arxiv.org/abs/2401.05659",
        "authors": [
            {
                "last_name": "Madugalla",
                "first_name": "Anuradha"
            },
            {
                "last_name": "Huang",
                "first_name": "Yutan"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            },
            {
                "last_name": "Cho",
                "first_name": "Min Hee"
            },
            {
                "last_name": "Gamage",
                "first_name": "Lasith Koswatta"
            },
            {
                "last_name": "Leao",
                "first_name": "Tristan"
            },
            {
                "last_name": "Thiele",
                "first_name": "Sam"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "SE"
        ],
        "abstract": "  Most software applications contain graphics such as charts, diagrams andmaps. Currently, these graphics are designed with a ``one size fits all\"approach and do not cater to the needs of people with disabilities. Therefore,when using software with graphics, a colour-impaired user may struggle tointerpret graphics with certain colours, and a person with dyslexia maystruggle to read the text labels in the graphic. Our research addresses thisissue by developing a framework that generates adaptive and accessibleinformation graphics for multiple disabilities. Uniquely, the approach alsoserves people with multiple simultaneous disabilities. To achieve these, weused a case study of public space floorplans presented via a web tool andworked with four disability groups: people with low vision, colour blindness,dyslexia and mobility impairment. Our research involved gathering requirementsfrom 3 accessibility experts and 80 participants with disabilities, developinga system to generate adaptive graphics that address the identifiedrequirements, and conducting an evaluation with 7 participants withdisabilities. The evaluation showed that users found our solution easy to useand suitable for most of their requirements. The study also providesrecommendations for front-end developers on engineering accessible graphics fortheir software and discusses the implications of our work on society from theperspective of public space owners and end users.",
        "title": "Engineering Adaptive Information Graphics for Disabled Communities: A  Case Study with Public Space Indoor Maps",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05661",
        "abstract_url": "http://arxiv.org/abs/2401.05661",
        "authors": [
            {
                "last_name": "Espinoza",
                "first_name": "Jes\u00fas F."
            },
            {
                "last_name": "Esquer-P\u00e9rez",
                "first_name": "Cynthia G."
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            ""
        ],
        "abstract": "  In this work we study the intersection properties of a finite disk system inthe euclidean space. We accomplish this by utilizing subsets of spheres withvarying dimensions and analyze specific points within them, referred to aspoles. Additionally, we introduce two applications: estimating the common scalefactor for the radii that makes the re-scaled disks intersects in a singlepoint, this is the \\v{C}ech scale, and constructing the minimal Axis-AlignedBounding Box (AABB) that encloses the intersection of all disks in the system.",
        "title": "Intersection properties of finite disk collections",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05664",
        "abstract_url": "http://arxiv.org/abs/2401.05664",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Jian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Energy efficiency is a big concern in industrial sectors. Finding the rootcause of anomaly state of energy efficiency can help to improve energyefficiency of industrial systems and therefore save energy cost. In thisresearch, we propose to use transfer entropy (TE) for root cause analysis onenergy efficiency of industrial systems. A method, called TE flow, is proposedin that a TE flow from physical measurements of each subsystem to the energyefficiency indicator along timeline is considered as causal strength fordiagnosing root cause of anomaly states of energy efficiency of a system. Thecopula entropy-based nonparametric TE estimator is used in the proposed method.We conducted experiments on real data collected from a compressing air systemto verify the proposed method. Experimental results show that the TE flowmethod successfully identified the root cause of the energy (in)efficiency ofthe system.",
        "title": "Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05665",
        "abstract_url": "http://arxiv.org/abs/2401.05665",
        "authors": [
            {
                "last_name": "Regal",
                "first_name": "Frank"
            },
            {
                "last_name": "Suarez",
                "first_name": "Chris"
            },
            {
                "last_name": "Parra",
                "first_name": "Fabian"
            },
            {
                "last_name": "Pryor",
                "first_name": "Mitch"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Multi-agent human-robot teaming allows for the potential to gatherinformation about various environments more efficiently by exploiting andcombining the strengths of humans and robots. In industries like defense,search and rescue, first-response, and others alike, heterogeneous human-robotteams show promise to accelerate data collection and improve team safety byremoving humans from unknown and potentially hazardous situations. This workbuilds upon AugRE, an Augmented Reality (AR) based scalable human-robot teamingframework. It enables users to localize and communicate with 50+ autonomousagents. Through our efforts, users are able to command, control, and superviseagents in large teams, both line-of-sight and non-line-of-sight, without theneed to modify the environment prior and without requiring users to use typicalhardware (i.e. joysticks, keyboards, laptops, tablets, etc.) in the field. Thedemonstrated work shows early indications that combining these AR-HMD-baseduser interaction modalities for command, control, and supervision will helpimprove human-robot team collaboration, robustness, and trust.",
        "title": "Augmented Reality User Interface for Command, Control, and Supervision  of Large Multi-Agent Teams",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05667",
        "abstract_url": "http://arxiv.org/abs/2401.05667",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Weijieying"
            },
            {
                "last_name": "Honavar",
                "first_name": "Vasant G"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  A key challenge in the continual learning setting is to efficiently learn asequence of tasks without forgetting how to perform previously learned tasks.Many existing approaches to this problem work by either retraining the model onprevious tasks or by expanding the model to accommodate new tasks. However,these approaches typically suffer from increased storage and computationalrequirements, a problem that is worsened in the case of sparse models due toneed for expensive re-training after sparsification. To address this challenge,we propose a new method for efficient continual learning of sparse models(EsaCL) that can automatically prune redundant parameters without adverselyimpacting the model's predictive power, and circumvent the need of retraining.We conduct a theoretical analysis of loss landscapes with parameter pruning,and design a directional pruning (SDP) strategy that is informed by thesharpness of the loss function with respect to the model parameters. SDPensures model with minimal loss of predictive accuracy, accelerating thelearning of sparse models at each stage. To accelerate model update, weintroduce an intelligent data selection (IDS) strategy that can identifycritical instances for estimating loss landscape, yielding substantiallyimproved data efficiency. The results of our experiments show that EsaCLachieves performance that is competitive with the state-of-the-art methods onthree continual learning benchmarks, while using substantially reduced memoryand computational resources.",
        "title": "EsaCL: Efficient Continual Learning of Sparse Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05668",
        "abstract_url": "http://arxiv.org/abs/2401.05668",
        "authors": [
            {
                "last_name": "Madugalla",
                "first_name": "Anuradha"
            },
            {
                "last_name": "Kanij",
                "first_name": "Tanjila"
            },
            {
                "last_name": "Hoda",
                "first_name": "Rashina"
            },
            {
                "last_name": "Hidellaarachchi",
                "first_name": "Dulaji"
            },
            {
                "last_name": "Pant",
                "first_name": "Aastha"
            },
            {
                "last_name": "Ferdousi",
                "first_name": "Samia"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The COVID-19 pandemic changed the way we live, work and the way we conductresearch. With the restrictions of lockdowns and social distancing, variousimpacts were experienced by many software engineering researchers, especiallywhose studies depend on human participants. We conducted a mixed methods studyto understand the extent of this impact. Through a detailed survey with 89software engineering researchers working with human participants around theworld and a further nine follow-up interviews, we identified the key challengesfaced, the adaptations made, and the surprising fringe benefits of conductingresearch involving human participants during the pandemic. Our findings alsorevealed that in retrospect, many researchers did not wish to revert to the oldways of conducting human-oriented research. Based on our analysis and insights,we share recommendations on how to conduct remote studies with humanparticipants effectively in an increasingly hybrid world when face-to-faceengagement is not possible or where remote participation is preferred.",
        "title": "Challenges, Adaptations, and Fringe Benefits of Conducting Software  Engineering Research with Human Participants during the COVID-19 Pandemic",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05669",
        "abstract_url": "http://arxiv.org/abs/2401.05669",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "Gu",
                "first_name": "Zhouhong"
            },
            {
                "last_name": "Liang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Lu",
                "first_name": "Dakuan"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yanghua"
            },
            {
                "last_name": "Wang",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pre-trained language models (PLMs) have been prevailing in state-of-the-artmethods for natural language processing, and knowledge-enhanced PLMs arefurther proposed to promote model performance in knowledge-intensive tasks.However, conceptual knowledge, one essential kind of knowledge for humancognition, still remains understudied in this line of research. This limitsPLMs' performance in scenarios requiring human-like cognition, such asunderstanding long-tail entities with concepts. In this paper, we proposeConcEPT, which stands for Concept-Enhanced Pre-Training for language models, toinfuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomieswith entity concept prediction, a novel pre-training objective to predict theconcepts of entities mentioned in the pre-training contexts. Unlike previousconcept-enhanced methods, ConcEPT can be readily adapted to various downstreamapplications without entity linking or concept mapping. Results of extensiveexperiments show the effectiveness of ConcEPT in four tasks such as entitytyping, which validates that our model gains improved conceptual knowledge withconcept-enhanced pre-training.",
        "title": "ConcEPT: Concept-Enhanced Pre-Training for Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05673",
        "abstract_url": "http://arxiv.org/abs/2401.05673",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Nick"
            },
            {
                "last_name": "Marsso",
                "first_name": "Lina"
            },
            {
                "last_name": "Yaman",
                "first_name": "Sinem Getir"
            },
            {
                "last_name": "Baatartogtokh",
                "first_name": "Yesugen"
            },
            {
                "last_name": "Ayad",
                "first_name": "Reem"
            },
            {
                "last_name": "de Mello",
                "first_name": "Vict\u00f3ria Oldemburgo"
            },
            {
                "last_name": "Townsend",
                "first_name": "Beverley"
            },
            {
                "last_name": "Standen",
                "first_name": "Isobel"
            },
            {
                "last_name": "Stefanakos",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Imrie",
                "first_name": "Calum"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Gena\u00edna Nunes"
            },
            {
                "last_name": "Cavalcanti",
                "first_name": "Ana"
            },
            {
                "last_name": "Calinescu",
                "first_name": "Radu"
            },
            {
                "last_name": "Chechik",
                "first_name": "Marsha"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  As software systems increasingly interact with humans in application domainssuch as transportation and healthcare, they raise concerns related to thesocial, legal, ethical, empathetic, and cultural (SLEEC) norms and values oftheir stakeholders. Normative non-functional requirements (N-NFRs) are used tocapture these concerns by setting SLEEC-relevant boundaries for systembehavior. Since N-NFRs need to be specified by multiple stakeholders withwidely different, non-technical expertise (ethicists, lawyers, regulators, endusers, etc.), N-NFR elicitation is very challenging. To address this challenge,we introduce N-Check, a novel tool-supported formal approach to N-NFR analysisand debugging. N-Check employs satisfiability checking to identify a broadspectrum of N-NFR well-formedness issues (WFI), such as conflicts, redundancy,restrictiveness, insufficiency, yielding diagnostics which pinpoint theircauses in a user-friendly way that enables non-technical stakeholders tounderstand and fix them. We show the effectiveness and usability of ourapproach through nine case studies in which teams of ethicists, lawyers,philosophers, psychologists, safety analysts, and engineers used N-Check toanalyse and debug 233 N-NFRs comprising 62 issues for the software underpinningthe operation of systems ranging from assistive-care robots and tree-diseasedetection drones to manufacturing collaborative robots.",
        "title": "Analyzing and Debugging Normative Requirements via Satisfiability  Checking",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05674",
        "abstract_url": "http://arxiv.org/abs/2401.05674",
        "authors": [
            {
                "last_name": "Feireisl",
                "first_name": "Eduard"
            },
            {
                "last_name": "Lukacova-Medvidova",
                "first_name": "Maria"
            },
            {
                "last_name": "She",
                "first_name": "Bangwei"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuhuan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We consider the Navier-Stokes-Fourier system governing the motion of ageneral compressible, heat conducting, Newtonian fluid driven by randominitial/boundary data. Convergence of the stochastic collocation and MonteCarlo numerical methods is shown under the hypothesis that approximatesolutions are bounded in probability. Abstract results are illustrated bynumerical experiments for the Rayleigh-Benard convection problem.",
        "title": "Convergence of numerical methods for the Navier-Stokes-Fourier system  driven by uncertain initial/boundary data",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05675",
        "abstract_url": "http://arxiv.org/abs/2401.05675",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Seung Hyun"
            },
            {
                "last_name": "Li",
                "first_name": "Yinxiao"
            },
            {
                "last_name": "Ke",
                "first_name": "Junjie"
            },
            {
                "last_name": "Yoo",
                "first_name": "Innfarn"
            },
            {
                "last_name": "Zhang",
                "first_name": "Han"
            },
            {
                "last_name": "Yu",
                "first_name": "Jiahui"
            },
            {
                "last_name": "Wang",
                "first_name": "Qifei"
            },
            {
                "last_name": "Deng",
                "first_name": "Fei"
            },
            {
                "last_name": "Entis",
                "first_name": "Glenn"
            },
            {
                "last_name": "He",
                "first_name": "Junfeng"
            },
            {
                "last_name": "Li",
                "first_name": "Gang"
            },
            {
                "last_name": "Kim",
                "first_name": "Sangpil"
            },
            {
                "last_name": "Essa",
                "first_name": "Irfan"
            },
            {
                "last_name": "Yang",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent works demonstrate that using reinforcement learning (RL) with qualityrewards can enhance the quality of generated images in text-to-image (T2I)generation. However, a simple aggregation of multiple rewards may causeover-optimization in certain metrics and degradation in others, and it ischallenging to manually find the optimal weights. An effective strategy tojointly optimize multiple rewards in RL for T2I generation is highly desirable.This paper introduces Parrot, a novel multi-reward RL framework for T2Igeneration. Through the use of the batch-wise Pareto optimal selection, Parrotautomatically identifies the optimal trade-off among different rewards duringthe RL optimization of the T2I generation. Additionally, Parrot employs a jointoptimization approach for the T2I model and the prompt expansion network,facilitating the generation of quality-aware text prompts, thus furtherenhancing the final image quality. To counteract the potential catastrophicforgetting of the original user prompt due to prompt expansion, we introduceoriginal prompt centered guidance at inference time, ensuring that thegenerated image remains faithful to the user input. Extensive experiments and auser study demonstrate that Parrot outperforms several baseline methods acrossvarious quality criteria, including aesthetics, human preference, imagesentiment, and text-image alignment.",
        "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for  Text-to-Image Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05676",
        "abstract_url": "http://arxiv.org/abs/2401.05676",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Weibo"
            },
            {
                "last_name": "Ren",
                "first_name": "Weihong"
            },
            {
                "last_name": "Tian",
                "first_name": "Jiandong"
            },
            {
                "last_name": "Qu",
                "first_name": "Liangqiong"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Liu",
                "first_name": "Honghai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human-Object Interaction (HOI) detection plays a vital role in sceneunderstanding, which aims to predict the HOI triplet in the form of <human,object, action>. Existing methods mainly extract multi-modal features (e.g.,appearance, object semantics, human pose) and then fuse them together todirectly predict HOI triplets. However, most of these methods focus on seekingfor self-triplet aggregation, but ignore the potential cross-tripletdependencies, resulting in ambiguity of action prediction. In this work, wepropose to explore Self- and Cross-Triplet Correlations (SCTC) for HOIdetection. Specifically, we regard each triplet proposal as a graph whereHuman, Object represent nodes and Action indicates edge, to aggregateself-triplet correlation. Also, we try to explore cross-triplet dependencies byjointly considering instance-level, semantic-level, and layout-level relations.Besides, we leverage the CLIP model to assist our SCTC obtain interaction-awarefeature by knowledge distillation, which provides useful action clues for HOIdetection. Extensive experiments on HICO-DET and V-COCO datasets verify theeffectiveness of our proposed SCTC.",
        "title": "Exploring Self- and Cross-Triplet Correlations for Human-Object  Interaction Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05680",
        "abstract_url": "http://arxiv.org/abs/2401.05680",
        "authors": [
            {
                "last_name": "Mitra",
                "first_name": "Shaswata"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Trisha"
            },
            {
                "last_name": "Neupane",
                "first_name": "Subash"
            },
            {
                "last_name": "Piplai",
                "first_name": "Aritran"
            },
            {
                "last_name": "Mittal",
                "first_name": "Sudip"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG",
            "NE"
        ],
        "abstract": "  In an increasingly interconnected world, where information is the lifebloodof modern society, regular cyber-attacks sabotage the confidentiality,integrity, and availability of digital systems and information. Additionally,cyber-attacks differ depending on the objective and evolve rapidly to disguisedefensive systems. However, a typical cyber-attack demonstrates a series ofstages from attack initiation to final resolution, called an attack life cycle.These diverse characteristics and the relentless evolution of cyber attackshave led cyber defense to adopt modern approaches like Machine Learning tobolster defensive measures and break the attack life cycle. Among the adoptedML approaches, Graph Neural Networks have emerged as a promising approach forenhancing the effectiveness of defensive measures due to their ability toprocess and learn from heterogeneous cyber threat data. In this paper, we lookinto the application of GNNs in aiding to break each stage of one of the mostrenowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We addresseach phase of CKC and discuss how GNNs contribute to preparing and preventingan attack from a defensive standpoint. Furthermore, We also discuss openresearch areas and further improvement scopes.",
        "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05683",
        "abstract_url": "http://arxiv.org/abs/2401.05683",
        "authors": [
            {
                "last_name": "Sankar",
                "first_name": "V. Udaya"
            },
            {
                "last_name": "Rao",
                "first_name": "Vishisht Srihari"
            },
            {
                "last_name": "Narahari",
                "first_name": "Y."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            ""
        ],
        "abstract": "  Mechanism design is essentially reverse engineering of games and involvesinducing a game among strategic agents in a way that the induced game satisfiesa set of desired properties in an equilibrium of the game. Desirable propertiesfor a mechanism include incentive compatibility, individual rationality,welfare maximisation, revenue maximisation (or cost minimisation), fairness ofallocation, etc. It is known from mechanism design theory that only certainstrict subsets of these properties can be simultaneously satisfied exactly byany given mechanism. Often, the mechanisms required by real-world applicationsmay need a subset of these properties that are theoretically impossible to besimultaneously satisfied. In such cases, a prominent recent approach is to usea deep learning based approach to learn a mechanism that approximatelysatisfies the required properties by minimizing a suitably defined lossfunction. In this paper, we present, from relevant literature, technicaldetails of using a deep learning approach for mechanism design and provide anoverview of key results in this topic. We demonstrate the power of thisapproach for three illustrative case studies: (a) efficient energy managementin a vehicular network (b) resource allocation in a mobile network (c)designing a volume discount procurement auction for agricultural inputs.Section 6 concludes the paper.",
        "title": "Deep Learning Meets Mechanism Design: Key Results and Some Novel  Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05684",
        "abstract_url": "http://arxiv.org/abs/2401.05684",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Sirui"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Li",
                "first_name": "Liang"
            },
            {
                "last_name": "Ding",
                "first_name": "Lingyun"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Multiscale metrics such as negative Sobolev norms are effective forquantifying the degree of mixedness of a passive scalar field advected by anincompressible flow in the absence of diffusion. In this paper we introduce amix norm that is motivated by Sobolev norm $H^{-1}$ for a general domain with ano-flux boundary. We then derive an explicit expression for the optimal flowthat maximizes the instantaneous decay rate of the mix norm under fixed energyand enstrophy constraints. Numerical simulations indicate that the mix normdecays exponentially or faster for various initial conditions and geometriesand the rate is closely related to the smallest non-zero eigenvalue of theLaplace operator. These results generalize previous findings restricted for aperiodic domain for its analytical and numerical simplicity. Additionally, weobserve that periodic boundaries tend to induce a faster decay in mix normcompared to no-flux conditions under the fixed energy constraint, while thecomparison is reversed for the fixed enstrophy constraint. In the special caseof even initial distributions, two types of boundary conditions yield the sameoptimal flow and mix norm decay.",
        "title": "Optimal Stirring Strategies for Passive Scalars in a Domain with a  General Shape and No-Flux Boundary Condition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05686",
        "abstract_url": "http://arxiv.org/abs/2401.05686",
        "authors": [
            {
                "last_name": "Appolinary",
                "first_name": "Blaise"
            },
            {
                "last_name": "Deaconu",
                "first_name": "Alex"
            },
            {
                "last_name": "Yang",
                "first_name": "Sophia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we present a novel method for dynamically expandingConvolutional Neural Networks (CNNs) during training, aimed at meeting theincreasing demand for efficient and sustainable deep learning models. Ourapproach, drawing from the seminal work on Self-Expanding Neural Networks(SENN), employs a natural expansion score as an expansion criteria to addressthe common issue of over-parameterization in deep convolutional neuralnetworks, thereby ensuring that the model's complexity is finely tuned to thetask's specific needs. A significant benefit of this method is its eco-friendlynature, as it obviates the necessity of training multiple models of differentsizes. We employ a strategy where a single model is dynamically expanded,facilitating the extraction of checkpoints at various complexity levels,effectively reducing computational resource use and energy consumption whilealso expediting the development cycle by offering diverse model complexitiesfrom a single training session. We evaluate our method on the CIFAR-10 datasetand our experimental results validate this approach, demonstrating thatdynamically adding layers not only maintains but also improves CNN performance,underscoring the effectiveness of our expansion criteria. This approach marks aconsiderable advancement in developing adaptive, scalable, and environmentallyconsiderate neural network architectures, addressing key challenges in thefield of deep learning.",
        "title": "Self Expanding Convolutional Neural Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05689",
        "abstract_url": "http://arxiv.org/abs/2401.05689",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Wang",
                "first_name": "Minghan"
            },
            {
                "last_name": "Qiao",
                "first_name": "Xiaosong"
            },
            {
                "last_name": "Wei",
                "first_name": "Daimeng"
            },
            {
                "last_name": "Shang",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Li",
                "first_name": "Zongyao"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhengzhe"
            },
            {
                "last_name": "Li",
                "first_name": "Yinglu"
            },
            {
                "last_name": "Su",
                "first_name": "Chang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            },
            {
                "last_name": "Tao",
                "first_name": "Shimin"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Error correction techniques have been used to refine the output sentencesfrom automatic speech recognition (ASR) models and achieve a lower word errorrate (WER). Previous works usually adopt end-to-end models and has strongdependency on Pseudo Paired Data and Original Paired Data. But when onlypre-training on Pseudo Paired Data, previous models have negative effect oncorrection. While fine-tuning on Original Paired Data, the source side datamust be transcribed by a well-trained ASR model, which takes a lot of time andnot universal. In this paper, we propose UCorrect, an unsupervisedDetector-Generator-Selector framework for ASR Error Correction. UCorrect has nodependency on the training data mentioned before. The whole procedure is firstto detect whether the character is erroneous, then to generate some candidatecharacters and finally to select the most confident one to replace the errorcharacter. Experiments on the public AISHELL-1 dataset and WenetSpeech datasetshow the effectiveness of UCorrect for ASR error correction: 1) it achievessignificant WER reduction, achieves 6.83\\% even without fine-tuning and 14.29\\%after fine-tuning; 2) it outperforms the popular NAR correction models by alarge margin with a competitive low latency; and 3) it is an universal method,as it reduces all WERs of the ASR model with different decoding strategies andreduces all WERs of ASR models trained on different scale datasets.",
        "title": "UCorrect: An Unsupervised Framework for Automatic Speech Recognition  Error Correction",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05690",
        "abstract_url": "http://arxiv.org/abs/2401.05690",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Cong"
            },
            {
                "last_name": "You",
                "first_name": "Changsheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haodong"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            },
            {
                "last_name": "Shi",
                "first_name": "Shuo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Extremely large-scale array (XL-array) has emerged as a promising technologyto enable near-field communications for achieving enhanced spectrum efficiencyand spatial resolution, by drastically increasing the number of antennas.However, this also inevitably incurs higher hardware and energy cost, which maynot be affordable in future wireless systems. To address this issue, we proposein this paper to exploit two types of sparse arrays (SAs) for enablingnear-field communications. Specifically, we first consider the linear sparsearray (LSA) and characterize its near-field beam pattern. It is shown thatdespite the achieved beam-focusing gain, the LSA introduces several undesiredgrating-lobes, which have comparable beam power with the main-lobe and arefocused on specific regions. An efficient hybrid beamforming design is thenproposed for the LSA to deal with the potential strong inter-user interference(IUI). Next, we consider another form of SA, called extended coprime array(ECA), which is composed of two LSA subarrays with different (coprime)inter-antenna spacing. By characterizing the ECA near-field beam pattern, weshow that compared with the LSA with the same array sparsity, the ECA cangreatly suppress the beam power of near-field grating-lobes thanks to theoffset effect of the two subarrays, albeit with a larger number ofgrating-lobes. This thus motivates us to propose a customized two-phase hybridbeamforming design for the ECA. Finally, numerical results are presented todemonstrate the rate performance gain of the proposed two SAs over theconventional uniform linear array (ULA).",
        "title": "Sparse Array Enabled Near-Field Communications: Beam Pattern Analysis  and Hybrid Beamforming Design",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05695",
        "abstract_url": "http://arxiv.org/abs/2401.05695",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Chengfeng"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Jiao",
                "first_name": "Wenpin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yongqiang"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhenwei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The use of large language models in medical dialogue generation has garneredsignificant attention, with a focus on improving response quality and fluency.While previous studies have made progress in optimizing model performance forsingle-round medical Q&A tasks, there is a need to enhance the model'scapability for multi-round conversations to avoid logical inconsistencies. Toaddress this, we propose an approach called preference learning from processfeedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPFinvolves rule modeling, preference data generation, and preference alignment totrain the model to adhere to the diagnostic process. Experimental results usingStandardized Patient Testing show that PLPF enhances the diagnostic accuracy ofthe baseline model in medical conversations by 17.6%, outperforming traditionalreinforcement learning from human feedback. Additionally, PLPF demonstrateseffectiveness in both multi-round and single-round dialogue tasks, showcasingits potential for improving medical dialogue generation.",
        "title": "Integrating Physician Diagnostic Logic into Large Language Models:  Preference Learning from Process Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05698",
        "abstract_url": "http://arxiv.org/abs/2401.05698",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Licai"
            },
            {
                "last_name": "Lian",
                "first_name": "Zheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Bin"
            },
            {
                "last_name": "Tao",
                "first_name": "Jianhua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  Audio-Visual Emotion Recognition (AVER) has garnered increasing attention inrecent years for its critical role in creating emotion-ware intelligentmachines. Previous efforts in this area are dominated by the supervisedlearning paradigm. Despite significant progress, supervised learning is meetingits bottleneck due to the longstanding data scarcity issue in AVER. Motivatedby recent advances in self-supervised learning, we propose HierarchicalContrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework thatleverages large-scale self-supervised pre-training on vast unlabeledaudio-visual data to promote the advancement of AVER. Following prior arts inself-supervised audio-visual representation learning, HiCMAE adopts two primaryforms of self-supervision for pre-training, namely masked data modeling andcontrastive learning. Unlike them which focus exclusively on top-layerrepresentations while neglecting explicit guidance of intermediate layers,HiCMAE develops a three-pronged strategy to foster hierarchical audio-visualfeature learning and improve the overall quality of learned representations. Toverify the effectiveness of HiCMAE, we conduct extensive experiments on 9datasets covering both categorical and dimensional AVER tasks. Experimentalresults show that our method significantly outperforms state-of-the-artsupervised and self-supervised audio-visual methods, which indicates thatHiCMAE is a powerful audio-visual emotion representation learner. Codes andmodels will be publicly available at https://github.com/sunlicai/HiCMAE.",
        "title": "HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised  Audio-Visual Emotion Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05700",
        "abstract_url": "http://arxiv.org/abs/2401.05700",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhanglin"
            },
            {
                "last_name": "Li",
                "first_name": "Zongyao"
            },
            {
                "last_name": "Shang",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Wei",
                "first_name": "Daimeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Rao",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Shaojun"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Incremental Decoding is an effective framework that enables the use of anoffline model in a simultaneous setting without modifying the original model,making it suitable for Low-Latency Simultaneous Speech Translation. However,this framework may introduce errors when the system outputs from incompleteinput. To reduce these output errors, several strategies such as Hold-$n$,LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to becarefully selected for optimal performance. Moreover, these strategies are moresuitable for end-to-end systems than cascade systems. In our paper, we proposea new adaptable and efficient policy named \"Regularized Batched Inputs\". Ourmethod stands out by enhancing input diversity to mitigate output errors. Wesuggest particular regularization techniques for both end-to-end and cascadesystems. We conducted experiments on IWSLT Simultaneous Speech Translation(SimulST) tasks, which demonstrate that our approach achieves low latency whilemaintaining no more than 2 BLEU points loss compared to offline systems.Furthermore, our SimulST systems attained several new state-of-the-art resultsin various language directions.",
        "title": "R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework  for Low-Latency Simultaneous Speech Translation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05702",
        "abstract_url": "http://arxiv.org/abs/2401.05702",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Hui"
            },
            {
                "last_name": "Sun",
                "first_name": "Qianru"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Anomaly Detection (VAD) aims to localize abnormal events on thetimeline of long-range surveillance videos. Anomaly-scoring-based methods havebeen prevailing for years but suffer from the high complexity of thresholdingand low explanability of detection results. In this paper, we conduct pioneerresearch on equipping video-based large language models (VLLMs) in theframework of VAD, making the VAD model free from thresholds and able to explainthe reasons for the detected anomalies. We introduce a novel network moduleLong-Term Context (LTC) to mitigate the incapability of VLLMs in long-rangecontext modeling. We design a three-phase training method to improve theefficiency of fine-tuning VLLMs by substantially minimizing the requirementsfor VAD data and lowering the costs of annotating instruction-tuning data. Ourtrained model achieves the top performance on the anomaly videos of theUCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%,respectively. More impressively, our approach can provide textual explanationsfor detected anomalies.",
        "title": "Video Anomaly Detection and Explanation via Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05705",
        "abstract_url": "http://arxiv.org/abs/2401.05705",
        "authors": [
            {
                "last_name": "Krivorotko",
                "first_name": "Olga"
            },
            {
                "last_name": "Zvonareva",
                "first_name": "Tatiana"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  A numerical algorithm for regularization of the solution of the sourceproblem for the diffusion-logistic model based on information about the processat fixed moments of time of integral type has been developed. The peculiarityof the problem under study is the discrete formulation in space andimpossibility to apply classical algorithms for its numerical solution. Theregularization of the problem is based on the application of A.N. Tikhonov'sapproach and a priori information about the source of the process. The problemwas formulated in a variational formulation and solved by the global tensoroptimization method. It is shown that in the case of noisy data regularizationimproves the accuracy of the reconstructed source.",
        "title": "Regularization of the discrete source problem in the nonlinear  diffusive-logistic equation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05707",
        "abstract_url": "http://arxiv.org/abs/2401.05707",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Xi",
                "first_name": "Dinghao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Liumin"
            },
            {
                "last_name": "Xu",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Text style transfer is increasingly prominent in online entertainment andsocial media. However, existing research mainly concentrates on style transferwithin individual English sentences, while ignoring the complexity of longChinese texts, which limits the wider applicability of style transfer indigital media realm. To bridge this gap, we propose a Chinese Article-styleTransfer framework (CAT-LLM), leveraging the capabilities of Large LanguageModels (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition(TSD) module aimed at comprehensively analyzing text features in articles,prompting LLMs to efficiently transfer Chinese article-style. The TSD moduleintegrates a series of machine learning algorithms to analyze article-stylefrom both words and sentences levels, thereby aiding LLMs thoroughly grasp thetarget style without compromising the integrity of the original text. Inaddition, this module supports dynamic expansion of internal style trees,showcasing robust compatibility and allowing flexible optimization insubsequent research. Moreover, we select five Chinese articles with distinctstyles and create five parallel datasets using ChatGPT, enhancing the models'performance evaluation accuracy and establishing a novel paradigm forevaluating subsequent research on article-style transfer. Extensiveexperimental results affirm that CAT-LLM outperforms current research in termsof transfer accuracy and content preservation, and has remarkable applicabilityto various types of LLMs.",
        "title": "CAT-LLM: Prompting Large Language Models with Text Style Definition for  Chinese Article-style Transfer",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05708",
        "abstract_url": "http://arxiv.org/abs/2401.05708",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Che-Kai"
            },
            {
                "last_name": "Li",
                "first_name": "Chao"
            },
            {
                "last_name": "Mao",
                "first_name": "Ruibin"
            },
            {
                "last_name": "Yang",
                "first_name": "Jianyi"
            },
            {
                "last_name": "K\u00e4mpfe",
                "first_name": "Thomas"
            },
            {
                "last_name": "Imani",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Li",
                "first_name": "Can"
            },
            {
                "last_name": "Zhuo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Yin",
                "first_name": "Xunzhao"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Rapid advancements in artificial intelligence have given rise totransformative models, profoundly impacting our lives. These models demandmassive volumes of data to operate effectively, exacerbating the data-transferbottleneck inherent in the conventional von-Neumann architecture.Compute-in-memory (CIM), a novel computing paradigm, tackles these issues byseamlessly embedding in-memory search functions, thereby obviating the need fordata transfers. However, existing non-volatile memory (NVM)-based acceleratorsare application specific. During the similarity based associative searchoperation, they only support a single, specific distance metric, such asHamming, Manhattan, or Euclidean distance in measuring the query against thestored data, calling for reconfigurable in-memory solutions adaptable tovarious applications. To overcome such a limitation, in this paper, we presentFeReX, a reconfigurable associative memory (AM) that accommodates variousdistance metrics including Hamming, Manhattan, and Euclidean distances.Leveraging multi-bit ferroelectric field-effect transistors (FeFETs) as theproxy and a hardware-software co-design approach, we introduce a constrainedsatisfaction problem (CSP)-based method to automate AM search input voltage andstored voltage configurations for different distance based search functions.Device-circuit co-simulations first validate the effectiveness of the proposedFeReX methodology for reconfigurable search distance functions. Then, webenchmark FeReX in the context of k-nearest neighbor (KNN) and hyperdimensionalcomputing (HDC), which highlights the robustness of FeReX and demonstrates upto 250x speedup and 10^4 energy savings compared with GPU.",
        "title": "FeReX: A Reconfigurable Design of Multi-bit Ferroelectric  Compute-in-Memory for Nearest Neighbor Search",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05709",
        "abstract_url": "http://arxiv.org/abs/2401.05709",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Penghong"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Wenrui"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Debin"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Localization is one of the pivotal issues in wireless sensor networkapplications. In 3D localization studies, most algorithms focus on enhancingthe location prediction process, lacking theoretical derivation of thedetection distance of an anchor node at the varying hops, engenders alocalization performance bottleneck. To address this issue, we propose aprobability-based average distance estimation (PADE) model that utilizes theprobability distribution of node distances detected by an anchor node. The aimis to mathematically derive the average distances of nodes detected by ananchor node at different hops. First, we develop a probability-based maximumdistance estimation (PMDE) model to calculate the upper bound of the distancedetected by an anchor node. Then, we present the PADE model, which relies onthe upper bound obtained of the distance by the PMDE model. Finally, theobtained average distance is used to construct a distance loss function, and itis embedded with the traditional distance loss function into a multi-objectivegenetic algorithm to predict the locations of unknown nodes. The experimentalresults demonstrate that the proposed method achieves state-of-the-artperformance in random and multimodal distributed sensor networks. The averagelocalization accuracy is improved by 3.49\\%-12.66\\% and 3.99%-22.34%,respectively.",
        "title": "Probability-based Distance Estimation Model for 3D DV-Hop Localization  in WSNs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05710",
        "abstract_url": "http://arxiv.org/abs/2401.05710",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zhihui"
            },
            {
                "last_name": "Perrault",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study reinforcement learning in the presence of an unknown rewardperturbation. Existing methodologies for this problem make strong assumptionsincluding reward smoothness, known perturbations, and/or perturbations that donot modify the optimal policy. We study the case of unknown arbitraryperturbations that discretize and shuffle reward space, but have the propertythat the true reward belongs to the most frequently observed class afterperturbation. This class of perturbations generalizes existing classes (and, inthe limit, all continuous bounded perturbations) and defeats existing methods.We introduce an adaptive distributional reward critic and show theoreticallythat it can recover the true rewards under technical conditions. Under thetargeted perturbation in discrete and continuous control tasks, we win/tie thehighest return in 40/57 settings (compared to 16/57 for the best baseline).Even under the untargeted perturbation, we still win an edge over the baselinedesigned especially for that setting.",
        "title": "The Distributional Reward Critic Architecture for Perturbed-Reward  Reinforcement Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05711",
        "abstract_url": "http://arxiv.org/abs/2401.05711",
        "authors": [
            {
                "last_name": "Jiao",
                "first_name": "Jiyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Han",
                "first_name": "Chenpei"
            },
            {
                "last_name": "Huang",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yizhuo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While fingerprinting localization is favored for its effectiveness, it ishindered by high data acquisition costs and the inaccuracy of staticdatabase-based estimates. Addressing these issues, this letter presents aninnovative indoor localization method using a data-efficient meta-learningalgorithm. This approach, grounded in the ``Learning to Learn'' paradigm ofmeta-learning, utilizes historical localization tasks to improve adaptabilityand learning efficiency in dynamic indoor environments. We introduce atask-weighted loss to enhance knowledge transfer within this framework. Ourcomprehensive experiments confirm the method's robustness and superiority overcurrent benchmarks, achieving a notable 23.13\\% average gain in Mean EuclideanDistance, particularly effective in scenarios with limited CSI data.",
        "title": "Dynamic Indoor Fingerprinting Localization based on Few-Shot  Meta-Learning with CSI Images",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05712",
        "abstract_url": "http://arxiv.org/abs/2401.05712",
        "authors": [
            {
                "last_name": "Hoang",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  Combining discovery and augmentation is important in the era of data usagewhen it comes to predicting the outcome of tasks. However, having to ask theuser the utility function to discover the goal to achieve the optimal smallrightful dataset is not an optimal solution. The existing solutions do not makegood use of this combination, hence underutilizing the data. In this paper, weintroduce a novel goal-oriented framework, called BOD: Blindly Optimal DataDiscovery, that involves humans in the loop and comparing utility scores everytime querying in the process without knowing the utility function. Thisestablishes the promise of using BOD: Blindly Optimal Data Discovery for moderndata science solutions.",
        "title": "BOD: Blindly Optimal Data Discovery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05715",
        "abstract_url": "http://arxiv.org/abs/2401.05715",
        "authors": [
            {
                "last_name": "Jackiewicz",
                "first_name": "Marcel"
            },
            {
                "last_name": "Kasperski",
                "first_name": "Adam"
            },
            {
                "last_name": "Zielinski",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In this paper, the recoverable robust shortest path problem under intervaluncertainty representations is discussed. This problem is known to be stronglyNP-hard and also hard to approximate in general digraphs. In this paper, theclass of acyclic digraphs is considered. It is shown that for the traditionalinterval uncertainty, the problem can be solved in polynomial time for allnatural, known from the literature, neighborhoods. Efficient algorithms forvarious classes of acyclic digraphs are constructed. Some negative results forgeneral digraphs are strengthened. Finally, some exact and approximate methodsof solving the problem under budgeted interval uncertainty are proposed.",
        "title": "Recoverable robust shortest path problem under interval uncertainty  representations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05716",
        "abstract_url": "http://arxiv.org/abs/2401.05716",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Xu"
            },
            {
                "last_name": "Scarlett",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In this paper, we study the problem of estimating the normalizing constant$\\int e^{-\\lambda f(x)}dx$ through queries to the black-box function $f$, where$f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\\lambda$ is aproblem parameter. We show that to estimate the normalizing constant within asmall relative error, the level of difficulty depends on the value of$\\lambda$: When $\\lambda$ approaches zero, the problem is similar to Bayesianquadrature (BQ), while when $\\lambda$ approaches infinity, the problem issimilar to Bayesian optimization (BO). More generally, the problem variesbetween BQ and BO. We find that this pattern holds true even when the functionevaluations are noisy, bringing new aspects to this topic. Our findings aresupported by both algorithm-independent lower bounds and algorithmic upperbounds, as well as simulation studies conducted on a variety of benchmarkfunctions.",
        "title": "Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature  and Bayesian Optimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05717",
        "abstract_url": "http://arxiv.org/abs/2401.05717",
        "authors": [
            {
                "last_name": "Salvi",
                "first_name": "Giampiero"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "LG",
            "SD",
            "",
            "",
            ""
        ],
        "abstract": "  This article investigates the possibility to use the class entropy of theoutput of a connectionist phoneme recogniser to predict time boundaries betweenphonetic classes. The rationale is that the value of the entropy shouldincrease in proximity of a transition between two segments that are wellmodelled (known) by the recognition network since it is a measure ofuncertainty. The advantage of this measure is its simplicity as the posteriorprobabilities of each class are available in connectionist phoneme recognition.The entropy and a number of measures based on differentiation of the entropyare used in isolation and in combination. The decision methods for predictingthe boundaries range from simple thresholds to neural network based procedure.The different methods are compared with respect to their precision, measured interms of the ratio between the number C of predicted boundaries within 10 or 20msec of the reference and the total number of predicted boundaries, and recall,measured as the ratio between C and the total number of reference boundaries.",
        "title": "Segment Boundary Detection via Class Entropy Measurements in  Connectionist Phoneme Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05722",
        "abstract_url": "http://arxiv.org/abs/2401.05722",
        "authors": [
            {
                "last_name": "Court\u00e8s",
                "first_name": "Cl\u00e9mentine"
            },
            {
                "last_name": "Boileau",
                "first_name": "Matthieu"
            },
            {
                "last_name": "C\u00f4te",
                "first_name": "Rapha\u00ebl"
            },
            {
                "last_name": "Hervieux",
                "first_name": "Paul-Antoine"
            },
            {
                "last_name": "Manfredi",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We solve the Landau-Lifshitz-Gilbert equation in the finite-temperatureregime, where thermal fluctuations are modeled by a random magnetic field whosevariance is proportional to the temperature. By rescaling the temperatureproportionally to the computational cell size $\\Delta x$ ($T \\to T\\,\\Deltax/a_{\\text{eff}}$, where $a_{\\text{eff}}$ is the lattice constant) [M. B. Hahn,J. Phys. Comm., 3:075009, 2019], we obtain Curie temperatures $T_{\\text{C}}$that are in line with the experimental values for cobalt, iron and nickel. Forfinite-sized objects such as nanowires (1D) and nanolayers (2D), the Curietemperature varies with the smallest size $d$ of the system. We show that thedifference between the computed finite-size $T_{\\text{C}}$ and the bulk$T_{\\text{C}}$ follows a power-law of the type: $(\\xi_0/d)^\\lambda$, where$\\xi_0$ is the correlation length at zero temperature, and $\\lambda$ is acritical exponent. We obtain values of $\\xi_0$ in the nanometer range, also inaccordance with other simulations and experiments. The computed criticalexponent is close to $\\lambda=2$ for all considered materials and geometries.This is the expected result for a mean-field approach, but slightly larger thanthe values observed experimentally.",
        "title": "Micromagnetic simulations of the size dependence of the Curie  temperature in ferromagnetic nanowires and nanolayers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05725",
        "abstract_url": "http://arxiv.org/abs/2401.05725",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Han"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Mu",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weile"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Wong",
                "first_name": "Kai-Kit"
            },
            {
                "last_name": "Yang",
                "first_name": "Kun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A simultaneously transmitting and reflecting reconfigurable intelligentsurface (STAR-RIS) enhanced unnamed aerial vehicle (UAV)-enabled multi-usermobile edge computing (MEC) scheme is proposed in this paper. Different fromthe existing MEC works, the proposed scheme allows bi-directional offloadingwhere users can simultaneously offload their computing tasks to the MEC serverssituated at the ground base station (BS) and aerial UAV with the assistance ofthe STARRIS. Specifically, we formulate an optimization problem aiming atmaximizing the energy efficiency of the system while ensuring the quality ofservice (QoS) constraint by jointly optimizing the resource allocation, userscheduling, passive beamforming of the STAR-RIS, and the UAV trajectory. Aniterative algorithm designed with the Dinkelbach's algorithm and the successiveconvex approximation (SCA) is proposed to effectively handle the formulatednon-convex optimization problem. Simulation results indicate that the proposedSTAR-RIS enhanced UAV-enabled MEC scheme possesses significant advantages inenhancing the system energy efficiency over other baseline schemes includingthe conventional RIS-aided scheme.",
        "title": "STAR-RIS Enhanced UAV-Enabled MEC Networks with Bi-Directional Task  Offloading",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05727",
        "abstract_url": "http://arxiv.org/abs/2401.05727",
        "authors": [
            {
                "last_name": "Chopra",
                "first_name": "Sahil"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Part of speech tagging in zero-resource settings can be an effective approachfor low-resource languages when no labeled training data is available. Existingsystems use two main techniques for POS tagging i.e. pretrained multilinguallarge language models(LLM) or project the source language labels into the zeroresource target language and train a sequence labeling model on it. We explorethe latter approach using the off-the-shelf alignment module and train a hiddenMarkov model(HMM) to predict the POS tags. We evaluate transfer learning setupwith English as a source language and French, German, and Spanish as targetlanguages for part-of-speech tagging. Our conclusion is that projectedalignment data in zero-resource language can be beneficial to predict POS tags.",
        "title": "Zero Resource Cross-Lingual Part Of Speech Tagging",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05730",
        "abstract_url": "http://arxiv.org/abs/2401.05730",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Jaeill"
            },
            {
                "last_name": "Hwang",
                "first_name": "Duhun"
            },
            {
                "last_name": "Lee",
                "first_name": "Eunjung"
            },
            {
                "last_name": "Suh",
                "first_name": "Jangwon"
            },
            {
                "last_name": "Kim",
                "first_name": "Jimyeong"
            },
            {
                "last_name": "Rhee",
                "first_name": "Wonjong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In the past few years, contrastive learning has played a central role for thesuccess of visual unsupervised representation learning. Around the same time,high-performance non-contrastive learning methods have been developed as well.While most of the works utilize only two views, we carefully review theexisting multi-view methods and propose a general multi-view strategy that canimprove learning speed and performance of any contrastive or non-contrastivemethod. We first analyze CMC's full-graph paradigm and empirically show thatthe learning speed of $K$-views can be increased by $_{K}\\mathrm{C}_{2}$ timesfor small learning rate and early training. Then, we upgrade CMC's full-graphby mixing views created by a crop-only augmentation, adopting small-size viewsas in SwAV multi-crop, and modifying the negative sampling. The resultingmulti-view strategy is called ECPP (Efficient Combinatorial Positive Pairing).We investigate the effectiveness of ECPP by applying it to SimCLR and assessingthe linear evaluation performance for CIFAR-10 and ImageNet-100. For eachbenchmark, we achieve a state-of-the-art performance. In case of ImageNet-100,ECPP boosted SimCLR outperforms supervised learning.",
        "title": "Enhancing Contrastive Learning with Efficient Combinatorial Positive  Pairing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05731",
        "abstract_url": "http://arxiv.org/abs/2401.05731",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Xiaohui"
            },
            {
                "last_name": "Li",
                "first_name": "Wenxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhongzhi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "",
            ""
        ],
        "abstract": "  In order to investigate the relationship between Shannon information measureof random variables, scholars such as Yeung utilized information diagrams toexplore the structured representation of information measures, establishingcorrespondences with sets. However, this method has limitations when studyinginformation measures of five or more random variables. In this paper, weconsider employing algebraic methods to study the relationship of informationmeasures of random variables. By introducing a semiring generated by randomvariables, we establish correspondences between sets and elements of thesemiring. Utilizing the Grobner-Shirshov basis, we present the structure of thesemiring and its standard form. Furthermore, we delve into the structure of thesemiring generated under Markov chain conditions (referred to as Markovsemiring), obtaining its standard form.",
        "title": "On Grobner-Shirshov bases for Markov semirings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05735",
        "abstract_url": "http://arxiv.org/abs/2401.05735",
        "authors": [
            {
                "last_name": "Kahatapitiya",
                "first_name": "Kumara"
            },
            {
                "last_name": "Karjauv",
                "first_name": "Adil"
            },
            {
                "last_name": "Abati",
                "first_name": "Davide"
            },
            {
                "last_name": "Porikli",
                "first_name": "Fatih"
            },
            {
                "last_name": "Asano",
                "first_name": "Yuki M."
            },
            {
                "last_name": "Habibian",
                "first_name": "Amirhossein"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion-based video editing have reached impressive quality and cantransform either the global style, local structure, and attributes of givenvideo inputs, following textual edit prompts. However, such solutions typicallyincur heavy memory and computational costs to generate temporally-coherentframes, either in the form of diffusion inversion and/or cross-frame attention.In this paper, we conduct an analysis of such inefficiencies, and suggestsimple yet effective modifications that allow significant speed-ups whilstmaintaining quality. Moreover, we introduce Object-Centric Diffusion, coined asOCD, to further reduce latency by allocating computations more towardsforeground edited regions that are arguably more important for perceptualquality. We achieve this by two novel proposals: i) Object-Centric Sampling,decoupling the diffusion steps spent on salient regions or background,allocating most of the model capacity to the former, and ii) Object-Centric 3DToken Merging, which reduces cost of cross-frame attention by fusing redundanttokens in unimportant background regions. Both techniques are readilyapplicable to a given video editing model \\textit{without} retraining, and candrastically reduce its memory and computational cost. We evaluate our proposalson inversion-based and control-signal-based editing pipelines, and show alatency reduction up to 10x for a comparable synthesis quality.",
        "title": "Object-Centric Diffusion for Efficient Video Editing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05736",
        "abstract_url": "http://arxiv.org/abs/2401.05736",
        "authors": [
            {
                "last_name": "Lerner",
                "first_name": "Paul"
            },
            {
                "last_name": "Ferret",
                "first_name": "Olivier"
            },
            {
                "last_name": "Guinaudeau",
                "first_name": "Camille"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Knowledge-based Visual Question Answering about Named Entities is achallenging task that requires retrieving information from a multimodalKnowledge Base. Named entities have diverse visual representations and aretherefore difficult to recognize. We argue that cross-modal retrieval may helpbridge the semantic gap between an entity and its depictions, and is foremostcomplementary with mono-modal retrieval. We provide empirical evidence throughexperiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three differentstrategies to fine-tune such a model: mono-modal, cross-modal, or jointtraining. Our method, which combines mono-and cross-modal retrieval, iscompetitive with billion-parameter models on the three datasets, while beingconceptually simpler and computationally cheaper.",
        "title": "Cross-modal Retrieval for Knowledge-based Visual Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05737",
        "abstract_url": "http://arxiv.org/abs/2401.05737",
        "authors": [
            {
                "last_name": "Manjavacas",
                "first_name": "Antonio"
            },
            {
                "last_name": "Campoy-Nieves",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Jim\u00e9nez-Raboso",
                "first_name": "Javier"
            },
            {
                "last_name": "Molina-Solana",
                "first_name": "Miguel"
            },
            {
                "last_name": "G\u00f3mez-Romero",
                "first_name": "Juan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            ""
        ],
        "abstract": "  Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driverof energy consumption in commercial and residential buildings. Recent studieshave shown that Deep Reinforcement Learning (DRL) algorithms can outperformtraditional reactive controllers. However, DRL-based solutions are generallydesigned for ad hoc setups and lack standardization for comparison. To fillthis gap, this paper provides a critical and reproducible evaluation, in termsof comfort and energy consumption, of several state-of-the-art DRL algorithmsfor HVAC control. The study examines the controllers' robustness, adaptability,and trade-off between optimization goals by using the Sinergym framework. Theresults obtained confirm the potential of DRL algorithms, such as SAC and TD3,in complex scenarios and reveal several challenges related to generalizationand incremental learning.",
        "title": "An experimental evaluation of Deep Reinforcement Learning algorithms for  HVAC control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05738",
        "abstract_url": "http://arxiv.org/abs/2401.05738",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Boheng"
            },
            {
                "last_name": "Lu",
                "first_name": "Yi"
            },
            {
                "last_name": "Shi",
                "first_name": "Pengbo"
            },
            {
                "last_name": "Chen",
                "first_name": "Qingzi"
            },
            {
                "last_name": "Liu",
                "first_name": "Jirui"
            },
            {
                "last_name": "Zhu",
                "first_name": "Lingyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We revisit the relationship between attention mechanisms and large kernelConvNets in visual transformers and propose a new spatial attention named LargeKernel Convolutional Attention (LKCA). It simplifies the attention operation byreplacing it with a single large kernel convolution. LKCA combines theadvantages of convolutional neural networks and visual transformers, possessinga large receptive field, locality, and parameter sharing. We explained thesuperiority of LKCA from both convolution and attention perspectives, providingequivalent code implementations for each view. Experiments confirm that LKCAimplemented from both the convolutional and attention perspectives exhibitequivalent performance. We extensively experimented with the LKCA variant ofViT in both classification and segmentation tasks. The experiments demonstratedthat LKCA exhibits competitive performance in visual tasks. Our code will bemade publicly available at https://github.com/CatworldLee/LKCA.",
        "title": "LKCA: Large Kernel Convolutional Attention",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05739",
        "abstract_url": "http://arxiv.org/abs/2401.05739",
        "authors": [
            {
                "last_name": "Jia",
                "first_name": "Ang"
            },
            {
                "last_name": "Fan",
                "first_name": "Ming"
            },
            {
                "last_name": "Xu",
                "first_name": "Xi"
            },
            {
                "last_name": "Jin",
                "first_name": "Wuxia"
            },
            {
                "last_name": "Wang",
                "first_name": "Haijun"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Binary function similarity detection plays an important role in a wide rangeof security applications. Existing works usually assume that the query functionand target function share equal semantics and compare their full semantics toobtain the similarity. However, we find that the function mapping is morecomplex, especially when function inlining happens.  In this paper, we will systematically investigate cross-inlining binaryfunction similarity detection. We first construct a cross-inlining dataset bycompiling 51 projects using 9 compilers, with 4 optimizations, to 6architectures, with 2 inlining flags, which results in two datasets both with216 combinations. Then we construct the cross-inlining function mappings bylinking the common source functions in these two datasets. Through analysis ofthis dataset, we find that three cross-inlining patterns widely exist whileexisting work suffers when detecting cross-inlining binary function similarity.Next, we propose a pattern-based model named CI-Detector for cross-inliningmatching. CI-Detector uses the attributed CFG to represent the semantics ofbinary functions and GNN to embed binary functions into vectors. CI-Detectorrespectively trains a model for these three cross-inlining patterns. Finally,the testing pairs are input to these three models and all the producedsimilarities are aggregated to produce the final similarity. We conduct severalexperiments to evaluate CI-Detector. Results show that CI-Detector can detectcross-inlining pairs with a precision of 81% and a recall of 97%, which exceedsall state-of-the-art works.",
        "title": "Cross-Inlining Binary Function Similarity Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05740",
        "abstract_url": "http://arxiv.org/abs/2401.05740",
        "authors": [
            {
                "last_name": "Berger",
                "first_name": "Andre"
            },
            {
                "last_name": "Rouhani",
                "first_name": "Arman"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Marc"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  In this paper, we introduce an improved upper bound for the efficiency ofNash equilibria in utilitarian scheduling games on related machines. Themachines have varying speeds and adhere to the Shortest Processing Time (SPT)policy as the global order for job processing. The goal of each job is tominimize its completion time, while the social objective is to minimize the sumof completion times. Our main result provides an upper bound of$2-\\frac{1}{2\\cdot(2m-1)}$ on the price of anarchy for the general case of $m$machines. We improve this bound to 3/2 for the case of two machines, and to$2-\\frac{1}{2\\cdot m}$ for the general case of $m$ machines when the machineshave divisible speeds.",
        "title": "An improved bound for the price of anarchy for related machine  scheduling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05743",
        "abstract_url": "http://arxiv.org/abs/2401.05743",
        "authors": [
            {
                "last_name": "Marconi",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Rosati",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We study consistent query answering over knowledge bases expressed byexistential rules. Specifically, we establish the data complexity of consistentquery answering and repair checking under tuple-deletion semantics for ageneral class of disjunctive existential rules and for several subclassesthereof (acyclic, linear, full, guarded, and sticky). In particular, weidentify several cases in which the above problems are tractable or evenfirst-order rewritable, and present new query rewriting techniques that can bethe basis for practical inconsistency-tolerant query answering systems.",
        "title": "Consistent Query Answering for Existential Rules under Tuple-Deletion  Semantics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05744",
        "abstract_url": "http://arxiv.org/abs/2401.05744",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yicong"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiangguo"
            },
            {
                "last_name": "Chen",
                "first_name": "Hongxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Sixiao"
            },
            {
                "last_name": "Yang",
                "first_name": "Yu"
            },
            {
                "last_name": "Xu",
                "first_name": "Guandong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Compared with only pursuing recommendation accuracy, the explainability of arecommendation model has drawn more attention in recent years. Many graph-basedrecommendations resort to informative paths with the attention mechanism forthe explanation. Unfortunately, these attention weights are intentionallydesigned for model accuracy but not explainability. Recently, some researchershave started to question attention-based explainability because the attentionweights are unstable for different reproductions, and they may not always alignwith human intuition. Inspired by the counterfactual reasoning from causalitylearning theory, we propose a novel explainable framework targeting path-basedrecommendations, wherein the explainable weights of paths are learned toreplace attention weights. Specifically, we design two counterfactual reasoningalgorithms from both path representation and path topological structureperspectives. Moreover, unlike traditional case studies, we also propose apackage of explainability evaluation solutions with both qualitative andquantitative methods. We conduct extensive experiments on three real-worlddatasets, the results of which further demonstrate the effectiveness andreliability of our method.",
        "title": "Attention Is Not the Only Choice: Counterfactual Reasoning for  Path-Based Explainable Recommendation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05745",
        "abstract_url": "http://arxiv.org/abs/2401.05745",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Barry Shichen"
            },
            {
                "last_name": "Liang",
                "first_name": "Siyun"
            },
            {
                "last_name": "Paetzold",
                "first_name": "Johannes"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose the use of a Transformer to accurately predict normals from pointclouds with noise and density variations. Previous learning-based methodsutilize PointNet variants to explicitly extract multi-scale features atdifferent input scales, then focus on a surface fitting method by which localpoint cloud neighborhoods are fitted to a geometric surface approximated byeither a polynomial function or a multi-layer perceptron (MLP). However,fitting surfaces to fixed-order polynomial functions can suffer fromoverfitting or underfitting, and learning MLP-represented hyper-surfacesrequires pre-generated per-point weights. To avoid these limitations, we firstunify the design choices in previous works and then propose a simplifiedTransformer-based model to extract richer and more robust geometric featuresfor the surface normal estimation task. Through extensive experiments, wedemonstrate that our Transformer-based method achieves state-of-the-artperformance on both the synthetic shape dataset PCPNet, and the real-worldindoor scene dataset SceneNN, exhibiting more noise-resilient behavior andsignificantly faster inference. Most importantly, we demonstrate that thesophisticated hand-designed modules in existing works are not necessary toexcel at the task of surface normal estimation.",
        "title": "Surface Normal Estimation with Transformers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05746",
        "abstract_url": "http://arxiv.org/abs/2401.05746",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Heqing"
            },
            {
                "last_name": "Shen",
                "first_name": "Meng"
            },
            {
                "last_name": "Hu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Chng",
                "first_name": "Eng Siong"
            },
            {
                "last_name": "Rajan",
                "first_name": "Deepu"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Audio-visual deepfake detection scrutinizes manipulations in public videousing complementary multimodal cues. Current methods, which train on fusedmultimodal data for multimodal targets face challenges due to uncertainties andinconsistencies in learned representations caused by independent modalitymanipulations in deepfake videos. To address this, we propose cross-modalityand within-modality regularization to preserve modality distinctions duringmultimodal representation learning. Our approach includes an audio-visualtransformer module for modality correspondence and a cross-modalityregularization module to align paired audio-visual signals, preserving modalitydistinctions. Simultaneously, a within-modality regularization module refinesunimodal representations with modality-specific targets to retainmodal-specific details. Experimental results on the public audio-visualdataset, FakeAVCeleb, demonstrate the effectiveness and competitiveness of ourapproach.",
        "title": "Cross-Modality and Within-Modality Regularization for Audio-Visual  DeepFake Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05749",
        "abstract_url": "http://arxiv.org/abs/2401.05749",
        "authors": [
            {
                "last_name": "Thompson",
                "first_name": "Brian"
            },
            {
                "last_name": "Dhaliwal",
                "first_name": "Mehak Preet"
            },
            {
                "last_name": "Frisch",
                "first_name": "Peter"
            },
            {
                "last_name": "Domhan",
                "first_name": "Tobias"
            },
            {
                "last_name": "Federico",
                "first_name": "Marcello"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  We show that content on the web is often translated into many languages, andthe low quality of these multi-way translations indicates they were likelycreated using Machine Translation (MT). Multi-way parallel, machine generatedcontent not only dominates the translations in lower resource languages; italso constitutes a large fraction of the total web content in those languages.We also find evidence of a selection bias in the type of content which istranslated into many languages, consistent with low quality English contentbeing translated en masse into many lower resource languages, via MT. Our workraises serious concerns about training models such as multilingual largelanguage models on both monolingual and bilingual data scraped from the web.",
        "title": "A Shocking Amount of the Web is Machine Translated: Insights from  Multi-Way Parallelism",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05750",
        "abstract_url": "http://arxiv.org/abs/2401.05750",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Peng"
            },
            {
                "last_name": "Tan",
                "first_name": "Feitong"
            },
            {
                "last_name": "Yu",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yinda"
            },
            {
                "last_name": "Qi",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Despite advances in 3D generation, the direct creation of 3D objects withinan existing 3D scene represented as NeRF remains underexplored. This processrequires not only high-quality 3D object generation but also seamlesscomposition of the generated 3D content into the existing NeRF. To this end, wepropose a new method, GO-NeRF, capable of utilizing scene context forhigh-quality and harmonious 3D object generation within an existing NeRF. Ourmethod employs a compositional rendering formulation that allows the generated3D objects to be seamlessly composited into the scene utilizing learned3D-aware opacity maps without introducing unintended scene modification.Moreover, we also develop tailored optimization objectives and trainingstrategies to enhance the model's ability to exploit scene context and mitigateartifacts, such as floaters, originating from 3D object generation within ascene. Extensive experiments on both feed-forward and $360^o$ scenes show thesuperior performance of our proposed GO-NeRF in generating objects harmoniouslycomposited with surrounding scenes and synthesizing high-quality novel viewimages. Project page at {\\url{https://daipengwa.github.io/GO-NeRF/}.",
        "title": "GO-NeRF: Generating Virtual Objects in Neural Radiance Fields",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05752",
        "abstract_url": "http://arxiv.org/abs/2401.05752",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Na"
            },
            {
                "last_name": "Qi",
                "first_name": "Lei"
            },
            {
                "last_name": "Guo",
                "first_name": "Jintao"
            },
            {
                "last_name": "Shi",
                "first_name": "Yinghuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Domain generalization (DG) intends to train a model on multiple sourcedomains to ensure that it can generalize well to an arbitrary unseen targetdomain. The acquisition of domain-invariant representations is pivotal for DGas they possess the ability to capture the inherent semantic information of thedata, mitigate the influence of domain shift, and enhance the generalizationcapability of the model. Adopting multiple perspectives, such as the sample andthe feature, proves to be effective. The sample perspective facilitates dataaugmentation through data manipulation techniques, whereas the featureperspective enables the extraction of meaningful generalization features. Inthis paper, we focus on improving the generalization ability of the model bycompelling it to acquire domain-invariant representations from both the sampleand feature perspectives by disentangling spurious correlations and enhancingpotential correlations. 1) From the sample perspective, we develop a frequencyrestriction module, guiding the model to focus on the relevant correlationsbetween object features and labels, thereby disentangling spuriouscorrelations. 2) From the feature perspective, the simple Tail Interactionmodule implicitly enhances potential correlations among all samples from allsource domains, facilitating the acquisition of domain-invariantrepresentations across multiple domains for the model. The experimental resultsshow that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons(MLPs) with a strong baseline embedded with these two modules can achievesuperior results, e.g., an average accuracy of 92.30% on Digits-DG.",
        "title": "Learning Generalizable Models via Disentangling Spurious and Enhancing  Potential Correlations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05753",
        "abstract_url": "http://arxiv.org/abs/2401.05753",
        "authors": [
            {
                "last_name": "Ko",
                "first_name": "Yousun"
            },
            {
                "last_name": "Burgstaller",
                "first_name": "Bernd"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Soft errors are a type of transient digital signal corruption that occurs indigital hardware components such as the internal flip-flops of CPU pipelines,the register file, memory cells, and even internal communication buses. Softerrors are caused by environmental radioactivity, magnetic interference,lasers, and temperature fluctuations, either unintentionally, or as part of adeliberate attempt to compromise a system and expose confidential data.  We propose a bit-level error coalescing (BEC) static program analysis and itstwo use cases to understand and improve program reliability against softerrors. The BEC analysis tracks each bit corruption in the register file andclassifies the effect of the corruption by its semantics at compile time. Theusefulness of the proposed analysis is demonstrated in two scenarios, faultinjection campaign pruning, and reliability-aware program transformation.Experimental results show that bit-level analysis pruned up to 30.04 % ofexhaustive fault injection campaigns (13.71 % on average), without loss ofaccuracy. Program vulnerability was reduced by up to 13.11 % (4.94 % onaverage) through bit-level vulnerability-aware instruction scheduling. Theanalysis has been implemented within LLVM and evaluated on the RISC-Varchitecture.  To the best of our knowledge, the proposed BEC analysis is the firstbit-level compiler analysis for program reliability against soft errors. Theproposed method is generic and not limited to a specific computer architecture.",
        "title": "BEC: Bit-Level Static Analysis for Reliability against Soft Errors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05757",
        "abstract_url": "http://arxiv.org/abs/2401.05757",
        "authors": [
            {
                "last_name": "Aramaki",
                "first_name": "Mitsuko"
            },
            {
                "last_name": "Bernard",
                "first_name": "Corentin"
            },
            {
                "last_name": "Kronland-Martinet",
                "first_name": "Richard"
            },
            {
                "last_name": "Poirot",
                "first_name": "Samuel"
            },
            {
                "last_name": "Ystad",
                "first_name": "S\u00f8lvi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  Intuitive control of synthesis processes is an ongoing challenge within thedomain of auditory perception and cognition. Previous works on sound modellingcombined with psychophysical tests have enabled our team to develop asynthesizer that provides intuitive control of actions and objects based onsemantic descriptions for sound sources. In this demo we present an augmentedversion of the synthesizer in which we added tactile stimulations to increasethe sensation of true continuous friction interactions (rubbing and scratching)with the simulated objects. This is of interest for several reasons. Firstly,it enables to evaluate the realism of our sound model in presence ofstimulations from other modalities. Secondly it enables to compare tactile andauditory signal structures linked to the same evocation, and thirdly itprovides a tool to investigate multimodal perception and how stimulations fromdifferent modalities should be combined to provide realistic user interfaces.",
        "title": "Intuitive Control of Scraping and Rubbing Through Audio-tactile  Synthesis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05759",
        "abstract_url": "http://arxiv.org/abs/2401.05759",
        "authors": [
            {
                "last_name": "Vaccon",
                "first_name": "Tristan"
            },
            {
                "last_name": "Verron",
                "first_name": "Thibaut"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC",
            "",
            ""
        ],
        "abstract": "  A universal analytic Gr{\\\"o}bner basis (UAGB) of an ideal of a Tate algebrais a set containing a local Gr{\\\"o}bner basis for all suitable convergenceradii. In a previous article, the authors proved the existence of finite UAGB'sfor polynomial ideals, leaving open the question of how to compute them. Inthis paper, we provide an algorithm computing a UAGB for a given polynomialideal, by traversing the Gr{\\\"o}bner fan of the ideal. As an application, itoffers a new point of view on algorithms for computing tropical varieties ofhomogeneous polynomial ideals, which typically rely on lifting the computationsto an algebra of power series. Motivated by effective computations in tropicalanalytic geometry, we also examine local bases for more general convergenceconditions, constraining the radii to a convex polyhedron. In this setting, weprovide an algorithm to compute local Gr{\\\"o}bner bases and discuss obstaclestowards proving the existence of finite UAGBs. CCS CONCEPTS $\\bullet$ Computingmethodologies $\\rightarrow$ Algebraic algorithms.",
        "title": "Universal Analytic Gr{\\\"o}bner Bases and Tropical Geometry",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05761",
        "abstract_url": "http://arxiv.org/abs/2401.05761",
        "authors": [
            {
                "last_name": "Caramancion",
                "first_name": "Kevin Matthe"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  This study embarked on a comprehensive exploration of user preferencesbetween Search Engines and Large Language Models (LLMs) in the context ofvarious information retrieval scenarios. Conducted with a sample size of 100internet users (N=100) from across the United States, the research delved into20 distinct use cases ranging from factual searches, such as looking upCOVID-19 guidelines, to more subjective tasks, like seeking interpretations ofcomplex concepts in layman's terms. Participants were asked to state theirpreference between using a traditional search engine or an LLM for eachscenario. This approach allowed for a nuanced understanding of how usersperceive and utilize these two predominant digital tools in differing contexts.The use cases were carefully selected to cover a broad spectrum of typicalonline queries, thus ensuring a comprehensive analysis of user preferences. Thefindings reveal intriguing patterns in user choices, highlighting a cleartendency for participants to favor search engines for direct, fact-basedqueries, while LLMs were more often preferred for tasks requiring nuancedunderstanding and language processing. These results offer valuable insightsinto the current state of digital information retrieval and pave the way forfuture innovations in this field. This study not only sheds light on thespecific contexts in which each tool is favored but also hints at the potentialfor developing hybrid models that leverage the strengths of both search enginesand LLMs. The insights gained from this research are pivotal for developers,researchers, and policymakers in understanding the evolving landscape ofdigital information retrieval and user interaction with these technologies.",
        "title": "Large Language Models vs. Search Engines: Evaluating User Preferences  Across Varied Information Retrieval Scenarios",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05765",
        "abstract_url": "http://arxiv.org/abs/2401.05765",
        "authors": [
            {
                "last_name": "Boschi",
                "first_name": "Tobia"
            },
            {
                "last_name": "Bonin",
                "first_name": "Francesca"
            },
            {
                "last_name": "Epperlein",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Ordonez-Hurtado",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Pascale",
                "first_name": "Alessandra"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Functional data analysis has emerged as a crucial tool in many contemporaryscientific domains that require the integration and interpretation of complexdata. Moreover, the advent of new technologies has facilitated the collectionof a large number of longitudinal variables, making feature selection pivotalfor avoiding overfitting and improving prediction performance. This paperintroduces a novel methodology called FSFC (Feature Selection for FunctionalClassification), that addresses the challenge of jointly performing featureselection and classification of functional data in scenarios with categoricalresponses and longitudinal features. Our approach tackles a newly definedoptimization problem that integrates logistic loss and functional features toidentify the most crucial features for classification. To address theminimization procedure, we employ functional principal components and develop anew adaptive version of the Dual Augmented Lagrangian algorithm that leveragesthe sparsity structure of the problem for dimensionality reduction. Thecomputational efficiency of FSFC enables handling high-dimensional scenarioswhere the number of features may considerably exceed the number of statisticalunits. Simulation experiments demonstrate that FSFC outperforms other machinelearning and deep learning methods in computational time and classificationaccuracy. Furthermore, the FSFC feature selection capability can be leveragedto significantly reduce the problem's dimensionality and enhance theperformances of other classification algorithms. The efficacy of FSFC is alsodemonstrated through a real data application, analyzing relationships betweenfour chronic diseases and other health and socio-demographic factors.",
        "title": "Feature Selection for Functional Data Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05767",
        "abstract_url": "http://arxiv.org/abs/2401.05767",
        "authors": [
            {
                "last_name": "Tran",
                "first_name": "Ly-Duyen"
            },
            {
                "last_name": "Gurrin",
                "first_name": "Cathal"
            },
            {
                "last_name": "Smeaton",
                "first_name": "Alan F."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "HC"
        ],
        "abstract": "  Personal data includes the digital footprints that we leave behind as part ofour everyday activities, both online and offline in the real world. It includesdata we collect ourselves, such as from wearables, as well as the datacollected by others about our online behaviour and activities. Sometimes we areable to use the personal data we ourselves collect, in order to examine someparts of our lives but for the most part, our personal data is leveraged bythird parties including internet companies, for services like targetedadvertising and recommendations. Lifelogging is a form of extreme personal datagathering and in this article we present an overview of the tools used tomanage access to lifelogs as demonstrated at the most recent of the annualLifelog Search Challenge benchmarking workshops. Here, experimental systems areshowcased in live, real time information seeking tasks by real users. Thisoverview of these systems' capabilities show the range of possibilities foraccessing our own personal data which may, in time, become more easilyavailable as consumer-level services.",
        "title": "Lifelogging As An Extreme Form of Personal Information Management --  What Lessons To Learn",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05768",
        "abstract_url": "http://arxiv.org/abs/2401.05768",
        "authors": [
            {
                "last_name": "Gheorghiu",
                "first_name": "Adrian"
            },
            {
                "last_name": "T\u0103iatu",
                "first_name": "Iulian-Marius"
            },
            {
                "last_name": "Cercel",
                "first_name": "Dumitru-Clementin"
            },
            {
                "last_name": "Marin",
                "first_name": "Iuliana"
            },
            {
                "last_name": "Pop",
                "first_name": "Florin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The detection and classification of diseases in Robusta coffee leaves areessential to ensure that plants are healthy and the crop yield is kept high.However, this job requires extensive botanical knowledge and much wasted time.Therefore, this task and others similar to it have been extensively researchedsubjects in image classification. Regarding leaf disease classification, mostapproaches have used the more popular PlantVillage dataset while completelydisregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. Asthe RoCoLe dataset is imbalanced and does not have many samples, fine-tuning ofpre-trained models and multiple augmentation techniques need to be used. Thecurrent paper uses the RoCoLe dataset and approaches based on deep learning forclassifying coffee leaf diseases from images, incorporating the pix2pix modelfor segmentation and cycle-generative adversarial network (CycleGAN) foraugmentation. Our study demonstrates the effectiveness of Transformer-basedmodels, online augmentations, and CycleGAN augmentation in improving leafdisease classification. While synthetic data has limitations, it complementsreal data, enhancing model performance. These findings contribute to developingrobust techniques for plant disease detection and classification.",
        "title": "Evaluating Data Augmentation Techniques for Coffee Leaf Disease  Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05771",
        "abstract_url": "http://arxiv.org/abs/2401.05771",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhiying"
            },
            {
                "last_name": "Guo",
                "first_name": "Yongxin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate lesion classification in Wireless Capsule Endoscopy (WCE) images isvital for early diagnosis and treatment of gastrointestinal (GI) cancers.However, this task is confronted with challenges like tiny lesions andbackground interference. Additionally, WCE images exhibit higher intra-classvariance and inter-class similarities, adding complexity. To tackle thesechallenges, we propose Decoupled Supervised Contrastive Learning for WCE imageclassification, learning robust representations from zoomed-in WCE imagesgenerated by Saliency Augmentor. Specifically, We use uniformly down-sampledWCE images as anchors and WCE images from the same class, especially theirzoomed-in images, as positives. This approach empowers the Feature Extractor tocapture rich representations from various views of the same image, facilitatedby Decoupled Supervised Contrastive Learning. Training a linear Classifier onthese representations within 10 epochs yields an impressive 92.01% overallaccuracy, surpassing the prior state-of-the-art (SOTA) by 0.72% on a blend oftwo publicly accessible WCE datasets. Code is available at:https://github.com/Qiukunpeng/DSCL.",
        "title": "Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image  Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05772",
        "abstract_url": "http://arxiv.org/abs/2401.05772",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Wujie"
            },
            {
                "last_name": "Chen",
                "first_name": "Defang"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Feng",
                "first_name": "Yan"
            },
            {
                "last_name": "Chen",
                "first_name": "Chun"
            },
            {
                "last_name": "Wang",
                "first_name": "Can"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Deep learning has witnessed significant advancements in recent years at thecost of increasing training, inference, and model storage overhead. Whileexisting model compression methods strive to reduce the number of modelparameters while maintaining high accuracy, they inevitably necessitate there-training of the compressed model or impose architectural constraints. Toovercome these limitations, this paper presents a novel framework, termed\\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' modelis trained to receive the parameters of a larger model and generate compressedparameters. The concept of KT draws inspiration from language translation,which effectively employs neural networks to convert different languages,maintaining identical meaning. Accordingly, we explore the potential of neuralnetworks to convert models of disparate sizes, while preserving theirfunctionality. We propose a comprehensive framework for KT, introduce dataaugmentation strategies to enhance model performance despite restrictedtraining data, and successfully demonstrate the feasibility of KT on the MNISTdataset. Code is available at \\url{https://github.com/zju-SWJ/KT}.",
        "title": "Knowledge Translation: A New Pathway for Model Compression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05777",
        "abstract_url": "http://arxiv.org/abs/2401.05777",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jinxin"
            },
            {
                "last_name": "Cao",
                "first_name": "Shulin"
            },
            {
                "last_name": "Shi",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tingjian"
            },
            {
                "last_name": "Hou",
                "first_name": "Lei"
            },
            {
                "last_name": "Li",
                "first_name": "Juanzi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancement in the capabilities of large language models (LLMs) hastriggered a new surge in LLMs' evaluation. Most recent evaluation works tendsto evaluate the comprehensive ability of LLMs over series of tasks. However,the deep structure understanding of natural language is rarely explored. Inthis work, we examine the ability of LLMs to deal with structured semantics onthe tasks of question answering with the help of the human-constructed formallanguage. Specifically, we implement the inter-conversion of natural and formallanguage through in-context learning of LLMs to verify their ability tounderstand and generate the structured logical forms. Extensive experimentswith models of different sizes and in different formal languages show thattoday's state-of-the-art LLMs' understanding of the logical forms can approachhuman level overall, but there still are plenty of room in generating correctlogical forms, which suggest that it is more effective to use LLMs to generatemore natural language training data to reinforce a small model than directlyanswering questions with LLMs. Moreover, our results also indicate that modelsexhibit considerable sensitivity to different formal languages. In general, theformal language with the lower the formalization level, i.e. the more similarit is to natural language, is more LLMs-friendly.",
        "title": "Probing Structured Semantics Understanding and Generation of Language  Models via Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05778",
        "abstract_url": "http://arxiv.org/abs/2401.05778",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanling"
            },
            {
                "last_name": "Fu",
                "first_name": "Chuanpu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yong"
            },
            {
                "last_name": "Li",
                "first_name": "Sijia"
            },
            {
                "last_name": "Deng",
                "first_name": "Xinhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinglin"
            },
            {
                "last_name": "Qiu",
                "first_name": "Ziyi"
            },
            {
                "last_name": "Li",
                "first_name": "Peiyang"
            },
            {
                "last_name": "Tan",
                "first_name": "Zhixing"
            },
            {
                "last_name": "Xiong",
                "first_name": "Junwu"
            },
            {
                "last_name": "Kong",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Wen",
                "first_name": "Zujie"
            },
            {
                "last_name": "Xu",
                "first_name": "Ke"
            },
            {
                "last_name": "Li",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) have strong capabilities in solving diversenatural language processing tasks. However, the safety and security issues ofLLM systems have become the major obstacle to their widespread application.Many studies have extensively investigated risks in LLM systems and developedthe corresponding mitigation strategies. Leading-edge enterprises such asOpenAI, Google, Meta, and Anthropic have also made lots of efforts onresponsible LLMs. Therefore, there is a growing need to organize the existingstudies and establish comprehensive taxonomies for the community. In thispaper, we delve into four essential modules of an LLM system, including aninput module for receiving prompts, a language model trained on extensivecorpora, a toolchain module for development and deployment, and an outputmodule for exporting LLM-generated content. Based on this, we propose acomprehensive taxonomy, which systematically analyzes potential risksassociated with each module of an LLM system and discusses the correspondingmitigation strategies. Furthermore, we review prevalent benchmarks, aiming tofacilitate the risk assessment of LLM systems. We hope that this paper can helpLLM participants embrace a systematic perspective to build their responsibleLLM systems.",
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language  Model Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05779",
        "abstract_url": "http://arxiv.org/abs/2401.05779",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jing"
            },
            {
                "last_name": "Le",
                "first_name": "Trung"
            },
            {
                "last_name": "Hayat",
                "first_name": "Munawar"
            },
            {
                "last_name": "Harandi",
                "first_name": "Mehrtash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In response to data protection regulations and the ``right to be forgotten'',in this work, we introduce an unlearning algorithm for diffusion models. Ouralgorithm equips a diffusion model with a mechanism to mitigate the concernsrelated to data memorization. To achieve this, we formulate the unlearningproblem as a bi-level optimization problem, wherein the outer objective is topreserve the utility of the diffusion model on the remaining data. The innerobjective aims to scrub the information associated with forgetting data bydeviating the learnable generative process from the ground-truth denoisingprocedure. To solve the resulting bi-level problem, we adopt a first-ordermethod, having superior practical performance while being vigilant about thediffusion process and solving a bi-level problem therein. Empirically, wedemonstrate that our algorithm can preserve the model utility, effectiveness,and efficiency while removing across two widely-used diffusion models and inboth conditional and unconditional image generation scenarios. In ourexperiments, we demonstrate the unlearning of classes, attributes, and even arace from face and object datasets such as UTKFace, CelebA, CelebA-HQ, andCIFAR10.",
        "title": "EraseDiff: Erasing Data Influence in Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05782",
        "abstract_url": "http://arxiv.org/abs/2401.05782",
        "authors": [
            {
                "last_name": "Noom",
                "first_name": "Jacques"
            },
            {
                "last_name": "Soloviev",
                "first_name": "Oleg"
            },
            {
                "last_name": "Smith",
                "first_name": "Carlas"
            },
            {
                "last_name": "Verhaegen",
                "first_name": "Michel"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Stochastic Closed-Loop Active Fault Diagnosis (CLAFD) aims to select theinput sequentially in order to improve the discrimination of different modelsby minimizing the predicted error probability. As computation of these errorprobabilities encompasses the evaluation of multidimensional probabilityintegrals, relaxation methods are of interest. This manuscript presents a newmethod that allows to make an improved trade-off between three factors --namely maximized accuracy of diagnosis, minimized number of consecutivemeasurements to achieve that accuracy, and minimized computational effort pertime step -- with respect to the state-of-the-art. It relies on minimizing anupper bound on the error probability, which is in the case of linear modelswith Gaussian noise proven to be concave in the most challenging discriminationconditions. A simulation study is conducted both for open-loop and feedbackcontrolled candidate models. The results demonstrate the favorable trade-offusing the new contributions in this manuscript.",
        "title": "Online input design for discrimination of linear models using concave  minimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05783",
        "abstract_url": "http://arxiv.org/abs/2401.05783",
        "authors": [
            {
                "last_name": "Vlachou",
                "first_name": "Maria"
            },
            {
                "last_name": "Macdonald",
                "first_name": "Craig"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In Conversational Recommendation Systems (CRS), a user can provide feedbackon recommended items at each interaction turn, leading the CRS towards moredesirable recommendations. Currently, different types of CRS offer variouspossibilities for feedback, i.e., natural language feedback, or answeringclarifying questions. In most cases, a user simulator is employed for trainingas well as evaluating the CRS. Such user simulators typically critique thecurrent retrieved items based on knowledge of a single target item. Still,evaluating systems in offline settings with simulators suffers from problems,such as focusing entirely on a single target item (not addressing theexploratory nature of a recommender system), and exhibiting extreme patience(consistent feedback over a large number of turns). To overcome theselimitations, we obtain extra judgements for a selection of alternative items incommon CRS datasets, namely Shoes and Fashion IQ Dresses. Going further, wepropose improved user simulators that allow simulated users not only to expresstheir preferences about alternative items to their original target, but also tochange their mind and level of patience. In our experiments using the relativeimage captioning CRS setting and different CRS models, we find that using theknowledge of alternatives by the simulator can have a considerable impact onthe evaluation of existing CRS models, specifically that the existingsingle-target evaluation underestimates their effectiveness, and when simulatedusers are allowed to instead consider alternatives, the system can rapidlyrespond to more quickly satisfy the user.",
        "title": "What Else Would I Like? A User Simulator using Alternatives for Improved  Evaluation of Fashion Conversational Recommendation Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05787",
        "abstract_url": "http://arxiv.org/abs/2401.05787",
        "authors": [
            {
                "last_name": "Parvez",
                "first_name": "Md Rizwan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While chain-of-thought (CoT) prompting has revolutionized how LLMs performreasoning tasks, its current methods and variations (e.g, Self-consistency,ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) sufferfrom limitations like slowness, limited context grounding, hallucination andinconsistent outputs. To overcome these challenges, we introduce Evidence toGenerate (E2G), a novel single-agent, two-step prompting framework. Instead ofunverified reasoning claims, this innovative approach leverages the power of\"evidence for decision making\" by first focusing exclusively on the thoughtsequences (the series of intermediate steps) explicitly mentioned in thecontext which then serve as extracted evidence, guiding the LLM's outputgeneration process with greater precision and efficiency. This simple yetpowerful approach unlocks the true potential of chain-of-thought likeprompting, paving the way for faster, more reliable, and more contextuallyaware reasoning in LLMs. \\tool achieves remarkable results robustly across awide range of knowledge-intensive reasoning and generation tasks, surpassingbaseline approaches with state-of-the-art LLMs. For example, (i) on LogiQAbenchmark using GPT-4 as backbone model, \\tool achieves a new state-of-theAccuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant ofE2G with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9F1 points, reaching an F1 score of 83.3 on a subset of DROP.",
        "title": "Evidence to Generate (E2G): A Single-agent Two-step Prompting for  Context Grounded and Retrieval Augmented Reasoning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05790",
        "abstract_url": "http://arxiv.org/abs/2401.05790",
        "authors": [
            {
                "last_name": "Dolata",
                "first_name": "Mateusz"
            },
            {
                "last_name": "Lange",
                "first_name": "Norbert"
            },
            {
                "last_name": "Schwabe",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The rise of generative AI has led many companies to hire freelancers toharness its potential. However, this technology presents unique challenges todevelopers who have not previously engaged with it. Freelancers may find thesechallenges daunting due to the absence of organizational support and theirreliance on positive client feedback. In a study involving 52 freelancedevelopers, we identified multiple challenges associated with developingsolutions based on generative AI. Freelancers often struggle with aspects theyperceive as unique to generative AI such as unpredictability of its output, theoccurrence of hallucinations, and the inconsistent effort required due totrial-and-error prompting cycles. Further, the limitations of specificframeworks, such as token limits and long response times, add to thecomplexity. Hype-related issues, such as inflated client expectations and arapidly evolving technological ecosystem, further exacerbate the difficulties.To address these issues, we propose Software Engineering for Generative AI(SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where thesoftware engineering community can provide effective guidance. This support isessential for freelancers working with generative AI and other emergingtechnologies.",
        "title": "Development in times of hype: How freelancers explore Generative AI?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05792",
        "abstract_url": "http://arxiv.org/abs/2401.05792",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhihui"
            },
            {
                "last_name": "Zhao",
                "first_name": "Handong"
            },
            {
                "last_name": "Yu",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Shuai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large pretrained multilingual language models (ML-LMs) have shown remarkablecapabilities of zero-shot cross-lingual transfer, without direct cross-lingualsupervision. While these results are promising, follow-up works found that,within the multilingual embedding spaces, there exists strong language identityinformation which hinders the expression of linguistic factors shared acrosslanguages. For semantic tasks like cross-lingual sentence retrieval, it isdesired to remove such language identity signals to fully leverage semanticinformation. In this work, we provide a novel view of projecting awaylanguage-specific factors from a multilingual embedding space. Specifically, wediscover that there exists a low-rank subspace that primarily encodesinformation irrelevant to semantics (e.g., syntactic information). To identifythis subspace, we present a simple but effective unsupervised method based onsingular value decomposition with multiple monolingual corpora as input. Oncethe subspace is found, we can directly project the original embeddings into thenull space to boost language agnosticism without finetuning. We systematicallyevaluate our method on various tasks including the challenginglanguage-agnostic QA retrieval task. Empirical results show that applying ourmethod consistently leads to improvements over commonly used ML-LMs.",
        "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual  Representations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05794",
        "abstract_url": "http://arxiv.org/abs/2401.05794",
        "authors": [
            {
                "last_name": "Geneson",
                "first_name": "Jesse"
            },
            {
                "last_name": "Tang",
                "first_name": "Linus"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DM",
            ""
        ],
        "abstract": "  We improve several worst-case bounds for various online learning scenariosfrom (Auer and Long, Machine Learning, 1999). In particular, we sharpen anupper bound for delayed ambiguous reinforcement learning by a factor of 2, anupper bound for learning compositions of families of functions by a factor of2.41, and an upper bound for agnostic learning by a factor of 1.09. We alsoimprove a lower bound from the same paper for learning compositions of $k$families of functions by a factor of $\\Theta(\\ln{k})$, matching the upper boundup to a constant factor. In addition, we solve a problem from (Long,Theoretical Computer Science, 2020) on the price of bandit feedback withrespect to standard feedback for multiclass learning, and we improve an upperbound from (Feng et al., Theoretical Computer Science, 2023) on the price of$r$-input delayed ambiguous reinforcement learning by a factor of $r$, matchinga lower bound from the same paper up to the leading term.",
        "title": "Bounds on the price of feedback for mistake-bounded online learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05797",
        "abstract_url": "http://arxiv.org/abs/2401.05797",
        "authors": [
            {
                "last_name": "Deb",
                "first_name": "Soubhik"
            },
            {
                "last_name": "Raynor",
                "first_name": "Robert"
            },
            {
                "last_name": "Kannan",
                "first_name": "Sreeram"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  As of July 15, 2023, Ethererum, which is a Proof-of-Stake (PoS) blockchain[1] has around 410 Billion USD in total assets on chain (popularly referred toas total-value-locked, TVL) but has only 33 Billion USD worth of ETH staked insecuring the underlying consensus of the chain [2]. A preliminary analysismight suggest that as the amount staked is far less (11x less) than the valuesecured, the Ethereum blockchain is insecure and \"over-leveraged\" in a purelycryptoeconomic sense. In this work, we investigate how Ethereum, or, moregenerally, any PoS blockchain can be made secure despite this apparentimbalance. Towards that end, we attempt to formalize a model for analyzing thecryptoeconomic safety of PoS blockchain, which separately analyzes thecost-of-corruption, the cost incurred by an attacker, and theprofit-from-corruption, the profit gained by an attacker. We derive sharperbounds on profit-from-corruption, as well as new confirmation rules thatsignificantly decrease this upper-bound. We evaluate cost-of-corruption andprofit-from-corruption only from the perspective of attacking safety. Finally,we present a new \"insurance\" mechanism, STAKESURE, for allocating the slashedfunds in a PoS system, that has several highly desirable properties: solvingcommon information problem in existing blockchains, creating a mechanism forprovably safe bridging, and providing the first sharp solution forautomatically adjusting how much economic security is sufficient in a PoSsystem. Finally, we show that the system satisfies a notion of strongcryptoeconomic safety, which guarantees that no honest transactor ever losesmoney, and creates a closed system of Karma, which not only ensures that theattacker suffers a loss of funds but also that the harmed parties aresufficiently compensated.",
        "title": "STAKESURE: Proof of Stake Mechanisms with Strong Cryptoeconomic Safety",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05799",
        "abstract_url": "http://arxiv.org/abs/2401.05799",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Frank"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "MA",
            ""
        ],
        "abstract": "  Large language models (LLMs) have drastically changed the possible ways todesign intelligent systems, shifting the focuses from massive data acquisitionand new modeling training to human alignment and strategical elicitation of thefull potential of existing pre-trained models. This paradigm shift, however, isnot fully realized in financial sentiment analysis (FSA), due to thediscriminative nature of this task and a lack of prescriptive knowledge of howto leverage generative models in such a context. This study investigates theeffectiveness of the new paradigm, i.e., using LLMs without fine-tuning forFSA. Rooted in Minsky's theory of mind and emotions, a design framework withheterogeneous LLM agents is proposed. The framework instantiates specializedagents using prior domain knowledge of the types of FSA errors and reasons onthe aggregated agent discussions. Comprehensive evaluation on FSA datasets showthat the framework yields better accuracies, especially when the discussionsare substantial. This study contributes to the design foundations and paves newavenues for LLMs-based FSA. Implications on business and management are alsodiscussed.",
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05800",
        "abstract_url": "http://arxiv.org/abs/2401.05800",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Yu"
            },
            {
                "last_name": "Koh",
                "first_name": "Huan Yee"
            },
            {
                "last_name": "Jin",
                "first_name": "Ming"
            },
            {
                "last_name": "Chi",
                "first_name": "Lianhua"
            },
            {
                "last_name": "Wang",
                "first_name": "Haishuai"
            },
            {
                "last_name": "Phan",
                "first_name": "Khoa T."
            },
            {
                "last_name": "Chen",
                "first_name": "Yi-Ping Phoebe"
            },
            {
                "last_name": "Pan",
                "first_name": "Shirui"
            },
            {
                "last_name": "Xiang",
                "first_name": "Wei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The detection of anomalies in multivariate time series data is crucial forvarious practical applications, including smart power grids, traffic flowforecasting, and industrial process control. However, real-world time seriesdata is usually not well-structured, posting significant challenges to existingapproaches: (1) The existence of missing values in multivariate time seriesdata along variable and time dimensions hinders the effective modeling ofinterwoven spatial and temporal dependencies, resulting in important patternsbeing overlooked during model training; (2) Anomaly scoring withirregularly-sampled observations is less explored, making it difficult to useexisting detectors for multivariate series without fully-observed values. Inthis work, we introduce a novel framework called GST-Pro, which utilizes agraph spatiotemporal process and anomaly scorer to tackle the aforementionedchallenges in detecting anomalies on irregularly-sampled multivariate timeseries. Our approach comprises two main components. First, we propose a graphspatiotemporal process based on neural controlled differential equations. Thisprocess enables effective modeling of multivariate time series from bothspatial and temporal perspectives, even when the data contains missing values.Second, we present a novel distribution-based anomaly scoring mechanism thatalleviates the reliance on complete uniform observations. By analyzing thepredictions of the graph spatiotemporal process, our approach allows anomaliesto be easily detected. Our experimental results show that the GST-Pro methodcan effectively detect anomalies in time series data and outperformsstate-of-the-art methods, regardless of whether there are missing valuespresent in the data. Our code is available: https://github.com/huankoh/GST-Pro.",
        "title": "Graph Spatiotemporal Process for Multivariate Time Series Anomaly  Detection with Missing Values",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05802",
        "abstract_url": "http://arxiv.org/abs/2401.05802",
        "authors": [
            {
                "last_name": "Johal",
                "first_name": "Wafa"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  With advancement of robotics and artificial intelligence, applications forrobotics are flourishing. Human-robot interaction (HRI) is an important area ofrobotics as it allows robots to work closer to humans (with them or for them).One crucial factor for the success of HRI research is transferability, whichrefers to the ability of research outputs to be adopted by industry and providebenefits to society. In this paper, we explore the potentials and challenges oftransferability in HRI research. Firstly, we examine the current state of HRIresearch and identify various types of contributions that could lead tosuccessful outcomes. Secondly, we discuss the potential benefits for each typeof contribution and identify factors that could facilitate industry adoption ofHRI research. However, we also recognize that there are several challengesassociated with transferability, such as the diversity of well-definedjob/skill-sets required from HRI practitioners, the lack of industry-ledresearch, and the lack of standardization in HRI research methods. We discussthese challenges and propose potential solutions to bridge the gap betweenindustry expectations and academic research in HRI.",
        "title": "Transferability of HRI Research: Potential and Challenges",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05806",
        "abstract_url": "http://arxiv.org/abs/2401.05806",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Dong",
                "first_name": "Neng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Liehuang"
            },
            {
                "last_name": "Peng",
                "first_name": "Hao"
            },
            {
                "last_name": "Tao",
                "first_name": "Dapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visible-infrared person re-identification (VIReID) primarily deals withmatching identities across person images from different modalities. Due to themodality gap between visible and infrared images, cross-modality identitymatching poses significant challenges. Recognizing that high-level semantics ofpedestrian appearance, such as gender, shape, and clothing style, remainconsistent across modalities, this paper intends to bridge the modality gap byinfusing visual features with high-level semantics. Given the capability ofCLIP to sense high-level semantic information corresponding to visualrepresentations, we explore the application of CLIP within the domain ofVIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network(CSDN) that consists of Modality-specific Prompt Learner, Semantic InformationIntegration (SII), and High-level Semantic Embedding (HSE). Specifically,considering the diversity stemming from modality discrepancies in languagedescriptions, we devise bimodal learnable text tokens to capturemodality-private semantic information for visible and infrared images,respectively. Additionally, acknowledging the complementary nature of semanticdetails across different modalities, we integrate text features from thebimodal language descriptions to achieve comprehensive semantics. Finally, weestablish a connection between the integrated text features and the visualfeatures across modalities. This process embed rich high-level semanticinformation into visual representations, thereby promoting the modalityinvariance of visual representations. The effectiveness and superiority of ourproposed CSDN over existing methods have been substantiated throughexperimental evaluations on multiple widely used benchmarks. The code will bereleased at \\url{https://github.com/nengdong96/CSDN}.",
        "title": "CLIP-Driven Semantic Discovery Network for Visible-Infrared Person  Re-Identification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05807",
        "abstract_url": "http://arxiv.org/abs/2401.05807",
        "authors": [
            {
                "last_name": "Cobo",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Valle",
                "first_name": "Roberto"
            },
            {
                "last_name": "Buenaposada",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Baumela",
                "first_name": "Luis"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Head pose estimation (HPE) is a problem of interest in computer vision toimprove the performance of face processing tasks in semi-frontal or profilesettings. Recent applications require the analysis of faces in the full360{\\deg} rotation range. Traditional approaches to solve the semi-frontal andprofile cases are not directly amenable for the full rotation case. In thispaper we analyze the methodology for short- and wide-range HPE and discusswhich representations and metrics are adequate for each case. We show that thepopular Euler angles representation is a good choice for short-range HPE, butnot at extreme rotations. However, the Euler angles' gimbal lock problemprevents them from being used as a valid metric in any setting. We also revisitthe current cross-data set evaluation methodology and note that the lack ofalignment between the reference systems of the training and test data setsnegatively biases the results of all articles in the literature. We introduce aprocedure to quantify this misalignment and a new methodology for cross-dataset HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwibenchmark. We also propose a generalization of the geodesic angular distancemetric that enables the construction of a loss that controls the contributionof each training sample to the optimization of the model. Finally, we introducea wide range HPE benchmark based on the CMU Panoptic data set.",
        "title": "On the representation and methodology for wide and short range head pose  estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05808",
        "abstract_url": "http://arxiv.org/abs/2401.05808",
        "authors": [
            {
                "last_name": "Azarbahram",
                "first_name": "Ali"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The paper proposes an intermittent communication mechanism for the trackingconsensus of high-order nonlinear multi-agent systems (MASs) surrounded byrandom disturbances. Each collaborating agent is described by a class ofhigh-order nonlinear uncertain strict-feedback dynamics which is disturbed by awide stationary process representing the external noise. The resiliency levelof this networked control system (NCS) to the failures of physical devices orunreliability of communication channels is analyzed by introducing a linearauxiliary trajectory of the system. More precisely, the unreliability ofcommunication channels sometimes makes an agent incapable of sensing the localinformation or receiving it from neighboring nodes. Therefore, an intermittentcommunication scheme is proposed among the follower agents as a consequence ofemploying the linear auxiliary dynamics. The closed-loop networked systemsignals are proved to be noise-to-state practically stable in probability(NSpS-P). It has been justified that each agent follows the trajectory of thecorresponding local auxiliary virtual system practically in probability. Thesimulation experiments finally quantify the effectiveness of our proposedapproach in terms of providing a resilient performance against unreliability ofcommunication channels and reaching the tracking consensus.",
        "title": "Tracking Consensus of Networked Random Nonlinear Multi-agent Systems  with Intermittent Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05809",
        "abstract_url": "http://arxiv.org/abs/2401.05809",
        "authors": [
            {
                "last_name": "Tomita",
                "first_name": "Yoshihide"
            },
            {
                "last_name": "Koyama",
                "first_name": "Shoichi"
            },
            {
                "last_name": "Saruwatari",
                "first_name": "Hiroshi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  A method for synthesizing the desired sound field while suppressing theexterior radiation power with directional weighting is proposed. The exteriorradiation from the loudspeakers in sound field synthesis systems can beproblematic in practical situations. Although several methods to suppress theexterior radiation have been proposed, suppression in all outward directions isgenerally difficult, especially when the number of loudspeakers is notsufficiently large. We propose the directionally weighted exterior radiationrepresentation to prioritize the suppression directions by incorporating itinto the optimization problem of sound field synthesis. By using the proposedrepresentation, the exterior radiation in the prioritized directions can besignificantly reduced while maintaining high interior synthesis accuracy, owingto the relaxed constraint on the exterior radiation. Its performance isevaluated with the application of the proposed representation to amplitudematching in numerical experiments.",
        "title": "Localizing Acoustic Energy in Sound Field Synthesis by Directionally  Weighted Exterior Radiation Suppression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05811",
        "abstract_url": "http://arxiv.org/abs/2401.05811",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Zhuoyuan"
            },
            {
                "last_name": "Yu",
                "first_name": "Yen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This article introduces contrastive alignment instructions (AlignInstruct) toaddress two challenges in machine translation (MT) on large language models(LLMs). One is the expansion of supported languages to previously unseen ones.The second relates to the lack of data in low-resource languages. Modelfine-tuning through MT instructions (MTInstruct) is a straightforward approachto the first challenge. However, MTInstruct is limited by weak cross-lingualsignals inherent in the second challenge. AlignInstruct emphasizescross-lingual supervision via a cross-lingual discriminator built usingstatistical word alignments. Our results based on fine-tuning the BLOOMZ models(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs caneffectively translate unseen languages using MTInstruct; (2) AlignInstruct ledto consistent improvements in translation quality across 48 translationdirections involving English; (3) Discriminator-based instructions outperformedtheir generative counterparts as cross-lingual instructions; (4) AlignInstructimproved performance in 30 zero-shot directions.",
        "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine  Translation in Unseen, Low-resource Languages",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05815",
        "abstract_url": "http://arxiv.org/abs/2401.05815",
        "authors": [
            {
                "last_name": "Kaiser",
                "first_name": "Jan"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenran"
            },
            {
                "last_name": "Eichler",
                "first_name": "Annika"
            },
            {
                "last_name": "Garcia",
                "first_name": "Andrea Santamaria"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Machine learning has emerged as a powerful solution to the modern challengesin accelerator physics. However, the limited availability of beam time, thecomputational cost of simulations, and the high-dimensionality of optimisationproblems pose significant challenges in generating the required data fortraining state-of-the-art machine learning models. In this work, we introduceCheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.Cheetah enables the fast collection of large data sets by reducing computationtimes by multiple orders of magnitude and facilitates efficient gradient-basedoptimisation for accelerator tuning and system identification. This positionsCheetah as a user-friendly, readily extensible tool that integrates seamlesslywith widely adopted machine learning tools. We showcase the utility of Cheetahthrough five examples, including reinforcement learning training,gradient-based beamline tuning, gradient-based system identification,physics-informed Bayesian optimisation priors, and modular neural networksurrogate modelling of space charge effects. The use of such a high-speeddifferentiable simulation code will simplify the development of machinelearning-based methods for particle accelerators and fast-track theirintegration into everyday operations of accelerator facilities.",
        "title": "Cheetah: Bridging the Gap Between Machine Learning and Particle  Accelerator Physics with High-Speed, Differentiable Simulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05818",
        "abstract_url": "http://arxiv.org/abs/2401.05818",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Raquel"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Alberto"
            },
            {
                "last_name": "Mekler",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Writing and genre conventions are extant to any scientific community, and CHIis no different. In this paper, we present the early phases of an AI tool wecreated called KITSUNE, which supports authors in placing their work into theformat of a CHI paper, taking into account many conventions that areever-present in CHI papers. We describe the development of the tool with theintent to promote discussion around how writing conventions are upheld andunquestioned by the CHI community, and how this translates to the workproduced. In addition, we bring up questions surrounding how the introductionof LLMs into academic writing fundamentally change how conventions will beupheld now and in the future",
        "title": "How to write a CHI paper (asking for a friend)",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05819",
        "abstract_url": "http://arxiv.org/abs/2401.05819",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yuting"
            },
            {
                "last_name": "Chen",
                "first_name": "Fei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Auditory spatial attention detection (ASAD) is used to determine thedirection of a listener's attention to a speaker by analyzing her/hiselectroencephalographic (EEG) signals. This study aimed to further improve theperformance of ASAD with a short decision window (i.e., <1 s) rather than withlong decision windows in previous studies. An end-to-end temporal attentionnetwork (i.e., TAnet) was introduced in this work. TAnet employs a multi-headattention (MHA) mechanism, which can more effectively capture the interactionsamong time steps in collected EEG signals and efficiently assign correspondingweights to those EEG time steps. Experiments demonstrated that, compared withthe CNN-based method and recent ASAD methods, TAnet provided improved decodingperformance in the KUL dataset, with decoding accuracies of 92.4% (decisionwindow 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s)with short decision windows (i.e., <1 s). As a new ASAD model with a shortdecision window, TAnet can potentially facilitate the design of EEG-controlledintelligent hearing aids and sound recognition systems.",
        "title": "TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial  Attention Decoding with a Short Decision Window",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05820",
        "abstract_url": "http://arxiv.org/abs/2401.05820",
        "authors": [
            {
                "last_name": "Emonds",
                "first_name": "Yannick"
            },
            {
                "last_name": "Xi",
                "first_name": "Kai"
            },
            {
                "last_name": "Fr\u00f6ning",
                "first_name": "Holger"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "ET",
            "PF"
        ],
        "abstract": "  Resistive memory is a promising alternative to SRAM, but is also aninherently unstable device that requires substantial effort to ensure correctread and write operations. To avoid the associated costs in terms of area, timeand energy, the present work is concerned with exploring how much noise inmemory operations can be tolerated by image classification tasks based onneural networks. We introduce a special noisy operator that mimics the noise inan exemplary resistive memory unit, explore the resilience of convolutionalneural networks on the CIFAR-10 classification task, and discuss a couple ofcountermeasures to improve this resilience.",
        "title": "Implications of Noise in Resistive Memory on Deep Neural Networks for  Image Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05821",
        "abstract_url": "http://arxiv.org/abs/2401.05821",
        "authors": [
            {
                "last_name": "Delfosse",
                "first_name": "Quentin"
            },
            {
                "last_name": "Sztwiertnia",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Stammer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Rothermel",
                "first_name": "Mark"
            },
            {
                "last_name": "Kersting",
                "first_name": "Kristian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SC"
        ],
        "abstract": "  Reward sparsity, difficult credit assignment, and misalignment are only a fewof the many issues that make it difficult, if not impossible, for deepreinforcement learning (RL) agents to learn optimal policies. Unfortunately,the black-box nature of deep networks impedes the inclusion of domain expertswho could interpret the model and correct wrong behavior. To this end, weintroduce Successive Concept Bottlenecks Agents (SCoBots), which make the wholedecision pipeline transparent via the integration of consecutive conceptbottleneck layers. SCoBots make use of not only relevant object properties butalso of relational concepts. Our experimental results provide strong evidencethat SCoBots allow domain experts to efficiently understand and regularizetheir behavior, resulting in potentially better human-aligned RL. In this way,SCoBots enabled us to identify a misalignment problem in the most simple andiconic video game, Pong, and resolve it.",
        "title": "Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05822",
        "abstract_url": "http://arxiv.org/abs/2401.05822",
        "authors": [
            {
                "last_name": "Free",
                "first_name": "Michael"
            },
            {
                "last_name": "Langworthy",
                "first_name": "Andrew"
            },
            {
                "last_name": "Dimitropoulaki",
                "first_name": "Mary"
            },
            {
                "last_name": "Thompson",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  The objective of this work is to train a chatbot capable of solving evolvingproblems through conversing with a user about a problem the chatbot cannotdirectly observe. The system consists of a virtual problem (in this case asimple game), a simulated user capable of answering natural language questionsthat can observe and perform actions on the problem, and a Deep Q-Network(DQN)-based chatbot architecture. The chatbot is trained with the goal ofsolving the problem through dialogue with the simulated user usingreinforcement learning. The contributions of this paper are as follows: aproposed architecture to apply a conversational DQN-based agent to evolvingproblems, an exploration of training methods such as curriculum learning onmodel performance and the effect of modified reward functions in the case ofincreasing environment complexity.",
        "title": "Towards Goal-Oriented Agents for Evolving Problems Observed via  Conversation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05824",
        "abstract_url": "http://arxiv.org/abs/2401.05824",
        "authors": [
            {
                "last_name": "Phang",
                "first_name": "Kenji"
            },
            {
                "last_name": "Pradhan",
                "first_name": "Siddharth Saarathi"
            },
            {
                "last_name": "Ikwuegbu",
                "first_name": "Chino"
            },
            {
                "last_name": "Ramos",
                "first_name": "Gonzalo"
            },
            {
                "last_name": "Ford",
                "first_name": "Denae"
            },
            {
                "last_name": "Okoli",
                "first_name": "Ebele"
            },
            {
                "last_name": "Chishti",
                "first_name": "Salman Muin Kayser"
            },
            {
                "last_name": "Suh",
                "first_name": "Jina"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Mental health is a pressing concern in today's digital age, particularlyamong youth who are deeply intertwined with technology. Despite the influx oftechnology solutions addressing mental health issues, youth often remainsidelined during the design process. While co-design methods have been employedto improve participation by youth, many such initiatives are limited to designactivities and lack training for youth to research and develop solutions forthemselves. In this case study, we detail our 8-week remote, collaborativeresearch initiative called Youth WellTech, designed to facilitate remoteco-design sprints aimed at equipping youth with the tools and knowledge toenvision and design tech futures for their own communities. We pilot thisinitiative with 12 student technology evangelists across 8 countries globallyto foster the sharing of mental health challenges and diverse perspectives. Wehighlight insights from our experiences running this global program remotely,its structure, and recommendations for co-research.",
        "title": "Youth WellTech: A Global Remote Co-Design Sprint for Youth Mental Health  Technology",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05826",
        "abstract_url": "http://arxiv.org/abs/2401.05826",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Nivedita"
            },
            {
                "last_name": "Do",
                "first_name": "Yejin"
            },
            {
                "last_name": "Fouad",
                "first_name": "Yongsang Yu. Imane"
            },
            {
                "last_name": "Kim",
                "first_name": "Jungrae"
            },
            {
                "last_name": "Kim",
                "first_name": "Hyoungshick"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Despite stringent data protection regulations such as the General DataProtection Regulation (GDPR), the California Consumer Privacy Act (CCPA), andother country-specific regulations, many websites continue to use cookies totrack user activities. Recent studies have revealed several data protectionviolations, resulting in significant penalties, especially for multinationalcorporations. Motivated by the question of why these data protection violationscontinue to occur despite strong data protection regulations, we examined 360popular e-commerce websites in multiple countries to analyze whether theycomply with regulations to protect user privacy from a cookie perspective.",
        "title": "Crumbled Cookie Exploring E-commerce Websites Cookie Policies with Data  Protection Regulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05827",
        "abstract_url": "http://arxiv.org/abs/2401.05827",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jinge"
            },
            {
                "last_name": "Kim",
                "first_name": "Yunsoo"
            },
            {
                "last_name": "Wu",
                "first_name": "Honghan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "CV"
        ],
        "abstract": "  The recent success of large language and vision models on vision questionanswering (VQA), particularly their applications in medicine (Med-VQA), hasshown a great potential of realizing effective visual assistants forhealthcare. However, these models are not extensively tested on thehallucination phenomenon in clinical settings. Here, we created a hallucinationbenchmark of medical images paired with question-answer sets and conducted acomprehensive evaluation of the state-of-the-art models. The study provides anin-depth analysis of current models limitations and reveals the effectivenessof various prompting strategies.",
        "title": "Hallucination Benchmark in Medical Visual Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05831",
        "abstract_url": "http://arxiv.org/abs/2401.05831",
        "authors": [
            {
                "last_name": "Vardakas",
                "first_name": "Georgios"
            },
            {
                "last_name": "Pavlopoulos",
                "first_name": "John"
            },
            {
                "last_name": "Likas",
                "first_name": "Aristidis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Silhouette coefficient is an established internal clustering evaluationmeasure that produces a score per data point, assessing the quality of itsclustering assignment. To assess the quality of the clustering of the wholedataset, the scores of all the points in the dataset are typically averagedinto a single value, a strategy which we call as micro-averaging. As weillustrate in this work, by using a synthetic example, this micro-averagingstrategy is sensitive both to cluster imbalance and outliers (backgroundnoise). To address these issues, we propose an alternative aggregationstrategy, which first averages the silhouette scores at a cluster level andthen (macro) averages the scores across the clusters. Based on the samesynthetic example, we show that the proposed macro-averaged silhouette score isrobust to cluster imbalance and background noise. We have conducted anexperimental study showing that our macro-averaged variant provides betterestimates of the ground truth number of clusters on several cases compared tothe typical micro-averaged score.",
        "title": "Revisiting Silhouette: From Micro to Macro Aggregation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05833",
        "abstract_url": "http://arxiv.org/abs/2401.05833",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Attaining ultra-reliable communication (URC) in fifth-generation (5G) andbeyond networks requires deriving statistics of channel in ultra-reliableregion by modeling the extreme events. Extreme value theory (EVT) has beenpreviously adopted in channel modeling to characterize the lower tail ofreceived powers in URC systems. In this paper, we propose a multivariate EVT(MEVT)-based channel modeling methodology for tail of the joint distribution ofmulti-channel by characterizing the multivariate extremes of multiple-inputmultiple-output (MIMO) system. The proposed approach derives lower tailstatistics of received power of each channel by using the generalized Paretodistribution (GPD). Then, tail of the joint distribution is modeled as afunction of estimated GPD parameters based on two approaches: logisticdistribution, which utilizes logistic distribution to determine dependencyfactors among the Frechet transformed tail sequence and obtain a bi-variateextreme value model, and Poisson point process, which estimates probabilitymeasure function of the Pickands angular component to model bi-variate extremevalues. Finally, validity of the proposed models is assessed by incorporatingthe mean constraint on probability measure function of Pichanks coordinates.Based on the data collected within the engine compartment of Fiat Linea, wedemonstrate the superiority of proposed methodology compared to theconventional extrapolation-based methods in providing the best fit to themultivariate extremes.",
        "title": "Multivariate Extreme Value Theory Based Channel Modeling for  Ultra-Reliable Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05834",
        "abstract_url": "http://arxiv.org/abs/2401.05834",
        "authors": [
            {
                "last_name": "Mari",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Anish"
            },
            {
                "last_name": "Ren",
                "first_name": "Runtian"
            },
            {
                "last_name": "Sankowski",
                "first_name": "Piotr"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Web requests are growing exponentially since the 90s due to the rapiddevelopment of the Internet. This process was further accelerated by theintroduction of cloud services. It has been observed statistically that memoryor web requests generally follow power-law distribution, Breslau et al.INFOCOM'99. That is, the $i^{\\text{th}}$ most popular web page is requestedwith a probability proportional to $1 / i^{\\alpha}$ ($\\alpha > 0$ is aconstant). Furthermore, this study, which was performed more than 20 years ago,indicated Zipf-like behavior, i.e., that $\\alpha \\le 1$. Surprisingly, thememory access traces coming from petabyte-size modern cloud systems not onlyshow that $\\alpha$ can be bigger than one but also illustrate a shiftedpower-law distribution -- called Pareto type II or Lomax. These previously notreported phenomenon calls for statistical explanation.  Our first contribution is a new statistical {\\it multi-core power-law} modelindicating that double-power law can be attributed to the presence of multiplecores running many virtual machines in parallel on such systems. We verifyexperimentally the applicability of this model using the Kolmogorov-Smirnovtest (K-S test).  The second contribution of this paper is a theoretical analysis indicatingwhy LRU and LFU-based algorithms perform well in practice on data satisfyingpower-law or multi-core assumptions. We provide an explanation by studying theonline paging problem in the stochastic input model, i.e., the input is arandom sequence with each request independently drawn from a page set accordingto a distribution $\\pi$. We derive formulas (as a function of the pageprobabilities in $\\pi$) to upper bound their ratio-of-expectations, which helpin establishing O(1) performance ratio given the random sequence followingpower-law and multi-core power-law distributions.",
        "title": "Modeling Online Paging in Multi-Core Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05835",
        "abstract_url": "http://arxiv.org/abs/2401.05835",
        "authors": [
            {
                "last_name": "Hosseinalizadeh",
                "first_name": "Teimour"
            },
            {
                "last_name": "Schl\u00fcter",
                "first_name": "Nils"
            },
            {
                "last_name": "Darup",
                "first_name": "Moritz Schulze"
            },
            {
                "last_name": "Monshizadeh",
                "first_name": "Nima"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Search for the optimizer in computationally demanding model predictivecontrol (MPC) setups can be facilitated by Cloud as a service provider incyber-physical systems. This advantage introduces the risk that Cloud canobtain unauthorized access to the privacy-sensitive parameters of the systemand cost function. To solve this issue, i.e., preventing Cloud from accessingthe parameters while benefiting from Cloud computation, random affinetransformations provide an exact yet light weight in computation solution. Thisresearch deals with analyzing privacy preserving properties of thesetransformations when they are adopted for MPC problems. We consider two commonstrategies for outsourcing the optimization required in MPC problems, namelyseparate and dense forms, and establish that random affine transformationsutilized in these forms are vulnerable to side-knowledge from Cloud.Specifically, we prove that the privacy guarantees of these methods and theirextensions for separate form are undermined when a mild side-knowledge aboutthe problem in terms of structure of MPC cost function is available. Inaddition, while we prove that outsourcing the MPC problem in the dense forminherently leads to some degree of privacy for the system and cost functionparameters, we also establish that affine transformations applied to this formare nevertheless prone to be undermined by a Cloud with mild side-knowledge.Numerical simulations confirm our results.",
        "title": "Privacy Analysis of Affine Transformations in Cloud-based MPC:  Vulnerability to Side-knowledge",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05836",
        "abstract_url": "http://arxiv.org/abs/2401.05836",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Feng"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xveqing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuantai"
            },
            {
                "last_name": "Chen",
                "first_name": "Weijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaohong"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The essential of navigation, perception, and decision-making which are basictasks for intelligent robots, is to estimate necessary system states. Amongthem, navigation is fundamental for other upper applications, providing preciseposition and orientation, by integrating measurements from multiple sensors.With observations of each sensor appropriately modelled, multi-sensor fusiontasks for navigation are reduced to the state estimation problem which can besolved by two approaches: optimization and filtering. Recent research has shownthat optimization-based frameworks outperform filtering-based ones in terms ofaccuracy. However, both methods are based on maximum likelihood estimation(MLE) and should be theoretically equivalent with the same linearizationpoints, observation model, measurements, and Gaussian noise assumption. In thispaper, we deeply dig into the theories and existing strategies utilized in bothoptimization-based and filtering-based approaches. It is demonstrated that thetwo methods are equal theoretically, but this equivalence corrupts due todifferent strategies applied in real-time operation. By adjusting existingstrategies of the filtering-based approaches, the Monte-Carlo simulation andvehicular ablation experiments based on visual odometry (VO) indicate that thestrategy adjusted filtering strictly equals to optimization. Therefore, futureresearch on sensor-fusion problems should concentrate on their own algorithmsand strategies rather than state estimation approaches.",
        "title": "On State Estimation in Multi-Sensor Fusion Navigation: Optimization and  Filtering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05840",
        "abstract_url": "http://arxiv.org/abs/2401.05840",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhuoyan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Yin",
                "first_name": "Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  With the rapid development of AI-based decision aids, different forms of AIassistance have been increasingly integrated into the human decision makingprocesses. To best support humans in decision making, it is essential toquantitatively understand how diverse forms of AI assistance influence humans'decision making behavior. To this end, much of the current research focuses onthe end-to-end prediction of human behavior using ``black-box'' models, oftenlacking interpretations of the nuanced ways in which AI assistance impacts thehuman decision making process. Meanwhile, methods that prioritize theinterpretability of human behavior predictions are often tailored for onespecific form of AI assistance, making adaptations to other forms of assistancedifficult. In this paper, we propose a computational framework that can providean interpretable characterization of the influence of different forms of AIassistance on decision makers in AI-assisted decision making. Byconceptualizing AI assistance as the ``{\\em nudge}'' in human decision makingprocesses, our approach centers around modelling how different forms of AIassistance modify humans' strategy in weighing different information in makingtheir decisions. Evaluations on behavior data collected from real humandecision makers show that the proposed framework outperforms various baselinesin accurately predicting human behavior in AI-assisted decision making. Basedon the proposed framework, we further provide insights into how individualswith different cognitive styles are nudged by AI assistance differently.",
        "title": "Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in  AI-assisted Decision Making",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05841",
        "abstract_url": "http://arxiv.org/abs/2401.05841",
        "authors": [
            {
                "last_name": "Br\u00fcning",
                "first_name": "Frederik"
            },
            {
                "last_name": "Driemel",
                "first_name": "Anne"
            },
            {
                "last_name": "Erg\u00fcr",
                "first_name": "Alperen"
            },
            {
                "last_name": "R\u00f6glin",
                "first_name": "Heiko"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  The DTW Barycenter Averaging (DBA) algorithm is a widely used algorithm forestimating the mean of a given set of point sequences. In this context, themean is defined as a point sequence that minimises the sum of dynamic timewarping distances (DTW). The algorithm is similar to the $k$-means algorithm inthe sense that it alternately repeats two steps: (1) computing an optimalassignment to the points of the current mean, and (2) computing an optimal meanunder the current assignment. The popularity of DBA can be attributed to thefact that it works well in practice, despite any theoretical guarantees to beknown. In our paper, we aim to initiate a theoretical study of the number ofiterations that DBA performs until convergence. We assume the algorithm isgiven $n$ sequences of $m$ points in $\\mathbb{R}^d$ and a parameter $k$ thatspecifies the length of the mean sequence to be computed. We show that, incontrast to its fast running time in practice, the number of iterations can beexponential in $k$ in the worst case - even if the number of input sequences is$n=2$. We complement these findings with experiments on real-world data thatsuggest this worst-case behaviour is likely degenerate. To better understandthe performance of the algorithm on non-degenerate input, we study DBA in themodel of smoothed analysis, upper-bounding the expected number of iterations inthe worst case under random perturbations of the input. Our smoothed upperbound is polynomial in $k$, $n$ and $d$, and for constant $n$, it is alsopolynomial in $m$. For our analysis, we adapt the set of techniques that weredeveloped for analysing $k$-means and observe that this set of techniques isnot sufficient to obtain tight bounds for general $n$.",
        "title": "On the number of iterations of the DBA algorithm",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05842",
        "abstract_url": "http://arxiv.org/abs/2401.05842",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Tao"
            },
            {
                "last_name": "Bao",
                "first_name": "Jialu"
            },
            {
                "last_name": "Hsu",
                "first_name": "Justin"
            },
            {
                "last_name": "Silva",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Zanasi",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  The logic of Dependence and Independence Bunched Implications (DIBI) is alogic to reason about conditional independence (CI); for instance, DIBIformulas can characterise CI in probability distributions and relationaldatabases, using the probabilistic and relational DIBI models, respectively.Despite the similarity of the probabilistic and relational models, a uniform,more abstract account remains unsolved. The laborious case-by-case verificationof the frame conditions required for constructing new models also calls forsuch a treatment. In this paper, we develop an abstract framework forsystematically constructing DIBI models, using category theory as the unifyingmathematical language. In particular, we use string diagrams -- a graphicalpresentation of monoidal categories -- to give a uniform definition of theparallel composition and subkernel relation in DIBI models. Our approach notonly generalises known models, but also yields new models of interest andreduces properties of DIBI models to structures in the underlying categories.Furthermore, our categorical framework enables a logical notion of CI, in termsof the satisfaction of specific DIBI formulas. We compare it with stringdiagrammatic approaches to CI and show that it is an extension of stringdiagrammatic CI under reasonable conditions.",
        "title": "A Categorical Approach to DIBI Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05845",
        "abstract_url": "http://arxiv.org/abs/2401.05845",
        "authors": [
            {
                "last_name": "Konrad",
                "first_name": "Christian"
            },
            {
                "last_name": "O'Sullivan",
                "first_name": "Conor"
            },
            {
                "last_name": "Traistaru",
                "first_name": "Victor"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We consider the Graph Reconstruction problem given only query access to theinput graph via a Maximal Independent Set oracle. In this setting, in eachround, the player submits a query consisting of a subset of vertices to theoracle, and the oracle returns any maximal independent set in the subgraphinduced by the queried vertices. The goal for the player is to learn all theedges of the input graph.  In this paper, we give tight (up to a logarithmic factor) upper and lowerbounds for this problem:  1. We give a randomized query algorithm that uses $O(\\Delta^2 \\log n)$non-adaptive queries and succeeds with high probability to reconstruct an$n$-vertex graph with maximum degree $\\Delta$. Using the probabilistic method,we also show that a non-adaptive deterministic algorithm that executes$O(\\Delta^3 \\log n)$ queries exists.  2. We give two lower bounds that apply to arbitrary adaptive randomizedalgorithms that succeed with probability greater than $\\frac{1}{2}$. We showthat, for such algorithms, $\\Omega(\\Delta^2)$ rounds are necessary in graphs ofmaximum degree $\\Delta$, and that $\\Omega(\\log n)$ rounds are necessary evenwhen the input graph is an $n$-vertex cycle.",
        "title": "Graph Reconstruction via MIS Queries",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05848",
        "abstract_url": "http://arxiv.org/abs/2401.05848",
        "authors": [
            {
                "last_name": "Riebesell",
                "first_name": "Janosh"
            },
            {
                "last_name": "Surta",
                "first_name": "T. Wesley"
            },
            {
                "last_name": "Goodall",
                "first_name": "Rhys"
            },
            {
                "last_name": "Gaultois",
                "first_name": "Michael"
            },
            {
                "last_name": "Lee",
                "first_name": "Alpha A"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  Materials with high-dielectric constant easily polarize under externalelectric fields, allowing them to perform essential functions in many modernelectronic devices. Their practical utility is determined by two conflictingproperties: high dielectric constants tend to occur in materials with narrowband gaps, limiting the operating voltage before dielectric breakdown. Wepresent a high-throughput workflow that combines element substitution, MLpre-screening, ab initio simulation and human expert intuition to efficientlyexplore the vast space of unknown materials for potential dielectrics, leadingto the synthesis and characterization of two novel dielectric materials,CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objectiveoptimization setting with concave Pareto front. While usually considered morechallenging than single-objective optimization, we argue and show preliminaryevidence that the $1/x$-correlation between band gap and permittivity in factmakes the task more amenable to ML methods by allowing separate models for bandgap and permittivity to each operate in regions of good training support whilestill predicting materials of exceptional merit. To our knowledge, this is thefirst instance of successful ML-guided multi-objective materials optimizationachieving experimental synthesis and characterization. CsTaTeO6 is a structuregenerated via element substitution not present in our reference data sources,thus exemplifying successful de-novo materials design. Meanwhile, we report thefirst high-purity synthesis and dielectric characterization of Bi2Zr2O7 with aband gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics ofour multi-objective search.",
        "title": "Pushing the Pareto front of band gap and permittivity: ML-guided search  for dielectric materials",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05849",
        "abstract_url": "http://arxiv.org/abs/2401.05849",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Litian"
            },
            {
                "last_name": "Molhoek",
                "first_name": "Jord"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CL",
            "HC",
            "",
            ""
        ],
        "abstract": "  Humans have good natural intuition to recognize when another person hassomething to say. It would be interesting if an AI can also recognizeintentions to speak. Especially in scenarios when an AI is guiding a groupdiscussion, this can be a useful skill. This work studies the inference ofsuccessful and unsuccessful intentions to speak from accelerometer data. Thisis chosen because it is privacy-preserving and feasible for in-the-wildsettings since it can be placed in a smart badge. Data from a real-life socialnetworking event is used to train a machine-learning model that aims to inferintentions to speak. A subset of unsuccessful intention-to-speak cases in thedata is annotated. The model is trained on the successful intentions to speakand evaluated on both the successful and unsuccessful cases. In conclusion,there is useful information in accelerometer data, but not enough to reliablycapture intentions to speak. For example, posture shifts are correlated withintentions to speak, but people also often shift posture without having anintention to speak, or have an intention to speak without shifting theirposture. More modalities are likely needed to reliably infer intentions tospeak.",
        "title": "Inferring Intentions to Speak Using Accelerometer Data In-the-Wild",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05850",
        "abstract_url": "http://arxiv.org/abs/2401.05850",
        "authors": [
            {
                "last_name": "Guan",
                "first_name": "Yadong"
            },
            {
                "last_name": "Han",
                "first_name": "Jiqing"
            },
            {
                "last_name": "Song",
                "first_name": "Hongwei"
            },
            {
                "last_name": "Song",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Zheng",
                "first_name": "Guibin"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieran"
            },
            {
                "last_name": "He",
                "first_name": "Yongjun"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  Overlapping sound events are ubiquitous in real-world environments, butexisting end-to-end sound event detection (SED) methods still struggle todetect them effectively. A critical reason is that these methods representoverlapping events using shared and entangled frame-wise features, whichdegrades the feature discrimination. To solve the problem, we propose adisentangled feature learning framework to learn a category-specificrepresentation. Specifically, we employ different projectors to learn theframe-wise features for each category. To ensure that these feature does notcontain information of other categories, we maximize the common informationbetween frame-wise features within the same category and propose a frame-wisecontrastive loss. In addition, considering that the labeled data used by theproposed method is limited, we propose a semi-supervised frame-wise contrastiveloss that can leverage large amounts of unlabeled data to achieve featuredisentanglement. The experimental results demonstrate the effectiveness of ourmethod.",
        "title": "Contrastive Loss Based Frame-wise Feature disentanglement for Polyphonic  Sound Event Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05856",
        "abstract_url": "http://arxiv.org/abs/2401.05856",
        "authors": [
            {
                "last_name": "Barnett",
                "first_name": "Scott"
            },
            {
                "last_name": "Kurniawan",
                "first_name": "Stefanus"
            },
            {
                "last_name": "Thudumu",
                "first_name": "Srikanth"
            },
            {
                "last_name": "Brannelly",
                "first_name": "Zach"
            },
            {
                "last_name": "Abdelrazek",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software engineers are increasingly adding semantic search capabilities toapplications using a strategy known as Retrieval Augmented Generation (RAG). ARAG system involves finding documents that semantically match a query and thenpassing the documents to a large language model (LLM) such as ChatGPT toextract the right answer using an LLM. RAG systems aim to: a) reduce theproblem of hallucinated responses from LLMs, b) link sources/references togenerated responses, and c) remove the need for annotating documents withmeta-data. However, RAG systems suffer from limitations inherent to informationretrieval systems and from reliance on LLMs. In this paper, we present anexperience report on the failure points of RAG systems from three case studiesfrom separate domains: research, education, and biomedical. We share thelessons learned and present 7 failure points to consider when designing a RAGsystem. The two key takeaways arising from our work are: 1) validation of a RAGsystem is only feasible during operation, and 2) the robustness of a RAG systemevolves rather than designed in at the start. We conclude with a list ofpotential research directions on RAG systems for the software engineeringcommunity.",
        "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation  System",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05857",
        "abstract_url": "http://arxiv.org/abs/2401.05857",
        "authors": [
            {
                "last_name": "Azarbahram",
                "first_name": "Ali"
            },
            {
                "last_name": "Amini",
                "first_name": "Amir"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This article proposes a secure implementation for consensus using a dynamicevent-triggered (DET) communication scheme in high-order nonlinear multi-agentsystems (MAS) under asynchronous (distributed) denial of service (DoS) attacks.By introducing a linear auxiliary trajectory of the system, the DET datatransmission scheme among the neighboring agents is employed to reduce thecommunication for each agent. The asynchronous DoS attacks can block eachcommunication channel among the cooperative agents independently in an unknownpattern. To guarantee state consensus of auxiliary MAS under DoS, a linearmatrix inequality (LMI) based optimization approach is proposed whichsimultaneously designs all the unknown DET communication parameters as well asthe state feedback control gain. In addition to asynchronous DoS attacks overthe graph topology, the destructive effects of independent DoS attacks over thecommunication links between actual and auxiliary states are compensated as anadditional layer of resiliency for the system. The output of each agentultimately tracks the auxiliary state of the system and this results in theoutput consensus.",
        "title": "Secure Dynamic Event-triggered Consensus Under Asynchronous Denial of  Service",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05859",
        "abstract_url": "http://arxiv.org/abs/2401.05859",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Wentu"
            },
            {
                "last_name": "Cai",
                "first_name": "Kui"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, for any fixed integer $q>2$, we construct $q$-ary codescorrecting a burst of at most $t$ deletions with redundancy $\\log n+8\\log\\logn+o(\\log\\log n)+\\gamma_{q,t}$ bits and near-linear encoding/decodingcomplexity, where $n$ is the message length and $\\gamma_{q,t}$ is a constantthat only depends on $q$ and $t$. In previous works there are constructions ofsuch codes with redundancy $\\log n+O(\\log q\\log\\log n)$ bits or $\\logn+O(t^2\\log\\log n)+O(t\\log q)$. The redundancy of our new construction isindependent of $q$ and $t$ in the second term.",
        "title": "New Construction of $q$-ary Codes Correcting a Burst of at most $t$  Deletions",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05860",
        "abstract_url": "http://arxiv.org/abs/2401.05860",
        "authors": [
            {
                "last_name": "Phan",
                "first_name": "Thomy"
            },
            {
                "last_name": "Driscoll",
                "first_name": "Joseph"
            },
            {
                "last_name": "Romberg",
                "first_name": "Justin"
            },
            {
                "last_name": "Koenig",
                "first_name": "Sven"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  A wide range of real-world applications can be formulated as Multi-Agent PathFinding (MAPF) problem, where the goal is to find collision-free paths formultiple agents with individual start and goal locations. State-of-the-art MAPFsolvers are mainly centralized and depend on global information, which limitstheir scalability and flexibility regarding changes or new maps that wouldrequire expensive replanning. Multi-agent reinforcement learning (MARL) offersan alternative way by learning decentralized policies that can generalize overa variety of maps. While there exist some prior works that attempt to connectboth areas, the proposed techniques are heavily engineered and very complex dueto the integration of many mechanisms that limit generality and are expensiveto use. We argue that much simpler and general approaches are needed to bringthe areas of MARL and MAPF closer together with significantly lower costs. Inthis paper, we propose Confidence-based Auto-Curriculum for Team UpdateStability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines asimple reverse curriculum scheme, where the goal of each agent is randomlyplaced within an allocation radius around the agent's start location. Theallocation radius increases gradually as all agents improve, which is assessedby a confidence-based measure. We evaluate CACTUS in various maps of differentsizes, obstacle densities, and numbers of agents. Our experiments demonstratebetter performance and generalization capabilities than state-of-the-art MARLapproaches with less than 600,000 trainable parameters, which is less than 5%of the neural network size of current MARL approaches to MAPF.",
        "title": "Confidence-Based Curriculum Learning for Multi-Agent Path Finding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05861",
        "abstract_url": "http://arxiv.org/abs/2401.05861",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Pengzhi"
            },
            {
                "last_name": "He",
                "first_name": "Zhongjun"
            },
            {
                "last_name": "Wu",
                "first_name": "Hua"
            },
            {
                "last_name": "Wang",
                "first_name": "Haifeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The training paradigm for machine translation has gradually shifted, fromlearning neural machine translation (NMT) models with extensive parallelcorpora to instruction finetuning on pretrained multilingual large languagemodels (LLMs) with high-quality translation pairs. In this paper, we focus onboosting the many-to-many multilingual translation performance of LLMs with anemphasis on zero-shot translation directions. We demonstrate that promptstrategies adopted during instruction finetuning are crucial to zero-shottranslation performance and introduce a cross-lingual consistencyregularization, XConST, to bridge the representation gap among differentlanguages and improve zero-shot translation performance. XConST is not a newmethod, but a version of CrossConST (Gao et al., 2023a) adapted formultilingual finetuning on LLMs with translation instructions. Experimentalresults on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show thatour approach consistently improves translation performance. Our implementationsare available at https://github.com/gpengzhi/CrossConST-LLM.",
        "title": "Towards Boosting Many-to-Many Multilingual Machine Translation with  Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05868",
        "abstract_url": "http://arxiv.org/abs/2401.05868",
        "authors": [
            {
                "last_name": "Ham",
                "first_name": "David A."
            },
            {
                "last_name": "Hapla",
                "first_name": "Vaclav"
            },
            {
                "last_name": "Knepley",
                "first_name": "Matthew G."
            },
            {
                "last_name": "Mitchell",
                "first_name": "Lawrence"
            },
            {
                "last_name": "Sagiyama",
                "first_name": "Koki"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "MS"
        ],
        "abstract": "  In this work, we introduce a new algorithm for N-to-M checkpointing in finiteelement simulations. This new algorithm allows efficient saving/loading offunctions representing physical quantities associated with the meshrepresenting the physical domain. Specifically, the algorithm allows for usingdifferent numbers of parallel processes for saving and loading, allowing forrestarting and post-processing on the process count appropriate to the givenphase of the simulation and other conditions. For demonstration, we implementedthis algorithm in PETSc, the Portable, Extensible Toolkit for ScientificComputation, and added a convenient high-level interface into Firedrake, asystem for solving partial differential equations using finite element methods.We evaluated our new implementation by saving and loading data involving 8.2billion finite element degrees of freedom using 8,192 parallel processes onARCHER2, the UK National Supercomputing Service.",
        "title": "Efficient N-to-M Checkpointing Algorithm for Finite Element Simulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05870",
        "abstract_url": "http://arxiv.org/abs/2401.05870",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Hanzhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Yang",
                "first_name": "Jinze"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhongrui"
            },
            {
                "last_name": "Xie",
                "first_name": "Zeke"
            },
            {
                "last_name": "Tian",
                "first_name": "Lei"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            },
            {
                "last_name": "Sun",
                "first_name": "Mingming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The goal of Arbitrary Style Transfer (AST) is injecting the artistic featuresof a style reference into a given image/video. Existing methods usually focuson pursuing the balance between style and content, whereas ignoring thesignificant demand for flexible and customized stylization results and therebylimiting their practical application. To address this critical issue, a novelAST approach namely HiCAST is proposed, which is capable of explicitlycustomizing the stylization results according to various source of semanticclues. In the specific, our model is constructed based on Latent DiffusionModel (LDM) and elaborately designed to absorb content and style instance asconditions of LDM. It is characterized by introducing of \\textit{StyleAdapter}, which allows user to flexibly manipulate the output results byaligning multi-level style information and intrinsic knowledge in LDM. Lastly,we further extend our model to perform video AST. A novel learning objective isleveraged for video diffusion model training, which significantly improvecross-frame temporal consistency in the premise of maintaining stylizationstrength. Qualitative and quantitative comparisons as well as comprehensiveuser studies demonstrate that our HiCAST outperforms the existing SoTA methodsin generating visually plausible stylization results.",
        "title": "HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced  Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05871",
        "abstract_url": "http://arxiv.org/abs/2401.05871",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Yahui"
            },
            {
                "last_name": "Song",
                "first_name": "Haiyue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Personality recognition is useful for enhancing robots' ability to tailoruser-adaptive responses, thus fostering rich human-robot interactions. One ofthe challenges in this task is a limited number of speakers in existingdialogue corpora, which hampers the development of robust, speaker-independentpersonality recognition models. Additionally, accurately modeling both theinterdependencies among interlocutors and the intra-dependencies within thespeaker in dialogues remains a significant issue. To address the firstchallenge, we introduce personality trait interpolation for speaker dataaugmentation. For the second, we propose heterogeneous conversational graphnetworks to independently capture both contextual influences and inherentpersonality traits. Evaluations on the RealPersonaChat corpus demonstrate ourmethod's significant improvements over existing baselines.",
        "title": "Enhancing Personality Recognition in Dialogue by Data Augmentation and  Heterogeneous Conversational Graph Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05872",
        "abstract_url": "http://arxiv.org/abs/2401.05872",
        "authors": [
            {
                "last_name": "Goncharov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Santamaria",
                "first_name": "Alessio"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Lutz"
            },
            {
                "last_name": "Tsampas",
                "first_name": "Stelios"
            },
            {
                "last_name": "Urbat",
                "first_name": "Henning"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL"
        ],
        "abstract": "  We present a systematic approach to logical predicates based on universalcoalgebra and higher-order abstract GSOS, thus making a first step towards aunifying theory of logical relations. We first observe that logical predicatesare special cases of coalgebraic invariants on mixed-variance functors. We thenintroduce the notion of a locally maximal logical refinement of a givenpredicate, with a view to enabling inductive reasoning, and identify sufficientconditions on the overall setup in which locally maximal logical refinementscanonically exist. Finally, we develop induction-up-to techniques that simplifyinductive proofs via logical predicates on systems encoded as (certain classesof) higher-order GSOS laws by identifying and abstracting away from theirboiler-plate part.",
        "title": "Logical Predicates in Higher-Order Mathematical Operational Semantics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05876",
        "abstract_url": "http://arxiv.org/abs/2401.05876",
        "authors": [
            {
                "last_name": "Baumann",
                "first_name": "Dominik"
            },
            {
                "last_name": "Sch\u00f6n",
                "first_name": "Thomas B."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  When deploying machine learning algorithms in the real world, guaranteeingsafety is an essential asset. Existing safe learning approaches typicallyconsider continuous variables, i.e., regression tasks. However, in practice,robotic systems are also subject to discrete, external environmental changes,e.g., having to carry objects of certain weights or operating on frozen, wet,or dry surfaces. Such influences can be modeled as discrete context variables.In the existing literature, such contexts are, if considered, mostly assumed tobe known. In this work, we drop this assumption and show how we can performsafe learning when we cannot directly measure the context variables. To achievethis, we derive frequentist guarantees for multi-class classification, allowingus to estimate the current context from measurements. Further, we propose anapproach for identifying contexts through experiments. We discuss under whichconditions we can retain theoretical guarantees and demonstrate theapplicability of our algorithm on a Furuta pendulum with camera measurements ofdifferent weights that serve as contexts.",
        "title": "Safe reinforcement learning in uncertain contexts",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05879",
        "abstract_url": "http://arxiv.org/abs/2401.05879",
        "authors": [
            {
                "last_name": "Jing",
                "first_name": "Yu"
            },
            {
                "last_name": "Yujuan",
                "first_name": "Tan"
            },
            {
                "last_name": "Ao",
                "first_name": "Ren"
            },
            {
                "last_name": "Duo",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Occlusions pose a significant challenge to optical flow algorithms that evenrely on global evidences. We consider an occluded point to be one that isimaged in the reference frame but not in the next. Estimating the motion ofthese points is extremely difficult, particularly in the two-frame setting.Previous work only used the current frame as the only input, which could notguarantee providing correct global reference information for occluded points,and had problems such as long calculation time and poor accuracy in predictingoptical flow at occluded points. To enable both high accuracy and efficiency,We fully mine and utilize the spatiotemporal information provided by the framepair, design a loopback judgment algorithm to ensure that correct globalreference information is obtained, mine multiple necessary global information,and design an efficient refinement module that fuses these global information.Specifically, we propose a YOIO framework, which consists of three maincomponents: an initial flow estimator, a multiple global information extractionmodule, and a unified refinement module. We demonstrate that optical flowestimates in the occluded regions can be significantly improved in only oneiteration without damaging the performance in non-occluded regions. Comparedwith GMA, the optical flow prediction accuracy of this method in the occludedarea is improved by more than 10%, and the occ_out area exceeds 15%, while thecalculation time is 27% shorter. This approach, running up to 18.9fps with436*1024 image resolution, obtains new state-of-the-art results on thechallenging Sintel dataset among all published and unpublished approaches thatcan run in real-time, suggesting a new paradigm for accurate and efficientoptical flow estimation.",
        "title": "YOIO: You Only Iterate Once by mining and fusing multiple necessary  global information in the optical flow estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05881",
        "abstract_url": "http://arxiv.org/abs/2401.05881",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Chendong"
            },
            {
                "last_name": "Yang",
                "first_name": "Dapeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Dai",
                "first_name": "Yiming"
            },
            {
                "last_name": "Jiang",
                "first_name": "Li"
            },
            {
                "last_name": "Liu",
                "first_name": "Hong"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The fabric-based pneumatic exosuit is now a hot research topic because it islighter and softer than traditional exoskeletons. Existing research focusedmore on the mechanical properties of the exosuit (e.g., torque and speed), butless on its wearability (e.g., appearance and comfort). This work presents anew design concept for fabric-based pneumatic exosuits Volume Transfer, whichmeans transferring the volume of pneumatic actuators beyond the garmentsprofile to the inside. This allows for a concealed appearance and a largerstress area while maintaining adequate torques. In order to verify thisconcept, we develop a fabric-based pneumatic exosuit for knee extensionassistance. Its profile is only 26mm and its stress area wraps around almosthalf of the leg. We use a mathematical model and simulation to determine theparameters of the exosuit, avoiding multiple iterations of the prototype.Experiment results show that the exosuit can generate a torque of 7.6Nm at apressure of 90kPa and produce a significant reduction in the electromyographyactivity of the knee extensor muscles. We believe that Volume Transfer could beutilized prevalently in future fabric-based pneumatic exosuit designs toachieve a significant improvement in wearability.",
        "title": "Volume Transfer: A New Design Concept for Fabric-Based Pneumatic  Exosuits",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05882",
        "abstract_url": "http://arxiv.org/abs/2401.05882",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Ultra-reliable low latency communication (URLLC) requires the packet errorrate to be on the order of $10^{-9}$-$10^{-5}$. Determining the appropriatetransmission rate to satisfy this ultra-reliability constraint requiresderiving the statistics of the channel in the ultra-reliable region and thenincorporating these statistics into the rate selection. In this paper, wepropose a framework for determining the rate selection for ultra-reliablecommunications based on the extreme value theory (EVT). We first model thewireless channel at URLLC by estimating the parameters of the generalizedPareto distribution (GPD) best fitting to the tail distribution of the receivedpowers, i.e., the power values below a certain threshold. Then, we determinethe maximum transmission rate by incorporating the Pareto distribution into therate selection function. Finally, we validate the selected rate by computingthe resulting error probability. Based on the data collected within the enginecompartment of Fiat Linea, we demonstrate the superior performance of theproposed methodology in determining the maximum transmission rate compared tothe traditional extrapolation-based approaches.",
        "title": "Extreme Value Theory Based Rate Selection for Ultra-Reliable  Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05883",
        "abstract_url": "http://arxiv.org/abs/2401.05883",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xianming"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Social media data is plagued by the redundancy problem caused by its noisynature, leading to increased training time and model bias. To address thisissue, we propose a novel approach called generative duplication. It aims toremove duplicate text from noisy social media data and mitigate model bias. Bydoing so, it can improve social media language understanding performance andsave training time. Extensive experiments demonstrate that the proposedgenerative deduplication can effectively reduce training samples whileimproving performance. This evidence suggests the effectiveness of generativededuplication and its importance in social media language understanding.",
        "title": "Generative Deduplication For Socia Media Data Selection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05888",
        "abstract_url": "http://arxiv.org/abs/2401.05888",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Proper determination of the transmission rate in ultra-reliable low latencycommunication (URLLC) needs to incorporate a confidence interval (CI) for theestimated parameters due to the large amount of data required for theiraccurate estimation. In this paper, we propose a framework based on the extremevalue theory (EVT) for determining the transmission rate along with itscorresponding CI for an ultra-reliable communication system. This frameworkconsists of characterizing the statistics of extreme events by fitting thegeneralized Pareto distribution (GPD) to the channel tail, deriving the GPDparameters and their associated CIs, and obtaining the transmission rate withina confidence interval. Based on the data collected within the enginecompartment of Fiat Linea, we demonstrate the accuracy of the estimated rateobtained through the EVT-based framework considering the confidence intervalfor the GPD parameters. Additionally, we show that proper estimation of thetransmission rate based on the proposed framework requires a lower number ofsamples compared to the traditional extrapolation-based approaches.",
        "title": "Incorporation of Confidence Interval into Rate Selection Based on the  Extreme Value Theory for Ultra-Reliable Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05891",
        "abstract_url": "http://arxiv.org/abs/2401.05891",
        "authors": [
            {
                "last_name": "Ciobotari",
                "first_name": "Ion"
            },
            {
                "last_name": "Pr\u00edncipe",
                "first_name": "Adriana"
            },
            {
                "last_name": "Oliveira",
                "first_name": "Maria Alexandra"
            },
            {
                "last_name": "Silva",
                "first_name": "Jo\u00e3o Nuno"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The collection of ecological data in the field is essential to diagnose,monitor and manage ecosystems in a sustainable way. Since acquisition of thisinformation through traditional methods are generally time-consuming, due tothe capability of recording large volumes of data in short time periods,automation of data acquisition sees a growing trend. Terrestrial laser scanners(TLS), particularly LiDAR sensors, have been used in ecology, allowing toreconstruct the 3D structure of vegetation, and thus, infer ecosystemcharacteristics based on the spatial variation of the density of points.However, the low amount of information obtained per beam, lack of data analysistools and the high cost of the equipment limit their use. This way, a low-costTLS (<10k$) was developed along with data acquisition and processing mechanismsapplicable in two case studies: an urban garden and a target area forecological restoration. The orientation of LiDAR was modified to makeobservations in the vertical plane and a motor was integrated for its rotation,enabling the acquisition of 360 degree data with high resolution. Motion andlocation sensors were also integrated for automatic error correction andgeoreferencing. From the data generated, histograms of point density variationalong the vegetation height were created, where shrub stratum was easilydistinguishable from tree stratum, and maximum tree height and shrub cover werecalculated. These results agreed with the field data, whereby the developed TLShas proved to be effective in calculating metrics of structural complexity ofvegetation.",
        "title": "LiDAR data acquisition and processing for ecology applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05892",
        "abstract_url": "http://arxiv.org/abs/2401.05892",
        "authors": [
            {
                "last_name": "Goetze",
                "first_name": "Miriam"
            },
            {
                "last_name": "Jungeblut",
                "first_name": "Paul"
            },
            {
                "last_name": "Ueckerdt",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            ""
        ],
        "abstract": "  We study the recognition complexity of subgraphs of 2- and 3-connected planarcubic graphs. Recently, we presented [ESA 2022] a quadratic-time algorithm torecognize subgraphs of planar cubic bridgeless (but not necessarily connected)graphs, both in the variable and fixed embedding setting (the latter only for2-connected inputs). Here, we extend our results in two directions: First, wepresent a quartic-time algorithm to recognize subgraphs of 2-connected planarcubic graphs in the fixed embedding setting, even for disconnected inputs.Second, we prove NP-hardness of recognizing subgraphs of 3-connected planarcubic graphs in the variable embedding setting.",
        "title": "Recognition Complexity of Subgraphs of 2- and 3-Connected Planar Cubic  Graphs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05894",
        "abstract_url": "http://arxiv.org/abs/2401.05894",
        "authors": [
            {
                "last_name": "Banaei",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Ebrahimy",
                "first_name": "Razgar"
            },
            {
                "last_name": "Madsen",
                "first_name": "Henrik"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper, a computationally lightweight algorithm is introduced forhybrid PV/Battery/Load systems that is price responsive, responds fast, doesnot require powerful hardware, and considers the operational limitations of thesystem. The method is applied to two buildings equipped with PV and battery.Simulation results show that the method can give results that are up to 3.9%more expensive than the Model predictive control (MPC) approach while theruntime of the program is up to 1000 times less than the MPC. Also, while theruntime of the proposed method is in the range of the self-consumptionmaximization (SCM) approach as the fastest method, its electricity cost isabout 3.2% cheaper than the SCM method. Simulation results also show that incase of providing grid services by the battery the difference betweenelectricity cost of the proposed approach and MPC can reduce which makes themethod good for such applications.",
        "title": "A Lightweight Energy Management Method for Hybrid PV/Battery/Load  Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05895",
        "abstract_url": "http://arxiv.org/abs/2401.05895",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Tianxiu"
            },
            {
                "last_name": "Gai",
                "first_name": "Keke"
            },
            {
                "last_name": "Yu",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Liehuang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CR",
            "DC"
        ],
        "abstract": "  Distributed machine learning enables parallel training of extensive datasetsby delegating computing tasks across multiple workers. Despite the costreduction benefits of distributed machine learning, the dissemination of finalmodel weights often leads to potential conflicts over model ownership asworkers struggle to substantiate their involvement in the training computation.To address the above ownership issues and prevent accidental failures andmalicious attacks, verifying the computational integrity and effectiveness ofworkers becomes particularly crucial in distributed machine learning. In thispaper, we proposed a novel binary linear tree commitment-based ownershipprotection model to ensure computational integrity with limited overhead andconcise proof. Due to the frequent updates of parameters during training, ourcommitment scheme introduces a maintainable tree structure to reduce the costsof updating proofs. Distinguished from SNARK-based verifiable computation, ourmodel achieves efficient proof aggregation by leveraging inner productarguments. Furthermore, proofs of model weights are watermarked by workeridentity keys to prevent commitments from being forged or duplicated. Theperformance analysis and comparison with SNARK-based hash commitments validatethe efficacy of our model in preserving computational integrity withindistributed machine learning.",
        "title": "Binary Linear Tree Commitment-based Ownership Protection for Distributed  Machine Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05896",
        "abstract_url": "http://arxiv.org/abs/2401.05896",
        "authors": [
            {
                "last_name": "Abdi",
                "first_name": "Nima"
            },
            {
                "last_name": "Albaseer",
                "first_name": "Abdullatif"
            },
            {
                "last_name": "Abdallah",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            "NI"
        ],
        "abstract": "  As smart grids (SG) increasingly rely on advanced technologies like sensorsand communication systems for efficient energy generation, distribution, andconsumption, they become enticing targets for sophisticated cyberattacks. Theseevolving threats demand robust security measures to maintain the stability andresilience of modern energy systems. While extensive research has beenconducted, a comprehensive exploration of proactive cyber defense strategiesutilizing Deep Learning (DL) in {SG} remains scarce in the literature. Thissurvey bridges this gap, studying the latest DL techniques for proactive cyberdefense. The survey begins with an overview of related works and our distinctcontributions, followed by an examination of SG infrastructure. Next, weclassify various cyber defense techniques into reactive and proactivecategories. A significant focus is placed on DL-enabled proactive defenses,where we provide a comprehensive taxonomy of DL approaches, highlighting theirroles and relevance in the proactive security of SG. Subsequently, we analyzethe most significant DL-based methods currently in use. Further, we exploreMoving Target Defense, a proactive defense strategy, and its interactions withDL methodologies. We then provide an overview of benchmark datasets used inthis domain to substantiate the discourse.{ This is followed by a criticaldiscussion on their practical implications and broader impact on cybersecurityin Smart Grids.} The survey finally lists the challenges associated withdeploying DL-based security systems within SG, followed by an outlook on futuredevelopments in this key field.",
        "title": "The Role of Deep Learning in Advancing Proactive Cybersecurity Measures  for Smart Grid Networks: A Survey",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05897",
        "abstract_url": "http://arxiv.org/abs/2401.05897",
        "authors": [
            {
                "last_name": "Bartels",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Tscherner",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  It is shown that discretizations based on variational or weak formulations ofthe plate bending problem with simple support boundary conditions do not leadto failure of convergence when polygonal domain approximations are used and theimposed boundary conditions are compatible with the nodal interpolation of therestriction of certain regular functions to approximating domains. It isfurther shown that this is optimal in the sense that a full realization of theboundary conditions leads to failure of convergence for conforming methods. Theabstract conditions imply that standard nonconforming and discontinuousGalerkin methods converge correctly while conforming methods require a suitablerelaxation of the boundary condition. The results are confirmed by numericalexperiments.",
        "title": "Necessary and Sufficient Conditions for Avoiding Babuska's Paradox on  Simplicial Meshes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05898",
        "abstract_url": "http://arxiv.org/abs/2401.05898",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Weihang"
            },
            {
                "last_name": "Shikh-Bahaei",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  In this work, we propose a novel partial compress-and-forward (PCF) schemefor improving the maximum achievable transmission rate of a diamond relaynetwork with two noisy relays. PCF combines conventional compress-and-forward(CF) and amplify-and-forward (AF) protocols, enabling one relay to operatealternately in the CF or the AF mode, while the other relay works purely in theCF mode. As the direct link from the source to the destination is unavailable,and there is no noiseless relay in the diamond network, messages received fromboth relays must act as side information for each other and must be decodedjointly. We propose a joint decoder to decode two Luby transform (LT) codesreceived from both relays corresponding to the same original message. Numericalresults show that PCF can achieve significant performance improvements comparedto decode-and-forward (DF) and pure CF protocols when at least the channelsconnected to one of the relays are of high quality.",
        "title": "A Partial Compress-and-Forward Strategy for Relay-assisted Wireless  Networks Based on Rateless Coding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05899",
        "abstract_url": "http://arxiv.org/abs/2401.05899",
        "authors": [
            {
                "last_name": "Zhai",
                "first_name": "Yuanzhao"
            },
            {
                "last_name": "Li",
                "first_name": "Yiying"
            },
            {
                "last_name": "Gao",
                "first_name": "Zijian"
            },
            {
                "last_name": "Gong",
                "first_name": "Xudong"
            },
            {
                "last_name": "Xu",
                "first_name": "Kele"
            },
            {
                "last_name": "Feng",
                "first_name": "Dawei"
            },
            {
                "last_name": "Bo",
                "first_name": "Ding"
            },
            {
                "last_name": "Wang",
                "first_name": "Huaimin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Model-based offline reinforcement learning (RL) has made remarkable progress,offering a promising avenue for improving generalization with synthetic modelrollouts. Existing works primarily focus on incorporating pessimism for policyoptimization, usually via constructing a Pessimistic Markov Decision Process(P-MDP). However, the P-MDP discourages the policies from learning inout-of-distribution (OOD) regions beyond the support of offline datasets, whichcan under-utilize the generalization ability of dynamics models. In contrast,we propose constructing an Optimistic MDP (O-MDP). We initially observed thepotential benefits of optimism brought by encouraging more OOD rollouts.Motivated by this observation, we present ORPO, a simple yet effectivemodel-based offline RL framework. ORPO generates Optimistic model Rollouts forPessimistic offline policy Optimization. Specifically, we train an optimisticrollout policy in the O-MDP to sample more OOD model rollouts. Then we relabelthe sampled state-action pairs with penalized rewards and optimize the outputpolicy in the P-MDP. Theoretically, we demonstrate that the performance ofpolicies trained with ORPO can be lower-bounded in linear MDPs. Experimentalresults show that our framework significantly outperforms P-MDP baselines by amargin of 30%, achieving state-of-the-art performance on the widely-usedbenchmark. Moreover, ORPO exhibits notable advantages in problems that requiregeneralization.",
        "title": "Optimistic Model Rollouts for Pessimistic Offline Policy Optimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05901",
        "abstract_url": "http://arxiv.org/abs/2401.05901",
        "authors": [
            {
                "last_name": "Rivas-Villar",
                "first_name": "David"
            },
            {
                "last_name": "Hervella",
                "first_name": "\u00c1lvaro S."
            },
            {
                "last_name": "Rouco",
                "first_name": "Jos\u00e9"
            },
            {
                "last_name": "Novo",
                "first_name": "Jorge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Retinal image registration is of utmost importance due to its wideapplications in medical practice. In this context, we propose ConKeD, a noveldeep learning approach to learn descriptors for retinal image registration. Incontrast to current registration methods, our approach employs a novelmulti-positive multi-negative contrastive learning strategy that enables theutilization of additional information from the available training samples. Thismakes it possible to learn high quality descriptors from limited training data.To train and evaluate ConKeD, we combine these descriptors with domain-specifickeypoints, particularly blood vessel bifurcations and crossovers, that aredetected using a deep neural network. Our experimental results demonstrate thebenefits of the novel multi-positive multi-negative strategy, as it outperformsthe widely used triplet loss technique (single-positive and single-negative) aswell as the single-positive multi-negative alternative. Additionally, thecombination of ConKeD with the domain-specific keypoints produces comparableresults to the state-of-the-art methods for retinal image registration, whileoffering important advantages such as avoiding pre-processing, utilizing fewertraining samples, and requiring fewer detected keypoints, among others.Therefore, ConKeD shows a promising potential towards facilitating thedevelopment and application of deep learning-based methods for retinal imageregistration.",
        "title": "ConKeD: Multiview contrastive descriptor learning for keypoint-based  retinal image registration",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05902",
        "abstract_url": "http://arxiv.org/abs/2401.05902",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Weihang"
            },
            {
                "last_name": "Shikh-Bahaei",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers downlink incremental redundancy Hybrid Automatic RepeatRequest (IR-HARQ) over unreliable feedback channels. Since the impact ofpositive feedback (i.e., ACK) error is smaller than that of negative feedback(i.e., NACK) error, an asymmetric feedback detection scheme is proposed toprotect NACK and further reduce the outage probability. We formulate the HARQprocess as a Markov Decision Process (MDP) model to adapt to the transmissionrate of each transmission attempt without enriched feedback and additionalfeedback cost. We aim to optimize the performance of HARQ process under certainoutage probability requirements by finding optimal asymmetric detectionthresholds. Numerical results obtained on the downlink Rayleigh fading channeland 5G new radio (NR) PUCCH feedback channel show that by applying asymmetricfeedback detection and adaptive rate allocation, higher throughput can beachieved under outage probability limitations.",
        "title": "Optimized Asymmetric Feedback Detection for Rate-adaptive HARQ with  Unreliable Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05906",
        "abstract_url": "http://arxiv.org/abs/2401.05906",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Hyunjin"
            },
            {
                "last_name": "Sung",
                "first_name": "Minhyuk"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3Dsegmentation lifting. Recent studies have highlighted the advantages ofutilizing 2D segmentation models to achieve high-quality 3D segmentationthrough few-shot adaptation. However, previous approaches have focused onadapting 2D segmentation models for domain shift to rendered images andsynthetic text descriptions, rather than optimizing the model specifically for3D segmentation. Our proposed task adaptation method finetunes a 2D boundingbox prediction model with an objective function for 3D segmentation. Weintroduce weights for 2D bounding boxes for adaptive merging and learn theweights using a small additional neural network. Additionally, we incorporateSAM, a foreground segmentation model on a bounding box, to improve theboundaries of 2D segments and consequently those of 3D segmentation. Ourexperiments on the PartNet-Mobility dataset show significant improvements withour task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%pimprovement in mAP_50 for semantic and instance segmentation compared to theSotA few-shot 3D segmentation model.",
        "title": "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05907",
        "abstract_url": "http://arxiv.org/abs/2401.05907",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Kang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanjie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This article introduces a sliding window model for defocus deblurring thatachieves the best performance to date with extremely low memory usage. NamedSwintormer, the method utilizes a diffusion model to generate latent priorfeatures that assist in restoring more detailed images. It also extends thesliding window strategy to specialized Transformer blocks for efficientinference. Additionally, we have further optimized Multiply-Accumulateoperations (Macs). Compared to the currently top-performing GRL method, ourSwintormer model drastically reduces computational complexity from 140.35 GMACsto 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocusdeblurring from 27.04 dB to 27.07 dB. This new method allows for the processingof higher resolution images on devices with limited memory, significantlyexpanding potential application scenarios. The article concludes with anablation study that provides an in-depth analysis of the impact of each networkmodule on final performance. The source code and model will be available at thefollowing website: https://github.com/bnm6900030/swintormer.",
        "title": "Efficient Image Deblurring Networks based on Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05908",
        "abstract_url": "http://arxiv.org/abs/2401.05908",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Xuyang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qibin"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Toshihisa"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  With large training datasets and massive amounts of computing sources, largelanguage models (LLMs) achieve remarkable performance in comprehensive andgenerative ability. Based on those powerful LLMs, the model fine-tuned withdomain-specific datasets posseses more specialized knowledge and thus is morepractical like medical LLMs. However, the existing fine-tuned medical LLMs arelimited to general medical knowledge with English language. Fordisease-specific problems, the model's response is inaccurate and sometimeseven completely irrelevant, especially when using a language other thanEnglish. In this work, we focus on the particular disease of Epilepsy withJapanese language and introduce a customized LLM termed as EpilepsyLLM. Ourmodel is trained from the pre-trained LLM by fine-tuning technique usingdatasets from the epilepsy domain. The datasets contain knowledge of basicinformation about disease, common treatment methods and drugs, and importantnotes in life and work. The experimental results demonstrate that EpilepsyLLMcan provide more reliable and specialized medical knowledge responses.",
        "title": "EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with  Epilepsy Medical Knowledge",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05909",
        "abstract_url": "http://arxiv.org/abs/2401.05909",
        "authors": [
            {
                "last_name": "Pavlichenko",
                "first_name": "Dmytro"
            },
            {
                "last_name": "Ficht",
                "first_name": "Grzegorz"
            },
            {
                "last_name": "Villar-Corrales",
                "first_name": "Angel"
            },
            {
                "last_name": "Denninger",
                "first_name": "Luis"
            },
            {
                "last_name": "Brocker",
                "first_name": "Julia"
            },
            {
                "last_name": "Sinen",
                "first_name": "Tim"
            },
            {
                "last_name": "Schreiber",
                "first_name": "Michael"
            },
            {
                "last_name": "Behnke",
                "first_name": "Sven"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The RoboCup Humanoid League holds annual soccer robot world championshipstowards the long-term objective of winning against the FIFA world champions by2050. The participating teams continuously improve their systems. This paperpresents the upgrades to our humanoid soccer system, leading our team NimbRo towin the Soccer Tournament in the Humanoid AdultSize League at RoboCup 2023 inBordeaux, France. The mentioned upgrades consist of: an updated modelarchitecture for visual perception, extended fused angles feedback mechanismsand an additional COM-ZMP controller for walking robustness, and parametricin-walk kicks through waveforms.",
        "title": "RoboCup 2023 Humanoid AdultSize Winner NimbRo: NimbRoNet3 Visual  Perception and Responsive Gait with Waveform In-walk Kicks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05912",
        "abstract_url": "http://arxiv.org/abs/2401.05912",
        "authors": [
            {
                "last_name": "Santos",
                "first_name": "Wesley Ramos dos"
            },
            {
                "last_name": "Paraboni",
                "first_name": "Ivandre"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This article presents a method for prompt-based mental health screening froma large and noisy dataset of social media text. Our method uses GPT 3.5.prompting to distinguish publications that may be more relevant to the task,and then uses a straightforward bag-of-words text classifier to predict actualuser labels. Results are found to be on pair with a BERT mixture of expertsclassifier, and incurring only a fraction of its computational costs.",
        "title": "Prompt-based mental health screening from social media text",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05914",
        "abstract_url": "http://arxiv.org/abs/2401.05914",
        "authors": [
            {
                "last_name": "Elkins",
                "first_name": "Sabina"
            },
            {
                "last_name": "Kochmar",
                "first_name": "Ekaterina"
            },
            {
                "last_name": "Cheung",
                "first_name": "Jackie C. K."
            },
            {
                "last_name": "Serban",
                "first_name": "Iulian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Question generation (QG) is a natural language processing task with anabundance of potential benefits and use cases in the educational domain. Inorder for this potential to be realized, QG systems must be designed andvalidated with pedagogical needs in mind. However, little research has assessedor designed QG approaches with the input from real teachers or students. Thispaper applies a large language model-based QG approach where questions aregenerated with learning goals derived from Bloom's taxonomy. The automaticallygenerated questions are used in multiple experiments designed to assess howteachers use them in practice. The results demonstrate that teachers prefer towrite quizzes with automatically generated questions, and that such quizzeshave no loss in quality compared to handwritten versions. Further, severalmetrics indicate that automatically generated questions can even improve thequality of the quizzes created, showing the promise for large scale use of QGin the classroom setting.",
        "title": "How Teachers Can Use Large Language Models and Bloom's Taxonomy to  Create Educational Quizzes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05916",
        "abstract_url": "http://arxiv.org/abs/2401.05916",
        "authors": [
            {
                "last_name": "Heikkinen",
                "first_name": "Mikko"
            },
            {
                "last_name": "Politis",
                "first_name": "Archontis"
            },
            {
                "last_name": "Virtanen",
                "first_name": "Tuomas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Ambisonics encoding of microphone array signals can enable various spatialaudio applications, such as virtual reality or telepresence, but it istypically designed for uniformly-spaced spherical microphone arrays. This paperproposes a method for Ambisonics encoding that uses a deep neural network (DNN)to estimate a signal transform from microphone inputs to Ambisonics signals.The approach uses a DNN consisting of a U-Net structure with a learnablepreprocessing as well as a loss function consisting of mean average error,spatial correlation, and energy preservation components. The method isvalidated on two microphone arrays with regular and irregular shapes havingfour microphones, on simulated reverberant scenes with multiple sources. Theresults of the validation show that the proposed method can meet or exceed theperformance of a conventional signal-independent Ambisonics encoder on a numberof error metrics.",
        "title": "Neural Ambisonics encoding for compact irregular microphone arrays",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05918",
        "abstract_url": "http://arxiv.org/abs/2401.05918",
        "authors": [
            {
                "last_name": "Boll",
                "first_name": "Bastian"
            },
            {
                "last_name": "Cassel",
                "first_name": "Jonas"
            },
            {
                "last_name": "Albers",
                "first_name": "Peter"
            },
            {
                "last_name": "Petra",
                "first_name": "Stefania"
            },
            {
                "last_name": "Schn\u00f6rr",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GT",
            ""
        ],
        "abstract": "  This paper studies a meta-simplex concept and geometric embedding frameworkfor multi-population replicator dynamics. Central results are two embeddingtheorems which constitute a formal reduction of multi-population replicatordynamics to single-population ones. In conjunction with a robust mathematicalformalism, this provides a toolset for analyzing complex multi-populationmodels. Our framework provides a unifying perspective on different populationdynamics in the literature which in particular enables to establish a formallink between multi-population and multi-game dynamics.",
        "title": "A Geometric Embedding Approach to Multiple Games and Multiple  Populations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05925",
        "abstract_url": "http://arxiv.org/abs/2401.05925",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Bin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Ma",
                "first_name": "Yongjia"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaohui"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zejian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), amethod for compact 3D-consistent scene segmentation at fast rendering speedwith only RGB images input. Previous NeRF-based 3D segmentation methods haverelied on implicit or voxel neural scene representation and ray-marching volumerendering which are time consuming. Recent 3D Gaussian Splatting significantlyimproves the rendering speed, however, existing Gaussians-based segmentationmethods(eg: Gaussian Grouping) fail to provide compact segmentation masksespecially in zero-shot segmentation, which is mainly caused by the lack ofrobustness and compactness for straightforwardly assigning learnable parametersto each Gaussian when encountering inconsistent 2D machine-generated labels.Our method aims to achieve compact and reliable zero-shot scene segmentationswiftly by mapping fused spatial and semantically meaningful features for eachGaussian point with a shallow decoding network. Specifically, our methodfirstly optimizes Gaussian points' position, convariance and color attributesunder the supervision of RGB images. After Gaussian Locating, we distillmulti-scale DINO features extracted from images through unprojection to eachGaussian, which is then incorporated with spatial features from the fast pointfeatures processing network, i.e. RandLA-Net. Then the shallow decoding MLP isapplied to the multi-scale fused features to obtain compact segmentation.Experimental results show that our model can perform high-quality zero-shotscene segmentation, as our model outperforms other segmentation methods on bothsemantic and panoptic segmentation task, meanwhile consumes approximately only10% segmenting time compared to NeRF-based segmentation. Code and more resultswill be available at https://David-Dou.github.io/CoSSegGaussians",
        "title": "CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05926",
        "abstract_url": "http://arxiv.org/abs/2401.05926",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Linghao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jingshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Chong"
            },
            {
                "last_name": "Liang",
                "first_name": "Peng"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  A commit message is a textual description of the code changes in a commit,which is a key part of the Git version control system (VCS). It captures theessence of software updating. Therefore, it can help developers understand codeevolution and facilitate efficient collaboration between developers. However,it is time-consuming and labor-intensive to write good and valuable commitmessages. Some researchers have conducted extensive studies on the automaticgeneration of commit messages and proposed several methods for this purpose,such as generation-based and retrieval-based models. However, seldom studiesexplored whether large language models (LLMs) can be effectively used for theautomatic generation of commit messages. To this end, this paper designed andconducted a series of experiments to comprehensively evaluate the performanceof popular open-source and closed-source LLMs, i.e., Llama 2 and ChatGPT, incommit message generation. The results indicate that considering the BLEU andRouge-L metrics, LLMs surpass existing methods in certain indicators but lagbehind in others. After human evaluations, however, LLMs show a distinctadvantage over all these existing methods. Especially, in 78% of the 366samples, the commit messages generated by LLMs were evaluated by humans as thebest. This work not only reveals the promising potential of using LLMs togenerate commit messages, but also explores the limitations of commonly usedmetrics in evaluating the quality of automatically generated commit messages.",
        "title": "Using Large Language Models for Commit Message Generation: A Preliminary  Study",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05928",
        "abstract_url": "http://arxiv.org/abs/2401.05928",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiashuo"
            },
            {
                "last_name": "Xu",
                "first_name": "Chunpu"
            },
            {
                "last_name": "Leong",
                "first_name": "Chak Tou"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  An emotional support conversation system aims to alleviate users' emotionaldistress and assist them in addressing their challenges. To generate supportiveresponses, it is critical to consider multiple factors such as empathy, supportstrategies, and response coherence, as established in prior methods.Nonetheless, previous models occasionally generate unhelpful responses, whichintend to provide support but display counterproductive effects. According topsychology and communication theories, poor performance in just onecontributing factor might cause a response to be unhelpful. From the modeltraining perspective, since these models have not been exposed to unhelpfulresponses during their training phase, they are unable to distinguish if thetokens they generate might result in unhelpful responses during inference. Toaddress this issue, we introduce a novel model-agnostic framework namedmitigating unhelpfulness with multifaceted AI feedback for emotional support(Muffin). Specifically, Muffin employs a multifaceted AI feedback module toassess the helpfulness of responses generated by a specific model withconsideration of multiple factors. Using contrastive learning, it then reducesthe likelihood of the model generating unhelpful responses compared to thehelpful ones. Experimental results demonstrate that Muffin effectivelymitigates the generation of unhelpful responses while slightly increasingresponse fluency and relevance.",
        "title": "Mitigating Unhelpfulness in Emotional Support Conversations with  Multifaceted AI Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05930",
        "abstract_url": "http://arxiv.org/abs/2401.05930",
        "authors": [
            {
                "last_name": "Kai",
                "first_name": "Jushi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Hu",
                "first_name": "Hai"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhouhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) demonstrate great performance in textgeneration. However, LLMs are still suffering from hallucinations. In thiswork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),to help LLMs decode more truthfully. SH2 is based on a simple fact rooted ininformation theory that for an LLM, the tokens predicted with lowerprobabilities are prone to be more informative than others. Our analysis showsthat the tokens assigned with lower probabilities by an LLM are more likely tobe closely related to factual information, such as nouns, proper nouns, andadjectives. Therefore, we propose to ''highlight'' the factual information byselecting the tokens with the lowest probabilities and concatenating them tothe original context, thus forcing the model to repeatedly read and hesitate onthese tokens before generation. During decoding, we also adopt contrastivedecoding to emphasize the difference in the output probabilities brought by thehesitation. Experimental results demonstrate that our SH2, requiring noadditional data or models, can effectively help LLMs elicit factual knowledgeand distinguish hallucinated contexts. Significant and consistent improvementsare achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.",
        "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05932",
        "abstract_url": "http://arxiv.org/abs/2401.05932",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Langwen"
            },
            {
                "last_name": "Gianinazzi",
                "first_name": "Lukas"
            },
            {
                "last_name": "Yu",
                "first_name": "Yuejiang"
            },
            {
                "last_name": "Dueben",
                "first_name": "Peter D."
            },
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            ""
        ],
        "abstract": "  The generation of initial conditions via accurate data assimilation iscrucial for reliable weather forecasting and climate modeling. We propose theDiffDA as a machine learning based data assimilation method capable ofassimilating atmospheric variables using predicted states and sparseobservations. We adapt the pretrained GraphCast weather forecast model as adenoising diffusion model. Our method applies two-phase conditioning: on thepredicted state during both training and inference, and on sparse observationsduring inference only. As a byproduct, this strategy also enables thepost-processing of predictions into the future, for which no observations areavailable.Through experiments based on a reanalysis dataset, we have verifiedthat our method can produce assimilated global atmospheric data consistent withobservations at 0.25degree resolution. The experiments also show that theinitial conditions that are generated via our approach can be used for forecastmodels with a loss of lead time of at most 24 hours when compared to initialconditions of state-of-the-art data assimilation suites. This enables to applythe method to real world applications such as the creation of reanalysisdatasets with autoregressive data assimilation.",
        "title": "DiffDA: a diffusion model for weather-scale data assimilation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05933",
        "abstract_url": "http://arxiv.org/abs/2401.05933",
        "authors": [
            {
                "last_name": "Aribe Jr.",
                "first_name": "Sales G."
            },
            {
                "last_name": "Gerardo",
                "first_name": "Bobby D."
            },
            {
                "last_name": "Medina",
                "first_name": "Ruji P."
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDSepidemic in the Philippines is the one that is spreading the quickest in thewestern Pacific. Although the full effects of COVID-19 on HIV services anddevelopment are still unknown, it is predicted that such disruptions could leadto a significant increase in HIV casualties. Therefore, the nation needs somemodeling and forecasting techniques to foresee the spread pattern and enhancethe governments prevention, treatment, testing, and care program. In thisstudy, the researcher uses Multilayer Perceptron Neural Network to forecasttime series during the period when the COVID-19 pandemic strikes the nation,using statistics taken from the HIV/AIDS and ART Registry of the Philippines.After training, validation, and testing of data, the study finds that thepredicted cumulative cases in the nation by 2030 will reach 145,273.Additionally, there is very little difference between observed and anticipatedHIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as wellas a greater coefficient of determination. Further research revealed that thePhilippines seems far from achieving Sustainable Development Goal 3 of Project2030 due to an increase in the nations rate of new HIV infections. Despite thedetrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, thePhilippine government, under the Marcos administration, must continue to adhereto the United Nations 90-90-90 targets by enhancing its ART program andensuring that all vital health services are readily accessible and available.",
        "title": "Time Series Forecasting of HIV/AIDS in the Philippines Using Deep  Learning: Does COVID-19 Epidemic Matter?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05938",
        "abstract_url": "http://arxiv.org/abs/2401.05938",
        "authors": [
            {
                "last_name": "Picasarri-Arrieta",
                "first_name": "Lucas"
            },
            {
                "last_name": "Rambaud",
                "first_name": "Cl\u00e9ment"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM"
        ],
        "abstract": "  Aboulker et al. proved that a digraph with large enough dichromatic numbercontains any fixed digraph as a subdivision. The dichromatic number of adigraph is the smallest order of a partition of its vertex set into acyclicinduced subdigraphs. A digraph is dicritical if the removal of any arc orvertex decreases its dichromatic number. In this paper we give sufficientconditions on a dicritical digraph of large order or large directed girth tocontain a given digraph as a subdivision. In particular, we prove that (i) forevery integers $k,\\ell$, large enough dicritical digraphs with dichromaticnumber $k$ contain an orientation of a cycle with at least $\\ell$ vertices;(ii) there are functions $f,g$ such that for every subdivision $F^*$ of adigraph $F$, digraphs with directed girth at least $f(F^*)$ and dichromaticnumber at least $g(F)$ contain a subdivision of $F^*$, and if $F$ is a tree,then $g(F)=|V(F)|$; (iii) there is a function $f$ such that for everysubdivision $F^*$ of $TT_3$ (the transitive tournament on three vertices),digraphs with directed girth at least $f(F^*)$ and minimum out-degree at least$2$ contain $F^*$ as a subdivision.",
        "title": "Subdivisions in dicritical digraphs with large order or digirth",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05939",
        "abstract_url": "http://arxiv.org/abs/2401.05939",
        "authors": [
            {
                "last_name": "Chatterjee",
                "first_name": "Shubham"
            },
            {
                "last_name": "Mackie",
                "first_name": "Iain"
            },
            {
                "last_name": "Dalton",
                "first_name": "Jeff"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            ""
        ],
        "abstract": "  While entity-oriented neural IR models have advanced significantly, theyoften overlook a key nuance: the varying degrees of influence individualentities within a document have on its overall relevance. Addressing this gap,we present DREQ, an entity-oriented dense document re-ranking model. Uniquely,we emphasize the query-relevant entities within a document's representationwhile simultaneously attenuating the less relevant ones, thus obtaining aquery-specific entity-centric document representation. We then combine thisentity-centric document representation with the text-centric representation ofthe document to obtain a \"hybrid\" representation of the document. We learn arelevance score for the document using this hybrid representation. Using fourlarge-scale benchmarks, we show that DREQ outperforms state-of-the-art neuraland non-neural re-ranking methods, highlighting the effectiveness of ourentity-oriented representation approach.",
        "title": "DREQ: Document Re-Ranking Using Entity-based Query Understanding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05940",
        "abstract_url": "http://arxiv.org/abs/2401.05940",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Shin",
                "first_name": "Donghwan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in processingboth natural and programming languages, which have enabled various applicationsin software engineering, such as requirement engineering, code generation, andsoftware testing. However, existing code generation benchmarks do notnecessarily assess the code understanding performance of LLMs, especially forthe subtle inconsistencies that may arise between code and its semanticsdescribed in natural language.  In this paper, we propose a novel method to systematically assess the codeunderstanding performance of LLMs, particularly focusing on subtle differencesbetween code and its descriptions, by introducing code mutations to existingcode generation datasets. Code mutations are small changes that alter thesemantics of the original code, creating a mismatch with the natural languagedescription. We apply different types of code mutations, such as operatorreplacement and statement deletion, to generate inconsistent code-descriptionpairs. We then use these pairs to test the ability of LLMs to correctly detectthe inconsistencies.  We propose a new LLM testing method, called Mutation-based ConsistencyTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 andGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, whichconsists of six programming languages (Python, C++, Java, Go, JavaScript, andRust). We compare the performance of the LLMs across different types of codemutations and programming languages and analyze the results. We find that theLLMs show significant variation in their code understanding performance andthat they have different strengths and weaknesses depending on the mutationtype and language.",
        "title": "Mutation-based Consistency Testing for Evaluating the Code Understanding  Capability of LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05943",
        "abstract_url": "http://arxiv.org/abs/2401.05943",
        "authors": [
            {
                "last_name": "Harnes",
                "first_name": "H\u00e5kon"
            },
            {
                "last_name": "Morrison",
                "first_name": "Donn"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  WebAssembly is a low-level bytecode language that allows high-level languageslike C, C++, and Rust to be executed in the browser at near-native performance.In recent years, WebAssembly has gained widespread adoption is now nativelysupported by all modern browsers. However, vulnerabilities in memory-unsafelanguages, like C and C++, can translate into vulnerabilities in WebAssemblybinaries. Unfortunately, most WebAssembly binaries are compiled from suchmemory-unsafe languages, and these vulnerabilities have been shown to bepractical in real-world scenarios. WebAssembly smart contracts have also beenfound to be vulnerable, causing significant financial loss. Additionally,WebAssembly has been used for malicious purposes like cryptojacking. To addressthese issues, several analysis techniques for WebAssembly binaries have beenproposed. In this paper, we conduct a comprehensive literature review of thesetechniques and categorize them based on their analysis strategy and objectives.Furthermore, we compare and evaluate the techniques using quantitative data,highlighting their strengths and weaknesses. In addition, one of the maincontributions of this paper is the identification of future research directionsbased on the thorough literature review conducted.",
        "title": "SoK: Analysis techniques for WebAssembly",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05946",
        "abstract_url": "http://arxiv.org/abs/2401.05946",
        "authors": [
            {
                "last_name": "Dedieu",
                "first_name": "Antoine"
            },
            {
                "last_name": "Lehrach",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guangyao"
            },
            {
                "last_name": "George",
                "first_name": "Dileep"
            },
            {
                "last_name": "L\u00e1zaro-Gredilla",
                "first_name": "Miguel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Despite their stellar performance on a wide range of tasks, includingin-context tasks only revealed during inference, vanilla transformers andvariants trained for next-token predictions (a) do not learn an explicit worldmodel of their environment which can be flexibly queried and (b) cannot be usedfor planning or navigation. In this paper, we consider partially observedenvironments (POEs), where an agent receives perceptually aliased observationsas it navigates, which makes path planning hard. We introduce a transformerwith (multiple) discrete bottleneck(s), TDB, whose latent codes learn acompressed representation of the history of observations and actions. Aftertraining a TDB to predict the future observation(s) given the history, weextract interpretable cognitive maps of the environment from its activebottleneck(s) indices. These maps are then paired with an external solver tosolve (constrained) path planning problems. First, we show that a TDB trainedon POEs (a) retains the near perfect predictive performance of a vanillatransformer or an LSTM while (b) solving shortest path problems exponentiallyfaster. Second, a TDB extracts interpretable representations from textdatasets, while reaching higher in-context accuracy than vanilla sequencemodels. Finally, in new POEs, a TDB (a) reaches near-perfect in-contextaccuracy, (b) learns accurate in-context cognitive maps (c) solves in-contextpath planning problems.",
        "title": "Learning Cognitive Maps from Transformer Representations for Efficient  Planning in Partially Observed Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05947",
        "abstract_url": "http://arxiv.org/abs/2401.05947",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhuolun"
            },
            {
                "last_name": "Majumdar",
                "first_name": "Srijoni"
            },
            {
                "last_name": "Pournaras",
                "first_name": "Evangelos"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Conditional Information Reveal (CIR) automates the release of informationupon meeting specific pre-defined conditions, such as time or location. Thispaper advances the understanding and implementation of CIR by introducing a newparadigm to highlight the security challenges in CIR design, and proposes adecentralized architecture as a design guideline for secure CIR systems.Furthermore, in the context of time-sensitive data sharing, this paper proposesa practical timed-release cryptography system employing the proposedarchitecture and a novel verifiable secret sharing scheme. Key achievements ofthis study include the creation of an open-source prototype for practicaldeployment and a comprehensive system evaluation that highlights the enhancedsecurity and efficiency of the proposed system. Furthermore, the paper delvesinto the application of this system in E-voting scenarios, illustrating itscapacity to secure and ensure fair electronic voting processes.",
        "title": "Blockchain-based Decentralized Time Lock Machines: Automated Reveal of  Time-sensitive Information",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05949",
        "abstract_url": "http://arxiv.org/abs/2401.05949",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Shuai"
            },
            {
                "last_name": "Jia",
                "first_name": "Meihuizi"
            },
            {
                "last_name": "Tuan",
                "first_name": "Luu Anh"
            },
            {
                "last_name": "Wen",
                "first_name": "Jinming"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In-context learning, a paradigm bridging the gap between pre-training andfine-tuning, has demonstrated high efficacy in several NLP tasks, especially infew-shot settings. Unlike traditional fine-tuning methods, in-context learningadapts pre-trained models to unseen tasks without updating any parameters.Despite being widely applied, in-context learning is vulnerable to maliciousattacks. In this work, we raise security concerns regarding this paradigm. Ourstudies demonstrate that an attacker can manipulate the behavior of largelanguage models by poisoning the demonstration context, without the need forfine-tuning the model. Specifically, we have designed a new backdoor attackmethod, named ICLAttack, to target large language models based on in-contextlearning. Our method encompasses two types of attacks: poisoning demonstrationexamples and poisoning prompts, which can make models behave in accordance withpredefined intentions. ICLAttack does not require additional fine-tuning toimplant a backdoor, thus preserving the model's generality. Furthermore, thepoisoned examples are correctly labeled, enhancing the natural stealth of ourattack method. Extensive experimental results across several language models,ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness ofour attack method, exemplified by a high average attack success rate of 95.0%across the three datasets on OPT models. Our findings highlight thevulnerabilities of language models, and we hope this work will raise awarenessof the possible security threats associated with in-context learning.",
        "title": "Universal Vulnerabilities in Large Language Models: In-context Learning  Backdoor Attacks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05950",
        "abstract_url": "http://arxiv.org/abs/2401.05950",
        "authors": [
            {
                "last_name": "Trombini",
                "first_name": "Sofia"
            },
            {
                "last_name": "Pasta",
                "first_name": "Edoardo"
            },
            {
                "last_name": "Fagiano",
                "first_name": "Lorenzo"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This study investigates deep offshore, pumping Airborne Wind Energy systems,focusing on the kite-platform interaction. The considered system includes a 360m2 soft-wing kite, connected by a tether to a winch installed on a10-meter-deep spar with four mooring lines. Wind power is converted intoelectricity with a feedback controlled periodic trajectory of the kite andcorresponding reeling motion of the tether. An analysis of the mutual influencebetween the platform and the kite dynamics, with different wave regimes,reveals a rather small sensitivity of the flight pattern to the platformoscillations; on the other hand, the frequency of tether force oscillations canbe close to the platform resonance peaks, resulting in possible increasedfatigue loads and damage of the floating and submerged components. A controldesign procedure is then proposed to avoid this problem, acting on the kitepath planner. Simulation results confirm the effectiveness of the approach.",
        "title": "On the Kite-Platform Interactions in Offshore Airborne Wind Energy  Systems: Frequency Analysis and Control Approach",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05952",
        "abstract_url": "http://arxiv.org/abs/2401.05952",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Chujie"
            },
            {
                "last_name": "Chen",
                "first_name": "Dongping"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qihui"
            },
            {
                "last_name": "Huang",
                "first_name": "Yue"
            },
            {
                "last_name": "Wan",
                "first_name": "Yao"
            },
            {
                "last_name": "Sun",
                "first_name": "Lichao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  With the remarkable development and widespread applications of large languagemodels (LLMs), the use of machine-generated text (MGT) is becoming increasinglycommon. This trend brings potential risks, particularly to the quality andcompleteness of information in fields such as news and education. Currentresearch predominantly addresses the detection of pure MGT without adequatelyaddressing mixed scenarios including AI-revised Human-Written Text (HWT) orhuman-revised MGT. To confront this challenge, we introduce mixcase, a novelconcept representing a hybrid text form involving both machine-generated andhuman-generated content. We collected mixcase instances generated from multipledaily text-editing scenarios and composed MixSet, the first dataset dedicatedto studying these mixed modification scenarios. We conduct experiments toevaluate the efficacy of popular MGT detectors, assessing their effectiveness,robustness, and generalization performance. Our findings reveal that existingdetectors struggle to identify mixcase as a separate class or MGT, particularlyin dealing with subtle modifications and style adaptability. This researchunderscores the urgent need for more fine-grain detectors tailored for mixcase,offering valuable insights for future research. Code and Models are availableat https://github.com/Dongping-Chen/MixSet.",
        "title": "LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05956",
        "abstract_url": "http://arxiv.org/abs/2401.05956",
        "authors": [
            {
                "last_name": "Rohwedder",
                "first_name": "Lars"
            },
            {
                "last_name": "Safari",
                "first_name": "Ashkan"
            },
            {
                "last_name": "Vredeveld",
                "first_name": "Tjark"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Local search is a widely used technique for tackling challenging optimizationproblems, offering significant advantages in terms of computational efficiencyand exhibiting strong empirical behavior across a wide range of problemdomains. In this paper, we address a scheduling problem on two identicalparallel machines with the objective of \\emph{makespan minimization}. For thisproblem, we consider a local search neighborhood, called \\emph{$k$-swap}, whichis a more generalized version of the widely-used \\emph{swap} and \\emph{jump}neighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$jobs between two machines in our schedule. First, we propose an algorithm forfinding an improving neighbor in the $k$-swap neighborhood which is faster thanthe naive approach, and prove an almost matching lower bound on any such analgorithm. Then, we analyze the number of local search steps required toconverge to a local optimum with respect to the $k$-swap neighborhood. For thecase $k = 2$ (similar to the swap neighborhood), we provide a polynomial upperbound on the number of local search steps, and for the case $k = 3$, we providean exponential lower bound. Finally, we conduct computational experiments onvarious families of instances, and we discuss extensions to more than twomachines in our schedule.",
        "title": "A k-swap Local Search for Makespan Scheduling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05960",
        "abstract_url": "http://arxiv.org/abs/2401.05960",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xijun"
            },
            {
                "last_name": "Zhu",
                "first_name": "Fangzhou"
            },
            {
                "last_name": "Zhen",
                "first_name": "Hui-Ling"
            },
            {
                "last_name": "Luo",
                "first_name": "Weilin"
            },
            {
                "last_name": "Lu",
                "first_name": "Meng"
            },
            {
                "last_name": "Huang",
                "first_name": "Yimin"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhenan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zirui"
            },
            {
                "last_name": "Kuang",
                "first_name": "Yufei"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhihai"
            },
            {
                "last_name": "Geng",
                "first_name": "Zijie"
            },
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Liu",
                "first_name": "Haoyang"
            },
            {
                "last_name": "An",
                "first_name": "Zhiwu"
            },
            {
                "last_name": "Yang",
                "first_name": "Muming"
            },
            {
                "last_name": "Li",
                "first_name": "Jianshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Jie"
            },
            {
                "last_name": "Yan",
                "first_name": "Junchi"
            },
            {
                "last_name": "Sun",
                "first_name": "Defeng"
            },
            {
                "last_name": "Zhong",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yong"
            },
            {
                "last_name": "Zeng",
                "first_name": "Jia"
            },
            {
                "last_name": "Yuan",
                "first_name": "Mingxuan"
            },
            {
                "last_name": "Hao",
                "first_name": "Jianye"
            },
            {
                "last_name": "Yao",
                "first_name": "Jun"
            },
            {
                "last_name": "Mao",
                "first_name": "Kun"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In an era of digital ubiquity, efficient resource management anddecision-making are paramount across numerous industries. To this end, wepresent a comprehensive study on the integration of machine learning (ML)techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate thescarcity of real-world mathematical programming instances, and to surpass thecapabilities of traditional optimization techniques. We showcase our methodsfor generating complex SAT and MILP instances utilizing generative models thatmirror multifaceted structures of real-world problem. Furthermore, we introducea training framework leveraging augmentation policies to maintain solvers'utility in dynamic environments. Besides the data generation and augmentation,our proposed approaches also include novel ML-driven policies for personalizedsolver strategies, with an emphasis on applications like graph convolutionalnetworks for initial basis selection and reinforcement learning for advancedpresolving and cut selection. Additionally, we detail the incorporation ofstate-of-the-art parameter tuning algorithms which markedly elevate solverperformance. Compared with traditional solvers such as Gurobi and SCIP, ourML-augmented OptVerse AI Solver demonstrates superior speed and precisionacross both established benchmarks and real-world scenarios, reinforcing thepractical imperative and effectiveness of machine learning techniques inmathematical programming solvers.",
        "title": "Machine Learning Insides OptVerse AI Solver: Design Principles and  Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05961",
        "abstract_url": "http://arxiv.org/abs/2401.05961",
        "authors": [
            {
                "last_name": "Cesarano",
                "first_name": "Carmine"
            },
            {
                "last_name": "Natella",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Application Layer Gateways (ALGs) play a crucial role in securing criticalsystems, including railways, industrial automation, and defense applications,by segmenting networks at different levels of criticality. However, theyrequire rigorous security testing to prevent software vulnerabilities, not onlyat the network level but also at the application layer (e.g., deep trafficinspection components). This paper presents a vulnerability-driven methodologyfor the comprehensive security testing of ALGs. We present the methodology inthe context of an industrial case study in the railways domain, and asimulation-based testing environment to support the methodology.",
        "title": "Securing an Application Layer Gateway: An Industrial Case Study",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05963",
        "abstract_url": "http://arxiv.org/abs/2401.05963",
        "authors": [
            {
                "last_name": "L\u00f3pez-Ure\u00f1a",
                "first_name": "Sergio"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper, we introduce a novel non-linear uniform subdivision scheme forthe generation of curves in $\\mathbb{R}^n$, $n\\geq2$. This scheme isdistinguished by its capacity to reproduce second-degree polynomial data onnon-uniform grids without necessitating prior knowledge of the gridspecificities. Our approach exploits the potential of annihilation operators toinfer the underlying grid, thereby obviating the need for end-users to specifysuch information. We define the scheme in a non-stationary manner, ensuringthat it progressively approaches a classical linear scheme as the iterationnumber increases, all while preserving its polynomial reproduction capability.  The convergence is established through two distinct theoretical methods.Firstly, we propose a new class of schemes, including ours, for which weestablish $\\mathcal{C}^1$ convergence by combining results from the analysis ofquasilinear schemes and asymptotically equivalent linear non-uniformnon-stationary schemes. Secondly, we adapt conventional analytical tools fornon-linear schemes to the non-stationary case, allowing us to again concludethe convergence of the proposed class of schemes.  We show its practical usefulness through numerical examples, showing that thegenerated curves are curvature continuous.",
        "title": "A uniform non-linear subdivision scheme reproducing polynomials at any  non-uniform grid",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05964",
        "abstract_url": "http://arxiv.org/abs/2401.05964",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hongjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Try to generate new bridge types using generative artificial intelligencetechnology. Using symmetric structured image dataset of three-span beam bridge,arch bridge, cable-stayed bridge and suspension bridge , based on Pythonprogramming language, TensorFlow and Keras deep learning platform framework ,PixelCNN is constructed and trained. The model can capture the statisticalstructure of the images and calculate the probability distribution of the nextpixel when the previous pixels are given. From the obtained latent spacesampling, new bridge types different from the training dataset can begenerated. PixelCNN can organically combine different structural components onthe basis of human original bridge types, creating new bridge types that have acertain degree of human original ability. Autoregressive models cannotunderstand the meaning of the sequence, while multimodal models combineregression and autoregressive models to understand the sequence. Multimodalmodels should be the way to achieve artificial general intelligence in thefuture.",
        "title": "An attempt to generate new bridge types from latent space of PixelCNN",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05965",
        "abstract_url": "http://arxiv.org/abs/2401.05965",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shiwei"
            },
            {
                "last_name": "Diao",
                "first_name": "Lansong"
            },
            {
                "last_name": "Wu",
                "first_name": "Chuan"
            },
            {
                "last_name": "Cao",
                "first_name": "Zongyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyu"
            },
            {
                "last_name": "Lin",
                "first_name": "Wei"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted totrain large deep neural networks (DNNs). Few studies have explored itsapplicability on heterogeneous clusters, to fully exploit available resourcesfor large model learning. This paper presents \\OurSystem, an automated systemdesigned to expedite SPMD DNN training on heterogeneous clusters. \\OurSystemjointly optimizes the tensor sharding strategy, sharding ratios acrossheterogeneous devices and the communication methods for tensor exchanges foroptimized distributed training with SPMD parallelism. We novelly formulatemodel partitioning as a program synthesis problem, in which we generate adistributed program from scratch on a distributed instruction set thatsemantically resembles the program designed for a single device, andsystematically explore the solution space with an A*-based search algorithm. Wederive the optimal tensor sharding ratios by formulating it as a linearprogramming problem. Additionally, \\OurSystem explores tensor communicationoptimization in a heterogeneous cluster and integrates it as part of theprogram synthesis process, for automatically choosing optimal collectivecommunication primitives and applying sufficient factor broadcasting technique.Extensive experiments on representative workloads demonstrate that \\OurSystemachieves up to 2.41x speed-up on heterogeneous clusters.",
        "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated  Program Synthesis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05967",
        "abstract_url": "http://arxiv.org/abs/2401.05967",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Yihua"
            },
            {
                "last_name": "Shimodaira",
                "first_name": "Hidetoshi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  The primary aim of Knowledge Graph embeddings (KGE) is to learnlow-dimensional representations of entities and relations for predictingmissing facts. While rotation-based methods like RotatE and QuatE perform wellin KGE, they face two challenges: limited model flexibility requiringproportional increases in relation size with entity dimension, and difficultiesin generalizing the model for higher-dimensional rotations. To address theseissues, we introduce OrthogonalE, a novel KGE model employing matrices forentities and block-diagonal orthogonal matrices with Riemannian optimizationfor relations. This approach enhances the generality and flexibility of KGEmodels. The experimental results indicate that our new KGE model, OrthogonalE,is both general and flexible, significantly outperforming state-of-the-art KGEmodels while substantially reducing the number of relation parameters.",
        "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph  Embedding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05968",
        "abstract_url": "http://arxiv.org/abs/2401.05968",
        "authors": [
            {
                "last_name": "Chaudhuri",
                "first_name": "Yashwardhan"
            },
            {
                "last_name": "Kumar",
                "first_name": "Ankit"
            },
            {
                "last_name": "Phukan",
                "first_name": "Orchid Chetia"
            },
            {
                "last_name": "Buduru",
                "first_name": "Arun Balaji"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Crowd counting finds direct applications in real-world situations, makingcomputational efficiency and performance crucial. However, most of the previousmethods rely on a heavy backbone and a complex downstream architecture thatrestricts the deployment. To address this challenge and enhance the versatilityof crowd-counting models, we introduce two lightweight models. These modelsmaintain the same downstream architecture while incorporating two distinctbackbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion toextract diverse scale features from a Pre-Trained Model (PTM) and subsequentlycombine these features seamlessly. This approach empowers our models to achieveimproved performance while maintaining a compact and efficient design. With thecomparison of our proposed models with previously available state-of-the-art(SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, itachieves comparable results while being the most computationally efficientmodel. Finally, we present a comparative study, an extensive ablation study,along with pruning to show the effectiveness of our models.",
        "title": "A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd  Counting",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05969",
        "abstract_url": "http://arxiv.org/abs/2401.05969",
        "authors": [
            {
                "last_name": "Strau\u00df",
                "first_name": "Niklas"
            },
            {
                "last_name": "Schubert",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The traveling officer problem (TOP) is a challenging stochastic optimizationtask. In this problem, a parking officer is guided through a city equipped withparking sensors to fine as many parking offenders as possible. A majorchallenge in TOP is the dynamic nature of parking offenses, which randomlyappear and disappear after some time, regardless of whether they have beenfined. Thus, solutions need to dynamically adjust to currently fineable parkingoffenses while also planning ahead to increase the likelihood that the officerarrives during the offense taking place. Though various solutions exist, thesemethods often struggle to take the implications of actions on the ability tofine future parking violations into account. This paper proposes SATOP, a novelspatial-aware deep reinforcement learning approach for TOP. Our novel stateencoder creates a representation of each action, leveraging the spatialrelationships between parking spots, the agent, and the action. Furthermore, wepropose a novel message-passing module for learning future inter-actioncorrelations in the given environment. Thus, the agent can estimate thepotential to fine further parking violations after executing an action. Weevaluate our method using an environment based on real-world data fromMelbourne. Our results show that SATOP consistently outperformsstate-of-the-art TOP agents and is able to fine up to 22% more parkingoffenses.",
        "title": "Spatial-Aware Deep Reinforcement Learning for the Traveling Officer  Problem",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05971",
        "abstract_url": "http://arxiv.org/abs/2401.05971",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Rouwan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xiaoya"
            },
            {
                "last_name": "Zhu",
                "first_name": "Juelin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xuxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Maojun"
            },
            {
                "last_name": "Yan",
                "first_name": "Shen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite significant progress in global localization of Unmanned AerialVehicles (UAVs) in GPS-denied environments, existing methods remain constrainedby the availability of datasets. Current datasets often focus on small-scalescenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAVbuild-in sensor data. To address these limitations, we introduce a large-scale6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoFlocalization pipeline (UAVLoc), which consists of offline synthetic datageneration and online visual localization. Additionally, based on the 6-DoFestimator, we design a hierarchical system for tracking ground target in 3Dspace. Experimental results on the new dataset demonstrate the effectiveness ofthe proposed approach. Code and dataset are available athttps://github.com/RingoWRW/UAVD4L",
        "title": "UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05972",
        "abstract_url": "http://arxiv.org/abs/2401.05972",
        "authors": [
            {
                "last_name": "Gahr",
                "first_name": "Constatin"
            },
            {
                "last_name": "Farcas",
                "first_name": "Ionut-Gabriel"
            },
            {
                "last_name": "Jenko",
                "first_name": "Frank"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "LG",
            ""
        ],
        "abstract": "  This paper focuses on the construction of non-intrusive Scientific MachineLearning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasmaturbulence simulations. In particular, we propose using Operator Inference(OpInf) to build low-cost physics-based ROMs from data for such simulations. Asa representative example, we focus on the Hasegawa-Wakatani (HW) equations usedfor modeling two-dimensional electrostatic drift-wave plasma turbulence. For acomprehensive perspective of the potential of OpInf to construct accurate ROMsfor this model, we consider a setup for the HW equations that leads to theformation of complex, nonlinear, and self-driven dynamics, and perform two setsof experiments. We first use the data obtained via a direct numericalsimulation of the HW equations starting from a specific initial condition andtrain OpInf ROMs for predictions beyond the training time horizon. In thesecond, more challenging set of experiments, we train ROMs using the samedataset as before but this time perform predictions for six other initialconditions. Our results show that the OpInf ROMs capture the important featuresof the turbulent dynamics and generalize to new and unseen initial conditionswhile reducing the evaluation time of the high-fidelity model by up to fiveorders of magnitude in single-core performance. In the broader context offusion research, this shows that non-intrusive SciML ROMs have the potential todrastically accelerate numerical studies, which can ultimately enable taskssuch as the design and real-time control of optimized fusion devices.",
        "title": "Learning physics-based reduced models from data for the  Hasegawa-Wakatani equations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05975",
        "abstract_url": "http://arxiv.org/abs/2401.05975",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shihao"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Ma",
                "first_name": "Yingwei"
            },
            {
                "last_name": "Ma",
                "first_name": "Jian"
            },
            {
                "last_name": "Zhong",
                "first_name": "Wenliang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guannan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kejun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xinwang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            ""
        ],
        "abstract": "  Mining users' intents plays a crucial role in sequential recommendation. Therecent approach, ICLRec, was introduced to extract underlying users' intentsusing contrastive learning and clustering. While it has shown effectiveness,the existing method suffers from complex and cumbersome alternatingoptimization, leading to two main issues. Firstly, the separation ofrepresentation learning and clustering optimization within a generalizedexpectation maximization (EM) framework often results in sub-optimalperformance. Secondly, performing clustering on the entire dataset hampersscalability for large-scale industry data. To address these challenges, wepropose a novel intent learning method called \\underline{ELCRec}, whichintegrates representation learning into an \\underline{E}nd-to-end\\underline{L}earnable \\underline{C}lustering framework for\\underline{Rec}ommendation. Specifically, we encode users' behavior sequencesand initialize the cluster centers as learnable network parameters.Additionally, we design a clustering loss that guides the networks todifferentiate between different cluster centers and pull similar samplestowards their respective cluster centers. This allows simultaneous optimizationof recommendation and clustering using mini-batch data. Moreover, we leveragethe learned cluster centers as self-supervision signals for representationlearning, resulting in further enhancement of recommendation performance.Extensive experiments conducted on open benchmarks and industry data validatethe superiority, effectiveness, and efficiency of our proposed ELCRec method.Code is available at: https://github.com/yueliu1999/ELCRec.",
        "title": "End-to-end Learnable Clustering for Intent Learning in Recommendation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05982",
        "abstract_url": "http://arxiv.org/abs/2401.05982",
        "authors": [
            {
                "last_name": "Zakrisson",
                "first_name": "Henning"
            },
            {
                "last_name": "Lindholm",
                "first_name": "Mathias"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The paper introduces a tree-based varying coefficient model (VCM) where thevarying coefficients are modelled using the cyclic gradient boosting machine(CGBM) from Delong et al. (2023). Modelling the coefficient functions using aCGBM allows for dimension-wise early stopping and feature importance scores.The dimension-wise early stopping not only reduces the risk ofdimension-specific overfitting, but also reveals differences in modelcomplexity across dimensions. The use of feature importance scores allows forsimple feature selection and easy model interpretation. The model is evaluatedon the same simulated and real data examples as those used in Richman andW\\\"uthrich (2023), and the results show that it produces results in terms ofout of sample loss that are comparable to those of their neural network-basedVCM called LocalGLMnet.",
        "title": "A tree-based varying coefficient model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05984",
        "abstract_url": "http://arxiv.org/abs/2401.05984",
        "authors": [
            {
                "last_name": "Tong",
                "first_name": "Hua"
            },
            {
                "last_name": "Halilaj",
                "first_name": "Eni"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongjie Jessica"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            ""
        ],
        "abstract": "  We present a new software package \"HybridOctree_Hex\" for adaptiveall-hexahedral mesh generation based on hybrid octree and quality improvementwith Jacobian control. The proposed HybridOctree_Hex begins by detectingcurvatures and narrow regions of the input boundary to identify key surfacefeatures and initialize an octree structure. Subsequently, a strongly balancedoctree is constructed using the balancing and pairing rules. Inspired by ourearlier preliminary hybrid octree-based work, templates are designed toguarantee an all-hexahedral dual mesh generation directly from the stronglybalanced octree. With these pre-defined templates, the sophisticated hybridoctree construction step is skipped to achieve an efficient implementation.After that, elements outside and around the boundary are removed to create acore mesh. The boundary points of the core mesh are connected to theircorresponding closest points on the surface to fill the buffer zone and buildthe final mesh. Coupled with smart Laplacian smoothing, HybridOctree_Hex takesadvantage of a delicate optimization-based quality improvement methodconsidering geometric fitting, Jacobian and scaled Jacobian to achieve theminimum scaled Jacobian higher than $0.5$. We empirically verify the robustnessand efficiency of our method by running the HybridOctree_Hex software on dozensof complex 3D models without any manual intervention or parameter adjustment.We provide the HybridOctree_Hex source codes, comprehensive results,encompassing mesh input/output files and statistical data presented athttps://github.com/CMU-CBML/HybridOctree_Hex.",
        "title": "HybridOctree_Hex: Hybrid Octree-Based Adaptive All-Hexahedral Mesh  Generation with Jacobian Control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05986",
        "abstract_url": "http://arxiv.org/abs/2401.05986",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Yifan"
            },
            {
                "last_name": "Chai",
                "first_name": "Bingxu"
            },
            {
                "last_name": "Yu",
                "first_name": "Siyu"
            },
            {
                "last_name": "Li",
                "first_name": "Ying"
            },
            {
                "last_name": "He",
                "first_name": "Pinjia"
            },
            {
                "last_name": "Jiang",
                "first_name": "Wei"
            },
            {
                "last_name": "Li",
                "first_name": "Jianguo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Due to the sheer size of software logs, developers rely on automated loganalysis. Log parsing, which parses semi-structured logs into a structuredformat, is a prerequisite of automated log analysis. However, existing logparsers are unsatisfactory when applied in practice because: 1) they ignorecategories of variables, and 2) have poor generalization ability. To addressthe limitations of existing approaches, we propose LogPTR, the first end-to-endvariable-aware log parser that can extract the static and dynamic parts inlogs, and further identify the categories of variables. The key of LogPTR isusing pointer network to copy words from the log message. We have performedextensive experiments on 16 public log datasets and the results show thatLogPTR outperforms state-of-the-art log parsers both on general log parsingthat extracts the log template and variable-aware log parsing that furtheridentifies the category of variables.",
        "title": "LogPTR: Variable-Aware Log Parsing with Pointer Network",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05987",
        "abstract_url": "http://arxiv.org/abs/2401.05987",
        "authors": [
            {
                "last_name": "von Gladiss",
                "first_name": "Anselm"
            },
            {
                "last_name": "Ahmadian",
                "first_name": "Amir Shayan"
            },
            {
                "last_name": "J\u00fcrjens",
                "first_name": "Jan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Magnetic particle imaging (MPI) is an emerging medical imaging modality whichoffers a unique combination of high temporal and spatial resolution,sensitivity and biocompatibility. For system-matrix (SM) based imagereconstruction in MPI, a huge amount of calibration data needs to be acquiredprior to reconstruction in a time-consuming procedure. Conventionally, the datais recorded on-site inside the scanning device, which significantly limits thetime that the scanning device is available for patient care in a clinicalsetting. Due to its size, handling the calibration data can be challenging. Tosolve these issues of recording and handling the data, data spaces could beused, as it has been shown that the calibration data can be measured indedicated devices off-site. We propose a data space aimed at improving theefficiency of SM-based image reconstruction in MPI. The data space consists ofimaging facilities, calibration data providers and reconstruction experts. Itsspecifications follow the reference architecture model of international dataspaces (IDS). Use-cases of image reconstruction in MPI are formulated. Thestakeholders and tasks are listed and mapped to the terminology of IDS. Thesignal chain in MPI is analysed to identify a minimum information model whichis used by the data space.",
        "title": "Reconstruction as a service: a data space for off-site image  reconstruction in magnetic particle imaging",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05993",
        "abstract_url": "http://arxiv.org/abs/2401.05993",
        "authors": [
            {
                "last_name": "Da R\u00f9",
                "first_name": "Pietro"
            },
            {
                "last_name": "Benoni",
                "first_name": "Arianna"
            },
            {
                "last_name": "Salucci",
                "first_name": "Marco"
            },
            {
                "last_name": "Rocca",
                "first_name": "Paolo"
            },
            {
                "last_name": "Massa",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In the framework of the \"Smart ElectroMagnetic Environment\" (SEME), aninnovative strategy leveraging Equivalence Source concepts is introduced forenhancing the performance of large-scale outdoor wireless communicationsystems. The proposed Opportunistic Sources Synthesis (OSS) approach is aimedat unconventionally synthesizing the primary source (i.e., the base transceiverstation (BTS) antenna array), so that the complex scattering phenomena inducedin the surrounding scatterers are profitably exploited to enhance the receivedpower within user-defined regions of interest (RoIs). To yield acomputationally feasible synthesis process, an innovative\"Embedded-plus-Environment Patterns\" (EPEPs) method is introduced. A set ofrepresentative numerical examples, concerned with realistic large-scale outdoorscenarios, is presented to assess the effectiveness and the efficiency of theproposed optimization-driven approach for a realistic SEME implementation.",
        "title": "An Opportunistic Source Synthesis Method for Smart Electromagnetic  Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05994",
        "abstract_url": "http://arxiv.org/abs/2401.05994",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Qian"
            },
            {
                "last_name": "Chen",
                "first_name": "Jieyang"
            },
            {
                "last_name": "Whitney",
                "first_name": "Ben"
            },
            {
                "last_name": "Liang",
                "first_name": "Xin"
            },
            {
                "last_name": "Reshniak",
                "first_name": "Viktor"
            },
            {
                "last_name": "Banerjee",
                "first_name": "Tania"
            },
            {
                "last_name": "Lee",
                "first_name": "Jaemoon"
            },
            {
                "last_name": "Rangarajan",
                "first_name": "Anand"
            },
            {
                "last_name": "Wan",
                "first_name": "Lipeng"
            },
            {
                "last_name": "Vidal",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Liu",
                "first_name": "Qing"
            },
            {
                "last_name": "Gainaru",
                "first_name": "Ana"
            },
            {
                "last_name": "Podhorszki",
                "first_name": "Norbert"
            },
            {
                "last_name": "Archibald",
                "first_name": "Richard"
            },
            {
                "last_name": "Ranka",
                "first_name": "Sanjay"
            },
            {
                "last_name": "Klasky",
                "first_name": "Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  We describe MGARD, a software providing MultiGrid Adaptive Reduction forfloating-point scientific data on structured and unstructured grids. Withexceptional data compression capability and precise error control, MGARDaddresses a wide range of requirements, including storage reduction,high-performance I/O, and in-situ data analysis. It features a unifiedapplication programming interface (API) that seamlessly operates across diversecomputing architectures. MGARD has been optimized with highly-tuned GPU kernelsand efficient memory and device management mechanisms, ensuring scalable andrapid operations.",
        "title": "MGARD: A multigrid framework for high-performance, error-controlled data  compression and refactoring",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05995",
        "abstract_url": "http://arxiv.org/abs/2401.05995",
        "authors": [
            {
                "last_name": "Dasgupta",
                "first_name": "Sankarshan"
            },
            {
                "last_name": "Buckley",
                "first_name": "James"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  In this new digital era, accessibility to real-world events is moving towardsweb-based modules. This is mostly visible on e-commerce websites where there islimited availability of physical verification. With this unforeseendevelopment, we depend on the verification in the virtual world to influenceour decisions. One of the decision making process is deeply based on reviewreading. Reviews play an important part in this transactional process. Andseeking a real review can be very tenuous work for the user. On the other hand,fake review heavily impacts these transaction records of a product. The articlepresents an implementation of a Siamese network for detecting fake reviews. Thefake reviews dataset, consisting of 40K reviews, preprocessed with differenttechniques. The cleaned data is passed through embeddings generated by MiniLMBERT for contextual relationship and Word2Vec for semantic relationship to formvectors. Further, the embeddings are trained in a Siamese network with LSTMlayers connected to fuzzy logic for decision-making. The results show that fakereviews can be detected with high accuracy on a siamese network for predictionand verification.",
        "title": "A Multi-Embedding Convergence Network on Siamese Architecture for Fake  Reviews",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05998",
        "abstract_url": "http://arxiv.org/abs/2401.05998",
        "authors": [
            {
                "last_name": "Chern",
                "first_name": "Steffi"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Andy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  While state-of-the-art language models have achieved impressive results, theyremain susceptible to inference-time adversarial attacks, such as adversarialprompts generated by red teams arXiv:2209.07858. One approach proposed toimprove the general quality of language model generations is multi-agentdebate, where language models self-evaluate through discussion and feedbackarXiv:2305.14325. We implement multi-agent debate between currentstate-of-the-art language models and evaluate models' susceptibility to redteam attacks in both single- and multi-agent settings. We find that multi-agentdebate can reduce model toxicity when jailbroken or less capable models areforced to debate with non-jailbroken or more capable models. We also findmarginal improvements through the general usage of multi-agent interactions. Wefurther perform adversarial prompt content classification via embeddingclustering, and analyze the susceptibility of different models to differenttypes of attack topics.",
        "title": "Combating Adversarial Attacks with Multi-Agent Debate",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05999",
        "abstract_url": "http://arxiv.org/abs/2401.05999",
        "authors": [
            {
                "last_name": "Margarido",
                "first_name": "Solange"
            },
            {
                "last_name": "Roque",
                "first_name": "Lic\u00ednio"
            },
            {
                "last_name": "Machado",
                "first_name": "Penousal"
            },
            {
                "last_name": "Martins",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In recent years, there has been a growing application of mixed-initiativeco-creative approaches in the creation of video games. The rapid advances inthe capabilities of artificial intelligence (AI) systems further propelcreative collaboration between humans and computational agents. In thistutorial, we present guidelines for researchers and practitioners to developgame design tools with a high degree of mixed-initiative co-creativity(MI-CCy). We begin by reviewing a selection of current works that will serve ascase studies and categorize them by the type of game content they address. Weintroduce the MI-CCy Quantifier, a framework that can be used by researchersand developers to assess co-creative tools on their level of MI-CCy through avisual scheme of quantifiable criteria scales. We demonstrate the usage of theMI-CCy Quantifier by applying it to the selected works. This analysis enabledus to discern prevalent patterns within these tools, as well as features thatcontribute to a higher level of MI-CCy. We highlight current gaps in MI-CCyapproaches within game design, which we propose as pivotal aspects to tackle inthe development of forthcoming approaches.",
        "title": "Boosting Mixed-Initiative Co-Creativity in Game Design: A Tutorial",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06000",
        "abstract_url": "http://arxiv.org/abs/2401.06000",
        "authors": [
            {
                "last_name": "Bian",
                "first_name": "Sizhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Lukowicz",
                "first_name": "Paul"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Due to the fact that roughly sixty percent of the human body is essentiallycomposed of water, the human body is inherently a conductive object, being ableto, firstly, form an inherent electric field from the body to the surroundingsand secondly, deform the distribution of an existing electric field near thebody. Body-area capacitive sensing, also called body-area electric fieldsensing, is becoming a promising alternative for wearable devices to accomplishcertain tasks in human activity recognition and human-computer interaction.Over the last decade, researchers have explored plentiful novel sensing systemsbacked by the body-area electric field. On the other hand, despite thepervasive exploration of the body-area electric field, a comprehensive surveydoes not exist for an enlightening guideline. Moreover, the various hardwareimplementations, applied algorithms, and targeted applications result in achallenging task to achieve a systematic overview of the subject. This paperaims to fill in the gap by comprehensively summarizing the existing works onbody-area capacitive sensing so that researchers can have a better view of thecurrent exploration status. To this end, we first sorted the explorations intothree domains according to the involved body forms: body-part electric field,whole-body electric field, and body-to-body electric field, and enumerated thestate-of-art works in the domains with a detailed survey of the backed sensingtricks and targeted applications. We then summarized the three types of sensingfrontends in circuit design, which is the most critical part in body-areacapacitive sensing, and analyzed the data processing pipeline categorized intothree kinds of approaches. Finally, we described the challenges and outlooks ofbody-area electric sensing.",
        "title": "Body-Area Capacitive or Electric Field Sensing for Human Activity  Recognition and Human-Computer Interaction: A Comprehensive Survey",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06003",
        "abstract_url": "http://arxiv.org/abs/2401.06003",
        "authors": [
            {
                "last_name": "Franke",
                "first_name": "Linus"
            },
            {
                "last_name": "R\u00fcckert",
                "first_name": "Darius"
            },
            {
                "last_name": "Fink",
                "first_name": "Laura"
            },
            {
                "last_name": "Stamminger",
                "first_name": "Marc"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "",
            ""
        ],
        "abstract": "  Point-based radiance field rendering has demonstrated impressive results fornovel view synthesis, offering a compelling blend of rendering quality andcomputational efficiency. However, also latest approaches in this domain arenot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.2023] struggles when tasked with rendering highly detailed scenes, due toblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]can accommodate crisper images, but the neural reconstruction network decreasesperformance, it grapples with temporal instability and it is unable toeffectively address large gaps in the point cloud.  In this paper, we present TRIPS (Trilinear Point Splatting), an approach thatcombines ideas from both Gaussian Splatting and ADOP. The fundamental conceptbehind our novel technique involves rasterizing points into a screen-spaceimage pyramid, with the selection of the pyramid layer determined by theprojected point size. This approach allows rendering arbitrarily large pointsusing a single trilinear write. A lightweight neural network is then used toreconstruct a hole-free image including detail beyond splat resolution.Importantly, our render pipeline is entirely differentiable, allowing forautomatic optimization of both point sizes and positions.  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-artmethods in terms of rendering quality while maintaining a real-time frame rateof 60 frames per second on readily available hardware. This performance extendsto challenging scenarios, such as scenes featuring intricate geometry,expansive landscapes, and auto-exposed footage.",
        "title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06005",
        "abstract_url": "http://arxiv.org/abs/2401.06005",
        "authors": [
            {
                "last_name": "Peters",
                "first_name": "Benjamin"
            },
            {
                "last_name": "DiCarlo",
                "first_name": "James J."
            },
            {
                "last_name": "Gureckis",
                "first_name": "Todd"
            },
            {
                "last_name": "Haefner",
                "first_name": "Ralf"
            },
            {
                "last_name": "Isik",
                "first_name": "Leyla"
            },
            {
                "last_name": "Tenenbaum",
                "first_name": "Joshua"
            },
            {
                "last_name": "Konkle",
                "first_name": "Talia"
            },
            {
                "last_name": "Naselaris",
                "first_name": "Thomas"
            },
            {
                "last_name": "Stachenfeld",
                "first_name": "Kimberly"
            },
            {
                "last_name": "Tavares",
                "first_name": "Zenna"
            },
            {
                "last_name": "Tsao",
                "first_name": "Doris"
            },
            {
                "last_name": "Yildirim",
                "first_name": "Ilker"
            },
            {
                "last_name": "Kriegeskorte",
                "first_name": "Nikolaus"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Vision is widely understood as an inference problem. However, two contrastingconceptions of the inference process have each been influential in research onbiological vision as well as the engineering of machine vision. The firstemphasizes bottom-up signal flow, describing vision as a largely feedforward,discriminative inference process that filters and transforms the visualinformation to remove irrelevant variation and represent behaviorally relevantinformation in a format suitable for downstream functions of cognition andbehavioral control. In this conception, vision is driven by the sensory data,and perception is direct because the processing proceeds from the data to thelatent variables of interest. The notion of \"inference\" in this conception isthat of the engineering literature on neural networks, where feedforwardconvolutional neural networks processing images are said to perform inference.The alternative conception is that of vision as an inference process inHelmholtz's sense, where the sensory evidence is evaluated in the context of agenerative model of the causal processes giving rise to it. In this conception,vision inverts a generative model through an interrogation of the evidence in aprocess often thought to involve top-down predictions of sensory data toevaluate the likelihood of alternative hypotheses. The authors includescientists rooted in roughly equal numbers in each of the conceptions andmotivated to overcome what might be a false dichotomy between them and engagethe other perspective in the realm of theory and experiment. The primate brainemploys an unknown algorithm that may combine the advantages of bothconceptions. We explain and clarify the terminology, review the key empiricalevidence, and propose an empirical research program that transcends thedichotomy and sets the stage for revealing the mysterious hybrid algorithm ofprimate vision.",
        "title": "How does the primate brain combine generative and discriminative  computations in vision?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06009",
        "abstract_url": "http://arxiv.org/abs/2401.06009",
        "authors": [
            {
                "last_name": "Rogers",
                "first_name": "Martin S J"
            },
            {
                "last_name": "Fox",
                "first_name": "Maria"
            },
            {
                "last_name": "Fleming",
                "first_name": "Andrew"
            },
            {
                "last_name": "van Zeeland",
                "first_name": "Louisa"
            },
            {
                "last_name": "Wilkinson",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Hosking",
                "first_name": "J. Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            ""
        ],
        "abstract": "  Synthetic Aperture Radar (SAR) imagery is the primary data type used for seaice mapping due to its spatio-temporal coverage and the ability to detect seaice independent of cloud and lighting conditions. Automatic sea ice detectionusing SAR imagery remains problematic due to the presence of ambiguous signaland noise within the image. Conversely, ice and water are easilydistinguishable using multispectral imagery (MSI), but in the polar regions theocean's surface is often occluded by cloud or the sun may not appear above thehorizon for many months. To address some of these limitations, this paperproposes a new tool trained using concurrent multispectral Visible and SARimagery for sea Ice Detection (ViSual\\_IceD). ViSual\\_IceD is a convolutionneural network (CNN) that builds on the classic U-Net architecture bycontaining two parallel encoder stages, enabling the fusion and concatenationof MSI and SAR imagery containing different spatial resolutions. Theperformance of ViSual\\_IceD is compared with U-Net models trained usingconcatenated MSI and SAR imagery as well as models trained exclusively on MSIor SAR imagery. ViSual\\_IceD outperforms the other networks, with a F1 score1.60\\% points higher than the next best network, and results indicate thatViSual\\_IceD is selective in the image type it uses during image segmentation.Outputs from ViSual\\_IceD are compared to sea ice concentration productsderived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight howViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularlyin coastal regions. As the spatial-temporal coverage of MSI and SAR imagerycontinues to increase, ViSual\\_IceD provides a new opportunity for robust,accurate sea ice coverage detection in polar regions.",
        "title": "Sea ice detection using concurrent multispectral and synthetic aperture  radar imagery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06010",
        "abstract_url": "http://arxiv.org/abs/2401.06010",
        "authors": [
            {
                "last_name": "del Amor",
                "first_name": "Roc\u00edo"
            },
            {
                "last_name": "Silva-Rodr\u00edguez",
                "first_name": "Julio"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The development of computer vision solutions for gigapixel images in digitalpathology is hampered by significant computational limitations due to the largesize of whole slide images. In particular, digitizing biopsies at highresolutions is a time-consuming process, which is necessary due to theworsening results from the decrease in image detail. To alleviate this issue,recent literature has proposed using knowledge distillation to enhance themodel performance at reduced image resolutions. In particular, soft labels andfeatures extracted at the highest magnification level are distilled into amodel that takes lower-magnification images as input. However, this approachfails to transfer knowledge about the most discriminative image regions in theclassification process, which may be lost when the resolution is decreased. Inthis work, we propose to distill this information by incorporating attentionmaps during training. In particular, our formulation leverages saliency maps ofthe target class via grad-CAMs, which guides the lower-resolution Student modelto match the Teacher distribution by minimizing the l2 distance between them.Comprehensive experiments on prostate histology image grading demonstrate thatthe proposed approach substantially improves the model performance acrossdifferent image resolutions compared to previous literature.",
        "title": "Attention to detail: inter-resolution knowledge distillation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06013",
        "abstract_url": "http://arxiv.org/abs/2401.06013",
        "authors": [
            {
                "last_name": "Beilei",
                "first_name": "Cui"
            },
            {
                "last_name": "Mobarakol",
                "first_name": "Islam"
            },
            {
                "last_name": "Long",
                "first_name": "Bai"
            },
            {
                "last_name": "Hongliang",
                "first_name": "Ren"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,surgical navigation and augmented reality visualization. Although thefoundation model exhibits outstanding performance in many vision tasks,including depth estimation (e.g., DINOv2), recent works observed itslimitations in medical and surgical domain-specific applications. This workpresents a low-ranked adaptation (LoRA) of the foundation model for surgicaldepth estimation. Methods: We design a foundation model-based depth estimationmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 fordepth estimation in endoscopic surgery. We build LoRA layers and integrate theminto DINO to adapt with surgery-specific domain knowledge instead ofconventional fine-tuning. During training, we freeze the DINO image encoder,which shows excellent visual representation capacity, and only optimize theLoRA layers and depth decoder to integrate features from the surgical scene.Results: Our model is extensively validated on a MICCAI challenge dataset ofSCARED, which is collected from da Vinci Xi endoscope surgery. We empiricallyshow that Surgical-DINO significantly outperforms all the state-of-the-artmodels in endoscopic depth estimation tasks. The analysis with ablation studieshas shown evidence of the remarkable effect of our LoRA layers and adaptation.Conclusion: Surgical-DINO shed some light on the successful adaptation of thefoundation models into the surgical domain for depth estimation. There is clearevidence in the results that zero-shot prediction on pre-trained weights incomputer vision datasets or naive fine-tuning is not sufficient to use thefoundation model in the surgical domain directly. Code is available athttps://github.com/BeileiCui/SurgicalDINO.",
        "title": "Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation  in Endoscopic Surgery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06019",
        "abstract_url": "http://arxiv.org/abs/2401.06019",
        "authors": [
            {
                "last_name": "Alonso",
                "first_name": "Pablo"
            },
            {
                "last_name": "de Gordoa",
                "first_name": "Jon Ander I\u00f1iguez"
            },
            {
                "last_name": "Ortega",
                "first_name": "Juan Diego"
            },
            {
                "last_name": "Garc\u00eda",
                "first_name": "Sara"
            },
            {
                "last_name": "Iriarte",
                "first_name": "Francisco Javier"
            },
            {
                "last_name": "Nieto",
                "first_name": "Marcos"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Runway and taxiway pavements are exposed to high stress during theirprojected lifetime, which inevitably leads to a decrease in their conditionover time. To make sure airport pavement condition ensure uninterrupted andresilient operations, it is of utmost importance to monitor their condition andconduct regular inspections. UAV-based inspection is recently gainingimportance due to its wide range monitoring capabilities and reduced cost. Inthis work, we propose a vision-based approach to automatically identifypavement distress using images captured by UAVs. The proposed method is basedon Deep Learning (DL) to segment defects in the image. The DL architectureleverages the low computational capacities of embedded systems in UAVs by usingan optimised implementation of EfficientNet feature extraction and FeaturePyramid Network segmentation. To deal with the lack of annotated data fortraining we have developed a synthetic dataset generation methodology to extendavailable distress datasets. We demonstrate that the use of a mixed datasetcomposed of synthetic and real training images yields better results whentesting the training models in real application scenarios.",
        "title": "Automatic UAV-based Airport Pavement Inspection Using Mixed Real and  Virtual Scenarios",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06021",
        "abstract_url": "http://arxiv.org/abs/2401.06021",
        "authors": [
            {
                "last_name": "de Groot",
                "first_name": "Oscar"
            },
            {
                "last_name": "Ferranti",
                "first_name": "Laura"
            },
            {
                "last_name": "Gavrila",
                "first_name": "Dariu"
            },
            {
                "last_name": "Alonso-Mora",
                "first_name": "Javier"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Ground robots navigating in complex, dynamic environments must computecollision-free trajectories to avoid obstacles safely and efficiently.Nonconvex optimization is a popular method to compute a trajectory inreal-time. However, these methods often converge to locally optimal solutionsand frequently switch between different local minima, leading to inefficientand unsafe robot motion. In this work, We propose a novel topology-driventrajectory optimization strategy for dynamic environments that plans multipledistinct evasive trajectories to enhance the robot's behavior and efficiency. Aglobal planner iteratively generates trajectories in distinct homotopy classes.These trajectories are then optimized by local planners working in parallel.While each planner shares the same navigation objectives, they are locallyconstrained to a specific homotopy class, meaning each local planner attempts adifferent evasive maneuver. The robot then executes the feasible trajectorywith the lowest cost in a receding horizon manner. We demonstrate, on a mobilerobot navigating among pedestrians, that our approach leads to faster and safertrajectories than existing planners.",
        "title": "Topology-Driven Parallel Trajectory Optimization in Dynamic Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06030",
        "abstract_url": "http://arxiv.org/abs/2401.06030",
        "authors": [
            {
                "last_name": "Sheng",
                "first_name": "Lijun"
            },
            {
                "last_name": "Liang",
                "first_name": "Jian"
            },
            {
                "last_name": "He",
                "first_name": "Ran"
            },
            {
                "last_name": "Wang",
                "first_name": "Zilei"
            },
            {
                "last_name": "Tan",
                "first_name": "Tieniu"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Model adaptation tackles the distribution shift problem with a pre-trainedmodel instead of raw data, becoming a popular paradigm due to its great privacyprotection. Existing methods always assume adapting to a clean target domain,overlooking the security risks of unlabeled samples. In this paper, we explorethe potential backdoor attacks on model adaptation launched by well-designedpoisoning target data. Concretely, we provide two backdoor triggers with twopoisoning strategies for different prior knowledge owned by attackers. Theseattacks achieve a high success rate and keep the normal performance on cleansamples in the test stage. To defend against backdoor embedding, we propose aplug-and-play method named MixAdapt, combining it with existing adaptationalgorithms. Experiments across commonly used benchmarks and adaptation methodsdemonstrate the effectiveness of MixAdapt. We hope this work will shed light onthe safety of learning with unlabeled data.",
        "title": "Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and  Defense on Model Adaptation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06031",
        "abstract_url": "http://arxiv.org/abs/2401.06031",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Huaming"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiayu"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhibo"
            },
            {
                "last_name": "Choo",
                "first_name": "Kim-Kwang Raymond"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Adversarial generative models, such as Generative Adversarial Networks(GANs), are widely applied for generating various types of data, i.e., images,text, and audio. Accordingly, its promising performance has led to theGAN-based adversarial attack methods in the white-box and black-box attackscenarios. The importance of transferable black-box attacks lies in theirability to be effective across different models and settings, more closelyaligning with real-world applications. However, it remains challenging toretain the performance in terms of transferable adversarial examples for suchmethods. Meanwhile, we observe that some enhanced gradient-based transferableadversarial attack algorithms require prolonged time for adversarial samplegeneration. Thus, in this work, we propose a novel algorithm named GE-AdvGAN toenhance the transferability of adversarial samples whilst improving thealgorithm's efficiency. The main approach is via optimising the trainingprocess of the generator parameters. With the functional and characteristicsimilarity analysis, we introduce a novel gradient editing (GE) mechanism andverify its feasibility in generating transferable samples on various models.Moreover, by exploring the frequency domain information to determine thegradient editing direction, GE-AdvGAN can generate highly transferableadversarial samples while minimizing the execution time in comparison to thestate-of-the-art transferable adversarial attack algorithms. The performance ofGE-AdvGAN is comprehensively evaluated by large-scale experiments on differentdatasets, which results demonstrate the superiority of our algorithm. The codefor our algorithm is available at: https://github.com/LMBTough/GE-advGAN",
        "title": "GE-AdvGAN: Improving the transferability of adversarial samples by  gradient editing-based adversarial generative model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06034",
        "abstract_url": "http://arxiv.org/abs/2401.06034",
        "authors": [
            {
                "last_name": "Adilazuarda",
                "first_name": "Muhammad Farid"
            },
            {
                "last_name": "Cahyawijaya",
                "first_name": "Samuel"
            },
            {
                "last_name": "Aji",
                "first_name": "Alham Fikri"
            },
            {
                "last_name": "Winata",
                "first_name": "Genta Indra"
            },
            {
                "last_name": "Purwarianti",
                "first_name": "Ayu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pretrained language models (PLMs) have shown remarkable generalization towardmultiple tasks and languages. Nonetheless, the generalization of PLMs towardsunseen languages is poor, resulting in significantly worse languageperformance, or even generating nonsensical responses that are comparable to arandom baseline. This limitation has been a longstanding problem of PLMsraising the problem of diversity and equal access to language modelingtechnology. In this work, we solve this limitation by introducing LinguAlchemy,a regularization technique that incorporates various aspects of languagescovering typological, geographical, and phylogenetic constraining the resultingrepresentation of PLMs to better characterize the corresponding linguisticsconstraints. LinguAlchemy significantly improves the accuracy performance ofmBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared tofully finetuned models and displaying a high degree of unseen languagegeneralization. We further introduce AlchemyScale and AlchemyTune, extension ofLinguAlchemy which adjusts the linguistic regularization weights automatically,alleviating the need for hyperparameter search. LinguAlchemy enables bettercross-lingual generalization to unseen languages which is vital for betterinclusivity and accessibility of PLMs.",
        "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen  Language Generalization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06035",
        "abstract_url": "http://arxiv.org/abs/2401.06035",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Partha"
            },
            {
                "last_name": "Sanyal",
                "first_name": "Soubhik"
            },
            {
                "last_name": "Schmid",
                "first_name": "Cordelia"
            },
            {
                "last_name": "Sch\u00f6lkopf",
                "first_name": "Bernhard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We present a novel unconditional video generative model designed to addresslong-term spatial and temporal dependencies. To capture these dependencies, ourapproach incorporates a hybrid explicit-implicit tri-plane representationinspired by 3D-aware generative frameworks developed for three-dimensionalobject representation and employs a singular latent code to model an entirevideo sequence. Individual video frames are then synthesized from anintermediate tri-plane representation, which itself is derived from the primarylatent code. This novel strategy reduces computational complexity by a factorof $2$ as measured in FLOPs. Consequently, our approach facilitates theefficient and temporally coherent generation of videos. Moreover, our jointframe modeling approach, in contrast to autoregressive methods, mitigates thegeneration of visual artifacts. We further enhance the model's capabilities byintegrating an optical flow-based module within our Generative AdversarialNetwork (GAN) based generator architecture, thereby compensating for theconstraints imposed by a smaller generator size. As a result, our model iscapable of synthesizing high-fidelity video clips at a resolution of$256\\times256$ pixels, with durations extending to more than $5$ seconds at aframe rate of 30 fps. The efficacy and versatility of our approach areempirically validated through qualitative and quantitative assessments acrossthree different datasets comprising both synthetic and real video clips.",
        "title": "RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane  Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06040",
        "abstract_url": "http://arxiv.org/abs/2401.06040",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Qipeng"
            },
            {
                "last_name": "Mallick",
                "first_name": "Tanwi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traffic forecasting is the foundation for intelligent transportation systems.Spatiotemporal graph neural networks have demonstrated state-of-the-artperformance in traffic forecasting. However, these methods do not explicitlymodel some of the natural characteristics in traffic data, such as themultiscale structure that encompasses spatial and temporal variations atdifferent levels of granularity or scale. To that end, we propose aWavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combinesmultiscale analysis (MSA)-based method with Deep Learning (DL)-based method. InWavGCRN, the traffic data is decomposed into time-frequency components withDiscrete Wavelet Transformation (DWT), constructing a multi-stream inputstructure; then Graph Convolutional Recurrent networks (GCRNs) are employed asencoders for each stream, extracting spatiotemporal features in differentscales; and finally the learnable Inversed DWT and GCRN are combined as thedecoder, fusing the information from all streams for traffic metricsreconstruction and prediction. Furthermore, road-network-informed graphs anddata-driven graph learning are combined to accurately capture spatialcorrelation. The proposed method can offer well-defined interpretability,powerful learning capability, and competitive forecasting performance onreal-world traffic data sets.",
        "title": "Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for  Traffic Forecasting",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06044",
        "abstract_url": "http://arxiv.org/abs/2401.06044",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Xun"
            },
            {
                "last_name": "Beillahi",
                "first_name": "Sidi Mohamed"
            },
            {
                "last_name": "Minwalla",
                "first_name": "Cyrus"
            },
            {
                "last_name": "Du",
                "first_name": "Han"
            },
            {
                "last_name": "Veneris",
                "first_name": "Andreas"
            },
            {
                "last_name": "Long",
                "first_name": "Fan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  This paper presents OVer, a framework designed to automatically analyze thebehavior of decentralized finance (DeFi) protocols when subjected to a \"skewed\"oracle input. OVer firstly performs symbolic analysis on the given contract andconstructs a model of constraints. Then, the framework leverages an SMT solverto identify parameters that allow its secure operation. Furthermore, guardstatements may be generated for smart contracts that may use the oracle values,thus effectively preventing oracle manipulation attacks. Empirical results showthat OVer can successfully analyze all 10 benchmarks collected, which encompassa diverse range of DeFi protocols. Additionally, this paper also illustratesthat current parameters utilized in the majority of benchmarks are inadequateto ensure safety when confronted with significant oracle deviations.",
        "title": "Safeguarding DeFi Smart Contracts against Oracle Deviations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06047",
        "abstract_url": "http://arxiv.org/abs/2401.06047",
        "authors": [
            {
                "last_name": "Agarwal",
                "first_name": "Pankaj K."
            },
            {
                "last_name": "Raychaudhury",
                "first_name": "Rahul"
            },
            {
                "last_name": "Sintos",
                "first_name": "Stavros"
            },
            {
                "last_name": "Yang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DB"
        ],
        "abstract": "  We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each$R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in[0,1]$ denotes its \\emph{selectivity}. The goal is to compute a small-size\\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots,(q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in [0,1]$ for each $1\\leq j\\leq m$,and $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most\\emph{consistent} with $\\mathcal{Z}$, i.e.,$\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\!\\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized. In adatabase setting, $\\mathcal{Z}$ corresponds to a workload of range queries oversome table, together with their observed selectivities (i.e., fraction oftuples returned), and $\\mathcal{D}$ can be used as compact model forapproximating the data distribution within the table without accessing theunderlying contents.  In this paper, we obtain both upper and lower bounds for this problem. Inparticular, we show that the problem of finding the best data distribution fromselectivity queries is $\\mathsf{NP}$-complete. On the positive side, wedescribe a Monte Carlo algorithm that constructs, in time$O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discretedistribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that$\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq\\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for$p=1,2,\\infty$) where the minimum is taken over all discrete distributions. Wealso establish conditional lower bounds, which strongly indicate theinfeasibility of relative approximations as well as removal of the exponentialdependency on the dimension for additive approximations. This suggests thatsignificant improvements to our algorithm are unlikely.",
        "title": "Computing Data Distribution from Query Selectivities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06048",
        "abstract_url": "http://arxiv.org/abs/2401.06048",
        "authors": [
            {
                "last_name": "Guettala",
                "first_name": "Walid"
            },
            {
                "last_name": "Guly\u00e1s",
                "first_name": "L\u00e1szl\u00f3"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            "LG"
        ],
        "abstract": "  This paper studies four Graph Neural Network architectures (GNNs) for a graphclassification task on a synthetic dataset created using classic generativemodels of Network Science. Since the synthetic networks do not contain (node oredge) features, five different augmentation strategies (artificial featuretypes) are applied to nodes. All combinations of the 4 GNNs (GCN withHierarchical and Global aggregation, GIN and GATv2) and the 5 feature types(constant 1, noise, degree, normalized degree and ID -- a vector of the numberof cycles of various lengths) are studied and their performances compared as afunction of the hidden dimension of artificial neural networks used in theGNNs. The generalisation ability of these models is also analysed using asecond synthetic network dataset (containing networks of different sizes).Ourresults point towards the balanced importance of the computational power of theGNN architecture and the the information level provided by the artificialfeatures. GNN architectures with higher computational power, like GIN andGATv2, perform well for most augmentation strategies. On the other hand,artificial features with higher information content, like ID or degree, notonly consistently outperform other augmentation strategies, but can also helpGNN architectures with lower computational power to achieve good performance.",
        "title": "On the Power of Graph Neural Networks and Feature Augmentation  Strategies to Classify Social Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06052",
        "abstract_url": "http://arxiv.org/abs/2401.06052",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Guanjun"
            },
            {
                "last_name": "Yi",
                "first_name": "Taoran"
            },
            {
                "last_name": "Fang",
                "first_name": "Jiemin"
            },
            {
                "last_name": "Liu",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinggang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Neural Radiances Fields (NeRF) and their extensions have shown great successin representing 3D scenes and synthesizing novel-view images. However, mostNeRF methods take in low-dynamic-range (LDR) images, which may lose details,especially with nonuniform illumination. Some previous NeRF methods attempt tointroduce high-dynamic-range (HDR) techniques but mainly target static scenes.To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRFframework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D imagescaptured with various exposures. A learnable exposure mapping function isconstructed to obtain adaptive exposure values for each image. Based on themonotonically increasing prior, a camera response function is designed forstable learning. With the proposed model, high-quality novel-view images at anytime point can be rendered with any desired exposure. We further construct adataset containing multiple dynamic scenes captured with diverse exposures forevaluation. All the datasets and code are available at\\url{https://guanjunwu.github.io/HDR-HexPlane/}.",
        "title": "Fast High Dynamic Range Radiance Fields for Dynamic Scenes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06056",
        "abstract_url": "http://arxiv.org/abs/2401.06056",
        "authors": [
            {
                "last_name": "Vecchio",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Deschaintre",
                "first_name": "Valentin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBRmaterials. Materials are crucial components of virtual relightable assets,defining the interaction of light at the surface of geometries. Given theirimportance, significant research effort was dedicated to their representation,creation and acquisition. However, in the past 6 years, most research inmaterial acquisiton or generation relied either on the same unique dataset, oron company-owned huge library of procedural materials. With this dataset wepropose a significantly larger, more diverse, and higher resolution set ofmaterials than previously publicly available. We carefully discuss the datacollection process and demonstrate the benefits of this dataset on materialacquisition and generation applications. The complete data further containsmetadata with each material's origin, license, category, tags, creation methodand, when available, descriptions and physical size, as well as 3M+ renderingsof the augmented materials, in 1K, under various environment lightings. TheMatSynth dataset is released through the project page at:https://www.gvecchio.com/matsynth.",
        "title": "MatSynth: A Modern PBR Materials Dataset",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06059",
        "abstract_url": "http://arxiv.org/abs/2401.06059",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Minhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Ken Ziyu"
            },
            {
                "last_name": "Zhong",
                "first_name": "Ming"
            },
            {
                "last_name": "Schaeffer",
                "first_name": "Rylan"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Siru"
            },
            {
                "last_name": "Han",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Koyejo",
                "first_name": "Sanmi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Language models pre-trained on web-scale corpora demonstrate impressivecapabilities on diverse downstream tasks. However, there is increasing concernwhether such capabilities might arise from evaluation datasets being includedin the pre-training corpus -- a phenomenon known as \\textit{data contamination}-- in a manner that artificially increases performance. There has been littleunderstanding of how this potential contamination might influence LMs'performance on downstream tasks. In this paper, we explore the impact of datacontamination at the pre-training stage by pre-training a series of GPT-2models \\textit{from scratch}. We highlight the effect of both textcontamination (\\textit{i.e.}\\ input text of the evaluation samples) andground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input andthe desired outputs) from evaluation data. We also investigate the effects ofrepeating contamination for various downstream tasks. Additionally, we examinethe prevailing n-gram-based definitions of contamination within current LLMreports, pinpointing their limitations and inadequacy. Our findings offer newinsights into data contamination's effects on language model capabilities andunderscore the need for independent, comprehensive contamination assessments inLLM studies.",
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06066",
        "abstract_url": "http://arxiv.org/abs/2401.06066",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Damai"
            },
            {
                "last_name": "Deng",
                "first_name": "Chengqi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chenggang"
            },
            {
                "last_name": "Xu",
                "first_name": "R. X."
            },
            {
                "last_name": "Gao",
                "first_name": "Huazuo"
            },
            {
                "last_name": "Chen",
                "first_name": "Deli"
            },
            {
                "last_name": "Li",
                "first_name": "Jiashi"
            },
            {
                "last_name": "Zeng",
                "first_name": "Wangding"
            },
            {
                "last_name": "Yu",
                "first_name": "Xingkai"
            },
            {
                "last_name": "Wu",
                "first_name": "Y."
            },
            {
                "last_name": "Xie",
                "first_name": "Zhenda"
            },
            {
                "last_name": "Li",
                "first_name": "Y. K."
            },
            {
                "last_name": "Huang",
                "first_name": "Panpan"
            },
            {
                "last_name": "Luo",
                "first_name": "Fuli"
            },
            {
                "last_name": "Ruan",
                "first_name": "Chong"
            },
            {
                "last_name": "Sui",
                "first_name": "Zhifang"
            },
            {
                "last_name": "Liang",
                "first_name": "Wenfeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the era of large language models, Mixture-of-Experts (MoE) is a promisingarchitecture for managing computational costs when scaling up model parameters.However, conventional MoE architectures like GShard, which activate the top-$K$out of $N$ experts, face challenges in ensuring expert specialization, i.e.each expert acquires non-overlapping and focused knowledge. In response, wepropose the DeepSeekMoE architecture towards ultimate expert specialization. Itinvolves two principal strategies: (1) finely segmenting the experts into $mN$ones and activating $mK$ from them, allowing for a more flexible combination ofactivated experts; (2) isolating $K_s$ experts as shared ones, aiming atcapturing common knowledge and mitigating redundancy in routed experts.Starting from a modest scale with 2B parameters, we demonstrate thatDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5times the expert parameters and computation. In addition, DeepSeekMoE 2B nearlyapproaches the performance of its dense counterpart with the same number oftotal parameters, which set the upper bound of MoE models. Subsequently, wescale up DeepSeekMoE to 16B parameters and show that it achieves comparableperformance with LLaMA2 7B, with only about 40% of computations. Further, ourpreliminary efforts to scale up DeepSeekMoE to 145B parameters consistentlyvalidate its substantial advantages over the GShard architecture, and show itsperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)of computations.",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06070",
        "abstract_url": "http://arxiv.org/abs/2401.06070",
        "authors": [
            {
                "last_name": "Jafarzadeh",
                "first_name": "Siavash"
            },
            {
                "last_name": "Silling",
                "first_name": "Stewart"
            },
            {
                "last_name": "Liu",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhongqiang"
            },
            {
                "last_name": "Yu",
                "first_name": "Yue"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Neural operators, which can act as implicit solution operators of hiddengoverning equations, have recently become popular tools for learning theresponses of complex real-world physical systems. Nevertheless, most neuraloperator applications have thus far been data-driven and neglect the intrinsicpreservation of fundamental physical laws in data. In this work, we introduce anovel integral neural operator architecture called the Peridynamic NeuralOperator (PNO) that learns a nonlocal constitutive law from data. This neuraloperator provides a forward model in the form of state-based peridynamics, withobjectivity and momentum balance laws automatically guaranteed. Asapplications, we demonstrate the expressivity and efficacy of our model inlearning complex material behaviors from both synthetic and experimental datasets. We show that, owing to its ability to capture complex responses, ourlearned neural operator achieves improved accuracy and efficiency compared tobaseline models that use predefined constitutive laws. Moreover, by preservingthe essential physical laws within the neural network architecture, the PNO isrobust in treating noisy data. The method shows generalizability to differentdomain configurations, external loadings, and discretizations.",
        "title": "Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model  for Complex Material Responses",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06071",
        "abstract_url": "http://arxiv.org/abs/2401.06071",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhaowei"
            },
            {
                "last_name": "Xu",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dong"
            },
            {
                "last_name": "Song",
                "first_name": "Hang"
            },
            {
                "last_name": "Cai",
                "first_name": "Yiqing"
            },
            {
                "last_name": "Qi",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ran"
            },
            {
                "last_name": "Pan",
                "first_name": "Junting"
            },
            {
                "last_name": "Li",
                "first_name": "Zefeng"
            },
            {
                "last_name": "Vu",
                "first_name": "Van Tu"
            },
            {
                "last_name": "Huang",
                "first_name": "Zhida"
            },
            {
                "last_name": "Wang",
                "first_name": "Tao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Multi-modal large language models have demonstrated impressive performanceacross various tasks in different modalities. However, existing multi-modalmodels primarily emphasize capturing global information within each modalitywhile neglecting the importance of perceiving local information acrossmodalities. Consequently, these models lack the ability to effectivelyunderstand the fine-grained details of input data, limiting their performancein tasks that require a more nuanced understanding. To address this limitation,there is a compelling need to develop models that enable fine-grainedunderstanding across multiple modalities, thereby enhancing their applicabilityto a wide range of tasks. In this paper, we propose LEGO, a language enhancedmulti-modal grounding model. Beyond capturing global information like othermulti-modal models, our proposed model excels at tasks demanding a detailedunderstanding of local information within the input. It demonstrates preciseidentification and localization of specific regions in images or moments invideos. To achieve this objective, we design a diversified dataset constructionpipeline, resulting in a multi-modal, multi-granularity dataset for modeltraining. The code, dataset, and demo of our model can be found at https://github.com/lzw-lzw/LEGO.",
        "title": "LEGO:Language Enhanced Multi-modal Grounding Model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06072",
        "abstract_url": "http://arxiv.org/abs/2401.06072",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Ruilin"
            },
            {
                "last_name": "Gu",
                "first_name": "Tianle"
            },
            {
                "last_name": "Li",
                "first_name": "Haoling"
            },
            {
                "last_name": "Li",
                "first_name": "Junzhe"
            },
            {
                "last_name": "Lin",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Li",
                "first_name": "Jiayi"
            },
            {
                "last_name": "Yang",
                "first_name": "Yujiu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Temporal Knowledge Graph Completion (TKGC) is a challenging task ofpredicting missing event links at future timestamps by leveraging establishedtemporal structural knowledge. Given the formidable generative capabilitiesinherent in LLMs (LLMs), this paper proposes a novel approach to conceptualizetemporal link prediction as an event generation task within the context of ahistorical event chain. We employ efficient fine-tuning methods to make LLMsadapt to specific graph textual information and patterns discovered in temporaltimelines. Furthermore, we introduce structure-based historical dataaugmentation and the integration of reverse knowledge to emphasize LLMs'awareness of structural information, thereby enhancing their reasoningcapabilities. We conduct thorough experiments on multiple widely used datasetsand find that our fine-tuned model outperforms existing embedding-based modelson multiple metrics, achieving SOTA results. We also carry out sufficientablation experiments to explore the key influencing factors when LLMs performstructured temporal knowledge inference tasks.",
        "title": "Chain of History: Learning and Forecasting with LLMs for Temporal  Knowledge Graph Completion",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06080",
        "abstract_url": "http://arxiv.org/abs/2401.06080",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Binghai"
            },
            {
                "last_name": "Zheng",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Lu"
            },
            {
                "last_name": "Liu",
                "first_name": "Yan"
            },
            {
                "last_name": "Dou",
                "first_name": "Shihan"
            },
            {
                "last_name": "Huang",
                "first_name": "Caishuang"
            },
            {
                "last_name": "Shen",
                "first_name": "Wei"
            },
            {
                "last_name": "Jin",
                "first_name": "Senjie"
            },
            {
                "last_name": "Zhou",
                "first_name": "Enyu"
            },
            {
                "last_name": "Shi",
                "first_name": "Chenyu"
            },
            {
                "last_name": "Gao",
                "first_name": "Songyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Nuo"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuhao"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiaoran"
            },
            {
                "last_name": "Xi",
                "first_name": "Zhiheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Ji",
                "first_name": "Tao"
            },
            {
                "last_name": "Yan",
                "first_name": "Hang"
            },
            {
                "last_name": "Shen",
                "first_name": "Lixing"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhan"
            },
            {
                "last_name": "Gui",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            },
            {
                "last_name": "Qiu",
                "first_name": "Xipeng"
            },
            {
                "last_name": "Huang",
                "first_name": "Xuanjing"
            },
            {
                "last_name": "Wu",
                "first_name": "Zuxuan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yu-Gang"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become a crucialtechnology for aligning language models with human values and intentions,enabling models to produce more helpful and harmless responses. Reward modelsare trained as proxies for human preferences to drive reinforcement learningoptimization. While reward models are often considered central to achievinghigh performance, they face the following challenges in practical applications:(1) Incorrect and ambiguous preference pairs in the dataset may hinder thereward model from accurately capturing human intent. (2) Reward models trainedon data from a specific distribution often struggle to generalize to examplesoutside that distribution and are not suitable for iterative RLHF training.  In this report, we attempt to address these two issues. (1) From a dataperspective, we propose a method to measure the strength of preferences withinthe data, based on a voting mechanism of multiple reward models. Experimentalresults confirm that data with varying preference strengths have differentimpacts on reward model performance. We introduce a series of novel methods tomitigate the influence of incorrect and ambiguous preferences in the datasetand fully leverage high-quality preference data. (2) From an algorithmicstandpoint, we introduce contrastive learning to enhance the ability of rewardmodels to distinguish between chosen and rejected responses, thereby improvingmodel generalization. Furthermore, we employ meta-learning to enable the rewardmodel to maintain the ability to differentiate subtle differences inout-of-distribution samples, and this approach can be utilized for iterativeRLHF optimization.",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06081",
        "abstract_url": "http://arxiv.org/abs/2401.06081",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kun"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Wan",
                "first_name": "Junchen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fuzheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Di"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Reinforcement learning (RL) has been widely used in training large languagemodels~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness anderrors. However, existing RL methods mostly adopt the instance-level reward,which is unable to provide fine-grained supervision for complex reasoningtasks, and can not focus on the few key tokens that lead to the incorrectness.To address it, we propose a new RL method named \\textbf{RLMEC} thatincorporates a generative model as the reward model, which is trained by theerroneous solution rewriting task under the minimum editing constraint, and canproduce token-level rewards for RL training. Based on the generative rewardmodel, we design the token-level RL objective for training and animitation-based regularization for stabilizing RL process. And the bothobjectives focus on the learning of the key tokens for the erroneous solution,reducing the effect of other unimportant tokens. The experiment results onmathematical tasks and question-answering tasks have demonstrated theeffectiveness of our approach. Our code and data are available at\\url{https://github.com/RUCAIBox/RLMEC}.",
        "title": "Improving Large Language Models via Fine-grained Reinforcement Learning  with Minimum Editing Constraint",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06085",
        "abstract_url": "http://arxiv.org/abs/2401.06085",
        "authors": [
            {
                "last_name": "Smaldore",
                "first_name": "Valentino"
            },
            {
                "last_name": "Zanella",
                "first_name": "Corrado"
            },
            {
                "last_name": "Zullo",
                "first_name": "Ferdinando"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  In this paper we will study the action of $\\mathbb{F}_{q^n}^{2 \\times 2}$ onthe graph of an $\\mathbb{F}_q$-linear function of $\\mathbb{F}_{q^n}$ intoitself. In particular we will see that, under certain combinatorialassumptions, its stabilizer (together with the sum and product of matrices) isa field. We will also see some examples for which this does not happen.Moreover, we will establish a connection between such a stabilizer and theright idealizer of the rank-metric code defined by the linear function and givesome structural results in the case in which the polynomials are partiallyscattered.",
        "title": "On the stabilizer of the graph of linear functions over finite fields",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06086",
        "abstract_url": "http://arxiv.org/abs/2401.06086",
        "authors": [
            {
                "last_name": "Terawong",
                "first_name": "Chawin"
            },
            {
                "last_name": "Cliff",
                "first_name": "Dave"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CE",
            "MA"
        ],
        "abstract": "  We present first results from the use of XGBoost, a highly effective machinelearning (ML) method, within the Bristol Betting Exchange (BBE), an open-sourceagent-based model (ABM) designed to simulate a contemporary sports-bettingexchange with in-play betting during track-racing events such as horse races.We use the BBE ABM and its array of minimally-simple bettor-agents as asynthetic data generator which feeds into our XGBoost ML system, with theintention that XGBoost discovers profitable dynamic betting strategies bylearning from the more profitable bets made by the BBE bettor-agents. Afterthis XGBoost training, which results in one or more decision trees, abettor-agent with a betting strategy determined by the XGBoost-learned decisiontree(s) is added to the BBE ABM and made to bet on a sequence of races undervarious conditions and betting-market scenarios, with profitability serving asthe primary metric of comparison and evaluation. Our initial findings presentedhere show that XGBoost trained in this way can indeed learn profitable bettingstrategies, and can generalise to learn strategies that outperform each of theset of strategies used for creation of the training data. To foster furtherresearch and enhancements, the complete version of our extended BBE, includingthe XGBoost integration, has been made freely available as an open-sourcerelease on GitHub.",
        "title": "XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an  Agent-Based Model of a Sports Betting Exchange",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06088",
        "abstract_url": "http://arxiv.org/abs/2401.06088",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "K M Sajjadul"
            },
            {
                "last_name": "Nipu",
                "first_name": "Ayesha Siddika"
            },
            {
                "last_name": "Madiraju",
                "first_name": "Praveen"
            },
            {
                "last_name": "Deshpande",
                "first_name": "Priya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  The Chief Complaint (CC) is a crucial component of a patient's medical recordas it describes the main reason or concern for seeking medical care. Itprovides critical information for healthcare providers to make informeddecisions about patient care. However, documenting CCs can be time-consumingfor healthcare providers, especially in busy emergency departments. To addressthis issue, an autocompletion tool that suggests accurate and well-formattedphrases or sentences for clinical notes can be a valuable resource for triagenurses. In this study, we utilized text generation techniques to developmachine learning models using CC data. In our proposed work, we train a LongShort-Term Memory (LSTM) model and fine-tune three different variants ofBiomedical Generative Pretrained Transformers (BioGPT), namelymicrosoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.Additionally, we tune a prompt by incorporating exemplar CC sentences,utilizing the OpenAI API of GPT-4. We evaluate the models' performance based onthe perplexity score, modified BERTScore, and cosine similarity score. Theresults show that BioGPT-Large exhibits superior performance compared to theother models. It consistently achieves a remarkably low perplexity score of1.65 when generating CC, whereas the baseline LSTM model achieves the bestperplexity score of 170. Further, we evaluate and assess the proposed models'performance and the outcome of GPT-4.0. Our study demonstrates that utilizingLLMs such as BioGPT, leads to the development of an effective autocompletiontool for generating CC documentation in healthcare settings.",
        "title": "Autocompletion of Chief Complaints in the Electronic Health Records  using Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06089",
        "abstract_url": "http://arxiv.org/abs/2401.06089",
        "authors": [
            {
                "last_name": "Sao",
                "first_name": "Piyush"
            },
            {
                "last_name": "Prokopenko",
                "first_name": "Andrey"
            },
            {
                "last_name": "Lebrun-Grandi\u00e9",
                "first_name": "Damien"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC",
            "",
            "",
            ""
        ],
        "abstract": "  This paper presents \\pandora, a novel parallel algorithm for efficientlyconstructing dendrograms for single-linkage hierarchical clustering, including\\hdbscan. Traditional dendrogram construction methods from a minimum spanningtree (MST), such as agglomerative or divisive techniques, often fail toefficiently parallelize, especially with skewed dendrograms common inreal-world data.  \\pandora addresses these challenges through a unique recursive treecontraction method, which simplifies the tree for initial dendrogramconstruction and then progressively reconstructs the complete dendrogram. Thisprocess makes \\pandora asymptotically work-optimal, independent of dendrogramskewness. All steps in \\pandora are fully parallel and suitable for massivelythreaded accelerators such as GPUs.  Our implementation is written in Kokkos, providing support for both CPUs andmulti-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \\pandora is2.2$\\times$ faster than the current best-multithreaded implementation, whilethe GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora. Theseadvancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over thecurrent best, which only offload MST construction to GPUs and performmultithreaded dendrogram construction.",
        "title": "PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage  Clustering on GPU",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06091",
        "abstract_url": "http://arxiv.org/abs/2401.06091",
        "authors": [
            {
                "last_name": "McDermott",
                "first_name": "Matthew B. A."
            },
            {
                "last_name": "Hansen",
                "first_name": "Lasse Hyldig"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Angelotti",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Gallifant",
                "first_name": "Jack"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In machine learning (ML), a widespread adage is that the area under theprecision-recall curve (AUPRC) is a superior metric for model comparison to thearea under the receiver operating characteristic (AUROC) for binaryclassification tasks with class imbalance. This paper challenges this notionthrough novel mathematical analysis, illustrating that AUROC and AUPRC can beconcisely related in probabilistic terms. We demonstrate that AUPRC, contraryto popular belief, is not superior in cases of class imbalance and might evenbe a harmful metric, given its inclination to unduly favor model improvementsin subpopulations with more frequent positive labels. This bias caninadvertently heighten algorithmic disparities. Prompted by these insights, athorough review of existing ML literature was conducted, utilizing largelanguage models to analyze over 1.5 million papers from arXiv. Ourinvestigation focused on the prevalence and substantiation of the purportedAUPRC superiority. The results expose a significant deficit in empiricalbacking and a trend of misattributions that have fuelled the widespreadacceptance of AUPRC's supposed advantages. Our findings represent a dualcontribution: a significant technical advancement in understanding metricbehaviors and a stark warning about unchecked assumptions in the ML community.All experiments are accessible athttps://github.com/mmcdermott/AUC_is_all_you_need.",
        "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06098",
        "abstract_url": "http://arxiv.org/abs/2401.06098",
        "authors": [
            {
                "last_name": "Bako",
                "first_name": "Laurent"
            },
            {
                "last_name": "Nadri",
                "first_name": "Madiha"
            },
            {
                "last_name": "Andrieu",
                "first_name": "Vincent"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper discusses a general framework for designing robust stateestimators for a class of discrete-time nonlinear systems. We consider systemsthat may be impacted by impulsive (sparse but otherwise arbitrary) measurementnoise sequences. We show that a family of state estimators, robust to this typeof undesired signal, can be obtained by minimizing a class of nonsmooth convexfunctions at each time step. The resulting state observers are defined throughproximal operators. We obtain a nonlinear implicit dynamical system in term ofestimation error and prove, in the noise-free setting, that it vanishesasymptotically when the minimized loss function and the to-beobserved systemenjoy appropriate properties. From a computational perspective, even though theproposed observers can be implemented via efficient numerical procedures, theydo not admit closed-form expressions. The paper argues that by adoptingappropriate relaxations, simple and fast analytic expressions can be derived.",
        "title": "Proximal observers for secure state estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06102",
        "abstract_url": "http://arxiv.org/abs/2401.06102",
        "authors": [
            {
                "last_name": "Ghandeharioun",
                "first_name": "Asma"
            },
            {
                "last_name": "Caciularu",
                "first_name": "Avi"
            },
            {
                "last_name": "Pearce",
                "first_name": "Adam"
            },
            {
                "last_name": "Dixon",
                "first_name": "Lucas"
            },
            {
                "last_name": "Geva",
                "first_name": "Mor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Inspecting the information encoded in hidden representations of largelanguage models (LLMs) can explain models' behavior and verify their alignmentwith human values. Given the capabilities of LLMs in generatinghuman-understandable text, we propose leveraging the model itself to explainits internal representations in natural language. We introduce a frameworkcalled Patchscopes and show how it can be used to answer a wide range ofresearch questions about an LLM's computation. We show that priorinterpretability methods based on projecting representations into thevocabulary space and intervening on the LLM computation, can be viewed asspecial instances of this framework. Moreover, several of their shortcomingssuch as failure in inspecting early layers or lack of expressivity can bemitigated by a Patchscope. Beyond unifying prior inspection techniques,Patchscopes also opens up new possibilities such as using a more capable modelto explain the representations of a smaller model, and unlocks new applicationssuch as self-correction in multi-hop reasoning.",
        "title": "Patchscope: A Unifying Framework for Inspecting Hidden Representations  of Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06104",
        "abstract_url": "http://arxiv.org/abs/2401.06104",
        "authors": [
            {
                "last_name": "Oren",
                "first_name": "Matanel"
            },
            {
                "last_name": "Hassid",
                "first_name": "Michael"
            },
            {
                "last_name": "Adi",
                "first_name": "Yossi"
            },
            {
                "last_name": "Schwartz",
                "first_name": "Roy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Transformers are considered conceptually different compared to the previousgeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).In this work, we demonstrate that decoder-only transformers can in fact beconceptualized as infinite multi-state RNNs - an RNN variant with unlimitedhidden state size. We further show that pretrained transformers can beconverted into $\\textit{finite}$ multi-state RNNs by fixing the size of theirhidden state. We observe that several existing transformers cache compressiontechniques can be framed as such conversion policies, and introduce a novelpolicy, TOVA, which is simpler compared to these policies. Our experiments withseveral long range tasks indicate that TOVA outperforms all other baselinepolicies, while being nearly on par with the full (infinite) model, and usingin some cases only $\\frac{1}{8}$ of the original cache size. Our resultsindicate that transformer decoder LLMs often behave in practice as RNNs. Theyalso lay out the option of mitigating one of their most painful computationalbottlenecks - the size of their cache memory. We publicly release our code athttps://github.com/schwartz-lab-NLP/TOVA.",
        "title": "Transformers are Multi-State RNNs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06105",
        "abstract_url": "http://arxiv.org/abs/2401.06105",
        "authors": [
            {
                "last_name": "Arar",
                "first_name": "Moab"
            },
            {
                "last_name": "Voynov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Hertz",
                "first_name": "Amir"
            },
            {
                "last_name": "Avrahami",
                "first_name": "Omri"
            },
            {
                "last_name": "Fruchter",
                "first_name": "Shlomi"
            },
            {
                "last_name": "Pritch",
                "first_name": "Yael"
            },
            {
                "last_name": "Cohen-Or",
                "first_name": "Daniel"
            },
            {
                "last_name": "Shamir",
                "first_name": "Ariel"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "GR",
            "LG"
        ],
        "abstract": "  Content creators often aim to create personalized images using personalsubjects that go beyond the capabilities of conventional text-to-image models.Additionally, they may want the resulting image to encompass a specificlocation, style, ambiance, and more. Existing personalization methods maycompromise personalization ability or the alignment to complex textual prompts.This trade-off can impede the fulfillment of user prompts and subject fidelity.We propose a new approach focusing on personalization methods for a\\emph{single} prompt to address this issue. We term our approach prompt-alignedpersonalization. While this may seem restrictive, our method excels inimproving text alignment, enabling the creation of images with complex andintricate prompts, which may pose a challenge for current techniques. Inparticular, our method keeps the personalized model aligned with a targetprompt using an additional score distillation sampling term. We demonstrate theversatility of our method in multi- and single-shot settings and further showthat it can compose multiple subjects or use inspiration from reference images,such as artworks. We compare our approach quantitatively and qualitatively withexisting baselines and state-of-the-art techniques.",
        "title": "PALP: Prompt Aligned Personalization of Text-to-Image Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06109",
        "abstract_url": "http://arxiv.org/abs/2401.06109",
        "authors": [
            {
                "last_name": "Szab\u00f3",
                "first_name": "D\u00e1niel"
            },
            {
                "last_name": "Apers",
                "first_name": "Simon"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DM",
            ""
        ],
        "abstract": "  We show that the graph property of having a (very) large $k$-th Betti number$\\beta_k$ for constant $k$ is testable with a constant number of queries in thedense graph model. More specifically, we consider a clique complex defined byan underlying graph and prove that for any $\\varepsilon>0$, there exists$\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta)d_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing$(k+2)$-clique-freeness, which is known to be testable. This complements aresult by Elek (2010) showing that Betti numbers are testable in thebounded-degree model. Our result combines the Euler characteristic, matroidtheory and the graph removal lemma.",
        "title": "Holey graphs: very large Betti numbers are testable",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06112",
        "abstract_url": "http://arxiv.org/abs/2401.06112",
        "authors": [
            {
                "last_name": "Yamagiwa",
                "first_name": "Hiroaki"
            },
            {
                "last_name": "Takase",
                "first_name": "Yusuke"
            },
            {
                "last_name": "Shimodaira",
                "first_name": "Hidetoshi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Word embedding is one of the most important components in natural languageprocessing, but interpreting high-dimensional embeddings remains a challengingproblem. To address this problem, Independent Component Analysis (ICA) isidentified as an effective solution. ICA-transformed word embeddings revealinterpretable semantic axes; however, the order of these axes are arbitrary. Inthis study, we focus on this property and propose a novel method, Axis Tour,which optimizes the order of the axes. Inspired by Word Tour, a one-dimensionalword embedding method, we aim to improve the clarity of the word embeddingspace by maximizing the semantic continuity of the axes. Furthermore, we showthrough experiments on downstream tasks that Axis Tour constructs betterlow-dimensional embeddings compared to both PCA and ICA.",
        "title": "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed  Embeddings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06116",
        "abstract_url": "http://arxiv.org/abs/2401.06116",
        "authors": [
            {
                "last_name": "Bolanos",
                "first_name": "Luis"
            },
            {
                "last_name": "Su",
                "first_name": "Shih-Yang"
            },
            {
                "last_name": "Rhodin",
                "first_name": "Helge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Neural character models can now reconstruct detailed geometry and texturefrom video, but they lack explicit shadows and shading, leading to artifactswhen generating novel views and poses or during relighting. It is particularlydifficult to include shadows as they are a global effect and the requiredcasting of secondary rays is costly. We propose a new shadow model using aGaussian density proxy that replaces sampling with a simple analytic formula.It supports dynamic motion and is tailored for shadow computation, therebyavoiding the affine projection approximation and sorting required by theclosely related Gaussian splatting. Combined with a deferred neural renderingmodel, our Gaussian shadows enable Lambertian shading and shadow casting withminimal overhead. We demonstrate improved reconstructions, with betterseparation of albedo, shading, and shadows in challenging outdoor scenes withdirect sun light and hard shadows. Our method is able to optimize the lightdirection without any input from the user. As a result, novel poses have fewershadow artifacts and relighting in novel scenes is more realistic compared tothe state-of-the-art methods, providing new ways to pose neural characters innovel environments, increasing their applicability.",
        "title": "Gaussian Shadow Casting for Neural Characters",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06118",
        "abstract_url": "http://arxiv.org/abs/2401.06118",
        "authors": [
            {
                "last_name": "Egiazarian",
                "first_name": "Vage"
            },
            {
                "last_name": "Panferov",
                "first_name": "Andrei"
            },
            {
                "last_name": "Kuznedelev",
                "first_name": "Denis"
            },
            {
                "last_name": "Frantar",
                "first_name": "Elias"
            },
            {
                "last_name": "Babenko",
                "first_name": "Artem"
            },
            {
                "last_name": "Alistarh",
                "first_name": "Dan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  The emergence of accurate open large language models (LLMs) has led to a racetowards quantization techniques for such models enabling execution on end-userdevices. In this paper, we revisit the problem of \"extreme\" LLMcompression--defined as targeting extremely low bit counts, such as 2 to 3 bitsper parameter, from the point of view of classic methods in Multi-CodebookQuantization (MCQ). Our work builds on top of Additive Quantization, a classicalgorithm from the MCQ family, and adapts it to the quantization of languagemodels. The resulting algorithm advances the state-of-the-art in LLMcompression, outperforming all recently-proposed techniques in terms ofaccuracy at a given compression budget. For instance, when compressing Llama 2models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93perplexity (a 1.29 improvement relative to the best prior work, and 1.81 pointsfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70Bmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release ourimplementation of Additive Quantization for Language Models AQLM as a baselineto facilitate future research in LLM quantization.",
        "title": "Extreme Compression of Large Language Models via Additive Quantization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06121",
        "abstract_url": "http://arxiv.org/abs/2401.06121",
        "authors": [
            {
                "last_name": "Maini",
                "first_name": "Pratyush"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhili"
            },
            {
                "last_name": "Schwarzschild",
                "first_name": "Avi"
            },
            {
                "last_name": "Lipton",
                "first_name": "Zachary C."
            },
            {
                "last_name": "Kolter",
                "first_name": "J. Zico"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Large language models trained on massive corpora of data from the web canmemorize and reproduce sensitive or private data raising both legal and ethicalconcerns. Unlearning, or tuning models to forget information present in theirtraining data, provides us with a way to protect private data after training.Although several methods exist for such unlearning, it is unclear to whatextent they result in models equivalent to those where the data to be forgottenwas never learned in the first place. To address this challenge, we presentTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepenour understanding of unlearning. We offer a dataset of 200 diverse syntheticauthor profiles, each consisting of 20 question-answer pairs, and a subset ofthese profiles called the forget set that serves as the target for unlearning.We compile a suite of metrics that work together to provide a holistic pictureof unlearning efficacy. Finally, we provide a set of baseline results fromexisting unlearning algorithms. Importantly, none of the baselines we considershow effective unlearning motivating continued efforts to develop approachesfor unlearning that effectively tune models so that they truly behave as ifthey were never trained on the forget data at all.",
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06122",
        "abstract_url": "http://arxiv.org/abs/2401.06122",
        "authors": [
            {
                "last_name": "Bareeva",
                "first_name": "Dilyara"
            },
            {
                "last_name": "H\u00f6hne",
                "first_name": "Marina M. -C."
            },
            {
                "last_name": "Warnecke",
                "first_name": "Alexander"
            },
            {
                "last_name": "Pirch",
                "first_name": "Lukas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Klaus-Robert"
            },
            {
                "last_name": "Rieck",
                "first_name": "Konrad"
            },
            {
                "last_name": "Bykov",
                "first_name": "Kirill"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Deep Neural Networks (DNNs) are capable of learning complex and versatilerepresentations, however, the semantic nature of the learned concepts remainsunknown. A common method used to explain the concepts learned by DNNs isActivation Maximization (AM), which generates a synthetic input signal thatmaximally activates a particular neuron in the network. In this paper, weinvestigate the vulnerability of this approach to adversarial modelmanipulations and introduce a novel method for manipulating featurevisualization without altering the model architecture or significantlyimpacting the model's decision-making process. We evaluate the effectiveness ofour method on several neural network models and demonstrate its capabilities tohide the functionality of specific neurons by masking the original explanationsof neurons with chosen target explanations during model auditing. As a remedy,we propose a protective measure against such manipulations and providequantitative evidence which substantiates our findings.",
        "title": "Manipulating Feature Visualizations with Gradient Slingshots",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06125",
        "abstract_url": "http://arxiv.org/abs/2401.06125",
        "authors": [
            {
                "last_name": "D\u00e6hli",
                "first_name": "Karen M."
            },
            {
                "last_name": "Obead",
                "first_name": "Sarah A"
            },
            {
                "last_name": "Lin",
                "first_name": "Hsuan-Yin"
            },
            {
                "last_name": "Rosnes",
                "first_name": "Eirik"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "IT"
        ],
        "abstract": "  In private computation, a user wishes to retrieve a function evaluation ofmessages stored on a set of databases without revealing the function's identityto the databases. Obead \\emph{et al.} introduced a capacity outer bound forprivate nonlinear computation, dependent on the order of the candidatefunctions. Focusing on private \\emph{quadratic monomial} computation, wepropose three methods for ordering candidate functions: a graph edge-coloringmethod, a graph-distance method, and an entropy-based greedy method. Weconfirm, via an exhaustive search, that all three methods yield an optimalordering for $f < 6$ messages. For $6 \\leq f \\leq 12$ messages, we numericallyevaluate the performance of the proposed methods compared with a directedrandom search. For almost all scenarios considered, the entropy-based greedymethod gives the smallest gap to the best-found ordering.",
        "title": "Improved Capacity Outer Bound for Private Quadratic Monomial Computation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06126",
        "abstract_url": "http://arxiv.org/abs/2401.06126",
        "authors": [
            {
                "last_name": "Saunders",
                "first_name": "Jack"
            },
            {
                "last_name": "Namboodiri",
                "first_name": "Vinay"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Visual dubbing is the process of generating lip motions of an actor in avideo to synchronise with given audio. Recent advances have made progresstowards this goal but have not been able to produce an approach suitable formass adoption. Existing methods are split into either person-generic orperson-specific models. Person-specific models produce results almostindistinguishable from reality but rely on long training times using largesingle-person datasets. Person-generic works have allowed for the visualdubbing of any video to any audio without further training, but these fail tocapture the person-specific nuances and often suffer from visual artefacts. Ourmethod, based on data-efficient neural rendering priors, overcomes thelimitations of existing approaches. Our pipeline consists of learning adeferred neural rendering prior network and actor-specific adaptation usingneural textures. This method allows for $\\textbf{high-quality visual dubbingwith just a few seconds of data}$, that enables video dubbing for any actor -from A-list celebrities to background actors. We show that we achievestate-of-the-art in terms of $\\textbf{visual quality}$ and$\\textbf{recognisability}$ both quantitatively, and qualitatively through twouser studies. Our prior learning and adaptation method $\\textbf{generalises tolimited data}$ better and is more $\\textbf{scalable}$ than existingperson-specific models. Our experiments on real-world, limited data scenariosfind that our model is preferred over all others. The project page may be foundat https://dubbingforeveryone.github.io/",
        "title": "Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural  Rendering Priors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06127",
        "abstract_url": "http://arxiv.org/abs/2401.06127",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Yifan"
            },
            {
                "last_name": "Zhan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Jin",
                "first_name": "Qing"
            },
            {
                "last_name": "Li",
                "first_name": "Yanyu"
            },
            {
                "last_name": "Idelbayev",
                "first_name": "Yerlan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xian"
            },
            {
                "last_name": "Zharkov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Aberman",
                "first_name": "Kfir"
            },
            {
                "last_name": "Tulyakov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanzhi"
            },
            {
                "last_name": "Ren",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  One highly promising direction for enabling flexible real-time on-deviceimage editing is utilizing data distillation by leveraging large-scaletext-to-image diffusion models, such as Stable Diffusion, to generate paireddatasets used for training generative adversarial networks (GANs). Thisapproach notably alleviates the stringent requirements typically imposed byhigh-end commercial GPUs for performing image editing with diffusion models.However, unlike text-to-image diffusion models, each distilled GAN isspecialized for a specific image editing task, necessitating costly trainingefforts to obtain models for various concepts. In this work, we introduce andaddress a novel research direction: can the process of distilling GANs fromdiffusion models be made significantly more efficient? To achieve this goal, wepropose a series of innovative techniques. First, we construct a base GAN modelwith generalized features, adaptable to different concepts through fine-tuning,eliminating the need for training from scratch. Second, we identify cruciallayers within the base GAN model and employ Low-Rank Adaptation (LoRA) with asimple yet effective rank search process, rather than fine-tuning the entirebase model. Third, we investigate the minimal amount of data necessary forfine-tuning, further reducing the overall training time. Extensive experimentsshow that we can efficiently empower GANs with the ability to perform real-timehigh-quality image editing on mobile devices with remarkable reduced trainingcost and storage for each concept.",
        "title": "E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image  Translation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06129",
        "abstract_url": "http://arxiv.org/abs/2401.06129",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Long"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jialin"
            },
            {
                "last_name": "Chu",
                "first_name": "Chun-Te"
            },
            {
                "last_name": "Miao",
                "first_name": "Hui"
            },
            {
                "last_name": "Schroff",
                "first_name": "Florian"
            },
            {
                "last_name": "Adam",
                "first_name": "Hartwig"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            },
            {
                "last_name": "Gong",
                "first_name": "Boqing"
            },
            {
                "last_name": "Kr\u00e4henb\u00fchl",
                "first_name": "Philipp"
            },
            {
                "last_name": "Yuan",
                "first_name": "Liangzhe"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The recent advance in vision-language models is largely attributed to theabundance of image-text data. We aim to replicate this success forvideo-language models, but there simply is not enough human-curated video-textdata available. We thus resort to fine-tuning a video-language model from astrong image-language baseline with synthesized instructional data. Theresulting video-language model is then used to auto-label millions of videos togenerate high-quality captions. We show the adapted video-language modelperforms well on a wide range of video-language benchmarks. For instance, itsurpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, ourmodel generates detailed descriptions for previously unseen videos, whichprovide better textual supervision than existing methods. Experiments show thata video-language dual-encoder model contrastively trained on theseauto-generated captions is 3.8% better than the strongest baseline that alsoleverages vision-language models. Our best model outperforms state-of-the-artmethods on MSR-VTT zero-shot text-to-video retrieval by 6%.",
        "title": "Distilling Vision-Language Models on Millions of Videos",
        "date": "2024-01-11",
        "group": "cs"
    }
]
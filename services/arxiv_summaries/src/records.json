[
    {
        "identifier": "oai:arXiv.org:1204.1134",
        "abstract_url": "http://arxiv.org/abs/1204.1134",
        "authors": [
            {
                "last_name": "Carlucci",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Zdanowski",
                "first_name": "Konrad"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LO",
            "",
            ""
        ],
        "abstract": "  We characterize the computational content and the proof-theoretic strength of\na Ramsey-type theorem for bi-colorings of so-called {\\em exactly large} sets.\nAn {\\it exactly large} set is a set $X\\subset\\Nat$ such that\n$\\card(X)=\\min(X)+1$. The theorem we analyze is as follows. For every infinite\nsubset $M$ of $\\Nat$, for every coloring $C$ of the exactly large subsets of\n$M$ in two colors, there exists and infinite subset $L$ of $M$ such that $C$ is\nconstant on all exactly large subsets of $L$. This theorem is essentially due\nto Pudl\\`ak and R\\\"odl and independently to Farmaki. We prove that --- over\nComputable Mathematics --- this theorem is equivalent to closure under the\n$\\omega$ Turing jump (i.e., under arithmetical truth). Natural combinatorial\ntheorems at this level of complexity are rare. Our results give a complete\ncharacterization of the theorem from the point of view of Computable\nMathematics and of the Proof Theory of Arithmetic. This nicely extends the\ncurrent knowledge about the strength of Ramsey Theorem. We also show that\nanalogous results hold for a related principle based on the Regressive Ramsey\nTheorem. In addition we give a further characterization in terms of truth\npredicates over Peano Arithmetic. We conjecture that analogous results hold for\nlarger ordinals.\n",
        "title": "The strength of Ramsey Theorem for coloring relatively large sets",
        "date": "2012-04-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.00118",
        "abstract_url": "http://arxiv.org/abs/2104.00118",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Peipei"
            },
            {
                "last_name": "Rupp",
                "first_name": "Andreas"
            },
            {
                "last_name": "Kanschat",
                "first_name": "Guido"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Uniform convergence of the geometric multigrid V-cycle is proven for HDG\nmethods with a new set of assumptions on the injection operators from coarser\nto finer meshes. The scheme involves standard smoothers and local solvers which\nare bounded, convergent, and consistent. Elliptic regularity is used in the\nproofs. The new assumptions admit injection operators local to a single coarse\ngrid cell. Examples for admissible injection operators are given. The analysis\napplies to the hybridized local discontinuous Galerkin method, hybridized\nRaviart-Thomas, and hybridized Brezzi-Douglas-Marini mixed element methods.\nNumerical experiments are provided to confirm the theoretical results.\n",
        "title": "Analysis of injection operators in multigrid solvers for hybridized\n  discontinuous Galerkin methods",
        "date": "2021-03-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.11448",
        "abstract_url": "http://arxiv.org/abs/2104.11448",
        "authors": [
            {
                "last_name": "Vizca\u00edno",
                "first_name": "Aurora"
            },
            {
                "last_name": "de Guzm\u00e1n",
                "first_name": "Ignacio Garc\u00eda-Rodr\u00edguez"
            },
            {
                "last_name": "Manjavacas",
                "first_name": "Antonio"
            },
            {
                "last_name": "Garc\u00eda",
                "first_name": "F\u00e9lix"
            },
            {
                "last_name": "Cruz-Lemus",
                "first_name": "Jos\u00e9 A."
            },
            {
                "last_name": "Serrano",
                "first_name": "Manuel \u00c1ngel"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Technology has changed both our way of life and the way in which we learn.\nStudents now attend lectures with laptops and mobile phones, and this situation\nis accentuated in the case of students on Computer Science degrees, since they\nrequire their computers in order to participate in both theoretical and\npractical lessons. Problems, however, arise when the students' social networks\nare opened on their computers and they receive notifications that interrupt\ntheir work. We set up a workshop regarding time, thoughts and attention\nmanagement with the objective of teaching our students techniques that would\nallow them to manage interruptions, concentrate better and definitively make\nbetter use of their time. Those who took part in the workshop were then\nevaluated to discover its effects. The results obtained are quite optimistic\nand are described in this paper with the objective of encouraging other\nuniversities to perform similar initiatives.\n",
        "title": "How to help university students to manage their interruptions and\n  improve their attention and time management",
        "date": "2021-04-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2107.12312",
        "abstract_url": "http://arxiv.org/abs/2107.12312",
        "authors": [
            {
                "last_name": "Pecho-Ninapaytan",
                "first_name": "Andrea"
            },
            {
                "last_name": "Zambrano-Zuta",
                "first_name": "Stefany"
            },
            {
                "last_name": "Vargas-Bianchi",
                "first_name": "Lizardo"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The spread of fake news has been increasing, which gives rise to a special\ninterest in the development of identification and coping skills among news\nconsumers so that they can filter out misleading information. Studies suggest\nthat older people share more fake news from social media. There is scarce\nliterature that analyse how baby boomers behave in the face of fake news. The\npurpose of this study is to examine how female baby boomers deal with fake news\non Facebook and their available resources to learn how to identify and handle\ndubious information. A qualitative study and thematic analysis were conducted\nusing information obtained from interviewing female baby boomers. Four themes\nemerge from the analysis, revealing that participants recognise that they can\nidentify fake news but may not always be able to do so due to limitations in\ntheir understanding of an issue or uncertainty about its source. Participants\nshow participants empirically develop critical identification and filtering\nskills with the assistance from close family members.\n",
        "title": "'No, auntie, that's false': Female baby boomers develop critical skills\n  to confront fake news with guidance from relatives",
        "date": "2021-07-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2107.12879",
        "abstract_url": "http://arxiv.org/abs/2107.12879",
        "authors": [
            {
                "last_name": "Vargas-Bianchi",
                "first_name": "Lizardo"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Much research has been conducted on how consumption is related to human\nrelations, for example, consumer communities organized around specific brands,\nor the way people use products to define their own identity and transmit a\ndesired image. However, only a scarcity of research has examined the\nconsumption behaviour when the fundamental intention is to leverage group\nbelonging. The literature comprises a single theoretical framework that\ndescribes this behaviour, a nascent proposition that has not been tested. This\nstudy examines the transferability of that theoretical framework in a different\ncontext than the one used for its proposal and its extent on the phenomenon of\nconsuming to leverage belonging. A qualitative deductive case study and a\npattern matching analysis technique were employed, followed by a structural\ncoding analysis of interview data. The findings revealed the model is\ntransferable, however its conceptual extent on the phenomenon it addresses\nfaces limitations. These findings allow the proposal of an alternative\nframework, the Belonging-Oriented Consumption Model. This model provides a\ntheoretical basis for future research on consumer belonging behaviour.\n",
        "title": "Consumer belonging behaviour: Qualitative testing of a theoretical\n  framework and proposal of an alternative model",
        "date": "2021-07-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2112.06333",
        "abstract_url": "http://arxiv.org/abs/2112.06333",
        "authors": [
            {
                "last_name": "Bradshaw",
                "first_name": "Peter"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            ""
        ],
        "abstract": "  We consider single-conflict colorings, a variant of graph colorings in which\neach edge of a graph has a single forbidden color pair. We show that for any\nassignment of forbidden color pairs to the edges of a $d$-degenerate graph $G$\non $n$ vertices of edge-multiplicity at most $\\log \\log n$, $O(\\sqrt{ d } \\log\nn)$ colors are always enough to color the vertices of $G$ in a way that avoids\nevery forbidden color pair. This answers a question of Dvo\\v{r}\\'ak, Esperet,\nKang, and Ozeki for simple graphs (Journal of Graph Theory 2021).\n",
        "title": "Single-conflict colorings of degenerate graphs",
        "date": "2021-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.12673",
        "abstract_url": "http://arxiv.org/abs/2201.12673",
        "authors": [
            {
                "last_name": "Rasetto",
                "first_name": "Marco"
            },
            {
                "last_name": "Wan",
                "first_name": "Qingzhou"
            },
            {
                "last_name": "Akolkar",
                "first_name": "Himanshu"
            },
            {
                "last_name": "Shi",
                "first_name": "Bertram"
            },
            {
                "last_name": "Xiong",
                "first_name": "Feng"
            },
            {
                "last_name": "Benosman",
                "first_name": "Ryad"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Neuromorphic engineering has led to the necessary process of rethinking of\nhow we process and integrate information, analyze data, and use the resulting\ninsights to improve computation and avoid the current high power and latency of\nArtificial Intelligence (AI) hardware. Current neuromorphic processors are,\nhowever, limited by digital technologies, which cannot reproduce the abilities\nof biological neural computation in terms of power, latency and area cost. In\nthis paper, we show that the combined use of the dynamic properties of\nmemristors to implement a model of synaptic integration and the determination\nof the correct level of abstraction of biological neural networks has the\npotential to open a new range of capabilities for neuromorphic processors. We\ntest this approach using a novel three-terminal LixWO3 electrochemical\nmemristor, by deriving its conductance model and using it to emulate synaptic\ntemporal kernel computation in the context of a pattern recognition task. We\nshow that these devices allow for robust results with no loss in precision\nwhile opening the path for an energy efficient approach to build novel\nbio-inspired processing units in silicon.\n",
        "title": "The Challenges Ahead for Bio-inspired Neuromorphic Event Processors: How\n  Memristors Dynamic Properties Could Revolutionize Machine Learning",
        "date": "2022-01-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2202.12438",
        "abstract_url": "http://arxiv.org/abs/2202.12438",
        "authors": [
            {
                "last_name": "Dvo\u0159\u00e1k",
                "first_name": "Pavel"
            },
            {
                "last_name": "Krawczyk",
                "first_name": "Monika"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Novotn\u00e1",
                "first_name": "Jana"
            },
            {
                "last_name": "Rz\u0105\u017cewski",
                "first_name": "Pawe\u0142"
            },
            {
                "last_name": "\u017buk",
                "first_name": "Aneta"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  A locally surjective homomorphism from a graph $G$ to a graph $H$ is an\nedge-preserving mapping from $V(G)$ to $V(H)$ that is surjective in the\nneighborhood of each vertex in $G$. In the list locally surjective homomorphism\nproblem, denoted by LLSHom($H$), the graph $H$ is fixed and the instance\nconsists of a graph $G$ whose every vertex is equipped with a subset of $V(H)$,\ncalled list. We ask for the existence of a locally surjective homomorphism from\n$G$ to $H$, where every vertex of $G$ is mapped to a vertex from its list. In\nthis paper, we study the complexity of the LLSHom($H$) problem in $F$-free\ngraphs, i.e., graphs that exclude a fixed graph $F$ as an induced subgraph. We\naim to understand for which pairs $(H,F)$ the problem can be solved in\nsubexponential time.\n  We show that for all graphs $H$, for which the problem is NP-hard in general\ngraphs, it cannot be solved in subexponential time in $F$-free graphs unless\n$F$ is a bounded-degree forest or the ETH fails. The initial study reveals that\na natural subfamily of bounded-degree forests $F$ that might lead to some\ntractability results is the family $\\mathcal S$ consisting of forests whose\nevery component has at most three leaves. In this case, we exhibit the\nfollowing dichotomy theorem: besides the cases that are polynomial-time\nsolvable in general graphs, the graphs $H \\in \\{P_3,C_4\\}$ are the only\nconnected ones that allow for a subexponential-time algorithm in $F$-free\ngraphs for every $F \\in \\mathcal S$ (unless the ETH fails).\n",
        "title": "List Locally Surjective Homomorphisms in Hereditary Graph Classes",
        "date": "2022-02-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2203.05103",
        "abstract_url": "http://arxiv.org/abs/2203.05103",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Wei",
                "first_name": "Shikui"
            },
            {
                "last_name": "Lu",
                "first_name": "Qiming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Neural Ordinary Differential Equations (Neural ODEs) construct the continuous\ndynamics of hidden units using ordinary differential equations specified by a\nneural network, demonstrating promising results on many tasks. However, Neural\nODEs still do not perform well on image recognition tasks. The possible reason\nis that the one-hot encoding vector commonly used in Neural ODEs can not\nprovide enough supervised information. We propose a new training based on\nknowledge distillation to construct more powerful and robust Neural ODEs\nfitting image recognition tasks. Specially, we model the training of Neural\nODEs into a teacher-student learning process, in which we propose ResNets as\nthe teacher model to provide richer supervised information. The experimental\nresults show that the new training manner can improve the classification\naccuracy of Neural ODEs by 24% on CIFAR10 and 5% on SVHN. In addition, we also\nquantitatively discuss the effect of both knowledge distillation and time\nhorizon in Neural ODEs on robustness against adversarial examples. The\nexperimental analysis concludes that introducing the knowledge distillation and\nincreasing the time horizon can improve the robustness of Neural ODEs against\nadversarial examples.\n",
        "title": "Improving Neural ODEs via Knowledge Distillation",
        "date": "2022-03-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2205.02065",
        "abstract_url": "http://arxiv.org/abs/2205.02065",
        "authors": [
            {
                "last_name": "Posso",
                "first_name": "Julien"
            },
            {
                "last_name": "Bois",
                "first_name": "Guy"
            },
            {
                "last_name": "Savaria",
                "first_name": "Yvon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Spacecraft pose estimation is an essential computer vision application that\ncan improve the autonomy of in-orbit operations. An ESA/Stanford competition\nbrought out solutions that seem hardly compatible with the constraints imposed\non spacecraft onboard computers. URSONet is among the best in the competition\nfor its generalization capabilities but at the cost of a tremendous number of\nparameters and high computational complexity. In this paper, we propose\nMobile-URSONet: a spacecraft pose estimation convolutional neural network with\n178 times fewer parameters while degrading accuracy by no more than four times\ncompared to URSONet.\n",
        "title": "Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose\n  Estimation",
        "date": "2022-05-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.10943",
        "abstract_url": "http://arxiv.org/abs/2206.10943",
        "authors": [
            {
                "last_name": "Linders",
                "first_name": "Viktor"
            },
            {
                "last_name": "Birken",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Conservation and consistency are fundamental properties of discretizations of\nsystems of hyperbolic conservation laws. Here, these concepts are extended to\nthe realm of iterative methods by formally defining locally conservative and\nflux consistent iterations. These concepts are of both theoretical and\npractical importance: Based on recent work by the authors, it is shown that\npseudo-time iterations using explicit Runge-Kutta methods are locally\nconservative but not necessarily flux consistent. An extension of the\nLax-Wendroff theorem is presented, revealing convergence towards weak solutions\nof a temporally retarded system of conservation laws. Each equation is modified\nin the same way, namely by a particular scalar factor multiplying the spatial\nflux terms. A technique for enforcing flux consistency, and thereby recovering\nconvergence, is presented. Further, local conservation is established for all\nKrylov subspace methods, with and without restarts, and for Newton's method\nunder certain assumptions on the discretization. Thus it is shown that\nNewton-Krylov methods are locally conservative, although not necessarily flux\nconsistent. Numerical experiments with the 2D compressible Euler equations\ncorroborate the theoretical results. Further numerical investigations of the\nimpact of flux consistency on Newton-Krylov methods indicate that its effect is\ncase dependent, and diminishes as the number of iterations grow.\n",
        "title": "Locally conservative and flux consistent iterative methods",
        "date": "2022-06-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.15464",
        "abstract_url": "http://arxiv.org/abs/2206.15464",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Andi"
            },
            {
                "last_name": "Cincio",
                "first_name": "Lukasz"
            },
            {
                "last_name": "Coles",
                "first_name": "Patrick J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We study the problem of learning the parameters for the Hamiltonian of a\nquantum many-body system, given limited access to the system. In this work, we\nbuild upon recent approaches to Hamiltonian learning via derivative estimation.\nWe propose a protocol that improves the scaling dependence of prior works,\nparticularly with respect to parameters relating to the structure of the\nHamiltonian (e.g., its locality $k$). Furthermore, by deriving exact bounds on\nthe performance of our protocol, we are able to provide a precise numerical\nprescription for theoretically optimal settings of hyperparameters in our\nlearning protocol, such as the maximum evolution time (when learning with\nunitary dynamics) or minimum temperature (when learning with Gibbs states).\nThanks to these improvements, our protocol is practical for large problems: we\ndemonstrate this with a numerical simulation of our protocol on an 80-qubit\nsystem.\n",
        "title": "Practical Black Box Hamiltonian Learning",
        "date": "2022-06-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.07425",
        "abstract_url": "http://arxiv.org/abs/2207.07425",
        "authors": [
            {
                "last_name": "Hatzel",
                "first_name": "Meike"
            },
            {
                "last_name": "Jaffke",
                "first_name": "Lars"
            },
            {
                "last_name": "Lima",
                "first_name": "Paloma T."
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Sharma",
                "first_name": "Roohani"
            },
            {
                "last_name": "Sorge",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We show fixed-parameter tractability of the Directed Multicut problem with\nthree terminal pairs (with a randomized algorithm). This problem, given a\ndirected graph $G$, pairs of vertices (called terminals) $(s_1,t_1)$,\n$(s_2,t_2)$, and $(s_3,t_3)$, and an integer $k$, asks to find a set of at most\n$k$ non-terminal vertices in $G$ that intersect all $s_1t_1$-paths, all\n$s_2t_2$-paths, and all $s_3t_3$-paths. The parameterized complexity of this\ncase has been open since Chitnis, Cygan, Hajiaghayi, and Marx proved\nfixed-parameter tractability of the 2-terminal-pairs case at SODA 2012, and\nPilipczuk and Wahlstr\\\"{o}m proved the W[1]-hardness of the 4-terminal-pairs\ncase at SODA 2016.\n  On the technical side, we use two recent developments in parameterized\nalgorithms. Using the technique of directed flow-augmentation [Kim, Kratsch,\nPilipczuk, Wahlstr\\\"{o}m, STOC 2022] we cast the problem as a CSP problem with\nfew variables and constraints over a large ordered domain.We observe that this\nproblem can be in turn encoded as an FO model-checking task over a structure\nconsisting of a few 0-1 matrices. We look at this problem through the lenses of\ntwin-width, a recently introduced structural parameter [Bonnet, Kim,\nThomass\\'{e}, Watrigant, FOCS 2020]: By a recent characterization [Bonnet,\nGiocanti, Ossona de Mendes, Simon, Thomass\\'{e}, Toru\\'{n}czyk, STOC 2022] the\nsaid FO model-checking task can be done in FPT time if the said matrices have\nbounded grid rank. To complete the proof, we show an irrelevant vertex rule: If\nany of the matrices in the said encoding has a large grid minor, a vertex\ncorresponding to the ``middle'' box in the grid minor can be proclaimed\nirrelevant -- not contained in the sought solution -- and thus reduced.\n",
        "title": "Fixed-parameter tractability of Directed Multicut with three terminal\n  pairs parameterized by the size of the cutset: twin-width meets\n  flow-augmentation",
        "date": "2022-07-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.07426",
        "abstract_url": "http://arxiv.org/abs/2207.07426",
        "authors": [
            {
                "last_name": "Jaffke",
                "first_name": "Lars"
            },
            {
                "last_name": "Lima",
                "first_name": "Paloma T."
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Souza",
                "first_name": "Ueverton S."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  We study a generalization of the classic Global Min-Cut problem, called\nGlobal Label Min-Cut (or sometimes Global Hedge Min-Cut): the edges of the\ninput (multi)graph are labeled (or partitioned into color classes or hedges),\nand removing all edges of the same label (color or from the same hedge) costs\none. The problem asks to disconnect the graph at minimum cost.\n  While the $st$-cut version of the problem is known to be NP-hard, the above\nglobal cut version is known to admit a quasi-polynomial randomized $n^{O(\\log\n\\mathrm{OPT})}$-time algorithm due to Ghaffari, Karger, and Panigrahi [SODA\n2017]. They consider this as ``strong evidence that this problem is in P''. We\nshow that this is actually not the case. We complete the study of the\ncomplexity of the Global Label Min-Cut problem by showing that the\nquasi-polynomial running time is probably optimal: We show that the existence\nof an algorithm with running time $(np)^{o(\\log n/ (\\log \\log n)^2)}$ would\ncontradict the Exponential Time Hypothesis, where $n$ is the number of\nvertices, and $p$ is the number of labels in the input. The key step for the\nlower bound is a proof that Global Label Min-Cut is W[1]-hard when\nparameterized by the number of uncut labels. In other words, the problem is\ndifficult in the regime where almost all labels need to be cut to disconnect\nthe graph. To turn this lower bound into a quasi-polynomial-time lower bound,\nwe also needed to revisit the framework due to Marx [Theory Comput. 2010] of\nproving lower bounds assuming Exponential Time Hypothesis through the Subgraph\nIsomorphism problem parameterized by the number of edges of the pattern. Here,\nwe provide an alternative simplified proof of the hardness of this problem that\nis more versatile with respect to the choice of the regimes of the parameters.\n",
        "title": "A tight quasi-polynomial bound for Global Label Min-Cut",
        "date": "2022-07-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.14403",
        "abstract_url": "http://arxiv.org/abs/2208.14403",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Ayoosh"
            },
            {
                "last_name": "Kim",
                "first_name": "Hunmin"
            },
            {
                "last_name": "Yu",
                "first_name": "Simon"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Hovakimyan",
                "first_name": "Naira"
            },
            {
                "last_name": "Caccamo",
                "first_name": "Marco"
            },
            {
                "last_name": "Sha",
                "first_name": "Lui"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Perception of obstacles remains a critical safety concern for autonomous\nvehicles. Real-world collisions have shown that the autonomy faults leading to\nfatal collisions originate from obstacle existence detection. Open source\nautonomous driving implementations show a perception pipeline with complex\ninterdependent Deep Neural Networks. These networks are not fully verifiable,\nmaking them unsuitable for safety-critical tasks.\n  In this work, we present a safety verification of an existing LiDAR based\nclassical obstacle detection algorithm. We establish strict bounds on the\ncapabilities of this obstacle detection algorithm. Given safety standards, such\nbounds allow for determining LiDAR sensor properties that would reliably\nsatisfy the standards. Such analysis has as yet been unattainable for neural\nnetwork based perception systems. We provide a rigorous analysis of the\nobstacle detection system with empirical results based on real-world sensor\ndata.\n",
        "title": "Verifiable Obstacle Detection",
        "date": "2022-08-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2209.06171",
        "abstract_url": "http://arxiv.org/abs/2209.06171",
        "authors": [
            {
                "last_name": "Cook",
                "first_name": "Linda"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            },
            {
                "last_name": "Pilipczuk",
                "first_name": "Marcin"
            },
            {
                "last_name": "Reinald",
                "first_name": "Amadeus"
            },
            {
                "last_name": "Souza",
                "first_name": "U\u00e9verton S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            ""
        ],
        "abstract": "  An oriented graph is a digraph that does not contain a directed cycle of\nlength two. An (oriented) graph $D$ is $H$-free if $D$ does not contain $H$ as\nan induced sub(di)graph. The Gy\\'arf\\'as-Sumner conjecture is a widely-open\nconjecture on simple graphs, which states that for any forest $F$, there is\nsome function $f$ such that every $F$-free graph $G$ with clique number\n$\\omega(G)$ has chromatic number at most $f(\\omega(G))$. Aboulker, Charbit, and\nNaserasr [Extension of Gy\\'arf\\'as-Sumner Conjecture to Digraphs; E-JC 2021]\nproposed an analogue of this conjecture to the dichromatic number of oriented\ngraphs. The dichromatic number of a digraph $D$ is the minimum number of colors\nrequired to color the vertex set of $D$ so that no directed cycle in $D$ is\nmonochromatic.\n  Aboulker, Charbit, and Naserasr's $\\overrightarrow{\\chi}$-boundedness\nconjecture states that for every oriented forest $F$, there is some function\n$f$ such that every $F$-free oriented graph $D$ has dichromatic number at most\n$f(\\omega(D))$, where $\\omega(D)$ is the size of a maximum clique in the graph\nunderlying $D$. In this paper, we perform the first step towards proving\nAboulker, Charbit, and Naserasr's $\\overrightarrow{\\chi}$-boundedness\nconjecture by showing that it holds when $F$ is any orientation of a path on\nfour vertices.\n",
        "title": "Proving a directed analogue of the Gy\\'arf\\'as-Sumner conjecture for\n  orientations of $P_4$",
        "date": "2022-09-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2210.11481",
        "abstract_url": "http://arxiv.org/abs/2210.11481",
        "authors": [
            {
                "last_name": "Clainche",
                "first_name": "Soledad Le"
            },
            {
                "last_name": "Ferrer",
                "first_name": "Esteban"
            },
            {
                "last_name": "Gibson",
                "first_name": "Sam"
            },
            {
                "last_name": "Cross",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Parente",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Vinuesa",
                "first_name": "Ricardo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  This review covers the new developments in machine learning (ML) that are\nimpacting the multi-disciplinary area of aerospace engineering, including\nfundamental fluid dynamics (experimental and numerical), aerodynamics,\nacoustics, combustion and structural health monitoring. We review the state of\nthe art, gathering the advantages and challenges of ML methods across different\naerospace disciplines and provide our view on future opportunities. The basic\nconcepts and the most relevant strategies for ML are presented together with\nthe most relevant applications in aerospace engineering, revealing that ML is\nimproving aircraft performance and that these techniques will have a large\nimpact in the near future.\n",
        "title": "Improving aircraft performance using machine learning: a review",
        "date": "2022-10-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2211.12049",
        "abstract_url": "http://arxiv.org/abs/2211.12049",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Ming"
            },
            {
                "last_name": "Xia",
                "first_name": "Zeke"
            },
            {
                "last_name": "Yue",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Huang",
                "first_name": "Yihao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Chen",
                "first_name": "Mingsong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  As a promising distributed machine learning paradigm that enables\ncollaborative training without compromising data privacy, Federated Learning\n(FL) has been increasingly used in AIoT (Artificial Intelligence of Things)\ndesign. However, due to the lack of efficient management of straggling devices,\nexisting FL methods greatly suffer from the problems of low inference accuracy\nand long training time. Things become even worse when taking various uncertain\nfactors (e.g., network delays, performance variances caused by process\nvariation) existing in AIoT scenarios into account. To address this issue, this\npaper proposes a novel asynchronous FL framework named GitFL, whose\nimplementation is inspired by the famous version control system Git. Unlike\ntraditional FL, the cloud server of GitFL maintains a master model (i.e., the\nglobal model) together with a set of branch models indicating the trained local\nmodels committed by selected devices, where the master model is updated based\non both all the pushed branch models and their version information, and only\nthe branch models after the pull operation are dispatched to devices. By using\nour proposed Reinforcement Learning (RL)-based device selection mechanism, a\npulled branch model with an older version will be more likely to be dispatched\nto a faster and less frequently selected device for the next round of local\ntraining. In this way, GitFL enables both effective control of model staleness\nand adaptive load balance of versioned models among straggling devices, thus\navoiding the performance deterioration. Comprehensive experimental results on\nwell-known models and datasets show that, compared with state-of-the-art\nasynchronous FL methods, GitFL can achieve up to 2.64X training acceleration\nand 7.88% inference accuracy improvements in various uncertain scenarios.\n",
        "title": "GitFL: Adaptive Asynchronous Federated Learning using Version Control",
        "date": "2022-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.09560",
        "abstract_url": "http://arxiv.org/abs/2212.09560",
        "authors": [
            {
                "last_name": "Llorente",
                "first_name": "Victor J."
            },
            {
                "last_name": "Kou",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Valero",
                "first_name": "Eusebio"
            },
            {
                "last_name": "Ferrer",
                "first_name": "Esteban"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The Immersed Boundary Method (IBM) is a popular numerical approach to impose\nboundary conditions without relying on body-fitted grids, thus reducing the\ncostly effort of mesh generation. To obtain enhanced accuracy, IBM can be\ncombined with high-order methods (e.g., discontinuous Galerkin). For this\ncombination to be effective, an analysis of the numerical errors is essential.\nIn this work, we apply, for the first time, a modified equation analysis to the\ncombination of IBM (based on volume penalization) and high-order methods (based\non nodal discontinuous Galerkin methods) to analyze a priori numerical errors\nand obtain practical guidelines on the selection of IBM parameters. The\nanalysis is performed on a linear advection-diffusion equation with Dirichlet\nboundary conditions. Three ways to penalize the immerse boundary are\nconsidered, the first penalizes the solution inside the IBM region (classic\napproach), whilst the second and third penalize the first and second\nderivatives of the solution. We find optimal combinations of the penalization\nparameters, including the first and second penalizing derivatives, resulting in\nminimum errors. We validate the theoretical analysis with numerical experiments\nfor one- and two-dimensional advection-diffusion equations.\n",
        "title": "A modified equation analysis for immersed boundary methods based on\n  volume penalization: applications to linear advection-diffusion and\n  high-order discontinuous Galerkin schemes",
        "date": "2022-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.07623",
        "abstract_url": "http://arxiv.org/abs/2302.07623",
        "authors": [
            {
                "last_name": "Nikolopoulos",
                "first_name": "Georgios M."
            },
            {
                "last_name": "Fischlin",
                "first_name": "Marc"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CR"
        ],
        "abstract": "  Quantum key-distribution protocols allow two honest distant parties to\nestablish a common truly random secret key in the presence of powerful\nadversaries, provided that the two users share beforehand a short secret key.\nThis pre-shared secret key is used mainly for authentication purposes in the\npost-processing of classical data that have been obtained during the quantum\ncommunication stage, and it prevents a man-in-the-middle attack. The necessity\nof a pre-shared key is usually considered as the main drawback of quantum\nkey-distribution protocols, which becomes even stronger for large networks\ninvolving more that two users. Here we discuss the conditions under which\nphysical unclonable function can be integrated in currently available quantum\nkey-distribution systems, in order to facilitate the generation and the\ndistribution of the necessary pre-shared key, with the smallest possible cost\nin the security of the systems. Moreover, the integration of physical\nunclonable functions in quantum key-distribution networks allows for real-time\nauthentication of the devices that are connected to the network.\n",
        "title": "Quantum key distribution with post-processing driven by physical\n  unclonable functions",
        "date": "2023-02-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.09813",
        "abstract_url": "http://arxiv.org/abs/2302.09813",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Juexiao"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Liao",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bin"
            },
            {
                "last_name": "He",
                "first_name": "Wenjia"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongxiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Longxi"
            },
            {
                "last_name": "Gao",
                "first_name": "Xin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Revoking personal private data is one of the basic human rights, which has\nalready been sheltered by several privacy-preserving laws in many countries.\nHowever, with the development of data science, machine learning and deep\nlearning techniques, this right is usually neglected or violated as more and\nmore patients' data are being collected and used for model training, especially\nin intelligent healthcare, thus making intelligent healthcare a sector where\ntechnology must meet the law, regulations, and privacy principles to ensure\nthat the innovation is for the common good. In order to secure patients' right\nto be forgotten, we proposed a novel solution by using auditing to guide the\nforgetting process, where auditing means determining whether a dataset has been\nused to train the model and forgetting requires the information of a query\ndataset to be forgotten from the target model. We unified these two tasks by\nintroducing a new approach called knowledge purification. To implement our\nsolution, we developed AFS, a unified open-source software, which is able to\nevaluate and revoke patients' private data from pre-trained deep learning\nmodels. We demonstrated the generality of AFS by applying it to four tasks on\ndifferent datasets with various data sizes and architectures of deep learning\nnetworks. The software is publicly available at\n\\url{https://github.com/JoshuaChou2018/AFS}.\n",
        "title": "Audit to Forget: A Unified Method to Revoke Patients' Private Data in\n  Intelligent Healthcare",
        "date": "2023-02-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.11571",
        "abstract_url": "http://arxiv.org/abs/2302.11571",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Juexiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Longxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Chu",
                "first_name": "Yuetan"
            },
            {
                "last_name": "Han",
                "first_name": "Wenkai"
            },
            {
                "last_name": "Gao",
                "first_name": "Xin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Heterogeneous data is endemic due to the use of diverse models and settings\nof devices by hospitals in the field of medical imaging. However, there are few\nopen-source frameworks for federated heterogeneous medical image analysis with\npersonalization and privacy protection simultaneously without the demand to\nmodify the existing model structures or to share any private data. In this\npaper, we proposed PPPML-HMI, an open-source learning paradigm for personalized\nand privacy-preserving federated heterogeneous medical image analysis. To our\nbest knowledge, personalization and privacy protection were achieved\nsimultaneously for the first time under the federated scenario by integrating\nthe PerFedAvg algorithm and designing our novel cyclic secure aggregation with\nthe homomorphic encryption algorithm. To show the utility of PPPML-HMI, we\napplied it to a simulated classification task namely the classification of\nhealthy people and patients from the RAD-ChestCT Dataset, and one real-world\nsegmentation task namely the segmentation of lung infections from COVID-19 CT\nscans. For the real-world task, PPPML-HMI achieved $\\sim$5\\% higher Dice score\non average compared to conventional FL under the heterogeneous scenario.\nMeanwhile, we applied the improved deep leakage from gradients to simulate\nadversarial attacks and showed the solid privacy-preserving capability of\nPPPML-HMI. By applying PPPML-HMI to both tasks with different neural networks,\na varied number of users, and sample sizes, we further demonstrated the strong\nrobustness of PPPML-HMI.\n",
        "title": "Personalized and privacy-preserving federated heterogeneous medical\n  image analysis with PPPML-HMI",
        "date": "2023-02-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10567",
        "abstract_url": "http://arxiv.org/abs/2303.10567",
        "authors": [
            {
                "last_name": "Jeong",
                "first_name": "Jinyeong"
            },
            {
                "last_name": "Kim",
                "first_name": "Min Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a decentralized passive impedance control scheme for\ncollaborative grasping using under-actuated aerial manipulators (AMs). The AM\nsystem is formulated, using a proper coordinate transformation, as an\ninertially decoupled dynamics with which a passivity-based control design is\nconducted. Since the interaction for grasping can be interpreted as a feedback\ninterconnection of passive systems, an arbitrary number of AMs can be modularly\ncombined, leading to a decentralized control scheme. Another interesting\nconsequence of the passivity property is that the AMs automatically converge to\na certain configuration to accomplish the grasping. Collaborative grasping\nusing 10 AMs is presented in simulation.\n",
        "title": "Passivity-based Decentralized Control for Collaborative Grasping of\n  Under-Actuated Aerial Manipulators",
        "date": "2023-03-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.14853",
        "abstract_url": "http://arxiv.org/abs/2304.14853",
        "authors": [
            {
                "last_name": "Manjunath",
                "first_name": "Shashank"
            },
            {
                "last_name": "Perea",
                "first_name": "Jose A."
            },
            {
                "last_name": "Sathyanarayana",
                "first_name": "Aarti"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CG"
        ],
        "abstract": "  Topological data analysis (TDA) is an emerging technique for biological\nsignal processing. TDA leverages the invariant topological features of signals\nin a metric space for robust analysis of signals even in the presence of noise.\nIn this paper, we leverage TDA on brain connectivity networks derived from\nelectroencephalogram (EEG) signals to identify statistical differences between\npediatric patients with obstructive sleep apnea (OSA) and pediatric patients\nwithout OSA. We leverage a large corpus of data, and show that TDA enables us\nto see a statistical difference between the brain dynamics of the two groups.\n",
        "title": "Topological Data Analysis of Electroencephalogram Signals for Pediatric\n  Obstructive Sleep Apnea",
        "date": "2023-04-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.10945",
        "abstract_url": "http://arxiv.org/abs/2305.10945",
        "authors": [
            {
                "last_name": "Heisler",
                "first_name": "Marcel"
            },
            {
                "last_name": "Becker-Asano",
                "first_name": "Christian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper describes, how current Machine Learning (ML) techniques combined\nwith simple rule-based animation routines make an android robot head an\nembodied conversational agent with ChatGPT as its core component. The android\nrobot head is described, technical details are given of how lip-sync animation\nis being achieved, and general software design decisions are presented. A\npublic presentation of the system revealed improvement opportunities that are\nreported and that lead our iterative implementation approach.\n",
        "title": "An Android Robot Head as Embodied Conversational Agent",
        "date": "2023-05-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11292",
        "abstract_url": "http://arxiv.org/abs/2305.11292",
        "authors": [
            {
                "last_name": "Vinod",
                "first_name": "Vivin"
            },
            {
                "last_name": "Maity",
                "first_name": "Sayan"
            },
            {
                "last_name": "Zaspel",
                "first_name": "Peter"
            },
            {
                "last_name": "Kleinekath\u00f6fer",
                "first_name": "Ulrich"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  The accurate but fast calculation of molecular excited states is still a very\nchallenging topic. For many applications, detailed knowledge of the energy\nfunnel in larger molecular aggregates is of key importance requiring highly\naccurate excited state energies. To this end, machine learning techniques can\nbe an extremely useful tool though the cost of generating highly accurate\ntraining datasets still remains a severe challenge. To overcome this hurdle,\nthis work proposes the use of multi-fidelity machine learning where very little\ntraining data from high accuracies is combined with cheaper and less accurate\ndata to achieve the accuracy of the costlier level. In the present study, the\napproach is employed to predict the first excited state energies for three\nmolecules of increasing size, namely, benzene, naphthalene, and anthracene. The\nenergies are trained and tested for conformations stemming from classical\nmolecular dynamics simulations and from real-time density functional\ntight-binding calculations. It can be shown that the multi-fidelity machine\nlearning model can achieve the same accuracy as a machine learning model built\nonly on high cost training data while having a much lower computational effort\nto generate the data. The numerical gain observed in these benchmark test\ncalculations was over a factor of 30 but certainly can be much higher for high\naccuracy data.\n",
        "title": "Multi-Fidelity Machine Learning for Excited State Energies of Molecules",
        "date": "2023-05-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11728",
        "abstract_url": "http://arxiv.org/abs/2305.11728",
        "authors": [
            {
                "last_name": "Tabatabaei",
                "first_name": "Zahra"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adrian"
            },
            {
                "last_name": "Moll",
                "first_name": "Javier Oliver"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Digital pathology has revolutionized cancer diagnosis by leveraging\nContent-Based Medical Image Retrieval (CBMIR) for analyzing histopathological\nWhole Slide Images (WSIs). CBMIR enables searching for similar content,\nenhancing diagnostic reliability and accuracy. In 2020, breast and prostate\ncancer constituted 11.7% and 14.1% of cases, respectively, as reported by the\nGlobal Cancer Observatory (GCO). The proposed Unsupervised CBMIR (UCBMIR)\nreplicates the traditional cancer diagnosis workflow, offering a dependable\nmethod to support pathologists in WSI-based diagnostic conclusions. This\napproach alleviates pathologists' workload, potentially enhancing diagnostic\nefficiency. To address the challenge of the lack of labeled histopathological\nimages in CBMIR, a customized unsupervised Convolutional Auto Encoder (CAE) was\ndeveloped, extracting 200 features per image for the search engine component.\nUCBMIR was evaluated using widely-used numerical techniques in CBMIR, alongside\nvisual evaluation and comparison with a classifier. The validation involved\nthree distinct datasets, with an external evaluation demonstrating its\neffectiveness. UCBMIR outperformed previous studies, achieving a top 5 recall\nof 99% and 80% on BreaKHis and SICAPv2, respectively, using the first\nevaluation technique. Precision rates of 91% and 70% were achieved for BreaKHis\nand SICAPv2, respectively, using the second evaluation technique. Furthermore,\nUCBMIR demonstrated the capability to identify various patterns in patches,\nachieving an 81% accuracy in the top 5 when tested on an external image from\nArvaniti.\n",
        "title": "Towards More Transparent and Accurate Cancer Diagnosis with an\n  Unsupervised CAE Approach",
        "date": "2023-05-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.12567",
        "abstract_url": "http://arxiv.org/abs/2305.12567",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Linyuan"
            },
            {
                "last_name": "Xiong",
                "first_name": "Chenyan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Bajaj",
                "first_name": "Payal"
            },
            {
                "last_name": "Xie",
                "first_name": "Yiqing"
            },
            {
                "last_name": "Cheung",
                "first_name": "Alvin"
            },
            {
                "last_name": "Gao",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Song",
                "first_name": "Xia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper explores the effectiveness of model-generated signals in improving\nzero-shot generalization of text-to-text Transformers such as T5. We study\nvarious designs to pretrain T5 using an auxiliary model to construct more\nchallenging token replacements for the main model to denoise. Key aspects under\nstudy include the decoding target, the location of the RTD head, and the\nmasking pattern. Based on these studies, we develop a new model, METRO-T0,\nwhich is pretrained using the redesigned ELECTRA-Style pretraining strategies\nand then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all\nsimilar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU,\nand rivals the state-of-the-art T0-11B model with only 8% of its parameters.\nOur analysis on model's neural activation and parameter sensitivity reveals\nthat the effectiveness of METRO-T0 stems from more balanced contribution of\nparameters and better utilization of their capacity. The code and model\ncheckpoints are available at https://github.com/gonglinyuan/metro_t0.\n",
        "title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of\n  Text-to-Text Transformers",
        "date": "2023-05-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.12722",
        "abstract_url": "http://arxiv.org/abs/2305.12722",
        "authors": [
            {
                "last_name": "Nilsson",
                "first_name": "Gustav"
            },
            {
                "last_name": "Aquino",
                "first_name": "Alejandro D. Owen"
            },
            {
                "last_name": "Coogan",
                "first_name": "Samuel"
            },
            {
                "last_name": "Molzahn",
                "first_name": "Daniel K."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The ongoing electrification of the transportation fleet will increase the\nload on the electric power grid. Since both the transportation network and the\npower grid already experience periods of significant stress, joint analyses of\nboth infrastructures will most likely be necessary to ensure acceptable\noperation in the future. To enable such analyses, this paper presents an\nopen-source testbed that jointly simulates high-fidelity models of both the\nelectric distribution system and the transportation network. The testbed\nutilizes two open-source simulators, OpenDSS to simulate the electric\ndistribution system and the microscopic traffic simulator SUMO to simulate the\ntraffic dynamics. Electric vehicle charging links the electric distribution\nsystem and the transportation network models at vehicle locations determined\nusing publicly available parcel data. Leveraging high-fidelity synthetic\nelectric distribution system data from the SMART-DS project and transportation\nsystem data from OpenStreetMap, this testbed models the city of Greensboro, NC\ndown to the household level. Moreover, the methodology and the supporting\nscripts released with the testbed allow adaption to other areas where\nhigh-fidelity geolocated OpenDSS datasets are available. After describing the\ncomponents and usage of the testbed, we exemplify applications enabled by the\ntestbed via two scenarios modeling the extreme stresses encountered during\nevacuations.\n",
        "title": "GreenEVT: Greensboro Electric Vehicle Testbed",
        "date": "2023-05-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.14361",
        "abstract_url": "http://arxiv.org/abs/2305.14361",
        "authors": [
            {
                "last_name": "Scheper",
                "first_name": "Tjeerd V. olde"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The representation of arbitrary data in a biological system is one of the\nmost elusive elements of biological information processing. The often\nlogarithmic nature of information in amplitude and frequency presented to\nbiosystems prevents simple encapsulation of the information contained in the\ninput. Criticality Analysis (CA) is a bio-inspired method of information\nrepresentation within a controlled self-organised critical system that allows\nscale-free representation. This is based on the concept of a reservoir of\ndynamic behaviour in which self-similar data will create dynamic nonlinear\nrepresentations. This unique projection of data preserves the similarity of\ndata within a multidimensional neighbourhood. The input can be reduced\ndimensionally to a projection output that retains the features of the overall\ndata, yet has much simpler dynamic response. The method depends only on the\nrate control of chaos applied to the underlying controlled models, that allows\nthe encoding of arbitrary data, and promises optimal encoding of data given\nbiological relevant networks of oscillators. The CA method allows for a\nbiologically relevant encoding mechanism of arbitrary input to biosystems,\ncreating a suitable model for information processing in varying complexity of\norganisms and scale-free data representation for machine learning.\n",
        "title": "Criticality Analysis: Bio-inspired Nonlinear Data Representation",
        "date": "2023-05-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.15099",
        "abstract_url": "http://arxiv.org/abs/2305.15099",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Yang",
                "first_name": "Meng"
            },
            {
                "last_name": "Feng",
                "first_name": "Minwei"
            },
            {
                "last_name": "Yin",
                "first_name": "Jingcheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinbing"
            },
            {
                "last_name": "Leng",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhouhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The transformer model is known to be computationally demanding, and\nprohibitively costly for long sequences, as the self-attention module uses a\nquadratic time and space complexity with respect to sequence length. Many\nresearchers have focused on designing new forms of self-attention or\nintroducing new parameters to overcome this limitation, however a large portion\nof them prohibits the model to inherit weights from large pretrained models. In\nthis work, the transformer's inefficiency has been taken care of from another\nperspective. We propose Fourier Transformer, a simple yet effective approach by\nprogressively removing redundancies in hidden sequence using the ready-made\nFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation\n(DCT). Fourier Transformer is able to significantly reduce computational costs\nwhile retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among\nall transformer-based models on the long-range modeling benchmark LRA with\nsignificant improvement in both speed and space. For generative seq-to-seq\ntasks including CNN/DailyMail and ELI5, by inheriting the BART weights our\nmodel outperforms the standard BART and other efficient models. \\footnote{Our\ncode is publicly available at\n\\url{https://github.com/LUMIA-Group/FourierTransformer}}\n",
        "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence\n  Redundancy with FFT Operator",
        "date": "2023-05-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.17594",
        "abstract_url": "http://arxiv.org/abs/2305.17594",
        "authors": [
            {
                "last_name": "Bian",
                "first_name": "Sizhen"
            },
            {
                "last_name": "Rupp",
                "first_name": "Alexander"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  In recent years, working out in the gym has gotten increasingly more\ndata-focused and many gym enthusiasts are recording their exercises to have a\nbetter overview of their historical gym activities and to make a better\nexercise plan for the future. As a side effect, this recording process has led\nto a lot of time spent painstakingly operating these apps by plugging in used\ntypes of equipment and repetitions. This project aims to automate this process\nusing an Internet of Things (IoT) approach. Specifically, beacons with embedded\nultra-low-power inertial measurement units (IMUs) are attached to the types of\nequipment to recognize the usage and transmit the information to gym-goers and\nmanagers. We have created a small ecosystem composed of beacons, a gateway,\nsmartwatches, android/iPhone applications, a firebase cloud server, and a\ndashboard, all communicating over a mixture of Bluetooth and Wifi to distribute\ncollected data from machines to users and gym managers in a compact and\nmeaningful way. The system we have implemented is a working prototype of a\nbigger end goal and is supposed to initialize progress toward a smarter, more\nefficient, and still privacy-respect gym environment in the future. A\nsmall-scale real-life test shows 94.6\\% accuracy in user gym session recording,\nwhich can reach up to 100\\% easily with a more suitable assembling of the\nbeacons. This promising result shows the potential of a fully automatic\nexercise recording system, which enables comprehensive monitoring and analysis\nof the exercise sessions and frees the user from manual recording. The\nestimated battery life of the beacon is 400 days with a 210 mAh coin battery.\nWe also discussed the shortcoming of the current demonstration system and the\nfuture work for a reliable and ready-to-deploy automatic gym workout recording\nsystem.\n",
        "title": "Fully Automatic Gym Exercises Recording: An IoT Solution",
        "date": "2023-05-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.00265",
        "abstract_url": "http://arxiv.org/abs/2307.00265",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Ying"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingqing"
            },
            {
                "last_name": "Chen",
                "first_name": "Wen"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Li",
                "first_name": "Ming"
            },
            {
                "last_name": "da Costa",
                "first_name": "Daniel Benevides"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper studies an intelligent reflecting surface (IRS)-aided\nmulti-antenna simultaneous wireless information and power transfer (SWIPT)\nsystem where an $M$-antenna access point (AP) serves $K$ single-antenna\ninformation users (IUs) and $J$ single-antenna energy users (EUs) with the aid\nof an IRS with phase errors. We explicitly concentrate on overloaded scenarios\nwhere $K + J > M$ and $K \\geq M$. Our goal is to maximize the minimum\nthroughput among all the IUs by optimizing the allocation of resources\n(including time, transmit beamforming at the AP, and reflect beamforming at the\nIRS), while guaranteeing the minimum amount of harvested energy at each EU.\nTowards this goal, we propose two user grouping (UG) schemes, namely, the\nnon-overlapping UG scheme and the overlapping UG scheme, where the difference\nlies in whether identical IUs can exist in multiple groups. Different IU groups\nare served in orthogonal time dimensions, while the IUs in the same group are\nserved simultaneously with all the EUs via spatial multiplexing. The two\nproblems corresponding to the two UG schemes are mixed-integer non-convex\noptimization problems and difficult to solve optimally. We propose efficient\nalgorithms for these two problems based on the big-M formulation, the penalty\nmethod, the block coordinate descent, and the successive convex approximation.\nSimulation results show that: 1) the non-robust counterparts of the proposed\nrobust designs are unsuitable for practical IRS-aided SWIPT systems with phase\nerrors since the energy harvesting constraints cannot be satisfied; 2) the\nproposed UG strategies can significantly improve the max-min throughput over\nthe benchmark schemes without UG or adopting random UG; 3) the overlapping UG\nscheme performs much better than its non-overlapping counterpart when the\nabsolute difference between $K$ and $M$ is small and the EH constraints are not\nstringent.\n",
        "title": "IRS-Aided Overloaded Multi-Antenna Systems: Joint User Grouping and\n  Resource Allocation",
        "date": "2023-07-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.03299",
        "abstract_url": "http://arxiv.org/abs/2308.03299",
        "authors": [
            {
                "last_name": "Gaba",
                "first_name": "Aimen"
            },
            {
                "last_name": "Kaufman",
                "first_name": "Zhanna"
            },
            {
                "last_name": "Chueng",
                "first_name": "Jason"
            },
            {
                "last_name": "Shvakel",
                "first_name": "Marie"
            },
            {
                "last_name": "Hall",
                "first_name": "Kyle Wm."
            },
            {
                "last_name": "Brun",
                "first_name": "Yuriy"
            },
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Machine learning technology has become ubiquitous, but, unfortunately, often\nexhibits bias. As a consequence, disparate stakeholders need to interact with\nand make informed decisions about using machine learning models in everyday\nsystems. Visualization technology can support stakeholders in understanding and\nevaluating trade-offs between, for example, accuracy and fairness of models.\nThis paper aims to empirically answer \"Can visualization design choices affect\na stakeholder's perception of model bias, trust in a model, and willingness to\nadopt a model?\" Through a series of controlled, crowd-sourced experiments with\nmore than 1,500 participants, we identify a set of strategies people follow in\ndeciding which models to trust. Our results show that men and women prioritize\nfairness and performance differently and that visual design choices\nsignificantly affect that prioritization. For example, women trust fairer\nmodels more often than men do, participants value fairness more when it is\nexplained using text than as a bar chart, and being explicitly told a model is\nbiased has a bigger impact than showing past biased performance. We test the\ngeneralizability of our results by comparing the effect of multiple textual and\nvisual design choices and offer potential explanations of the cognitive\nmechanisms behind the difference in fairness perception and trust. Our research\nguides design considerations to support future work developing visualization\nsystems for machine learning.\n",
        "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and\n  Perceived Bias in Machine Learning",
        "date": "2023-08-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.04769",
        "abstract_url": "http://arxiv.org/abs/2308.04769",
        "authors": [
            {
                "last_name": "Hidaka",
                "first_name": "Ryo"
            },
            {
                "last_name": "Hamakawa",
                "first_name": "Yohei"
            },
            {
                "last_name": "Nakayama",
                "first_name": "Jun"
            },
            {
                "last_name": "Tatsumura",
                "first_name": "Kosuke"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "CE",
            ""
        ],
        "abstract": "  Correlation-diversified portfolios can be constructed by finding the maximum\nindependent sets (MISs) in market graphs with edges corresponding to\ncorrelations between two stocks. The computational complexity to find the MIS\nincreases exponentially as the size of the market graph increases, making the\nMIS selection in a large-scale market graph difficult. Here we construct a\ndiversified portfolio by solving the MIS problem for a large-scale market graph\nwith a combinatorial optimization solver (an Ising machine) based on a\nquantum-inspired algorithm called simulated bifurcation (SB) and investigate\nthe investment performance of the constructed portfolio using long-term\nhistorical market data. Comparisons using stock universes of various sizes\n[TOPIX 100, Nikkei 225, TOPIX 1000, and TOPIX (including approximately 2,000\nconstituents)] show that the SB-based solver outperforms conventional MIS\nsolvers in terms of computation-time and solution-accuracy. By using the\nSB-based solver, we optimized the parameters of a MIS portfolio strategy\nthrough iteration of the backcast simulation that calculates the performance of\nthe MIS portfolio strategy based on a large-scale universe covering more than\n1,700 Japanese stocks for a long period of 10 years. It has been found that the\nbest MIS portfolio strategy (Sharpe ratio = 1.16, annualized return/risk =\n16.3%/14.0%) outperforms the major indices such as TOPIX (0.66, 10.0%/15.2%)\nand MSCI Japan Minimum Volatility Index (0.64, 7.7%/12.1%) for the period from\n2013 to 2023.\n",
        "title": "Correlation-diversified portfolio construction by finding maximum\n  independent set in large-scale market graph",
        "date": "2023-08-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.06085",
        "abstract_url": "http://arxiv.org/abs/2308.06085",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jinqiang"
            },
            {
                "last_name": "Dwarka",
                "first_name": "Vandana"
            },
            {
                "last_name": "Vuik",
                "first_name": "Cornelis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  The Helmholtz equation is related to seismic exploration, sonar, antennas,\nand medical imaging applications. It is one of the most challenging problems to\nsolve in terms of accuracy and convergence due to the scalability issues of the\nnumerical solvers. For 3D large-scale applications, high-performance parallel\nsolvers are also needed. In this paper, a matrix-free parallel iterative solver\nis presented for the three-dimensional (3D) heterogeneous Helmholtz equation.\nWe consider the preconditioned Krylov subspace methods for solving the linear\nsystem obtained from finite-difference discretization. The Complex Shifted\nLaplace Preconditioner (CSLP) is employed since it results in a linear increase\nin the number of iterations as a function of the wavenumber. The preconditioner\nis approximately inverted using one parallel 3D multigrid cycle. For parallel\ncomputing, the global domain is partitioned blockwise. The matrix-vector\nmultiplication and preconditioning operator are implemented in a matrix-free\nway instead of constructing large, memory-consuming coefficient matrices.\nNumerical experiments of 3D model problems demonstrate the robustness and\noutstanding strong scaling of our matrix-free parallel solution method.\nMoreover, the weak parallel scalability indicates our approach is suitable for\nrealistic 3D heterogeneous Helmholtz problems with minimized pollution error.\n",
        "title": "A matrix-free parallel solution method for the three-dimensional\n  heterogeneous Helmholtz equation",
        "date": "2023-08-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.13694",
        "abstract_url": "http://arxiv.org/abs/2308.13694",
        "authors": [
            {
                "last_name": "McDermott",
                "first_name": "Matthew"
            },
            {
                "last_name": "Rife",
                "first_name": "Jason"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Because scanning-LIDAR sensors require finite time to create a point cloud,\nsensor motion during a scan warps the resulting image, a phenomenon known as\nmotion distortion or rolling shutter. Motion-distortion correction methods\nexist, but they rely on external measurements or Bayesian filtering over\nmultiple LIDAR scans. In this paper we propose a novel algorithm that performs\nsnapshot processing to obtain a motion-distortion correction. Snapshot\nprocessing, which registers a current LIDAR scan to a reference image without\nusing external sensors or Bayesian filtering, is particularly relevant for\nlocalization to a high-definition (HD) map. Our approach, which we call\nVelocity-corrected Iterative Compact Ellipsoidal Transformation (VICET),\nextends the well-known Normal Distributions Transform (NDT) algorithm to solve\njointly for both a 6 Degree-of-Freedom (DOF) rigid transform between two LIDAR\nscans and a set of 6DOF motion states that describe distortion within the\ncurrent LIDAR scan. Using experiments, we show that VICET achieves\nsignificantly higher accuracy than NDT or Iterative Closest Point (ICP)\nalgorithms when localizing a distorted raw LIDAR scan against an undistorted HD\nMap. We recommend the reader explore our open-source code and visualizations at\nhttps://github.com/mcdermatt/VICET, which supplements this manuscript.\n",
        "title": "Correcting Motion Distortion for LIDAR HD-Map Localization",
        "date": "2023-08-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.09676",
        "abstract_url": "http://arxiv.org/abs/2309.09676",
        "authors": [
            {
                "last_name": "Bogdoll",
                "first_name": "Daniel"
            },
            {
                "last_name": "Pavlitska",
                "first_name": "Svetlana"
            },
            {
                "last_name": "Klaus",
                "first_name": "Simon"
            },
            {
                "last_name": "Z\u00f6llner",
                "first_name": "J. Marius"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "RO"
        ],
        "abstract": "  Anomalies in the domain of autonomous driving are a major hindrance to the\nlarge-scale deployment of autonomous vehicles. In this work, we focus on\nhigh-resolution camera data from urban scenes that include anomalies of various\ntypes and sizes. Based on a Variational Autoencoder, we condition its latent\nspace to classify samples as either normal data or anomalies. In order to\nemphasize especially small anomalies, we perform experiments where we provide\nthe VAE with a discrepancy map as an additional input, evaluating its impact on\nthe detection performance. Our method separates normal data and anomalies into\nisolated clusters while still reconstructing high-quality images, leading to\nmeaningful latent representations.\n",
        "title": "Conditioning Latent-Space Clusters for Real-World Anomaly Classification",
        "date": "2023-09-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.11285",
        "abstract_url": "http://arxiv.org/abs/2309.11285",
        "authors": [
            {
                "last_name": "Sarvazyan",
                "first_name": "Areg Mikael"
            },
            {
                "last_name": "Gonz\u00e1lez",
                "first_name": "Jos\u00e9 \u00c1ngel"
            },
            {
                "last_name": "Franco-Salvador",
                "first_name": "Marc"
            },
            {
                "last_name": "Rangel",
                "first_name": "Francisco"
            },
            {
                "last_name": "Chulvi",
                "first_name": "Berta"
            },
            {
                "last_name": "Rosso",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  This paper presents the overview of the AuTexTification shared task as part\nof the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the\nframework of the SEPLN 2023 conference. AuTexTification consists of two\nsubtasks: for Subtask 1, participants had to determine whether a text is\nhuman-authored or has been generated by a large language model. For Subtask 2,\nparticipants had to attribute a machine-generated text to one of six different\ntext generation models. Our AuTexTification 2023 dataset contains more than\n160.000 texts across two languages (English and Spanish) and five domains\n(tweets, reviews, news, legal, and how-to articles). A total of 114 teams\nsigned up to participate, of which 36 sent 175 runs, and 20 of them sent their\nworking notes. In this overview, we present the AuTexTification dataset and\ntask, the submitted participating systems, and the results.\n",
        "title": "Overview of AuTexTification at IberLEF 2023: Detection and Attribution\n  of Machine-Generated Text in Multiple Domains",
        "date": "2023-09-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.00658",
        "abstract_url": "http://arxiv.org/abs/2310.00658",
        "authors": [
            {
                "last_name": "Prather",
                "first_name": "James"
            },
            {
                "last_name": "Denny",
                "first_name": "Paul"
            },
            {
                "last_name": "Leinonen",
                "first_name": "Juho"
            },
            {
                "last_name": "Becker",
                "first_name": "Brett A."
            },
            {
                "last_name": "Albluwi",
                "first_name": "Ibrahim"
            },
            {
                "last_name": "Craig",
                "first_name": "Michelle"
            },
            {
                "last_name": "Keuning",
                "first_name": "Hieke"
            },
            {
                "last_name": "Kiesler",
                "first_name": "Natalie"
            },
            {
                "last_name": "Kohn",
                "first_name": "Tobias"
            },
            {
                "last_name": "Luxton-Reilly",
                "first_name": "Andrew"
            },
            {
                "last_name": "MacNeil",
                "first_name": "Stephen"
            },
            {
                "last_name": "Peterson",
                "first_name": "Andrew"
            },
            {
                "last_name": "Pettit",
                "first_name": "Raymond"
            },
            {
                "last_name": "Reeves",
                "first_name": "Brent N."
            },
            {
                "last_name": "Savelka",
                "first_name": "Jaromir"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "",
            "HC"
        ],
        "abstract": "  Recent advancements in artificial intelligence (AI) are fundamentally\nreshaping computing, with large language models (LLMs) now effectively being\nable to generate and interpret source code and natural language instructions.\nThese emergent capabilities have sparked urgent questions in the computing\neducation community around how educators should adapt their pedagogy to address\nthe challenges and to leverage the opportunities presented by this new\ntechnology. In this working group report, we undertake a comprehensive\nexploration of LLMs in the context of computing education and make five\nsignificant contributions. First, we provide a detailed review of the\nliterature on LLMs in computing education and synthesise findings from 71\nprimary articles. Second, we report the findings of a survey of computing\nstudents and instructors from across 20 countries, capturing prevailing\nattitudes towards LLMs and their use in computing education contexts. Third, to\nunderstand how pedagogy is already changing, we offer insights collected from\nin-depth interviews with 22 computing educators from five continents who have\nalready adapted their curricula and assessments. Fourth, we use the ACM Code of\nEthics to frame a discussion of ethical issues raised by the use of large\nlanguage models in computing education, and we provide concrete advice for\npolicy makers, educators, and students. Finally, we benchmark the performance\nof LLMs on various computing education datasets, and highlight the extent to\nwhich the capabilities of current models are rapidly improving. Our aim is that\nthis report will serve as a focal point for both researchers and practitioners\nwho are exploring, adapting, using, and evaluating LLMs and LLM-based tools in\ncomputing classrooms.\n",
        "title": "The Robots are Here: Navigating the Generative AI Revolution in\n  Computing Education",
        "date": "2023-10-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.02045",
        "abstract_url": "http://arxiv.org/abs/2310.02045",
        "authors": [
            {
                "last_name": "Rogenmoser",
                "first_name": "Michael"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  One of the key challenges when operating microcontrollers in harsh\nenvironments such as space is radiation-induced Single Event Upsets (SEUs),\nwhich can lead to errors in computation. Common countermeasures rely on\nproprietary radiation-hardened technologies, low density technologies, or\nextensive replication, leading to high costs and low performance and\nefficiency. To combat this, we present Trikarenos, a fault-tolerant 32-bit\nRISC-V microcontroller SoC in an advanced TSMC 28nm technology. Trikarenos\nalleviates the replication cost by employing a configurable triple-core\nlockstep configuration, allowing three Ibex cores to execute applications\nreliably, operating on ECC-protected memory. If reliability is not needed for a\ngiven application, the cores can operate independently in parallel for higher\nperformance and efficiency. Trikarenos consumes 15.7mW at 250MHz executing a\nfault-tolerant matrix-matrix multiplication, a 21.5x efficiency gain over\nstate-of-the-art, and performance is increased by 2.96x when reliability is not\nneeded for processing, with a 2.36x increase in energy efficiency.\n",
        "title": "Trikarenos: A Fault-Tolerant RISC-V-based Microcontroller for CubeSats\n  in 28nm",
        "date": "2023-10-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.08261",
        "abstract_url": "http://arxiv.org/abs/2310.08261",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Wei",
                "first_name": "Haiyue"
            },
            {
                "last_name": "Bai",
                "first_name": "Lin"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  LiDAR and cameras are complementary sensors for 3D object detection in\nautonomous driving. However, it is challenging to explore the unnatural\ninteraction between point clouds and images, and the critical factor is how to\nconduct feature alignment of heterogeneous modalities. Currently, many methods\nachieve feature alignment by projection calibration only, without considering\nthe problem of coordinate conversion accuracy errors between sensors, leading\nto sub-optimal performance. In this paper, we present GraphAlign, a more\naccurate feature alignment strategy for 3D object detection by graph matching.\nSpecifically, we fuse image features from a semantic segmentation encoder in\nthe image branch and point cloud features from a 3D Sparse CNN in the LiDAR\nbranch. To save computation, we construct the nearest neighbor relationship by\ncalculating Euclidean distance within the subspaces that are divided into the\npoint cloud features. Through the projection calibration between the image and\npoint cloud, we project the nearest neighbors of point cloud features onto the\nimage features. Then by matching the nearest neighbors with a single point\ncloud to multiple images, we search for a more appropriate feature alignment.\nIn addition, we provide a self-attention module to enhance the weights of\nsignificant relations to fine-tune the feature alignment between heterogeneous\nmodalities. Extensive experiments on nuScenes benchmark demonstrate the\neffectiveness and efficiency of our GraphAlign.\n",
        "title": "GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for\n  Multi-Modal 3D Object Detection",
        "date": "2023-10-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.13320",
        "abstract_url": "http://arxiv.org/abs/2310.13320",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shaoan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Mingzhu"
            },
            {
                "last_name": "Hu",
                "first_name": "Yaoqing"
            },
            {
                "last_name": "Li",
                "first_name": "Dongyue"
            },
            {
                "last_name": "Yuan",
                "first_name": "Fusong"
            },
            {
                "last_name": "Yu",
                "first_name": "Junzhi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "RO"
        ],
        "abstract": "  High-precision pose estimation based on visual markers has been a thriving\nresearch topic in the field of computer vision. However, the suitability of\ntraditional flat markers on curved objects is limited due to the diverse shapes\nof curved surfaces, which hinders the development of high-precision pose\nestimation for curved objects. Therefore, this paper proposes a novel visual\nmarker called CylinderTag, which is designed for developable curved surfaces\nsuch as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmly\nattached to objects with a cylindrical shape. Leveraging the manifold\nassumption, the cross-ratio in projective invariance is utilized for encoding\nin the direction of zero curvature on the surface. Additionally, to facilitate\nthe usage of CylinderTag, we propose a heuristic search-based marker generator\nand a high-performance recognizer as well. Moreover, an all-encompassing\nevaluation of CylinderTag properties is conducted by means of extensive\nexperimentation, covering detection rate, detection speed, dictionary size,\nlocalization jitter, and pose estimation accuracy. CylinderTag showcases\nsuperior detection performance from varying view angles in comparison to\ntraditional visual markers, accompanied by higher localization accuracy.\nFurthermore, CylinderTag boasts real-time detection capability and an extensive\nmarker dictionary, offering enhanced versatility and practicality in a wide\nrange of applications. Experimental results demonstrate that the CylinderTag is\na highly promising visual marker for use on cylindrical-like surfaces, thus\noffering important guidance for future research on high-precision visual\nlocalization of cylinder-shaped objects. The code is available at:\nhttps://github.com/wsakobe/CylinderTag.\n",
        "title": "CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects\n  Pose Estimation Based on Projective Invariants",
        "date": "2023-10-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.00253",
        "abstract_url": "http://arxiv.org/abs/2311.00253",
        "authors": [
            {
                "last_name": "Pashazad",
                "first_name": "Hossein"
            },
            {
                "last_name": "Song",
                "first_name": "Xiaoyu"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Dynamic shearing banding and fracturing in unsaturated porous media is a\nsignificant problem in engineering and science. This article proposes a\nmultiphase micro-periporomechanics (uPPM) paradigm for modeling dynamic shear\nbanding and fracturing in unsaturated porous media. Periporomechanics (PPM) is\na nonlocal reformulation of classical poromechanics to model continuous and\ndiscontinuous deformation/fracture and fluid flow in porous media through a\nsingle framework. In PPM, a multiphase porous material is postulated as a\ncollection of a finite number of mixed material points. The length scale in PPM\nthat dictates the nonlocal interaction between material points is a\nmathematical object that lacks a direct physical meaning. As a novelty, in the\ncoupled uPPM, a microstructure-based material length scale is incorporated by\nconsidering micro-rotations of the solid skeleton following the Cosserat\ncontinuum theory for solids. As a new contribution, we reformulate the\nsecond-order work for detecting material instability and the energy-based crack\ncriterion and J-integral for modeling fracturing in the uPPM paradigm. The\nstabilized Cosserat PPM correspondence principle that mitigates the multiphase\nzero-energy mode instability is augmented to include unsaturated fluid flow. We\nhave numerically implemented the novel uPPM paradigm through a dual-way\nfractional-step algorithm in time and a hybrid Lagrangian-Eulerian meshfree\nmethod in space. Numerical examples are presented to demonstrate the robustness\nand efficacy of the proposed uPPM paradigm for modeling shear banding and\nfracturing in unsaturated porous media.\n",
        "title": "Computational multiphase micro-periporomechanics for dynamic shear\n  banding and fracturing of unsaturated porous media",
        "date": "2023-10-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.01773",
        "abstract_url": "http://arxiv.org/abs/2311.01773",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Yin",
                "first_name": "Fukun"
            },
            {
                "last_name": "Fan",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Li",
                "first_name": "Hui"
            },
            {
                "last_name": "Chen",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Wen"
            },
            {
                "last_name": "Lu",
                "first_name": "Chongshan"
            },
            {
                "last_name": "YU",
                "first_name": "Gang"
            },
            {
                "last_name": "Chen",
                "first_name": "Tao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advances in implicit neural representations have achieved impressive\nresults by sampling and fusing individual points along sampling rays in the\nsampling space. However, due to the explosively growing sampling space, finely\nrepresenting and synthesizing detailed textures remains a challenge for\nunbounded large-scale outdoor scenes. To alleviate the dilemma of using\nindividual points to perceive the entire colossal space, we explore learning\nthe surface distribution of the scene to provide structural priors and reduce\nthe samplable space and propose a Point Diffusion implicit Function, PDF, for\nlarge-scale scene neural representation. The core of our method is a\nlarge-scale point cloud super-resolution diffusion module that enhances the\nsparse point cloud reconstructed from several training images into a dense\npoint cloud as an explicit prior. Then in the rendering stage, only sampling\npoints with prior points within the sampling radius are retained. That is, the\nsampling space is reduced from the unbounded space to the scene surface.\nMeanwhile, to fill in the background of the scene that cannot be provided by\npoint clouds, the region sampling based on Mip-NeRF 360 is employed to model\nthe background representation. Expensive experiments have demonstrated the\neffectiveness of our method for large-scale scene novel view synthesis, which\noutperforms relevant state-of-the-art baselines.\n",
        "title": "PDF: Point Diffusion Implicit Function for Large-scale Scene Neural\n  Representation",
        "date": "2023-11-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.06067",
        "abstract_url": "http://arxiv.org/abs/2311.06067",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Xin"
            },
            {
                "last_name": "Chen",
                "first_name": "Shikun"
            },
            {
                "last_name": "Cao",
                "first_name": "Yichao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xin"
            },
            {
                "last_name": "Lu",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "CV"
        ],
        "abstract": "  In recent years, hashing methods have been popular in the large-scale media\nsearch for low storage and strong representation capabilities. To describe\nobjects with similar overall appearance but subtle differences, more and more\nstudies focus on hashing-based fine-grained image retrieval. Existing hashing\nnetworks usually generate both local and global features through attention\nguidance on the same deep activation tensor, which limits the diversity of\nfeature representations. To handle this limitation, we substitute convolutional\ndescriptors for attention-guided features and propose an Attributes Grouping\nand Mining Hashing (AGMH), which groups and embeds the category-specific visual\nattributes in multiple descriptors to generate a comprehensive feature\nrepresentation for efficient fine-grained image retrieval. Specifically, an\nAttention Dispersion Loss (ADL) is designed to force the descriptors to attend\nto various local regions and capture diverse subtle details. Moreover, we\npropose a Stepwise Interactive External Attention (SIEA) to mine critical\nattributes in each descriptor and construct correlations between fine-grained\nattributes and objects. The attention mechanism is dedicated to learning\ndiscrete attributes, which will not cost additional computations in hash codes\ngeneration. Finally, the compact binary codes are learned by preserving\npairwise similarities. Experimental results demonstrate that AGMH consistently\nyields the best performance against state-of-the-art methods on fine-grained\nbenchmark datasets.\n",
        "title": "Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval",
        "date": "2023-11-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.06826",
        "abstract_url": "http://arxiv.org/abs/2311.06826",
        "authors": [
            {
                "last_name": "Meding",
                "first_name": "Kristof"
            },
            {
                "last_name": "Hagendorff",
                "first_name": "Thilo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CY"
        ],
        "abstract": "  Fairness in machine learning (ML) is an ever-growing field of research due to\nthe manifold potential for harm from algorithmic discrimination. To prevent\nsuch harm, a large body of literature develops new approaches to quantify\nfairness. Here, we investigate how one can divert the quantification of\nfairness by describing a practice we call \"fairness hacking\" for the purpose of\nshrouding unfairness in algorithms. This impacts end-users who rely on learning\nalgorithms, as well as the broader community interested in fair AI practices.\nWe introduce two different categories of fairness hacking in reference to the\nestablished concept of p-hacking. The first category, intra-metric fairness\nhacking, describes the misuse of a particular metric by adding or removing\nsensitive attributes from the analysis. In this context, countermeasures that\nhave been developed to prevent or reduce p-hacking can be applied to similarly\nprevent or reduce fairness hacking. The second category of fairness hacking is\ninter-metric fairness hacking. Inter-metric fairness hacking is the search for\na specific fair metric with given attributes. We argue that countermeasures to\nprevent or reduce inter-metric fairness hacking are still in their infancy.\nFinally, we demonstrate both types of fairness hacking using real datasets. Our\npaper intends to serve as a guidance for discussions within the fair ML\ncommunity to prevent or reduce the misuse of fairness metrics, and thus reduce\noverall harm from ML applications.\n",
        "title": "Fairness Hacking: The Malicious Practice of Shrouding Unfairness in\n  Algorithms",
        "date": "2023-11-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.09061",
        "abstract_url": "http://arxiv.org/abs/2311.09061",
        "authors": [
            {
                "last_name": "Karlsson",
                "first_name": "T."
            },
            {
                "last_name": "\u00c5blad",
                "first_name": "E."
            },
            {
                "last_name": "Hermansson",
                "first_name": "T."
            },
            {
                "last_name": "Carlson",
                "first_name": "J. S."
            },
            {
                "last_name": "Tenf\u00e4lt",
                "first_name": "G."
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "",
            "",
            ""
        ],
        "abstract": "  Designing cable harnesses can be time-consuming and complex due to many\ndesign and manufacturing aspects and rules. Automating the design process can\nhelp to fulfil these rules, speed up the process, and optimize the design. To\naccommodate this, we formulate a harness routing optimization problem to\nminimize cable lengths, maximize bundling by rewarding shared paths, and\noptimize the cables' spatial location with respect to case-specific information\nof the routing environment, e.g., zones to avoid. A deterministic and\ncomputationally effective cable harness routing algorithm has been developed to\nsolve the routing problem and is used to generate a set of cable harness\ntopology candidates and approximate the Pareto front. Our approach was tested\nagainst a stochastic and an exact solver and our routing algorithm generated\nobjective function values better than the stochastic approach and close to the\nexact solver. Our algorithm was able to find solutions, some of them being\nproven to be near-optimal, for three industrial-sized 3D cases within\nreasonable time (in magnitude of seconds to minutes) and the computation times\nwere comparable to those of the stochastic approach.\n",
        "title": "Automatic cable harness layout routing in a customizable 3D environment",
        "date": "2023-11-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10653",
        "abstract_url": "http://arxiv.org/abs/2311.10653",
        "authors": [
            {
                "last_name": "Keyvanian",
                "first_name": "Shafagh"
            },
            {
                "last_name": "Johnson",
                "first_name": "Michelle J."
            },
            {
                "last_name": "Figueroa",
                "first_name": "Nadia"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  A realistic human kinematic model that satisfies anatomical constraints is\nessential for human-robot interaction, biomechanics and robot-assisted\nrehabilitation. Modeling realistic joint constraints, however, is challenging\nas human arm motion is constrained by joint limits, inter- and intra-joint\ndependencies, self-collisions, individual capabilities and muscular or\nneurological constraints which are difficult to represent. Hence, physicians\nand researchers have relied on simple box-constraints, ignoring important\nanatomical factors. In this paper, we propose a data-driven method to learn\nrealistic anatomically constrained upper-limb range of motion (RoM) boundaries\nfrom motion capture data. This is achieved by fitting a one-class support\nvector machine to a dataset of upper-limb joint space exploration motions with\nan efficient hyper-parameter tuning scheme. Our approach outperforms similar\nworks focused on valid RoM learning. Further, we propose an impairment index\n(II) metric that offers a quantitative assessment of capability/impairment when\ncomparing healthy and impaired arms. We validate the metric on healthy subjects\nphysically constrained to emulate hemiplegia and different disability levels as\nstroke patients.\n",
        "title": "Learning Realistic Joint Space Boundaries for Range of Motion Analysis\n  of Healthy and Impaired Human Arms",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10765",
        "abstract_url": "http://arxiv.org/abs/2311.10765",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yufeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The challenge of improving translation accuracy in GPT-4 is being addressed\nby harnessing a method known as in-context learning. This paper introduces a\nstrategic approach to utilize in-context learning specifically for machine\ntranslation, aiming to significantly boost accuracy. The crux of this method\nlies in the judicious selection of demonstrations that are most effective for\nin-context learning. By selecting these examples carefully, GPT-4 can utilize\nthem to achieve remarkably accurate machine translations, eliminating the need\nfor task-specific fine-tuning. This technique is anchored in the semantic\nsimilarities between the user's prompt and the chosen dataset. Sentences from\nthis dataset, carefully picked for their relevance and clarity, serve as potent\ndemonstrations for in-context learning. This approach not only enhances\ntranslation accuracy but also enriches the understanding of nuanced linguistic\nstructures. It represents a significant step forward in machine learning,\nleveraging the inherent capabilities of GPT-4 to provide translations that are\nnot only accurate but also contextually rich and linguistically sophisticated.\nThis method demonstrates the potential of in-context learning in overcoming\nlanguage barriers, opening new avenues for cross-cultural communication and\nglobal collaboration.\n",
        "title": "Enhancing Machine Translation through Advanced In-Context Learning: A\n  Methodological Strategy for GPT-4 Improvement",
        "date": "2023-11-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.16065",
        "abstract_url": "http://arxiv.org/abs/2311.16065",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Xianghua"
            },
            {
                "last_name": "Hu",
                "first_name": "Chen"
            },
            {
                "last_name": "Ren",
                "first_name": "Hanchi"
            },
            {
                "last_name": "Deng",
                "first_name": "Jingjing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  This review paper takes a comprehensive look at malicious attacks against FL,\ncategorizing them from new perspectives on attack origins and targets, and\nproviding insights into their methodology and impact. In this survey, we focus\non threat models targeting the learning process of FL systems. Based on the\nsource and target of the attack, we categorize existing threat models into four\ntypes, Data to Model (D2M), Model to Data (M2D), Model to Model (M2M) and\ncomposite attacks. For each attack type, we discuss the defense strategies\nproposed, highlighting their effectiveness, assumptions and potential areas for\nimprovement. Defense strategies have evolved from using a singular metric to\nexcluding malicious clients, to employing a multifaceted approach examining\nclient models at various phases. In this survey paper, our research indicates\nthat the to-learn data, the learning gradients, and the learned model at\ndifferent stages all can be manipulated to initiate malicious attacks that\nrange from undermining model performance, reconstructing private local data,\nand to inserting backdoors. We have also seen these threat are becoming more\ninsidious. While earlier studies typically amplified malicious gradients,\nrecent endeavors subtly alter the least significant weights in local models to\nbypass defense measures. This literature review provides a holistic\nunderstanding of the current FL threat landscape and highlights the importance\nof developing robust, efficient, and privacy-preserving defenses to ensure the\nsafe and trusted adoption of FL in real-world applications.\n",
        "title": "A Survey on Vulnerability of Federated Learning: A Learning Algorithm\n  Perspective",
        "date": "2023-11-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.02937",
        "abstract_url": "http://arxiv.org/abs/2312.02937",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Ayoosh"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhu",
                "first_name": "James"
            },
            {
                "last_name": "Cheng",
                "first_name": "Sheng"
            },
            {
                "last_name": "Gu",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Yoon",
                "first_name": "Hyung-Jin"
            },
            {
                "last_name": "Kim",
                "first_name": "Hunmin"
            },
            {
                "last_name": "Hovakimyan",
                "first_name": "Naira"
            },
            {
                "last_name": "Sha",
                "first_name": "Lui"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Perception, Planning, and Control form the essential components of autonomy\nin advanced air mobility. This work advances the holistic integration of these\ncomponents to enhance the performance and robustness of the complete\ncyber-physical system. We adapt Perception Simplex, a system for verifiable\ncollision avoidance amidst obstacle detection faults, to the vertical landing\nmaneuver for autonomous air mobility vehicles. We improve upon this system by\nreplacing static assumptions of control capabilities with dynamic confirmation,\ni.e., real-time confirmation of control limitations of the system, ensuring\nreliable fulfillment of safety maneuvers and overrides, without dependence on\noverly pessimistic assumptions. Parameters defining control system capabilities\nand limitations, e.g., maximum deceleration, are continuously tracked within\nthe system and used to make safety-critical decisions. We apply these\ntechniques to propose a verifiable collision avoidance solution for autonomous\naerial mobility vehicles operating in cluttered and potentially unsafe\nenvironments.\n",
        "title": "Synergistic Perception and Control Simplex for Verifiable Safe Vertical\n  Landing",
        "date": "2023-12-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.04219",
        "abstract_url": "http://arxiv.org/abs/2312.04219",
        "authors": [
            {
                "last_name": "Ferrer-i-Cancho",
                "first_name": "Ramon"
            },
            {
                "last_name": "Namboodiripad",
                "first_name": "Savithry"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Distance minimization is a general principle of language. A special case of\nthis principle in the domain of word order is swap distance minimization. This\nprinciple predicts that variations from a canonical order that are reached by\nfewer swaps of adjacent constituents are lest costly and thus more likely. Here\nwe investigate the principle in the context of the triple formed by subject\n(S), object (O) and verb (V). We introduce the concept of word order rotation\nas a cognitive underpinning of that prediction. When the canonical order of a\nlanguage is SOV, the principle predicts SOV < SVO, OSV < VSO, OVS < VOS, in\norder of increasing cognitive cost. We test the prediction in three flexible\norder SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese\n(Indo-European). Evidence of swap distance minimization is found in all three\nlanguages, but it is weaker in Sinhalese. Swap distance minimization is\nstronger than a preference for the canonical order in Korean and especially\nMalayalam.\n",
        "title": "Swap distance minimization in SOV languages. Cognitive and mathematical\n  foundations",
        "date": "2023-12-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.07895",
        "abstract_url": "http://arxiv.org/abs/2312.07895",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Yuqi"
            },
            {
                "last_name": "You",
                "first_name": "Li"
            },
            {
                "last_name": "Wang",
                "first_name": "Jue"
            },
            {
                "last_name": "Xu",
                "first_name": "Hao"
            },
            {
                "last_name": "Wong",
                "first_name": "Kai-Kit"
            },
            {
                "last_name": "Gao",
                "first_name": "Xiqi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  In conventional multiple-input multiple-output (MIMO) communication systems,\nthe positions of antennas are fixed. To take full advantage of spatial degrees\nof freedom, a new technology called fluid antenna (FA) is proposed to obtain\nhigher achievable rate and diversity gain. Most existing works on FA exploit\ninstantaneous channel state information (CSI). However, in FA-assisted systems,\nit is difficult to obtain instantaneous CSI since changes in the antenna\nposition will lead to channel variation. In this letter, we investigate a\nFA-assisted MIMO system using relatively slow-varying statistical CSI.\nSpecifically, in the criterion of rate maximization, we propose an algorithmic\nframework for transmit precoding and transmit/receive FAs position designs with\nstatistical CSI. Simulation results show that our proposed algorithm in\nFA-assisted systems significantly outperforms baselines in terms of rate\nperformance.\n",
        "title": "Fluid Antenna-Assisted MIMO Transmission Exploiting Statistical CSI",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.14848",
        "abstract_url": "http://arxiv.org/abs/2312.14848",
        "authors": [
            {
                "last_name": "Graber",
                "first_name": "Vanessa"
            },
            {
                "last_name": "Ronchi",
                "first_name": "Michele"
            },
            {
                "last_name": "Pardo-Araujo",
                "first_name": "Celsa"
            },
            {
                "last_name": "Rea",
                "first_name": "Nanda"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  We combine pulsar population synthesis with simulation-based inference to\nconstrain the magneto-rotational properties of isolated Galactic radio pulsars.\nWe first develop a flexible framework to model neutron-star birth properties\nand evolution, focusing on their dynamical, rotational and magnetic\ncharacteristics. In particular, we sample initial magnetic-field strengths,\n$B$, and spin periods, $P$, from log-normal distributions and capture the\nlate-time magnetic-field decay with a power law. Each log-normal is described\nby a mean, $\\mu_{\\log B}, \\mu_{\\log P}$, and standard deviation, $\\sigma_{\\log\nB}, \\sigma_{\\log P}$, while the power law is characterized by the index,\n$a_{\\rm late}$, resulting in five free parameters. We subsequently model the\nstars' radio emission and observational biases to mimic detections with three\nradio surveys, and produce a large database of synthetic $P$-$\\dot{P}$ diagrams\nby varying our input parameters. We then follow a simulation-based inference\napproach that focuses on neural posterior estimation and employ this database\nto train deep neural networks to directly infer the posterior distributions of\nthe five model parameters. After successfully validating these individual\nneural density estimators on simulated data, we use an ensemble of networks to\ninfer the posterior distributions for the observed pulsar population. We obtain\n$\\mu_{\\log B} = 13.10^{+0.08}_{-0.10}$, $\\sigma_{\\log B} =\n0.45^{+0.05}_{-0.05}$ and $\\mu_{\\log P} = -1.00^{+0.26}_{-0.21}$, $\\sigma_{\\log\nP} = 0.38^{+0.33}_{-0.18}$ for the log-normal distributions, and $a_{\\rm late}\n= -1.80^{+0.65}_{-0.61}$ for the power law at $95\\%$ credible interval. Our\napproach represents a crucial step towards robust statistical inference for\ncomplex population-synthesis frameworks and forms the basis for future\nmulti-wavelength analyses of Galactic pulsars.\n",
        "title": "Isolated pulsar population synthesis with simulation-based inference",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.16814",
        "abstract_url": "http://arxiv.org/abs/2312.16814",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Jindan"
            },
            {
                "last_name": "Xu",
                "first_name": "Wei"
            },
            {
                "last_name": "Yuen",
                "first_name": "Chau"
            },
            {
                "last_name": "Swindlehurst",
                "first_name": "A. Lee"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chunming"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Reconfigurable intelligent surface (RIS) technology is emerging as a\npromising technique for performance enhancement for next-generation wireless\nnetworks. This paper investigates the physical layer security of an\nRIS-assisted multiple-antenna communication system in the presence of random\nspatially distributed eavesdroppers. The RIS-to-ground channels are assumed to\nexperience Rician fading. Using stochastic geometry, exact distributions of the\nreceived signal-to-noise-ratios (SNRs) at the legitimate user and the\neavesdroppers located according to a Poisson point process (PPP) are derived,\nand closed-form expressions for the secrecy outage probability (SOP) and the\nergodic secrecy capacity (ESC) are obtained to provide insightful guidelines\nfor system design. First, the secrecy diversity order is obtained as\n$\\frac{2}{\\alpha_2}$, where $\\alpha_2$ denotes the path loss exponent of the\nRIS-to-ground links. Then, it is revealed that the secrecy performance is\nmainly affected by the number of RIS reflecting elements, $N$, and the impact\nof the number of transmit antennas and transmit power at the base station is\nmarginal. In addition, when the locations of the randomly located eavesdroppers\nare unknown, deploying the RIS closer to the legitimate user rather than to the\nbase station is shown to be more efficient. Moreover, it is also found that the\ndensity of randomly located eavesdroppers, $\\lambda_e$, has an additive effect\non the asymptotic ESC performance given by\n$\\log_2{\\left({1}/{\\lambda_e}\\right)}$. Finally, numerical simulations are\nconducted to verify the accuracy of these theoretical observations.\n",
        "title": "On Secrecy Performance of RIS-Assisted MISO Systems over Rician Channels\n  with Spatially Random Eavesdroppers",
        "date": "2023-12-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.17127",
        "abstract_url": "http://arxiv.org/abs/2312.17127",
        "authors": [
            {
                "last_name": "Ackerman",
                "first_name": "Nathanael L."
            },
            {
                "last_name": "Freer",
                "first_name": "Cameron E."
            },
            {
                "last_name": "Kaddar",
                "first_name": "Younesse"
            },
            {
                "last_name": "Karwowski",
                "first_name": "Jacek"
            },
            {
                "last_name": "Moss",
                "first_name": "Sean K."
            },
            {
                "last_name": "Roy",
                "first_name": "Daniel M."
            },
            {
                "last_name": "Staton",
                "first_name": "Sam"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongseok"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "LO",
            ""
        ],
        "abstract": "  We study semantic models of probabilistic programming languages over graphs,\nand establish a connection to graphons from graph theory and combinatorics. We\nshow that every well-behaved equational theory for our graph probabilistic\nprogramming language corresponds to a graphon, and conversely, every graphon\narises in this way.\n  We provide three constructions for showing that every graphon arises from an\nequational theory. The first is an abstract construction, using Markov\ncategories and monoidal indeterminates. The second and third are more concrete.\nThe second is in terms of traditional measure theoretic probability, which\ncovers 'black-and-white' graphons. The third is in terms of probability monads\non the nominal sets of Gabbay and Pitts. Specifically, we use a variation of\nnominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi\ngraphons. In this way, we build new models of graph probabilistic programming\nfrom graphons.\n",
        "title": "Probabilistic programming interfaces for random graphs: Markov\n  categories, graphons, and nominal sets",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01060",
        "abstract_url": "http://arxiv.org/abs/2401.01060",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Shuzheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Wenxin"
            },
            {
                "last_name": "Gao",
                "first_name": "Cuiyun"
            },
            {
                "last_name": "Li",
                "first_name": "Li"
            },
            {
                "last_name": "Hu",
                "first_name": "Xing"
            },
            {
                "last_name": "Xia",
                "first_name": "Xin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Michael R."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Pre-trained code models have recently achieved substantial improvements in\nmany code intelligence tasks. These models are first pre-trained on large-scale\nunlabeled datasets in a task-agnostic manner using self-supervised learning,\nand then fine-tuned on labeled datasets in downstream tasks. However, the\nlabeled datasets are usually limited in size (i.e., human intensive efforts),\nwhich may hinder the performance of pre-trained code models in specific tasks.\nTo mitigate this, one possible solution is to leverage the large-scale\nunlabeled data in the tuning stage by pseudo-labeling. However, directly\nemploying the pseudo-labeled data can bring a large amount of noise, i.e.,\nincorrect labels, leading to suboptimal performance. How to effectively\nleverage the noisy pseudo-labeled data is a challenging yet under-explored\nproblem.In this paper, we propose a novel approach named HINT to improve\npre-trained code models with large-scale unlabeled datasets by better utilizing\nthe pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled\ndata selection and Noise-tolerant Training. In the hybrid pseudo-data selection\nmodule, considering the robustness issue, apart from directly measuring the\nquality of pseudo labels through training loss, we further propose to employ a\nretrieval-based method to filter low-quality pseudo-labeled data. The\nnoise-tolerant training module aims to further mitigate the influence of errors\nin pseudo labels by training the model with a noise-tolerant loss function and\nby regularizing the consistency of model predictions.The experimental results\nshow that HINT can better leverage those unlabeled data in a task-specific way\nand provide complementary benefits for pre-trained models, e.g., improving the\nbest baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect\ndetection, and assertion generation, respectively.\n",
        "title": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively\n  Tuning Pre-trained Code Models",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01921",
        "abstract_url": "http://arxiv.org/abs/2401.01921",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Kai-Hsin"
            },
            {
                "last_name": "Lin",
                "first_name": "Chang-Teng"
            },
            {
                "last_name": "Hsu",
                "first_name": "Ke"
            },
            {
                "last_name": "Hung",
                "first_name": "Hao-Ti"
            },
            {
                "last_name": "Schneider",
                "first_name": "Manuel"
            },
            {
                "last_name": "Chung",
                "first_name": "Chia-Min"
            },
            {
                "last_name": "Kao",
                "first_name": "Ying-Jer"
            },
            {
                "last_name": "Chen",
                "first_name": "Pochung"
            }
        ],
        "primary_category": "MS",
        "categories": [
            "MS",
            ""
        ],
        "abstract": "  We introduce a tensor network library designed for classical and quantum\nphysics simulations called Cytnx (pronounced as sci-tens). This library\nprovides almost an identical interface and syntax for both C++ and Python,\nallowing users to effortlessly switch between two languages. Aiming at a quick\nlearning process for new users of tensor network algorithms, the interfaces\nresemble the popular Python scientific libraries like NumPy, Scipy, and\nPyTorch. Not only multiple global Abelian symmetries can be easily defined and\nimplemented, Cytnx also provides a new tool called Network that allows users to\nstore large tensor networks and perform tensor network contractions in an\noptimal order automatically. With the integration of cuQuantum, tensor\ncalculations can also be executed efficiently on GPUs. We present benchmark\nresults for tensor operations on both devices, CPU and GPU. We also discuss\nfeatures and higher-level interfaces to be added in the future.\n",
        "title": "The Cytnx Library for Tensor Networks",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02702",
        "abstract_url": "http://arxiv.org/abs/2401.02702",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Xie",
                "first_name": "Jun"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            },
            {
                "last_name": "Xu",
                "first_name": "Shaoqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhepeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  LiDAR-camera fusion can enhance the performance of 3D object detection by\nutilizing complementary information between depth-aware LiDAR points and\nsemantically rich images. Existing voxel-based methods face significant\nchallenges when fusing sparse voxel features with dense image features in a\none-to-one manner, resulting in the loss of the advantages of images, including\nsemantic and continuity information, leading to sub-optimal detection\nperformance, especially at long distances. In this paper, we present\nVoxelNextFusion, a multi-modal 3D object detection framework specifically\ndesigned for voxel-based methods, which effectively bridges the gap between\nsparse point clouds and dense images. In particular, we propose a voxel-based\nimage pipeline that involves projecting point clouds onto images to obtain both\npixel- and patch-level features. These features are then fused using a\nself-attention to obtain a combined representation. Moreover, to address the\nissue of background features present in patches, we propose a feature\nimportance module that effectively distinguishes between foreground and\nbackground features, thus minimizing the impact of the background features.\nExtensive experiments were conducted on the widely used KITTI and nuScenes 3D\nobject detection benchmarks. Notably, our VoxelNextFusion achieved around\n+3.20% in AP@0.7 improvement for car detection in hard level compared to the\nVoxel R-CNN baseline on the KITTI test dataset\n",
        "title": "VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework\n  for Multi-Modal 3D Object Detection",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02899",
        "abstract_url": "http://arxiv.org/abs/2401.02899",
        "authors": [
            {
                "last_name": "Lan\u010da",
                "first_name": "Luka"
            },
            {
                "last_name": "Jakac",
                "first_name": "Karlo"
            },
            {
                "last_name": "Ivi\u0107",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA",
            ""
        ],
        "abstract": "  This research addresses the challenge of executing multi-UAV survey missions\nover diverse terrains characterized by varying elevations. The approach\nintegrates advanced two-dimensional ergodic search technique with model\npredictive control of UAV altitude and velocity. Optimization of altitude and\nvelocity is performed along anticipated UAV ground routes, considering multiple\nobjectives and constraints. This yields a flight regimen tailored to the\nterrain, as well as the motion and sensing characteristics of the UAVs. The\nproposed UAV motion control strategy is assessed through simulations of\nrealistic search missions and actual terrain models. Results demonstrate the\nsuccessful integration of model predictive altitude and velocity control with a\ntwo-dimensional potential field-guided ergodic search. Adjusting UAV altitudes\nto near-ideal levels facilitates the utilization of sensing ranges, thereby\nenhancing the effectiveness of the search. Furthermore, the control algorithm\nis capable of real-time computation, encouraging its practical application in\nreal-world scenarios.\n",
        "title": "Model predictive altitude and velocity control in ergodic potential\n  field directed multi-UAV search",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03085",
        "abstract_url": "http://arxiv.org/abs/2401.03085",
        "authors": [
            {
                "last_name": "Brimoh",
                "first_name": "Paul"
            },
            {
                "last_name": "Olisah",
                "first_name": "Chollette C."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  A genuine signer's signature is naturally unstable even at short\ntime-intervals whereas, expert forgers always try to perfectly mimic a genuine\nsigner's signature. This presents a challenge which puts a genuine signer at\nrisk of being denied access, while a forge signer is granted access. The\nimplication is a high false acceptance rate (FAR) which is the percentage of\nforge signature classified as belonging to a genuine class. Existing work have\nonly scratched the surface of signature verification because the\nmisclassification error remains high. In this paper, a consensus-threshold\ndistance-based classifier criterion is proposed for offline writer-dependent\nsignature verification. Using features extracted from SigNet and SigNet-F deep\nconvolutional neural network models, the proposed classifier minimizes FAR.\nThis is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR\nand Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier\nimproves the state-of-the-art performance by achieving a 1.27% FAR compared to\n8.73% and 17.31% recorded in literature. This performance is consistent across\nother datasets and guarantees that the risk of imposters gaining access to\nsensitive documents or transactions is minimal.\n",
        "title": "Consensus-Threshold Criterion for Offline Signature Verification using\n  Convolutional Neural Network Learned Representations",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03231",
        "abstract_url": "http://arxiv.org/abs/2401.03231",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Seongbeom"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "",
            ""
        ],
        "abstract": "  Many countries around the world, including Korea, use the school choice\nlottery system. However, this method has a problem in that many students are\nassigned to less-preferred schools based on the lottery results. In addition,\nthe task of finding a good assignment with ties often has a time complexity of\nNP, making it a very difficult problem to improve the quality of the\nassignment.\n  In this paper, we prove that the problem of finding a stable matching that\nmaximizes the student-oriented preference utility in a two-sided market with\none-sided preference can be solved in polynomial time, and we verify through\nexperiments that the quality of assignment is improved. The main contributions\nof this paper are as follows. We found that stable student-oriented allocation\nin a two-sided market with one-sided preferences is the same as stable\nallocation in a two-sided market with symmetric preferences. In addition, we\ndefined a method to quantify the quality of allocation from a preference\nutilitarian perspective. Based on the above two, it was proven that the problem\nof finding a stable match that maximizes the preference utility in a two-sided\nmarket with homogeneous preferences can be reduced to an allocation problem. In\nthis paper, through an experiment, we quantitatively verified that optimal\nstudent assignment assigns more students to schools of higher preference, even\nin situations where many students are assigned to schools of low preference\nusing the existing assignment method.\n",
        "title": "Stable Marriage with One-Sided Preference",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03301",
        "abstract_url": "http://arxiv.org/abs/2401.03301",
        "authors": [
            {
                "last_name": "Nguyen-Tang",
                "first_name": "Thanh"
            },
            {
                "last_name": "Arora",
                "first_name": "Raman"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We seek to understand what facilitates sample-efficient learning from\nhistorical datasets for sequential decision-making, a problem that is popularly\nknown as offline reinforcement learning (RL). Further, we are interested in\nalgorithms that enjoy sample efficiency while leveraging (value) function\napproximation. In this paper, we address these fundamental questions by (i)\nproposing a notion of data diversity that subsumes the previous notions of\ncoverage measures in offline RL and (ii) using this notion to {unify} three\ndistinct classes of offline RL algorithms based on version spaces (VS),\nregularized optimization (RO), and posterior sampling (PS). We establish that\nVS-based, RO-based, and PS-based algorithms, under standard assumptions,\nachieve \\emph{comparable} sample efficiency, which recovers the\nstate-of-the-art sub-optimality bounds for finite and linear model classes with\nthe standard assumptions. This result is surprising, given that the prior work\nsuggested an unfavorable sample complexity of the RO-based algorithm compared\nto the VS-based algorithm, whereas posterior sampling is rarely considered in\noffline RL due to its explorative nature. Notably, our proposed model-free\nPS-based algorithm for offline RL is {novel}, with sub-optimality bounds that\nare {frequentist} (i.e., worst-case) in nature.\n",
        "title": "On Sample-Efficient Offline Reinforcement Learning: Data Diversity,\n  Posterior Sampling, and Beyond",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03800",
        "abstract_url": "http://arxiv.org/abs/2401.03800",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Dong"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Gao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Lu",
                "first_name": "Yuxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingming"
            },
            {
                "last_name": "Guo",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  High-quality imaging is crucial for ensuring safety supervision and\nintelligent deployment in fields like transportation and industry. It enables\nprecise and detailed monitoring of operations, facilitating timely detection of\npotential hazards and efficient management. However, adverse weather\nconditions, such as atmospheric haziness and precipitation, can have a\nsignificant impact on image quality. When the atmosphere contains dense haze or\nwater droplets, the incident light scatters, leading to degraded captured\nimages. This degradation is evident in the form of image blur and reduced\ncontrast, increasing the likelihood of incorrect assessments and\ninterpretations by intelligent imaging systems (IIS). To address the challenge\nof restoring degraded images in hazy and rainy conditions, this paper proposes\na novel multi-view knowledge-guided scene recovery network (termed MvKSR).\nSpecifically, guided filtering is performed on the degraded image to separate\nhigh/low-frequency components. Subsequently, an en-decoder-based multi-view\nfeature coarse extraction module (MCE) is used to coarsely extract features\nfrom different views of the degraded image. The multi-view feature fine fusion\nmodule (MFF) will learn and infer the restoration of degraded images through\nmixed supervision under different views. Additionally, we suggest an atrous\nresidual block to handle global restoration and local repair in\nhazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR\noutperforms other state-of-the-art methods in terms of efficiency and stability\nfor restoring degraded scenarios in IIS.\n",
        "title": "MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy\n  Degradation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03904",
        "abstract_url": "http://arxiv.org/abs/2401.03904",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yongjie"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Yang",
                "first_name": "Liying"
            },
            {
                "last_name": "Nie",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Chaoxiong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yiwen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Time-optimal control of a multi-rotor remains an open problem due to the\nunder-actuation and nonlinearity of its dynamics, which make it difficult to\nsolve this problem directly. In this paper, the time-optimal control problem of\nthe multi-rotor is studied. Firstly, a thrust limit optimal decomposition\nmethod is proposed, which can reasonably decompose the limited thrust into\nthree directions according to the current state and the target state. As a\nresult, the thrust limit constraint is decomposed as a linear constraint. With\nthe linear constraint and decoupled dynamics, a time-optimal guidance\ntrajectory can be obtained. Then, a cost function is defined based on the\ntime-optimal guidance trajectory, which has a quadratic form and can be used to\nevaluate the time-optimal performance of the system outputs. Finally, based on\nthe cost function, the time-optimal control problem is reformulated as an MPC\n(Model Predictive Control) problem. The experimental results demonstrate the\nfeasibility and validity of the proposed methods.\n",
        "title": "Guided Time-optimal Model Predictive Control of a Multi-rotor",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03991",
        "abstract_url": "http://arxiv.org/abs/2401.03991",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Fangjun"
            },
            {
                "last_name": "Hogg",
                "first_name": "David C."
            },
            {
                "last_name": "Cohn",
                "first_name": "Anthony G."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "DB",
            "LO"
        ],
        "abstract": "  Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT's spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT's ``cognitive process\", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.\n",
        "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth\n  Evaluation and Enhancement Using the StepGame Benchmark",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04068",
        "abstract_url": "http://arxiv.org/abs/2401.04068",
        "authors": [
            {
                "last_name": "Mathiesen",
                "first_name": "Frederik Baymler"
            },
            {
                "last_name": "Lahijanian",
                "first_name": "Morteza"
            },
            {
                "last_name": "Laurenti",
                "first_name": "Luca"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LO"
        ],
        "abstract": "  In this paper, we present IntervalMDP.jl, a Julia package for probabilistic\nanalysis of interval Markov Decision Processes (IMDPs). IntervalMDP.jl\nfacilitates the synthesis of optimal strategies and verification of IMDPs\nagainst reachability specifications and discounted reward properties. The\nlibrary supports sparse matrices and is compatible with common tools for\nanalysis of probabilistic models, such as PRISM. A key feature of\nIntervalMDP.jl is that it presents both a multi-threaded CPU and a\nGPU-accelerated implementation of value iteration algorithms for IMDPs. In\nparticular, IntervalMDP.jl takes advantage of the Julia type system and the\ninherently parallelizable nature of value iteration to improve the efficiency\nof performing analysis of IMDPs. On a set of examples, we show that\nIntervalMDP.jl substantially outperforms existing tools for verification and\nstrategy synthesis for IMDPs in both computation time and memory consumption.\n",
        "title": "IntervalMDP.jl: Accelerated Value Iteration for Interval Markov Decision\n  Processes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04108",
        "abstract_url": "http://arxiv.org/abs/2401.04108",
        "authors": [
            {
                "last_name": "F\u00f6rster",
                "first_name": "Frank"
            },
            {
                "last_name": "Romeo",
                "first_name": "Marta"
            },
            {
                "last_name": "Holthaus",
                "first_name": "Patrick"
            },
            {
                "last_name": "Trigo",
                "first_name": "Maria Jose Galvez"
            },
            {
                "last_name": "Fischer",
                "first_name": "Joel E."
            },
            {
                "last_name": "Nesset",
                "first_name": "Birthe"
            },
            {
                "last_name": "Dondrup",
                "first_name": "Christian"
            },
            {
                "last_name": "Murad",
                "first_name": "Christine"
            },
            {
                "last_name": "Munteanu",
                "first_name": "Cosmin"
            },
            {
                "last_name": "Cowan",
                "first_name": "Benjamin R."
            },
            {
                "last_name": "Clark",
                "first_name": "Leigh"
            },
            {
                "last_name": "Porcheron",
                "first_name": "Martin"
            },
            {
                "last_name": "Candello",
                "first_name": "Heloisa"
            },
            {
                "last_name": "Langevin",
                "first_name": "Raina"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL",
            "RO",
            "",
            "",
            ""
        ],
        "abstract": "  Workshop proceedings of two co-located workshops \"Working with Troubles and\nFailures in Conversation with Humans and Robots\" (WTF 2023) and \"Is CUI Design\nReady Yet?\", both of which were part of the ACM conference on conversational\nuser interfaces 2023.\n  WTF 23 aimed at bringing together researchers from human-robot interaction,\ndialogue systems, human-computer interaction, and conversation analysis.\nDespite all progress, robotic speech interfaces continue to be brittle in a\nnumber of ways and the experience of failure of such interfaces is commonplace\namongst roboticists. However, the technical literature is positively skewed\ntoward their good performance. The workshop aims to provide a platform for\ndiscussing communicative troubles and failures in human-robot interactions and\nrelated failures in non-robotic speech interfaces. Aims include a scrupulous\ninvestigation into communicative failures, to begin working on a taxonomy of\nsuch failures, and enable a preliminary discussion on possible mitigating\nstrategies. Workshop website: https://sites.google.com/view/wtf2023/overview\n  Is CUI Design Ready Yet? As CUIs become more prevalent in both academic\nresearch and the commercial market, it becomes more essential to design usable\nand adoptable CUIs. While research has been growing on the methods for\ndesigning CUIs for commercial use, there has been little discussion on the\noverall community practice of developing design resources to aid in practical\nCUI design. The aim of this workshop, therefore, is to bring the CUI community\ntogether to discuss the current practices for developing tools and resources\nfor practical CUI design, the adoption (or non-adoption) of these tools and\nresources, and how these resources are utilized in the training and education\nof new CUI designers entering the field. Workshop website:\nhttps://speech-interaction.org/cui2023_design_workshop/index.html\n",
        "title": "Working with Trouble and Failures in Conversation between Humans and\n  Robots (WTF 2023) & Is CUI Design Ready Yet?",
        "date": "2023-09-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04109",
        "abstract_url": "http://arxiv.org/abs/2401.04109",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Youngchan"
            },
            {
                "last_name": "Hwang",
                "first_name": "Eunseung"
            },
            {
                "last_name": "Kai",
                "first_name": "Chang"
            },
            {
                "last_name": "Xu",
                "first_name": "Kaichen"
            },
            {
                "last_name": "Pan",
                "first_name": "Heng"
            },
            {
                "last_name": "Hong",
                "first_name": "Sukjoon"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Recently, the growing interest in wearable technology for personal healthcare\nand smart VR/AR applications newly imposed a need for development of facile\nfabrication method. Regarding the issue, laser has long been proposing original\nanswers to such challenging technological demands with its remote, sterile,\nrapid, and site-selective processing characteristics for arbitrary materials.\nIn this review, recent developments in relevant laser processes are summarized\nin two separate categories. Firstly, transformative approaches represented by\nlaser-induced graphene (LIG) are introduced. Apart from design optimization and\nalteration of native substrate, latest advancements in the transformative\napproach now enable not only more complex material compositions but also\nmultilayer device configurations by simultaneous transformation of\nheterogeneous precursor or sequential addition of functional layers coupled\nwith other electronic elements. Besides, more conventional laser techniques\nsuch as ablation, sintering and synthesis are still accessible for enhancing\nthe functionality of the entire system through expansion of applicable\nmaterials and adoption of new mechanisms. Various wearable device components\ndeveloped through the corresponding laser processes are then organized with\nemphasis on chemical/physical sensors and energy devices. At the same time,\nspecial attention is given to the applications utilizing multiple laser sources\nor multiple laser processes, which pave the way towards all-laser fabrication\nof wearable devices.\n",
        "title": "Recent developments of selective laser processes for wearable devices",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04111",
        "abstract_url": "http://arxiv.org/abs/2401.04111",
        "authors": [
            {
                "last_name": "Orun",
                "first_name": "Ahmet"
            },
            {
                "last_name": "Orun",
                "first_name": "Emre"
            },
            {
                "last_name": "Kurugollu",
                "first_name": "Fatih"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Cyber-attacks keep threatening global networks and information\ninfrastructures. The threat is getting more and more destructive and hard to\ncounter day by day as the global networks continue to enlarge exponentially\nwith limited security counter-measures. As this fact requires more\nsophisticated methods and techniques in urgency, a multidisciplinary remote\ncognitive observation technique is proposed in this paper to meet today's\ncybersecurity needs. The proposed method introduces a non-traditional Cognitive\nPsychology and Artificial Intelligence (AI) based remote threat identification\nwhich can be considered during the cyber security system design. It also\nenables to access the cognitive behavioural parameters of an intruder/hacker\nremotely without any physical contact via online connection, disregarding the\ndistance of the thread. The ultimate goal of this work is to develop a\nsupplementary cognitive cyber security tool for next generations secure online\nbanking, finance or trade systems.\n",
        "title": "Recognition of Cyber-Intrusion patterns in user cognitive behavioural\n  characteristics for remote identification",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04112",
        "abstract_url": "http://arxiv.org/abs/2401.04112",
        "authors": [
            {
                "last_name": "Rosenberg",
                "first_name": "Louis"
            },
            {
                "last_name": "Willcox",
                "first_name": "Gregg"
            },
            {
                "last_name": "Schumann",
                "first_name": "Hans"
            },
            {
                "last_name": "Mani",
                "first_name": "Ganesh"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Conversational Swarm Intelligence (CSI) is a communication technology that\nenables large, networked groups (25 to 2500 people) to hold real-time\nconversational deliberations online. Modeled on the dynamics of biological\nswarms, CSI enables the reasoning benefits of small-groups with the collective\nintelligence benefits of large-groups. In this pilot study, groups of 25 to 30\nparticipants were asked to select players for a weekly Fantasy Football contest\nover an 11-week period. As a baseline, participants filled out a survey to\nrecord their player selections. As an experimental method, participants engaged\nin a real-time text-chat deliberation using a CSI platform called Thinkscape to\ncollaboratively select sets of players. The results show that the real-time\nconversational group using CSI outperformed 66% of survey participants,\ndemonstrating significant amplification of intelligence versus the median\nindividual (p=0.020). The CSI method also significantly outperformed the most\npopular choices from the survey (the Wisdom of Crowd, p<0.001). These results\nsuggest that CSI is an effective technology for amplifying the intelligence of\ngroups engaged in real-time large-scale conversational deliberation and may\noffer a path to collective superintelligence.\n",
        "title": "Conversational Swarm Intelligence amplifies the accuracy of networked\n  groupwise deliberations",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04114",
        "abstract_url": "http://arxiv.org/abs/2401.04114",
        "authors": [
            {
                "last_name": "Kaur",
                "first_name": "Harleen"
            },
            {
                "last_name": "Mendling",
                "first_name": "Jan"
            },
            {
                "last_name": "Rubensson",
                "first_name": "Christoffer"
            },
            {
                "last_name": "Kampik",
                "first_name": "Timotheus"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV",
            "LG"
        ],
        "abstract": "  A key concern of automatic process discovery is to provide insights into\nperformance aspects of business processes. Waiting times are of particular\nimportance in this context. For that reason, it is surprising that current\ntechniques for automatic process discovery generate directly-follows graphs and\ncomparable process models, but often miss the opportunity to explicitly\nrepresent the time axis. In this paper, we present an approach for\nautomatically constructing process models that explicitly align with a time\naxis. We exemplify our approach for directly-follows graphs. Our evaluation\nusing two BPIC datasets and a proprietary dataset highlight the benefits of\nthis representation in comparison to standard layout techniques.\n",
        "title": "Timeline-based Process Discovery",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04116",
        "abstract_url": "http://arxiv.org/abs/2401.04116",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Huaqiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Yangkai"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CV"
        ],
        "abstract": "  Text-to-image generation is conducted through Generative Adversarial Networks\n(GANs) or transformer models. However, the current challenge lies in accurately\ngenerating images based on textual descriptions, especially in scenarios where\nthe content and theme of the target image are ambiguous. In this paper, we\npropose a method that utilizes artificial intelligence models for thematic\ncreativity, followed by a classification modeling of the actual painting\nprocess. The method involves converting all visual elements into quantifiable\ndata structures before creating images. We evaluate the effectiveness of this\napproach in terms of semantic accuracy, image reproducibility, and\ncomputational efficiency, in comparison with existing image generation\nalgorithms.\n",
        "title": "Semantic Draw Engineering for Text-to-Image Creation",
        "date": "2023-12-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04117",
        "abstract_url": "http://arxiv.org/abs/2401.04117",
        "authors": [
            {
                "last_name": "Choudhary",
                "first_name": "Safal"
            },
            {
                "last_name": "Randhawa",
                "first_name": "Princy"
            },
            {
                "last_name": "Jinka",
                "first_name": "Sampath Kumar P"
            },
            {
                "last_name": "C",
                "first_name": "Shiva Prasad H."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Juvenile Idiopathic Arthritis (JIA) is a widespread and chronic condition\nthat affects children and adolescents worldwide. The person suffering from JIA\nis characterized by chronic joint inflammation leading to pain, swelling,\nstiffness, and limited body movements. Individuals suffering from JIA require\nongoing treatment for their lifetime. Beyond inflammation, JIA patients have\nexpressed concerns about various factors and the lack of responsive services\naddressing their challenges. The implementation of smart garments offers a\npromising solution to assist individuals with Juvenile Idiopathic Arthritis in\nperforming their daily activities. These garments are designed to seamlessly\nintegrate technology and clothing, providing not only physical support but also\naddressing the psychological and emotional aspects of living with a chronic\ncondition. By incorporating sensors, these smart garments can monitor joint\nmovement, detect inflammation, and provide real-time feedback to both patients\nand healthcare providers. To tackle these comprehensive challenges, the\nresearch aims to offer a solution through the design of a smart garment,\ncreated with a holistic approach. This smart garment is intended to improve the\noverall well-being of JIA patients by enhancing their mobility, comfort, and\noverall quality of life. The integration of technology into clothing can\npotentially revolutionize the way JIA is managed, allowing patients to better\nmanage their condition and minimize its impact on their daily lives. The\nsynergy between healthcare and technology holds great potential in addressing\nthe multifaceted challenges posed by Juvenile Idiopathic Arthritis patients.\nThrough innovation and empathy, this research aims to pave the way for a\nbrighter future for individuals living with Juvenile Idiopathic Arthritis.\n",
        "title": "A Holistic Approach on Smart Garment for Patients with Juvenile\n  Idiopathic Arthritis",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04118",
        "abstract_url": "http://arxiv.org/abs/2401.04118",
        "authors": [
            {
                "last_name": "Bhattacharya",
                "first_name": "Aditya"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  With Artificial Intelligence (AI) becoming ubiquitous in every application\ndomain, the need for explanations is paramount to enhance transparency and\ntrust among non-technical users. Despite the potential shown by Explainable AI\n(XAI) for enhancing understanding of complex AI systems, most XAI methods are\ndesigned for technical AI experts rather than non-technical consumers.\nConsequently, such explanations are overwhelmingly complex and seldom guide\nusers in achieving their desired predicted outcomes. This paper presents\nongoing research for crafting XAI systems tailored to guide users in achieving\ndesired outcomes through improved human-AI interactions. This paper highlights\nthe research objectives and methods, key takeaways and implications learned\nfrom user studies. It outlines open questions and challenges for enhanced\nhuman-AI collaboration, which the author aims to address in future work.\n",
        "title": "Towards Directive Explanations: Crafting Explainable AI Systems for\n  Actionable Human-AI Interactions",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04119",
        "abstract_url": "http://arxiv.org/abs/2401.04119",
        "authors": [
            {
                "last_name": "Yada",
                "first_name": "Yuki"
            },
            {
                "last_name": "Matsumoto",
                "first_name": "Tsuneo"
            },
            {
                "last_name": "Kido",
                "first_name": "Fuyuko"
            },
            {
                "last_name": "Yamana",
                "first_name": "Hayato"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "LG"
        ],
        "abstract": "  Dark patterns are deceptive user interface designs for online services that\nmake users behave in unintended ways. Dark patterns, such as privacy invasion,\nfinancial loss, and emotional distress, can harm users. These issues have been\nthe subject of considerable debate in recent years. In this paper, we study\ninterpretable dark pattern auto-detection, that is, why a particular user\ninterface is detected as having dark patterns. First, we trained a model using\ntransformer-based pre-trained language models, BERT, on a text-based dataset\nfor the automatic detection of dark patterns in e-commerce. Then, we applied\npost-hoc explanation techniques, including local interpretable model agnostic\nexplanation (LIME) and Shapley additive explanations (SHAP), to the trained\nmodel, which revealed which terms influence each prediction as a dark pattern.\nIn addition, we extracted and analyzed terms that affected the dark patterns.\nOur findings may prevent users from being manipulated by dark patterns, and aid\nin the construction of more equitable internet services. Our code is available\nat https://github.com/yamanalab/why-darkpattern.\n",
        "title": "Why is the User Interface a Dark Pattern? : Explainable Auto-Detection\n  and its Analysis",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04120",
        "abstract_url": "http://arxiv.org/abs/2401.04120",
        "authors": [
            {
                "last_name": "Ramu",
                "first_name": "Dhruv"
            },
            {
                "last_name": "Jain",
                "first_name": "Rishab"
            },
            {
                "last_name": "Jain",
                "first_name": "Aditya"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL"
        ],
        "abstract": "  The growing popularity of generative artificial intelligence (AI) chatbots\nsuch as ChatGPT is having transformative effects on social media. As the\nprevalence of AI-generated content grows, concerns have been raised regarding\nprivacy and misinformation online. Among social media platforms, Discord\nenables AI integrations -- making their primarily \"Generation Z\" userbase\nparticularly exposed to AI-generated content. We surveyed Generation Z aged\nindividuals (n = 335) to evaluate their proficiency in discriminating between\nAI-generated and human-authored text on Discord. The investigation employed\none-shot prompting of ChatGPT, disguised as a text message received on the\nDiscord.com platform. We explore the influence of demographic factors on\nability, as well as participants' familiarity with Discord and artificial\nintelligence technologies. We find that Generation Z individuals are unable to\ndiscern between AI and human-authored text (p = 0.011), and that those with\nlower self-reported familiarity with Discord demonstrated an improved ability\nin identifying human-authored compared to those with self-reported experience\nwith AI (p << 0.0001). Our results suggest that there is a nuanced relationship\nbetween AI technology and popular modes of communication for Generation Z,\ncontributing valuable insights into human-computer interactions, digital\ncommunication, and artificial intelligence literacy.\n",
        "title": "Generation Z's Ability to Discriminate Between AI-generated and\n  Human-Authored Text on Discord",
        "date": "2023-12-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04122",
        "abstract_url": "http://arxiv.org/abs/2401.04122",
        "authors": [
            {
                "last_name": "Shah",
                "first_name": "Chirag"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  As LLMs make their way into many aspects of our lives, one place that\nwarrants increased scrutiny with LLM usage is scientific research. Using LLMs\nfor generating or analyzing data for research purposes is gaining popularity.\nBut when such application is marred with ad-hoc decisions and engineering\nsolutions, we need to be concerned about how it may affect that research, its\nfindings, or any future works based on that research. We need a more scientific\napproach to using LLMs in our research. While there are several active efforts\nto support more systematic construction of prompts, they are often focused more\non achieving desirable outcomes rather than producing replicable and\ngeneralizable knowledge with sufficient transparency, objectivity, or rigor.\nThis article presents a new methodology inspired by codebook construction\nthrough qualitative methods to address that. Using humans in the loop and a\nmulti-phase verification processes, this methodology lays a foundation for more\nsystematic, objective, and trustworthy way of applying LLMs for analyzing data.\nSpecifically, we show how a set of researchers can work through a rigorous\nprocess of labeling, deliberating, and documenting to remove subjectivity and\nbring transparency and replicability to prompt generation process. A set of\nexperiments are presented to show how this methodology can be put in practice.\n",
        "title": "From Prompt Engineering to Prompt Science With Human in the Loop",
        "date": "2023-12-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04124",
        "abstract_url": "http://arxiv.org/abs/2401.04124",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Tinghe"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Agents centered around Large Language Models (LLMs) are now capable of\nautomating mobile device operations for users. After fine-tuning to learn a\nuser's mobile operations, these agents can adhere to high-level user\ninstructions online. They execute tasks such as goal decomposition, sequencing\nof sub-goals, and interactive environmental exploration, until the final\nobjective is achieved. However, privacy concerns related to personalized user\ndata arise during mobile operations, requiring user confirmation. Moreover,\nusers' real-world operations are exploratory, with action data being complex\nand redundant, posing challenges for agent learning. To address these issues,\nin our practical application, we have designed interactive tasks between agents\nand humans to identify sensitive information and align with personalized user\nneeds. Additionally, we integrated Standard Operating Procedure (SOP)\ninformation within the model's in-context learning to enhance the agent's\ncomprehension of complex task execution. Our approach is evaluated on the new\ndevice control benchmark AitW, which encompasses 30K unique instructions across\nmulti-step tasks, including application operation, web searching, and web\nshopping. Experimental results show that the SOP-based agent achieves\nstate-of-the-art performance without incurring additional inference costs,\nboasting an overall action success rate of 66.92%.\n",
        "title": "MobileAgent: enhancing mobile control via human-machine interaction and\n  SOP integration",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04125",
        "abstract_url": "http://arxiv.org/abs/2401.04125",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Zili"
            },
            {
                "last_name": "Chen",
                "first_name": "Keyan"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Liang",
                "first_name": "Shunlin"
            },
            {
                "last_name": "Zou",
                "first_name": "Zhengxia"
            },
            {
                "last_name": "Shi",
                "first_name": "Zhenwei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Accurate weather forecasting holds significant importance to human\nactivities. Currently, there are two paradigms for weather forecasting:\nNumerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP).\nNWP utilizes atmospheric physics for weather modeling but suffers from poor\ndata utilization and high computational costs, while DLP can learn weather\npatterns from vast amounts of data directly but struggles to incorporate\nphysical laws. Both paradigms possess their respective strengths and\nweaknesses, and are incompatible, because physical laws adopted in NWP describe\nthe relationship between coordinates and meteorological variables, while DLP\ndirectly learns the relationships between meteorological variables without\nconsideration of coordinates. To address these problems, we introduce the\nDeepPhysiNet framework, incorporating physical laws into deep learning models\nfor accurate and continuous weather system modeling. First, we construct\nphysics networks based on multilayer perceptrons (MLPs) for individual\nmeteorological variable, such as temperature, pressure, and wind speed. Physics\nnetworks establish relationships between variables and coordinates by taking\ncoordinates as input and producing variable values as output. The physical laws\nin the form of Partial Differential Equations (PDEs) can be incorporated as a\npart of loss function. Next, we construct hyper-networks based on deep learning\nmethods to directly learn weather patterns from a large amount of\nmeteorological data. The output of hyper-networks constitutes a part of the\nweights for the physics networks. Experimental results demonstrate that, upon\nsuccessful integration of physical laws, DeepPhysiNet can accomplish multiple\ntasks simultaneously, not only enhancing forecast accuracy but also obtaining\ncontinuous spatiotemporal resolution results, which is unattainable by either\nthe NWP or DLP.\n",
        "title": "DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for\n  Accurate and Continuous Weather Modeling",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04127",
        "abstract_url": "http://arxiv.org/abs/2401.04127",
        "authors": [
            {
                "last_name": "Millot",
                "first_name": "Laurent"
            },
            {
                "last_name": "Pel\u00e9",
                "first_name": "G\u00e9rard"
            },
            {
                "last_name": "Elliq",
                "first_name": "Mohammed"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD",
            "",
            ""
        ],
        "abstract": "  Audio scene cartography for real or simulated stereo recordings is presented.\nThis audio scene analysis is performed doing successively: a perceptive\n10-subbands analysis, calculation of temporal laws for relative delays and\ngains between both channels of each subband using a short-time cons\\-tant scene\nassumption and channels inter-correlation which permit to follow a mobile\nsource in its moves, calculation of global and subbands histograms whose peaks\ngive the incidence information for fixed sources. Audio scenes composed of 2 to\n4 fixed sources or with a fixed source and a mobile one have been already\nsuccessfully tested. Further extensions and applications will be discussed.\nAudio illustrations of audio scenes, subband analysis and demonstration of\nreal-time stereo recording simulations will be given.Paper 6340 presented at\nthe 118th Convention of the Audio Engineering Society, Barcelona, 2005\n",
        "title": "Using perceptive subbands analysis to perform audio scenes cartography",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04131",
        "abstract_url": "http://arxiv.org/abs/2401.04131",
        "authors": [
            {
                "last_name": "Acay",
                "first_name": "Co\u015fku"
            },
            {
                "last_name": "Gancher",
                "first_name": "Joshua"
            },
            {
                "last_name": "Recto",
                "first_name": "Rolph"
            },
            {
                "last_name": "Myers",
                "first_name": "Andrew C."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "PL"
        ],
        "abstract": "  Developing secure distributed systems is difficult, and even harder when\nadvanced cryptography must be used to achieve security goals. Following prior\nwork, we advocate using secure program partitioning to synthesize cryptographic\napplications: instead of implementing a system of communicating processes, the\nprogrammer implements a centralized, sequential program, which is automatically\ncompiled into a secure distributed version that uses cryptography.\n  While this approach is promising, formal results for the security of such\ncompilers are limited in scope. In particular, no security proof yet\nsimultaneously addresses subtleties essential for robust, efficient\napplications: multiple cryptographic mechanisms, malicious corruption, and\nasynchronous communication.\n  In this work, we develop a compiler security proof that handles these\nsubtleties. Our proof relies on a novel unification of simulation-based\nsecurity, information-flow control, choreographic programming, and\nsequentialization techniques for concurrent programs. While our proof targets\nhybrid protocols, which abstract cryptographic mechanisms as idealized\nfunctionalities, our approach offers a clear path toward leveraging Universal\nComposability to obtain end-to-end, modular security results with fully\ninstantiated cryptographic mechanisms.\n  Finally, following prior observations about simulation-based security, we\nprove that our result guarantees robust hyperproperty preservation, an\nimportant criterion for compiler correctness that preserves all source-level\nsecurity properties in target programs.\n",
        "title": "Secure Synthesis of Distributed Cryptographic Applications (Technical\n  Report)",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04133",
        "abstract_url": "http://arxiv.org/abs/2401.04133",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Ming-Yi"
            },
            {
                "last_name": "Huang",
                "first_name": "Yi-Hsiang"
            },
            {
                "last_name": "Teng",
                "first_name": "You-Chen"
            },
            {
                "last_name": "Wang",
                "first_name": "Chih-Yu"
            },
            {
                "last_name": "Lin",
                "first_name": "Che"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "SI"
        ],
        "abstract": "  Graph Neural Networks (GNNs) excel in various domains, from detecting\ne-commerce spam to social network classification problems. However, the lack of\npublic graph datasets hampers research progress, particularly in heterogeneous\ninformation networks (HIN). The demand for datasets for fair HIN comparisons is\ngrowing due to advancements in GNN interpretation models. In response, we\npropose SynHIN, a unique method for generating synthetic heterogeneous\ninformation networks. SynHIN identifies motifs in real-world datasets,\nsummarizes graph statistics, and constructs a synthetic network. Our approach\nutilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN\nfrom primary motif clusters. After In/Our-Cluster mergers and a post-pruning\nprocess fitting the real dataset constraints, we ensure the synthetic graph\nstatistics align closely with the reference one. SynHIN generates a synthetic\nheterogeneous graph dataset for node classification tasks, using the primary\nmotif as the explanation ground truth. It can adapt and address the lack of\nheterogeneous graph datasets and motif ground truths, proving beneficial for\nassessing heterogeneous graph neural network explainers. We further present a\nbenchmark dataset for future heterogeneous graph explainer model research. Our\nwork marks a significant step towards explainable AI in HGNNs.\n",
        "title": "SynHIN: Generating Synthetic Heterogeneous Information Network for\n  Explainable AI",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04134",
        "abstract_url": "http://arxiv.org/abs/2401.04134",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Frank"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            ""
        ],
        "abstract": "  This paper introduces a new neural network model that aims to mimic the\nbiological brain more closely by structuring the network as a complete directed\ngraph that processes continuous data for each timestep. Current neural networks\nhave structures that vaguely mimic the brain structure, such as neurons,\nconvolutions, and recurrence. The model proposed in this paper adds additional\nstructural properties by introducing cycles into the neuron connections and\nremoving the sequential nature commonly seen in other network layers.\nFurthermore, the model has continuous input and output, inspired by spiking\nneural networks, which allows the network to learn a process of classification,\nrather than simply returning the final result.\n",
        "title": "Web Neural Network with Complete DiGraphs",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04135",
        "abstract_url": "http://arxiv.org/abs/2401.04135",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Haiyang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Chunjiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Detian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Traffic flow prediction plays a crucial role in alleviating traffic\ncongestion and enhancing transport efficiency. While combining graph\nconvolution networks with recurrent neural networks for spatial-temporal\nmodeling is a common strategy in this realm, the restricted structure of\nrecurrent neural networks limits their ability to capture global information.\nFor spatial modeling, many prior studies learn a graph structure that is\nassumed to be fixed and uniform at all time steps, which may not be true. This\npaper introduces a novel traffic prediction framework, Global-Aware Enhanced\nSpatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core\ncomponents: a spatial-temporal graph recurrent neural network and a global\nawareness layer. Within this framework, three innovative prediction models are\nformulated. A sequence-aware graph neural network is proposed and integrated\ninto the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time\nsteps and capture local temporal relationships. To enhance the model's global\nperception, three distinct global spatial-temporal transformer-like\narchitectures (GST^2) are devised for the global awareness layer. We conduct\nextensive experiments on four real traffic datasets and the results demonstrate\nthe superiority of our framework and the three concrete models.\n",
        "title": "Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New\n  Framework For Traffic Flow Prediction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04136",
        "abstract_url": "http://arxiv.org/abs/2401.04136",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haonan"
            },
            {
                "last_name": "Shen",
                "first_name": "Qianli"
            },
            {
                "last_name": "Tong",
                "first_name": "Yao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yang"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The commercialization of diffusion models, renowned for their ability to\ngenerate high-quality images that are often indistinguishable from real ones,\nbrings forth potential copyright concerns. Although attempts have been made to\nimpede unauthorized access to copyrighted material during training and to\nsubsequently prevent DMs from generating copyrighted images, the effectiveness\nof these solutions remains unverified. This study explores the vulnerabilities\nassociated with copyright protection in DMs by introducing a backdoor data\npoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.\nOur attack method operates without requiring access to or control over the\ndiffusion model's training or fine-tuning processes; it merely involves the\ninsertion of poisoning data into the clean training dataset. This data,\ncomprising poisoning images equipped with prompts, is generated by leveraging\nthe powerful capabilities of multimodal large language models and text-guided\nimage inpainting techniques. Our experimental results and analysis confirm the\nmethod's effectiveness. By integrating a minor portion of\nnon-copyright-infringing stealthy poisoning data into the clean\ndataset-rendering it free from suspicion-we can prompt the finetuned diffusion\nmodels to produce copyrighted content when activated by specific trigger\nprompts. These findings underline potential pitfalls in the prevailing\ncopyright protection strategies and underscore the necessity for increased\nscrutiny and preventative measures against the misuse of DMs.\n",
        "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data\n  Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04138",
        "abstract_url": "http://arxiv.org/abs/2401.04138",
        "authors": [
            {
                "last_name": "Torii",
                "first_name": "Maya Grace"
            },
            {
                "last_name": "Murakami",
                "first_name": "Takahito"
            },
            {
                "last_name": "Ochiai",
                "first_name": "Yoichi"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  How would research be like if we still needed to \"send\" papers typed with a\ntypewriter? Our life and research environment have continually evolved, often\naccompanied by controversial opinions about new methodologies. In this paper,\nwe embrace this change by introducing a new approach to qualitative analysis in\nHCI using Large Language Models (LLMs). We detail a method that uses LLMs for\nqualitative data analysis and present a quantitative framework using SBART\ncosine similarity for performance evaluation. Our findings indicate that LLMs\nnot only match the efficacy of traditional analysis methods but also offer\nunique insights. Through a novel dataset and benchmark, we explore LLMs'\ncharacteristics in HCI research, suggesting potential avenues for further\nexploration and application in the field.\n",
        "title": "Expanding Horizons in HCI Research Through LLM-Driven Qualitative\n  Analysis",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04139",
        "abstract_url": "http://arxiv.org/abs/2401.04139",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Hanbeot"
            },
            {
                "last_name": "Cho",
                "first_name": "Yunjeong"
            },
            {
                "last_name": "Kim",
                "first_name": "Hoon-Hee"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study introduces CCNETS (Causal Learning with Causal Cooperative Nets),\na novel generative model-based classifier designed to tackle the challenge of\ngenerating data for imbalanced datasets in pattern recognition. CCNETS is\nuniquely crafted to emulate brain-like information processing and comprises\nthree main components: Explainer, Producer, and Reasoner. Each component is\ndesigned to mimic specific brain functions, which aids in generating\nhigh-quality datasets and enhancing classification performance.\n  The model is particularly focused on addressing the common and significant\nchallenge of handling imbalanced datasets in machine learning. CCNETS's\neffectiveness is demonstrated through its application to a \"fraud dataset,\"\nwhere normal transactions significantly outnumber fraudulent ones (99.83% vs.\n0.17%). Traditional methods often struggle with such imbalances, leading to\nskewed performance metrics. However, CCNETS exhibits superior classification\nability, as evidenced by its performance metrics. Specifically, it achieved an\nF1-score of 0.7992, outperforming traditional models like Autoencoders and\nMulti-layer Perceptrons (MLP) in the same context. This performance indicates\nCCNETS's proficiency in more accurately distinguishing between normal and\nfraudulent patterns.\n  The innovative structure of CCNETS enhances the coherence between generative\nand classification models, helping to overcome the limitations of pattern\nrecognition that rely solely on generative models. This study emphasizes\nCCNETS's potential in diverse applications, especially where quality data\ngeneration and pattern recognition are key. It proves effective in machine\nlearning, particularly for imbalanced datasets. CCNETS overcomes current\nchallenges in these datasets and advances machine learning with brain-inspired\napproaches.\n",
        "title": "CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition\n  in Imbalanced Datasets",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04141",
        "abstract_url": "http://arxiv.org/abs/2401.04141",
        "authors": [
            {
                "last_name": "Zini",
                "first_name": "Julia El"
            },
            {
                "last_name": "Musharrafieh",
                "first_name": "Bassel"
            },
            {
                "last_name": "Awad",
                "first_name": "Mariette"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The fractal dimension provides a statistical index of object complexity by\nstudying how the pattern changes with the measuring scale. Although useful in\nseveral classification tasks, the fractal dimension is under-explored in deep\nlearning applications. In this work, we investigate the features that are\nlearned by deep models and we study whether these deep networks are able to\nencode features as complex and high-level as the fractal dimensions.\nSpecifically, we conduct a correlation analysis experiment to show that deep\nnetworks are not able to extract such a feature in none of their layers. We\ncombine our analytical study with a human evaluation to investigate the\ndifferences between deep learning networks and models that operate on the\nfractal feature solely. Moreover, we show the effectiveness of fractal features\nin applications where the object structure is crucial for the classification\ntask. We empirically show that training a shallow network on fractal features\nachieves performance comparable, even superior in specific cases, to that of\ndeep networks trained on raw data while requiring less computational resources.\nFractals improved the accuracy of the classification by 30% on average while\nrequiring up to 84% less time to train. We couple our empirical study with a\ncomplexity analysis of the computational cost of extracting the proposed\nfractal features, and we study its limitation.\n",
        "title": "On The Potential of The Fractal Geometry and The CNNs Ability to Encode\n  it",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04143",
        "abstract_url": "http://arxiv.org/abs/2401.04143",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Xianghui"
            },
            {
                "last_name": "Wang",
                "first_name": "Xi"
            },
            {
                "last_name": "Athanasiou",
                "first_name": "Nikos"
            },
            {
                "last_name": "Bhatnagar",
                "first_name": "Bharat Lal"
            },
            {
                "last_name": "Huang",
                "first_name": "Chun-Hao P."
            },
            {
                "last_name": "Mo",
                "first_name": "Kaichun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Jia",
                "first_name": "Xia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zerui"
            },
            {
                "last_name": "Cui",
                "first_name": "Liangxian"
            },
            {
                "last_name": "Lin",
                "first_name": "Xiao"
            },
            {
                "last_name": "Qian",
                "first_name": "Bingqiao"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jie"
            },
            {
                "last_name": "Yang",
                "first_name": "Wenfei"
            },
            {
                "last_name": "Nam",
                "first_name": "Hyeongjin"
            },
            {
                "last_name": "Jung",
                "first_name": "Daniel Sungho"
            },
            {
                "last_name": "Kim",
                "first_name": "Kihoon"
            },
            {
                "last_name": "Lee",
                "first_name": "Kyoung Mu"
            },
            {
                "last_name": "Hilliges",
                "first_name": "Otmar"
            },
            {
                "last_name": "Pons-Moll",
                "first_name": "Gerard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Modeling the interaction between humans and objects has been an emerging\nresearch direction in recent years. Capturing human-object interaction is\nhowever a very challenging task due to heavy occlusion and complex dynamics,\nwhich requires understanding not only 3D human pose, and object pose but also\nthe interaction between them. Reconstruction of 3D humans and objects has been\ntwo separate research fields in computer vision for a long time. We hence\nproposed the first RHOBIN challenge: reconstruction of human-object\ninteractions in conjunction with the RHOBIN workshop. It was aimed at bringing\nthe research communities of human and object reconstruction as well as\ninteraction modeling together to discuss techniques and exchange ideas. Our\nchallenge consists of three tracks of 3D reconstruction from monocular RGB\nimages with a focus on dealing with challenging interaction scenarios. Our\nchallenge attracted more than 100 participants with more than 300 submissions,\nindicating the broad interest in the research communities. This paper describes\nthe settings of our challenge and discusses the winning methods of each track\nin more detail. We observe that the human reconstruction task is becoming\nmature even under heavy occlusion settings while object pose estimation and\njoint reconstruction remain challenging tasks. With the growing interest in\ninteraction modeling, we hope this report can provide useful insights and\nfoster future research in this direction. Our workshop website can be found at\n\\href{https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}.\n",
        "title": "RHOBIN Challenge: Reconstruction of Human Object Interaction",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04144",
        "abstract_url": "http://arxiv.org/abs/2401.04144",
        "authors": [
            {
                "last_name": "Gilda",
                "first_name": "Sankalp"
            },
            {
                "last_name": "Bhandari",
                "first_name": "Neel"
            },
            {
                "last_name": "Mak",
                "first_name": "Wendy"
            },
            {
                "last_name": "Panizza",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In this paper, we present results on improving out-of-domain weather\nprediction and uncertainty estimation as part of the \\texttt{Shifts Challenge\non Robustness and Uncertainty under Real-World Distributional Shift} challenge.\nWe find that by leveraging a mixture of experts in conjunction with an advanced\ndata augmentation technique borrowed from the computer vision domain, in\nconjunction with robust \\textit{post-hoc} calibration of predictive\nuncertainties, we can potentially achieve more accurate and better-calibrated\nresults with deep neural networks than with boosted tree models for tabular\ndata. We quantify our predictions using several metrics and propose several\nfuture lines of inquiry and experimentation to boost performance.\n",
        "title": "Robust Calibration For Improved Weather Prediction Under Distributional\n  Shift",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04145",
        "abstract_url": "http://arxiv.org/abs/2401.04145",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Guoming"
            },
            {
                "last_name": "Hou",
                "first_name": "Mingxin"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xiaofang"
            },
            {
                "last_name": "Huang",
                "first_name": "Shuqiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaonan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "RO"
        ],
        "abstract": "  Deep reinforcement learning (DRL) methods have recently shown promise in path\nplanning tasks. However, when dealing with global planning tasks, these methods\nface serious challenges such as poor convergence and generalization. To this\nend, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan\nArbitrarily) in this paper. Firstly, we analyze the reasons of these problems\nfrom the perspective of DRL's observation, revealing that the traditional\ndesign causes DRL to be interfered by irrelevant map information. Secondly, we\ndevelop the LOPA which utilizes a novel attention-enhanced mechanism to attain\nan improved attention capability towards the key information of the\nobservation. Such a mechanism is realized by two steps: (1) an attention model\nis built to transform the DRL's observation into two dynamic views: local and\nglobal, significantly guiding the LOPA to focus on the key information on the\ngiven maps; (2) a dual-channel network is constructed to process these two\nviews and integrate them to attain an improved reasoning capability. The LOPA\nis validated via multi-objective global path planning experiments. The result\nsuggests the LOPA has improved convergence and generalization performance as\nwell as great path planning efficiency.\n",
        "title": "Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep\n  Reinforcement Learning Method for Global Path Planning",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04148",
        "abstract_url": "http://arxiv.org/abs/2401.04148",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Pengxin"
            },
            {
                "last_name": "Jin",
                "first_name": "Pengrong"
            },
            {
                "last_name": "Li",
                "first_name": "Ziyue"
            },
            {
                "last_name": "Bai",
                "first_name": "Lei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Accurate spatial-temporal traffic flow forecasting is crucial in aiding\ntraffic managers in implementing control measures and assisting drivers in\nselecting optimal travel routes. Traditional deep-learning based methods for\ntraffic flow forecasting typically rely on historical data to train their\nmodels, which are then used to make predictions on future data. However, the\nperformance of the trained model usually degrades due to the temporal drift\nbetween the historical and future data. To make the model trained on historical\ndata better adapt to future data in a fully online manner, this paper conducts\nthe first study of the online test-time adaptation techniques for\nspatial-temporal traffic flow forecasting problems. To this end, we propose an\nAdaptive Double Correction by Series Decomposition (ADCSD) method, which first\ndecomposes the output of the trained model into seasonal and trend-cyclical\nparts and then corrects them by two separate modules during the testing phase\nusing the latest observed data entry by entry. In the proposed ADCSD method,\ninstead of fine-tuning the whole trained model during the testing phase, a lite\nnetwork is attached after the trained model, and only the lite network is\nfine-tuned in the testing process each time a data entry is observed. Moreover,\nto satisfy that different time series variables may have different levels of\ntemporal drift, two adaptive vectors are adopted to provide different weights\nfor different time series variables. Extensive experiments on four real-world\ntraffic flow forecasting datasets demonstrate the effectiveness of the proposed\nADCSD method. The code is available at https://github.com/Pengxin-Guo/ADCSD.\n",
        "title": "Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04150",
        "abstract_url": "http://arxiv.org/abs/2401.04150",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Long"
            },
            {
                "last_name": "Li",
                "first_name": "Ziqiang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bingxin"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhongming"
            },
            {
                "last_name": "Li",
                "first_name": "Ao"
            },
            {
                "last_name": "Ge",
                "first_name": "Yongxin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Although few-shot action recognition based on metric learning paradigm has\nachieved significant success, it fails to address the following issues: (1)\ninadequate action relation modeling and underutilization of multi-modal\ninformation; (2) challenges in handling video matching problems with different\nlengths and speeds, and video matching problems with misalignment of video\nsub-actions. To address these issues, we propose a Two-Stream Joint Matching\nmethod based on contrastive learning (TSJM), which consists of two modules:\nMulti-modal Contrastive Learning Module (MCL) and Joint Matching Module (JMM).\nThe objective of the MCL is to extensively investigate the inter-modal mutual\ninformation relationships, thereby thoroughly extracting modal information to\nenhance the modeling of action relationships. The JMM aims to simultaneously\naddress the aforementioned video matching problems. The effectiveness of the\nproposed method is evaluated on two widely used few shot action recognition\ndatasets, namely, SSv2 and Kinetics. Comprehensive ablation experiments are\nalso conducted to substantiate the efficacy of our proposed approach.\n",
        "title": "Two-stream joint matching method based on contrastive learning for\n  few-shot action recognition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04151",
        "abstract_url": "http://arxiv.org/abs/2401.04151",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Qin",
                "first_name": "Chengwei"
            },
            {
                "last_name": "Hazan",
                "first_name": "Elad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Fine-tuning is the primary methodology for tailoring pre-trained large\nlanguage models to specific tasks. As the model's scale and the diversity of\ntasks expand, parameter-efficient fine-tuning methods are of paramount\nimportance. One of the most widely used family of methods is low-rank\nadaptation (LoRA) and its variants. LoRA encodes weight update as the product\nof two low-rank matrices. Despite its advantages, LoRA falls short of\nfull-parameter fine-tuning in terms of generalization error for certain tasks.\n  We introduce Chain of LoRA (COLA), an iterative optimization framework\ninspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full\nparameter fine-tuning, without incurring additional computational costs or\nmemory overheads. COLA employs a residual learning procedure where it merges\nlearned LoRA modules into the pre-trained language model parameters and\nre-initilize optimization for new born LoRA modules. We provide theoretical\nconvergence guarantees as well as empirical results to validate the\neffectiveness of our algorithm. Across various models (OPT and llama-2) and\nseven benchmarking tasks, we demonstrate that COLA can consistently outperform\nLoRA without additional computational or memory costs.\n",
        "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual\n  Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04152",
        "abstract_url": "http://arxiv.org/abs/2401.04152",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Meng",
                "first_name": "Lingwei"
            },
            {
                "last_name": "Cui",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Guo",
                "first_name": "Haohan"
            },
            {
                "last_name": "Wu",
                "first_name": "Xixin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xunying"
            },
            {
                "last_name": "Meng",
                "first_name": "Helen"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "CL",
            ""
        ],
        "abstract": "  End-to-end multi-talker speech recognition has garnered great interest as an\neffective approach to directly transcribe overlapped speech from multiple\nspeakers. Current methods typically adopt either 1) single-input\nmultiple-output (SIMO) models with a branched encoder, or 2) single-input\nsingle-output (SISO) models based on attention-based encoder-decoder\narchitecture with serialized output training (SOT). In this work, we propose a\nCross-Speaker Encoding (CSE) network to address the limitations of SIMO models\nby aggregating cross-speaker representations. Furthermore, the CSE model is\nintegrated with SOT to leverage both the advantages of SIMO and SISO while\nmitigating their drawbacks. To the best of our knowledge, this work represents\nan early effort to integrate SIMO and SISO for multi-talker speech recognition.\nExperiments on the two-speaker LibrispeechMix dataset show that the CES model\nreduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model\nreduces WER by 10% overall and by 16% on high-overlap speech compared to the\nSOT model.\n",
        "title": "Cross-Speaker Encoding Network for Multi-Talker Speech Recognition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04153",
        "abstract_url": "http://arxiv.org/abs/2401.04153",
        "authors": [
            {
                "last_name": "Zimmermann",
                "first_name": "Janos"
            },
            {
                "last_name": "Motejat",
                "first_name": "Michael"
            },
            {
                "last_name": "R\u00f6ssl",
                "first_name": "Christian"
            },
            {
                "last_name": "Theisel",
                "first_name": "Holger"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GR"
        ],
        "abstract": "  FTLE (Finite Time Lyapunov Exponent) computation is one of the standard\napproaches to Lagrangian flow analysis. The main features of interest in FTLE\nfields are ridges that represent hyperbolic Lagrangian Coherent Structures.\nFTLE ridges tend to become sharp and crisp with increasing integration time,\nwhere the sharpness of the ridges is an indicator of the strength of\nseparation. The additional consideration of uncertainty in flows leads to more\nblurred ridges in the FTLE fields. There are multiple causes for such blurred\nridges: either the locations of the ridges are uncertain, or the strength of\nthe ridges is uncertain, or there is low uncertainty but weak separation.\nExisting approaches for uncertain FTLE computation are unable to distinguish\nthese different sources of uncertainty in the ridges. We introduce a new\napproach to define and visualize FTLE fields for flow ensembles. Before\ncomputing and comparing FTLE fields for the ensemble members, we compute\noptimal displacements of the domains to mutually align the ridges of the\nensemble members as much as possible. We do so in a way that an explicit\ngeometry extraction and alignment of the ridges is not necessary. The\nadditional consideration of these displacements allows for a visual distinction\nbetween uncertainty in ridge location, ridge sharpness, and separation\nstrength. We apply the approach to several synthetic and real ensemble data\nsets.\n",
        "title": "FTLE for Flow Ensembles by Optimal Domain Displacement",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04154",
        "abstract_url": "http://arxiv.org/abs/2401.04154",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wentao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.\n",
        "title": "Efficient Selective Audio Masked Multimodal Bottleneck Transformer for\n  Audio-Video Classification",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04155",
        "abstract_url": "http://arxiv.org/abs/2401.04155",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiajia"
            },
            {
                "last_name": "Yang",
                "first_name": "Mengyuan"
            },
            {
                "last_name": "Yu",
                "first_name": "Yankai"
            },
            {
                "last_name": "Xu",
                "first_name": "Haixia"
            },
            {
                "last_name": "Li",
                "first_name": "Kang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Large language models (LLMs) are a class of artificial intelligence models\nbased on deep learning, which have great performance in various tasks,\nespecially in natural language processing (NLP). Large language models\ntypically consist of artificial neural networks with numerous parameters,\ntrained on large amounts of unlabeled input using self-supervised or\nsemi-supervised learning. However, their potential for solving bioinformatics\nproblems may even exceed their proficiency in modeling human language. In this\nreview, we will present a summary of the prominent large language models used\nin natural language processing, such as BERT and GPT, and focus on exploring\nthe applications of large language models at different omics levels in\nbioinformatics, mainly including applications of large language models in\ngenomics, transcriptomics, proteomics, drug discovery and single cell analysis.\nFinally, this review summarizes the potential and prospects of large language\nmodels in solving bioinformatic problems.\n",
        "title": "Large language models in bioinformatics: applications and perspectives",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04157",
        "abstract_url": "http://arxiv.org/abs/2401.04157",
        "authors": [
            {
                "last_name": "Skreta",
                "first_name": "Marta"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zihan"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jia Lin"
            },
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Aspuru-Guzik",
                "first_name": "Al\u00e1n"
            },
            {
                "last_name": "Garg",
                "first_name": "Animesh"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Advancements in large language models (LLMs) have demonstrated their\npotential in facilitating high-level reasoning, logical reasoning and robotics\nplanning. Recently, LLMs have also been able to generate reward functions for\nlow-level robot actions, effectively bridging the interface between high-level\nplanning and low-level robot control. However, the challenge remains that even\nwith syntactically correct plans, robots can still fail to achieve their\nintended goals. This failure can be attributed to imperfect plans proposed by\nLLMs or to unforeseeable environmental circumstances that hinder the execution\nof planned subtasks due to erroneous assumptions about the state of objects.\nOne way to prevent these challenges is to rely on human-provided step-by-step\ninstructions, limiting the autonomy of robotic systems. Vision Language Models\n(VLMs) have shown remarkable success in tasks such as visual question answering\nand image captioning. Leveraging the capabilities of VLMs, we present a novel\nframework called Robotic Replanning with Perception and Language Models\n(RePLan) that enables real-time replanning capabilities for long-horizon tasks.\nThis framework utilizes the physical grounding provided by a VLM's\nunderstanding of the world's state to adapt robot actions when the initial plan\nfails to achieve the desired goal. We test our approach within four\nenvironments containing seven long-horizion tasks. We find that RePLan enables\na robot to successfully adapt to unforeseen obstacles while accomplishing\nopen-ended, long-horizon goals, where baseline models cannot. Find more\ninformation at https://replan-lm.github.io/replan.github.io/\n",
        "title": "RePLan: Robotic Replanning with Perception and Language Models",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04168",
        "abstract_url": "http://arxiv.org/abs/2401.04168",
        "authors": [
            {
                "last_name": "Mart\u00ednez",
                "first_name": "Francisco Ard\u00e9vol"
            },
            {
                "last_name": "Min",
                "first_name": "Michiel"
            },
            {
                "last_name": "Huppenkothen",
                "first_name": "Daniela"
            },
            {
                "last_name": "Kamp",
                "first_name": "Inga"
            },
            {
                "last_name": "Palmer",
                "first_name": "Paul I."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Interpreting the observations of exoplanet atmospheres to constrain physical\nand chemical properties is typically done using Bayesian retrieval techniques.\nBecause these methods require many model computations, a compromise is made\nbetween model complexity and run time. Reaching this compromise leads to the\nsimplification of many physical and chemical processes (e.g. parameterised\ntemperature structure). Here we implement and test sequential neural posterior\nestimation (SNPE), a machine learning inference algorithm, for exoplanet\natmospheric retrievals. The goal is to speed up retrievals so they can be run\nwith more computationally expensive atmospheric models, such as those computing\nthe temperature structure using radiative transfer. We generate 100 synthetic\nobservations using ARCiS (ARtful Modeling Code for exoplanet Science, an\natmospheric modelling code with the flexibility to compute models in varying\ndegrees of complexity) and perform retrievals on them to test the faithfulness\nof the SNPE posteriors. The faithfulness quantifies whether the posteriors\ncontain the ground truth as often as we expect. We also generate a synthetic\nobservation of a cool brown dwarf using the self-consistent capabilities of\nARCiS and run a retrieval with self-consistent models to showcase the\npossibilities that SNPE opens. We find that SNPE provides faithful posteriors\nand is therefore a reliable tool for exoplanet atmospheric retrievals. We are\nable to run a self-consistent retrieval of a synthetic brown dwarf spectrum\nusing only 50,000 forward model evaluations. We find that SNPE can speed up\nretrievals between $\\sim2\\times$ and $\\geq10\\times$ depending on the\ncomputational load of the forward model, the dimensionality of the observation,\nand the signal-to-noise ratio of the observation. We make the code publicly\navailable for the community on Github.\n",
        "title": "FlopPITy: Enabling self-consistent exoplanet atmospheric retrievals with\n  machine learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04181",
        "abstract_url": "http://arxiv.org/abs/2401.04181",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Minjie"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yichen"
            },
            {
                "last_name": "Li",
                "first_name": "Jinming"
            },
            {
                "last_name": "Wen",
                "first_name": "Junjie"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Che",
                "first_name": "Zhengping"
            },
            {
                "last_name": "Shen",
                "first_name": "Chaomin"
            },
            {
                "last_name": "Peng",
                "first_name": "Yaxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Dong"
            },
            {
                "last_name": "Feng",
                "first_name": "Feifei"
            },
            {
                "last_name": "Tang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  The language-conditioned robotic manipulation aims to transfer natural\nlanguage instructions into executable actions, from simple pick-and-place to\ntasks requiring intent recognition and visual reasoning. Inspired by the dual\nprocess theory in cognitive science, which suggests two parallel systems of\nfast and slow thinking in human decision-making, we introduce Robotics with\nFast and Slow Thinking (RFST), a framework that mimics human cognitive\narchitecture to classify tasks and makes decisions on two systems based on\ninstruction types. Our RFST consists of two key components: 1) an instruction\ndiscriminator to determine which system should be activated based on the\ncurrent user instruction, and 2) a slow-thinking system that is comprised of a\nfine-tuned vision language model aligned with the policy networks, which allows\nthe robot to recognize user intention or perform reasoning tasks. To assess our\nmethodology, we built a dataset featuring real-world trajectories, capturing\nactions ranging from spontaneous impulses to tasks requiring deliberate\ncontemplation. Our results, both in simulation and real-world scenarios,\nconfirm that our approach adeptly manages intricate tasks that demand intent\nrecognition and reasoning. The project is available at\nhttps://jlm-z.github.io/RSFT/\n",
        "title": "Language-Conditioned Robotic Manipulation with Fast and Slow Thinking",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04190",
        "abstract_url": "http://arxiv.org/abs/2401.04190",
        "authors": [
            {
                "last_name": "D\u00edaz-Pach\u00f3n",
                "first_name": "Daniel Andr\u00e9s"
            },
            {
                "last_name": "H\u00f6ssjer",
                "first_name": "Ola"
            },
            {
                "last_name": "Mathew",
                "first_name": "Calvin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "",
            ""
        ],
        "abstract": "  Fine-tuning studies whether some physical parameters, or relevant ratios\nbetween them, are located within so-called life-permitting intervals of small\nprobability outside of which carbon-based life would not be possible. Recent\ndevelopments have found estimates of these probabilities that circumvent\nprevious concerns of measurability and selection bias. However, the question\nremains if fine-tuning can indeed be known. Using a mathematization of the\nepistemological concepts of learning and knowledge acquisition, we argue that\nmost examples that have been touted as fine-tuned cannot be formally assessed\nas such. Nevertheless, fine-tuning can be known when the physical parameter is\nseen as a random variable and it is supported in the nonnegative real line,\nprovided the size of the life-permitting interval is small in relation to the\nobserved value of the parameter.\n",
        "title": "Is it possible to know cosmological fine-tuning?",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04191",
        "abstract_url": "http://arxiv.org/abs/2401.04191",
        "authors": [
            {
                "last_name": "Th\u00e9riault",
                "first_name": "Robin"
            },
            {
                "last_name": "Tantari",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Dense Hopfield networks are known for their feature to prototype transition\nand adversarial robustness. However, previous theoretical studies have been\nmostly concerned with their storage capacity. We bridge this gap by studying\nthe phase diagram of p-body Hopfield networks in the teacher-student setting of\nan unsupervised learning problem, uncovering ferromagnetic phases reminiscent\nof the prototype and feature learning regimes. On the Nishimori line, we find\nthe critical size of the training set necessary for efficient pattern\nretrieval. Interestingly, we find that that the paramagnetic to ferromagnetic\ntransition of the teacher-student setting coincides with the paramagnetic to\nspin-glass transition of the direct model, i.e. with random patterns. Outside\nof the Nishimori line, we investigate the learning performance in relation to\nthe inference temperature and dataset noise. Moreover, we show that using a\nlarger p for the student than the teacher gives the student an extensive\ntolerance to noise. We then derive a closed-form expression measuring the\nadversarial robustness of such a student at zero temperature, corroborating the\npositive correlation between number of parameters and robustness observed in\nlarge neural networks. We also use our model to clarify why the prototype phase\nof modern Hopfield networks is adversarially robust.\n",
        "title": "Dense Hopfield Networks in the Teacher-Student Setting",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04192",
        "abstract_url": "http://arxiv.org/abs/2401.04192",
        "authors": [
            {
                "last_name": "Ram\u00edrez",
                "first_name": "Aurora"
            },
            {
                "last_name": "Romero",
                "first_name": "Jos\u00e9 Ra\u00fal"
            },
            {
                "last_name": "Ventura",
                "first_name": "Sebasti\u00e1n"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "NE",
            "",
            "",
            ""
        ],
        "abstract": "  While working on a software specification, designers usually need to evaluate\ndifferent architectural alternatives to be sure that quality criteria are met.\nEven when these quality aspects could be expressed in terms of multiple\nsoftware metrics, other qualitative factors cannot be numerically measured, but\nthey are extracted from the engineer's know-how and prior experiences. In fact,\ndetecting not only strong but also weak points in the different solutions seems\nto fit better with the way humans make their decisions. Putting the human in\nthe loop brings new challenges to the search-based software engineering field,\nespecially for those human-centered activities within the early analysis phase.\nThis paper explores how the interactive evolutionary computation can serve as a\nbasis for integrating the human's judgment into the search process. An\ninteractive approach is proposed to discover software architectures, in which\nboth quantitative and qualitative criteria are applied to guide a\nmulti-objective evolutionary algorithm. The obtained feedback is incorporated\ninto the fitness function using architectural preferences allowing the\nalgorithm to discern between promising and poor solutions. Experimentation with\nreal users has revealed that the proposed interaction mechanism can effectively\nguide the search towards those regions of the search space that are of real\ninterest to the expert.\n",
        "title": "Interactive Multi-Objective Evolutionary Optimization of Software\n  Architectures",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04196",
        "abstract_url": "http://arxiv.org/abs/2401.04196",
        "authors": [
            {
                "last_name": "Blanes",
                "first_name": "Sergio"
            },
            {
                "last_name": "Casas",
                "first_name": "Fernando"
            },
            {
                "last_name": "Gonz\u00e1lez",
                "first_name": "Ces\u00e1reo"
            },
            {
                "last_name": "Thalhammer",
                "first_name": "Mechthild"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  The present work provides a comprehensive study of symmetric-conjugate\noperator splitting methods in the context of linear parabolic problems and\ndemonstrates their additional benefits compared to symmetric splitting methods.\nRelevant applications include nonreversible systems and ground state\ncomputations for linear Schr\\\"odinger equations based on the imaginary time\npropagation. Numerical examples confirm the favourable error behaviour of\nhigher-order symmetric-conjugate splitting methods and illustrate the\nusefulness of a time stepsize control, where the local error estimation relies\non the computation of the imaginary parts and thus requires negligible costs.\n",
        "title": "Symmetric-conjugate splitting methods for evolution equations of\n  parabolic type",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04198",
        "abstract_url": "http://arxiv.org/abs/2401.04198",
        "authors": [
            {
                "last_name": "Dewan",
                "first_name": "Shaurya"
            },
            {
                "last_name": "Jain",
                "first_name": "Anisha"
            },
            {
                "last_name": "LaLena",
                "first_name": "Zoe"
            },
            {
                "last_name": "Yu",
                "first_name": "Lifan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The authors of 'Unsupervised Reinforcement Learning in Multiple environments'\npropose a method, alpha-MEPOL, to tackle unsupervised RL across multiple\nenvironments. They pre-train a task-agnostic exploration policy using\ninteractions from an entire environment class and then fine-tune this policy\nfor various tasks using supervision. We expanded upon this work, with the goal\nof improving performance. We primarily propose and experiment with five new\nmodifications to the original work: sampling trajectories using an\nentropy-based probability distribution, dynamic alpha, higher KL Divergence\nthreshold, curiosity-driven exploration, and alpha-percentile sampling on\ncuriosity. Dynamic alpha and higher KL-Divergence threshold both provided a\nsignificant improvement over the baseline from the earlier work. PDF-sampling\nfailed to provide any improvement due to it being approximately equivalent to\nthe baseline method when the sample space is small. In high-dimensional\nenvironments, the addition of curiosity-driven exploration enhances learning by\nencouraging the agent to seek diverse experiences and explore the unknown more.\nHowever, its benefits are limited in low-dimensional and simpler environments\nwhere exploration possibilities are constrained and there is little that is\ntruly unknown to the agent. Overall, some of our experiments did boost\nperformance over the baseline and there are a few directions that seem\npromising for further research.\n",
        "title": "Curiosity & Entropy Driven Unsupervised RL in Multiple Environments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04210",
        "abstract_url": "http://arxiv.org/abs/2401.04210",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhi-Song"
            },
            {
                "last_name": "Courant",
                "first_name": "Robin"
            },
            {
                "last_name": "Kalogeiton",
                "first_name": "Vicky"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  Automatically understanding funny moments (i.e., the moments that make people\nlaugh) when watching comedy is challenging, as they relate to various features,\nsuch as body language, dialogues and culture. In this paper, we propose\nFunnyNet-W, a model that relies on cross- and self-attention for visual, audio\nand text data to predict funny moments in videos. Unlike most methods that rely\non ground truth data in the form of subtitles, in this work we exploit\nmodalities that come naturally with videos: (a) video frames as they contain\nvisual information indispensable for scene understanding, (b) audio as it\ncontains higher-level cues associated with funny moments, such as intonation,\npitch and pauses and (c) text automatically extracted with a speech-to-text\nmodel as it can provide rich information when processed by a Large Language\nModel. To acquire labels for training, we propose an unsupervised approach that\nspots and labels funny audio moments. We provide experiments on five datasets:\nthe sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive\nexperiments and analysis show that FunnyNet-W successfully exploits visual,\nauditory and textual cues to identify funny moments, while our findings reveal\nFunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the\nnew state of the art for funny moment detection with multimodal cues on all\ndatasets with and without using ground truth information.\n",
        "title": "FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04212",
        "abstract_url": "http://arxiv.org/abs/2401.04212",
        "authors": [
            {
                "last_name": "Rodriguez-Fernandez",
                "first_name": "Victor"
            },
            {
                "last_name": "Sarangerel",
                "first_name": "Sumiyajav"
            },
            {
                "last_name": "Siew",
                "first_name": "Peng Mun"
            },
            {
                "last_name": "Machuca",
                "first_name": "Pablo"
            },
            {
                "last_name": "Jang",
                "first_name": "Daniel"
            },
            {
                "last_name": "Linares",
                "first_name": "Richard"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  With the rapid increase in the number of Anthropogenic Space Objects (ASOs),\nLow Earth Orbit (LEO) is facing significant congestion, thereby posing\nchallenges to space operators and risking the viability of the space\nenvironment for varied uses. Current models for examining this evolution, while\ndetailed, are computationally demanding. To address these issues, we propose a\nnovel machine learning-based model, as an extension of the MIT Orbital Capacity\nTool (MOCAT). This advanced model is designed to accelerate the propagation of\nASO density distributions, and it is trained on hundreds of simulations\ngenerated by an established and accurate model of the space environment\nevolution. We study how different deep learning-based solutions can potentially\nbe good candidates for ASO propagation and manage the high-dimensionality of\nthe data. To assess the model's capabilities, we conduct experiments in long\nterm forecasting scenarios (around 100 years), analyze how and why the\nperformance degrades over time, and discuss potential solutions to make this\nsolution better.\n",
        "title": "Towards a Machine Learning-Based Approach to Predict Space Object\n  Density Distributions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04218",
        "abstract_url": "http://arxiv.org/abs/2401.04218",
        "authors": [
            {
                "last_name": "Fulman",
                "first_name": "Nir"
            },
            {
                "last_name": "Memduho\u011flu",
                "first_name": "Abdulkadir"
            },
            {
                "last_name": "Zipf",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We present a benchmark for assessing the capability of Large Language Models\n(LLMs) to discern intercardinal directions between geographic locations and\napply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark\nspecifically evaluates whether LLMs exhibit a hierarchical spatial bias similar\nto humans, where judgments about individual locations' spatial relationships\nare influenced by the perceived relationships of the larger groups that contain\nthem. To investigate this, we formulated 14 questions focusing on well-known\nAmerican cities. Seven questions were designed to challenge the LLMs with\nscenarios potentially influenced by the orientation of larger geographical\nunits, such as states or countries, while the remaining seven targeted\nlocations less susceptible to such hierarchical categorization. Among the\ntested models, GPT-4 exhibited superior performance with 55.3% accuracy,\nfollowed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed\nsignificantly reduced accuracy on tasks with suspected hierarchical bias. For\nexample, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on\nothers. Despite these inaccuracies, the models identified the nearest cardinal\ndirection in most cases, suggesting associative learning, embodying human-like\nmisconceptions. We discuss the potential of text-based data representing\ngeographic relationships directly to improve the spatial reasoning capabilities\nof LLMs.\n",
        "title": "Distortions in Judged Spatial Relations in Large Language Models: The\n  Dawn of Natural Language Geographic Data?",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04221",
        "abstract_url": "http://arxiv.org/abs/2401.04221",
        "authors": [
            {
                "last_name": "Malakar",
                "first_name": "Sanjay"
            },
            {
                "last_name": "Haider",
                "first_name": "Tameem Bin"
            },
            {
                "last_name": "Shahriar",
                "first_name": "Rifat"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "PL"
        ],
        "abstract": "  Fixing software bugs has always been an essential and time-consuming process\nin software development. Fixing concurrency bugs has become especially critical\nin the multicore era. However, fixing concurrency bugs is challenging due to\nnon-deterministic failures and tricky parallel reasoning. Beyond correctly\nfixing the original problem in the software, a good patch should also avoid\nintroducing new bugs, degrading performance unnecessarily, or damaging software\nreadability. Existing tools cannot automate the whole fixing process and\nprovide good-quality patches. We present RaceFixer, a tool that automates the\nprocess of fixing one common type of concurrency bug: single-variable atomicity\nviolations. RaceFixer starts from the bug reports of an existing bug-detection\ntool ThreadSanitizer. It augments these with static analysis to construct a\nsuitable patch for each bug report. It tries to combine the patches of multiple\nbugs for better performance and code readability. Finally, we test RaceFixer on\nbenchmarks from TheadSanitizer.\n",
        "title": "RaceFixer -- An Automated Data Race Fixer",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04226",
        "abstract_url": "http://arxiv.org/abs/2401.04226",
        "authors": [
            {
                "last_name": "Huin",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Martin",
                "first_name": "S\u00e9bastien"
            },
            {
                "last_name": "Leguay",
                "first_name": "J\u00e9r\u00e9mie"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Multi-topology routing (MTR) provides an attractive alternative to segment\nrouting for traffic engineering when network devices cannot be upgraded.\nHowever, due to a high overhead in terms of link state messages exchanged by\ntopologies and the need to frequently update link weights to follow evolving\nnetwork conditions, MTR is often limited to a small number of topologies and\nthe satisfaction of loose QoS constraints. To overcome these limitations we\npropose vMTR, an MTR extension where demands are routed over virtual topologies\nthat are silent, i.e., they do not exchange LSA messages, and that are\ncontinuously derived from a very limited set of real topologies, optimizing\neach a QoS parameter. In this context, we present a polynomial and exact\nalgorithm for vMTR and, as a benchmark, a local search algorithm for MTR. We\nshow that vMTR helps reducing drastically the number of real topologies and\nthat it is more robust to QoS changes.\n",
        "title": "Virtual Multi-Topology Routing for QoS Constraints",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04230",
        "abstract_url": "http://arxiv.org/abs/2401.04230",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Chengjie"
            },
            {
                "last_name": "Abdelzad",
                "first_name": "Vahdat"
            },
            {
                "last_name": "Sedwards",
                "first_name": "Sean"
            },
            {
                "last_name": "Czarnecki",
                "first_name": "Krzysztof"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We consider the problem of cross-sensor domain adaptation in the context of\nLiDAR-based 3D object detection and propose Stationary Object Aggregation\nPseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary\nobjects. In contrast to the current state-of-the-art in-domain practice of\naggregating just a few input scans, SOAP aggregates entire sequences of point\nclouds at the input level to reduce the sensor domain gap. Then, by means of\nwhat we call quasi-stationary training and spatial consistency post-processing,\nthe SOAP model generates accurate pseudo-labels for stationary objects, closing\na minimum of 30.3% domain gap compared to few-frame detectors. Our results also\nshow that state-of-the-art domain adaptation approaches can achieve even\ngreater performance in combination with SOAP, in both the unsupervised and\nsemi-supervised settings.\n",
        "title": "SOAP: Cross-sensor Domain Adaptation for 3D Object Detection Using\n  Stationary Object Aggregation Pseudo-labelling",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04232",
        "abstract_url": "http://arxiv.org/abs/2401.04232",
        "authors": [
            {
                "last_name": "Alves",
                "first_name": "Caio"
            },
            {
                "last_name": "Restrepo",
                "first_name": "Juan M."
            },
            {
                "last_name": "Ramirez",
                "first_name": "Jorge M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  In this paper we revisit the problem of decomposing a signal into a tendency\nand a residual. The tendency describes an executive summary of a signal that\nencapsulates its notable characteristics while disregarding seemingly random,\nless interesting aspects. Building upon the Intrinsic Time Decomposition (ITD)\nand information-theoretical analysis, we introduce two alternative procedures\nfor selecting the tendency from the ITD baselines. The first is based on the\nmaximum extrema prominence, namely the maximum difference between extrema\nwithin each baseline. Specifically this method selects the tendency as the\nbaseline from which an ITD step would produce the largest decline of the\nmaximum prominence. The second method uses the rotations from the ITD and\nselects the tendency as the last baseline for which the associated rotation is\nstatistically stationary. We delve into a comparative analysis of the\ninformation content and interpretability of the tendencies obtained by our\nproposed methods and those obtained through conventional low-pass filtering\nschemes, particularly the Hodrik-Prescott (HP) filter. Our findings underscore\na fundamental distinction in the nature and interpretability of these\ntendencies, highlighting their context-dependent utility with emphasis in\nmulti-scale signals. Through a series of real-world applications, we\ndemonstrate the computational robustness and practical utility of our proposed\ntendencies, emphasizing their adaptability and relevance in diverse time series\ncontexts.\n",
        "title": "Estimating an Executive Summary of a Time Series: The Tendency",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04235",
        "abstract_url": "http://arxiv.org/abs/2401.04235",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Christopher"
            },
            {
                "last_name": "Wang",
                "first_name": "Gary"
            },
            {
                "last_name": "Kastner",
                "first_name": "Kyle"
            },
            {
                "last_name": "Su",
                "first_name": "Heng"
            },
            {
                "last_name": "Chen",
                "first_name": "Allen"
            },
            {
                "last_name": "Rosenberg",
                "first_name": "Andrew"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhehuai"
            },
            {
                "last_name": "Wu",
                "first_name": "Zelin"
            },
            {
                "last_name": "Velikovich",
                "first_name": "Leonid"
            },
            {
                "last_name": "Rondon",
                "first_name": "Pat"
            },
            {
                "last_name": "Caseiro",
                "first_name": "Diamantino"
            },
            {
                "last_name": "Aleksic",
                "first_name": "Petar"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Automatic speech recognition (ASR) systems can suffer from poor recall for\nvarious reasons, such as noisy audio, lack of sufficient training data, etc.\n  Previous work has shown that recall can be improved by retrieving rewrite\ncandidates from a large database of likely, contextually-relevant alternatives\nto the hypothesis text using nearest-neighbors search over embeddings of the\nASR hypothesis text to correct and candidate corrections.\n  However, ASR-hypothesis-based retrieval can yield poor precision if the\ntextual hypotheses are too phonetically dissimilar to the transcript truth. In\nthis paper, we eliminate the hypothesis-audio mismatch problem by querying the\ncorrection database directly using embeddings derived from the utterance audio;\nthe embeddings of the utterance audio and candidate corrections are produced by\nmultimodal speech-text embedding networks trained to place the embedding of the\naudio of an utterance and the embedding of its corresponding textual transcript\nclose together.\n  After locating an appropriate correction candidate using nearest-neighbor\nsearch, we score the candidate with its speech-text embedding distance before\nadding the candidate to the original n-best list.\n  We show a relative word error rate (WER) reduction of 6% on utterances whose\ntranscripts appear in the candidate set, without increasing WER on general\nutterances.\n",
        "title": "High-precision Voice Search Query Correction via Retrievable Speech-text\n  Embedings",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04237",
        "abstract_url": "http://arxiv.org/abs/2401.04237",
        "authors": [
            {
                "last_name": "Iommazzo",
                "first_name": "Gabriele"
            },
            {
                "last_name": "D'Ambrosio",
                "first_name": "Claudia"
            },
            {
                "last_name": "Frangioni",
                "first_name": "Antonio"
            },
            {
                "last_name": "Liberti",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We propose a methodology, based on machine learning and optimization, for\nselecting a solver configuration for a given instance. First, we employ a set\nof solved instances and configurations in order to learn a performance function\nof the solver. Secondly, we formulate a mixed-integer nonlinear program where\nthe objective/constraints explicitly encode the learnt information, and which\nwe solve, upon the arrival of an unknown instance, to find the best solver\nconfiguration for that instance, based on the performance function. The main\nnovelty of our approach lies in the fact that the configuration set search\nproblem is formulated as a mathematical program, which allows us to a) enforce\nhard dependence and compatibility constraints on the configurations, and b)\nsolve it efficiently with off-the-shelf optimization tools.\n",
        "title": "A learning-based mathematical programming formulation for the automatic\n  configuration of optimization solvers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04241",
        "abstract_url": "http://arxiv.org/abs/2401.04241",
        "authors": [
            {
                "last_name": "Leyva",
                "first_name": "Roberto"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Victor"
            },
            {
                "last_name": "Epiphaniou",
                "first_name": "Gregory"
            },
            {
                "last_name": "Maple",
                "first_name": "Carsten"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face image synthesis detection is considerably gaining attention because of\nthe potential negative impact on society that this type of synthetic data\nbrings. In this paper, we propose a data-agnostic solution to detect the face\nimage synthesis process. Specifically, our solution is based on an anomaly\ndetection framework that requires only real data to learn the inference\nprocess. It is therefore data-agnostic in the sense that it requires no\nsynthetic face images. The solution uses the posterior probability with respect\nto the reference data to determine if new samples are synthetic or not. Our\nevaluation results using different synthesizers show that our solution is very\ncompetitive against the state-of-the-art, which requires synthetic data for\ntraining.\n",
        "title": "Data-Agnostic Face Image Synthesis Detection Using Bayesian CNNs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04242",
        "abstract_url": "http://arxiv.org/abs/2401.04242",
        "authors": [
            {
                "last_name": "Loregian",
                "first_name": "Fosco"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "FL"
        ],
        "abstract": "  We study generalized automata (in the sense of Ad\\'amek-Trnkov\\'a) in Joyal's\ncategory of (set-valued) combinatorial species, and as an important preliminary\nstep, we study coalgebras for its derivative endofunctor $\\partial$ and for the\n`Euler homogeneity operator' $L\\circ\\partial$ arising from the adjunction\n$L\\dashv\\partial\\dashv R$.\n",
        "title": "Automata and coalgebras in categories of species",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04244",
        "abstract_url": "http://arxiv.org/abs/2401.04244",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xingguang"
            },
            {
                "last_name": "Chimitt",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Chi",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Chan",
                "first_name": "Stanley H."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Recovering images distorted by atmospheric turbulence is a challenging\ninverse problem due to the stochastic nature of turbulence. Although numerous\nturbulence mitigation (TM) algorithms have been proposed, their efficiency and\ngeneralization to real-world dynamic scenarios remain severely limited.\nBuilding upon the intuitions of classical TM algorithms, we present the Deep\nAtmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major\nchallenges when transitioning from classical to deep learning approaches. By\ncarefully integrating the merits of classical multi-frame TM methods into a\ndeep network structure, we demonstrate that DATUM can efficiently perform\nlong-range temporal aggregation using a recurrent fashion, while deformable\nattention and temporal-channel attention seamlessly facilitate pixel\nregistration and lucky imaging. With additional supervision, tilt and blur\ndegradation can be jointly mitigated. These inductive biases empower DATUM to\nsignificantly outperform existing methods while delivering a tenfold increase\nin processing speed. A large-scale training dataset, ATSyn, is presented as a\nco-invention to enable generalization in real turbulence. Our code and datasets\nwill be available at\n\\href{https://xg416.github.io/DATUM}{\\textcolor{pink}{https://xg416.github.io/DATUM}}\n",
        "title": "Spatio-Temporal Turbulence Mitigation: A Translational Perspective",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04246",
        "abstract_url": "http://arxiv.org/abs/2401.04246",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Joseph C."
            },
            {
                "last_name": "Bloore",
                "first_name": "David"
            },
            {
                "last_name": "Kapoor",
                "first_name": "Karan"
            },
            {
                "last_name": "Feng",
                "first_name": "Jun"
            },
            {
                "last_name": "Hao",
                "first_name": "Ming-Hong"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengdi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The Boltzmann distribution of a protein provides a roadmap to all of its\nfunctional states. Normalizing flows are a promising tool for modeling this\ndistribution, but current methods are intractable for typical pharmacological\ntargets; they become computationally intractable due to the size of the system,\nheterogeneity of intra-molecular potential energy, and long-range interactions.\nTo remedy these issues, we present a novel flow architecture that utilizes\nsplit channels and gated attention to efficiently learn the conformational\ndistribution of proteins defined by internal coordinates. We show that by\nutilizing a 2-Wasserstein loss, one can smooth the transition from maximum\nlikelihood training to energy-based training, enabling the training of\nBoltzmann Generators for macromolecules. We evaluate our model and training\nstrategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein\nG, a 56-residue protein. We demonstrate that standard architectures and\ntraining strategies, such as maximum likelihood alone, fail while our novel\narchitecture and multi-stage training strategy are able to model the\nconformational distributions of protein G and HP35.\n",
        "title": "Scalable Normalizing Flows Enable Boltzmann Generators for\n  Macromolecules",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04247",
        "abstract_url": "http://arxiv.org/abs/2401.04247",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lijun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Martin",
                "first_name": "Antoni Viros"
            },
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            },
            {
                "last_name": "Brun",
                "first_name": "Yuriy"
            },
            {
                "last_name": "Guan",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Watermarking images is critical for tracking image provenance and claiming\nownership. With the advent of generative models, such as stable diffusion, able\nto create fake but realistic images, watermarking has become particularly\nimportant, e.g., to make generated images reliably identifiable. Unfortunately,\nthe very same stable diffusion technology can remove watermarks injected using\nexisting methods. To address this problem, we present a ZoDiac, which uses a\npre-trained stable diffusion model to inject a watermark into the trainable\nlatent space, resulting in watermarks that can be reliably detected in the\nlatent vector, even when attacked. We evaluate ZoDiac on three benchmarks,\nMS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against\nstate-of-the-art watermark attacks, with a watermark detection rate over 98%\nand a false positive rate below 6.4%, outperforming state-of-the-art\nwatermarking methods. Our research demonstrates that stable diffusion is a\npromising approach to robust watermarking, able to withstand even\nstable-diffusion-based attacks.\n",
        "title": "Robust Image Watermarking using Stable Diffusion",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04248",
        "abstract_url": "http://arxiv.org/abs/2401.04248",
        "authors": [
            {
                "last_name": "Dytso",
                "first_name": "Alex"
            },
            {
                "last_name": "Cardone",
                "first_name": "Martina"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper investigates the rate-distortion function, under a squared error\ndistortion $D$, for an $n$-dimensional random vector uniformly distributed on\nan $(n-1)$-sphere of radius $R$. First, an expression for the rate-distortion\nfunction is derived for any values of $n$, $D$, and $R$. Second, two types of\nasymptotics with respect to the rate-distortion function of a Gaussian source\nare characterized. More specifically, these asymptotics concern the\nlow-distortion regime (that is, $D \\to 0$) and the high-dimensional regime\n(that is, $n \\to \\infty$).\n",
        "title": "Uniform Distribution on $(n-1)$-Sphere: Rate-Distortion under Squared\n  Error Distortion",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04249",
        "abstract_url": "http://arxiv.org/abs/2401.04249",
        "authors": [
            {
                "last_name": "Ghahremani",
                "first_name": "Behzad"
            },
            {
                "last_name": "Babaee",
                "first_name": "Hessam"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We introduce a Tucker tensor cross approximation method that constructs a\nlow-rank representation of a $d$-dimensional tensor by sparsely sampling its\nfibers. These fibers are selected using the discrete empirical interpolation\nmethod (DEIM). Our proposed algorithm is referred to as DEIM fiber sampling\n(DEIM-FS). For a rank-$r$ approximation of an $\\mathcal{O}(N^d)$ tensor,\nDEIM-FS requires access to only $dNr^{d-1}$ tensor entries, a requirement that\nscales linearly with the tensor size along each mode. We demonstrate that\nDEIM-FS achieves an approximation accuracy close to the Tucker-tensor\napproximation obtained via higher-order singular value decomposition at a\nsignificantly reduced cost. We also present DEIM-FS (iterative) that does not\nrequire access to singular vectors of the target tensor unfolding and can be\nviewed as a black-box Tucker tensor algorithm. We employ DEIM-FS to reduce the\ncomputational cost associated with solving nonlinear tensor differential\nequations (TDEs) using dynamical low-rank approximation (DLRA). The\ncomputational cost of solving DLRA equations can become prohibitive when the\nexact rank of the right-hand side tensor is large. This issue arises in many\nTDEs, especially in cases involving non-polynomial nonlinearities, where the\nright-hand side tensor has full rank. This necessitates the storage and\ncomputation of tensors of size $\\mathcal{O}(N^d)$. We show that DEIM-FS results\nin significant computational savings for DLRA by constructing a low-rank Tucker\napproximation of the right-hand side tensor on the fly. Another advantage of\nusing DEIM-FS is to significantly simplify the implementation of DLRA\nequations, irrespective of the type of TDEs. We demonstrate the efficiency of\nthe algorithm through several examples including solving high-dimensional\npartial differential equations.\n",
        "title": "A DEIM Tucker Tensor Cross Algorithm and its Application to Dynamical\n  Low-Rank Approximation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04250",
        "abstract_url": "http://arxiv.org/abs/2401.04250",
        "authors": [
            {
                "last_name": "Taiwo",
                "first_name": "Funmilola Mary"
            },
            {
                "last_name": "Islambekov",
                "first_name": "Umar"
            },
            {
                "last_name": "Akcora",
                "first_name": "Cuneyt Gurcan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Topological Data Analysis (TDA) has been praised by researchers for its\nability to capture intricate shapes and structures within data. TDA is\nconsidered robust in handling noisy and high-dimensional datasets, and its\ninterpretability is believed to promote an intuitive understanding of model\nbehavior. However, claims regarding the power and usefulness of TDA have only\nbeen partially tested in application domains where TDA-based models are\ncompared to other graph machine learning approaches, such as graph neural\nnetworks. We meticulously test claims on TDA through a comprehensive set of\nexperiments and validate their merits. Our results affirm TDA's robustness\nagainst outliers and its interpretability, aligning with proponents' arguments.\nHowever, we find that TDA does not significantly enhance the predictive power\nof existing methods in our specific experiments, while incurring significant\ncomputational costs. We investigate phenomena related to graph characteristics,\nsuch as small diameters and high clustering coefficients, to mitigate the\ncomputational expenses of TDA computations. Our results offer valuable\nperspectives on integrating TDA into graph machine learning tasks.\n",
        "title": "Explaining the Power of Topological Data Analysis in Graph Machine\n  Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04257",
        "abstract_url": "http://arxiv.org/abs/2401.04257",
        "authors": [
            {
                "last_name": "Leyva",
                "first_name": "Roberto"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Victor"
            },
            {
                "last_name": "Epiphaniou",
                "first_name": "Gregory"
            },
            {
                "last_name": "Maple",
                "first_name": "Carsten"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face image synthesis is gaining more attention in computer security due to\nconcerns about its potential negative impacts, including those related to fake\nbiometrics. Hence, building models that can detect the synthesized face images\nis an important challenge to tackle. In this paper, we propose a fusion-based\nstrategy to detect face image synthesis while providing resiliency to several\nattacks. The proposed strategy uses a late fusion of the outputs computed by\nseveral undisclosed models by relying on random polynomial coefficients and\nexponents to conceal a new feature space. Unlike existing concealing solutions,\nour strategy requires no quantization, which helps to preserve the feature\nspace. Our experiments reveal that our strategy achieves state-of-the-art\nperformance while providing protection against poisoning, perturbation,\nbackdoor, and reverse model attacks.\n",
        "title": "Detecting Face Synthesis Using a Concealed Fusion Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04259",
        "abstract_url": "http://arxiv.org/abs/2401.04259",
        "authors": [
            {
                "last_name": "D'Arcy",
                "first_name": "Mike"
            },
            {
                "last_name": "Hope",
                "first_name": "Tom"
            },
            {
                "last_name": "Birnbaum",
                "first_name": "Larry"
            },
            {
                "last_name": "Downey",
                "first_name": "Doug"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We study the ability of LLMs to generate feedback for scientific papers and\ndevelop MARG, a feedback generation approach using multiple LLM instances that\nengage in internal discussion. By distributing paper text across agents, MARG\ncan consume the full text of papers beyond the input length limitations of the\nbase LLM, and by specializing agents and incorporating sub-tasks tailored to\ndifferent comment types (experiments, clarity, impact) it improves the\nhelpfulness and specificity of feedback. In a user study, baseline methods\nusing GPT-4 were rated as producing generic or very generic comments more than\nhalf the time, and only 1.7 comments per paper were rated as good overall in\nthe best baseline. Our system substantially improves the ability of GPT-4 to\ngenerate specific and helpful feedback, reducing the rate of generic comments\nfrom 60% to 29% and generating 3.7 good comments per paper (a 2.2x\nimprovement).\n",
        "title": "MARG: Multi-Agent Review Generation for Scientific Papers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04261",
        "abstract_url": "http://arxiv.org/abs/2401.04261",
        "authors": [
            {
                "last_name": "Langhammer",
                "first_name": "Martin"
            },
            {
                "last_name": "Constantinides",
                "first_name": "George A."
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Current soft processor architectures for FPGAs do not utilize the potential\nof the massive parallelism available. FPGAs now support many thousands of\nembedded floating point operators, and have similar computational densities to\nGPGPUs. Several soft GPGPU or SIMT processors have been published, but the\nreported large areas and modest Fmax makes their widespread use unlikely for\ncommercial designs. In this paper we take an alternative approach, building the\nsoft GPU microarchitecture around the FPGA resource mix available. We\ndemonstrate a statically scalable soft GPGPU processor (where both parameters\nand feature set can be determined at configuration time) that always closes\ntiming at the peak speed of the slowest embedded component in the FPGA (DSP or\nhard memory), with a completely unconstrained compile into a current Intel\nAgilex FPGA. We also show dynamic scalability, where a subset of the thread\nspace can be specified on an instruction-by-instruction basis.\n  For one example core type, we show a logic range -- depending on the\nconfiguration -- of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to\n250 M20K memories. All of these instances close timing at 771 MHz, a\nperformance level limited only by the DSP Blocks. We describe our methodology\nfor reliably achieving this clock rate by matching the processor pipeline\nstructure to the physical structure of the FPGA fabric. We also benchmark\nseveral algorithms across a range of data sizes, and compare to a commercial\nsoft RISC processor.\n",
        "title": "A Statically and Dynamically Scalable Soft GPGPU",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04264",
        "abstract_url": "http://arxiv.org/abs/2401.04264",
        "authors": [
            {
                "last_name": "Diamond",
                "first_name": "N'yoma"
            },
            {
                "last_name": "Murai",
                "first_name": "Fabricio"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "MA",
            "",
            ""
        ],
        "abstract": "  Many high-stakes decision-making problems, such as those found within\ncybersecurity and economics, can be modeled as competitive resource allocation\ngames. In these games, multiple players must allocate limited resources to\novercome their opponent(s), while minimizing any induced individual losses.\nHowever, existing means of assessing the performance of resource allocation\nalgorithms are highly disparate and problem-dependent. As a result, evaluating\nsuch algorithms is unreliable or impossible in many contexts and applications,\nespecially when considering differing levels of feedback. To resolve this\nproblem, we propose a generalized definition of payoff which uses an arbitrary\nuser-provided function. This unifies performance evaluation under all contexts\nand levels of feedback. Using this definition, we develop metrics for\nevaluating player performance, and estimators to approximate them under\nuncertainty (i.e., bandit or semi-bandit feedback). These metrics and their\nrespective estimators provide a problem-agnostic means to contextualize and\nevaluate algorithm performance. To validate the accuracy of our estimator, we\nexplore the Colonel Blotto ($\\mathcal{CB}$) game as an example. To this end, we\npropose a graph-pruning approach to efficiently identify feasible opponent\ndecisions, which are used in computing our estimation metrics. Using various\nresource allocation algorithms and game parameters, a suite of $\\mathcal{CB}$\ngames are simulated and used to compute and evaluate the quality of our\nestimates. These simulations empirically show our approach to be highly\naccurate at estimating the metrics associated with the unseen outcomes of an\nopponent's latent behavior.\n",
        "title": "General Performance Evaluation for Competitive Resource Allocation Games\n  via Unseen Payoff Estimation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04266",
        "abstract_url": "http://arxiv.org/abs/2401.04266",
        "authors": [
            {
                "last_name": "Rabbani",
                "first_name": "Shourav B."
            },
            {
                "last_name": "Medri",
                "first_name": "Ivan V."
            },
            {
                "last_name": "Samad",
                "first_name": "Manar D."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Despite groundbreaking success in image and text learning, deep learning has\nnot achieved significant improvements against traditional machine learning (ML)\nwhen it comes to tabular data. This performance gap underscores the need for\ndata-centric treatment and benchmarking of learning algorithms. Recently,\nattention and contrastive learning breakthroughs have shifted computer vision\nand natural language processing paradigms. However, the effectiveness of these\nadvanced deep models on tabular data is sparsely studied using a few data sets\nwith very large sample sizes, reporting mixed findings after benchmarking\nagainst a limited number of baselines. We argue that the heterogeneity of\ntabular data sets and selective baselines in the literature can bias the\nbenchmarking outcomes. This article extensively evaluates state-of-the-art\nattention and contrastive learning methods on a wide selection of 28 tabular\ndata sets (14 easy and 14 hard-to-classify) against traditional deep and\nmachine learning. Our data-centric benchmarking demonstrates when traditional\nML is preferred over deep learning and vice versa because no best learning\nmethod exists for all tabular data sets. Combining between-sample and\nbetween-feature attentions conquers the invincible traditional ML on tabular\ndata sets by a significant margin but fails on high dimensional data, where\ncontrastive learning takes a robust lead. While a hybrid attention-contrastive\nlearning strategy mostly wins on hard-to-classify data sets, traditional\nmethods are frequently superior on easy-to-classify data sets with presumably\nsimpler decision boundaries. To the best of our knowledge, this is the first\nbenchmarking paper with statistical analyses of attention and contrastive\nlearning performances on a diverse selection of tabular data sets against\ntraditional deep and machine learning baselines to facilitate further advances\nin this field.\n",
        "title": "Attention versus Contrastive Learning of Tabular Data -- A Data-centric\n  Benchmarking",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04268",
        "abstract_url": "http://arxiv.org/abs/2401.04268",
        "authors": [
            {
                "last_name": "Kutzke",
                "first_name": "Demetrious T."
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Gustavo E. Miranda"
            },
            {
                "last_name": "Herman",
                "first_name": "Robert J."
            },
            {
                "last_name": "Philippeaux",
                "first_name": "Harryel"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We introduce a launch device, called the remotely-enabled modular release\nmechanism, to augment rapid testing and prototyping of cooperative autonomy\nmaritime applications by facilitating autonomous deployment of an autonomous\nunderwater vehicle (AUV) from an autonomous surface vessel (ASV). While we\nfocus our development on a specific application of deploying an AUV from a\ncatamaran style ASV, the release mechanism can be adapted to different\ndeployable objects and towing vehicles, such as buoys and sensors for\noceanographic surveys or mono-hull ASVs. In this paper we explore a number of\nhardware and software design considerations to facilitate ease of integration\nwith existing maritime autonomy systems. We expound on bench tests and in-water\ntests used to explore the utility of the release system and diagnose system\nissues. Additionally, we make a first-principles argument, based on a\nhydrodynamics physics model, for assured deployment that is virtually\nindependent of sea state, making the release system a suitable alternative for\ndifferent maritime applications in varying environmental conditions.\n",
        "title": "Design and Development of a Remotely-enabled Modular Release Mechanism\n  for Autonomous Underwater Vehicles",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04280",
        "abstract_url": "http://arxiv.org/abs/2401.04280",
        "authors": [
            {
                "last_name": "Kandanaarachchi",
                "first_name": "Sevvandi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI",
            ""
        ],
        "abstract": "  Dynamic graph embeddings, inductive and incremental learning facilitate\npredictive tasks such as node classification and link prediction. However,\npredicting the structure of a graph at a future time step from a time series of\ngraphs, allowing for new nodes has not gained much attention. In this paper, we\npresent such an approach. We use time series methods to predict the node degree\nat future time points and combine it with flux balance analysis -- a linear\nprogramming method used in biochemistry -- to obtain the structure of future\ngraphs. Furthermore, we explore the predictive graph distribution for different\nparameter values. We evaluate this method using synthetic and real datasets and\ndemonstrate its utility and applicability.\n",
        "title": "Predicting the structure of dynamic graphs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04282",
        "abstract_url": "http://arxiv.org/abs/2401.04282",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Qinwu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study develops a graph search algorithm to find the optimal\ndiscrimination path for the binary classification problem. The objective\nfunction is defined as the difference of variations between the true positive\n(TP) and false positive (FP). It uses the depth first search (DFS) algorithm to\nfind the top-down paths for discrimination. It proposes a dynamic optimization\nprocedure to optimize TP at the upper levels and then reduce FP at the lower\nlevels. To accelerate computing speed with improving accuracy, it proposes a\nreduced histogram algorithm with variable bin size instead of looping over all\ndata points, to find the feature threshold of discrimination. The algorithm is\napplied on top of a Support Vector Machine (SVM) model for a binary\nclassification problem on whether a person is fit or unfit. It significantly\nimproves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a\nloss of only\\ 5% TP). The graph search auto-generates 39 ranked discrimination\npaths within 9 seconds on an input of total 328,464 objects, using a dual-core\nLaptop computer with a processor of 2.59 GHz.\n",
        "title": "A Fast Graph Search Algorithm with Dynamic Optimization and Reduced\n  Histogram for Discrimination of Binary Classification Problem",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04283",
        "abstract_url": "http://arxiv.org/abs/2401.04283",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Wan",
                "first_name": "Li"
            },
            {
                "last_name": "Li",
                "first_name": "Yun"
            },
            {
                "last_name": "Huang",
                "first_name": "Yiteng"
            },
            {
                "last_name": "Sun",
                "first_name": "Ming"
            },
            {
                "last_name": "Luan",
                "first_name": "James"
            },
            {
                "last_name": "Shi",
                "first_name": "Yangyang"
            },
            {
                "last_name": "Lei",
                "first_name": "Xin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Despite the potential of diffusion models in speech enhancement, their\ndeployment in Acoustic Echo Cancellation (AEC) has been restricted. In this\npaper, we propose DI-AEC, pioneering a diffusion-based stochastic regeneration\napproach dedicated to AEC. Further, we propose FADI-AEC, fast score-based\ndiffusion AEC framework to save computational demands, making it favorable for\nedge devices. It stands out by running the score model once per frame,\nachieving a significant surge in processing efficiency. Apart from that, we\nintroduce a novel noise generation technique where far-end signals are\nutilized, incorporating both far-end and near-end signals to refine the score\nmodel's accuracy. We test our proposed method on the ICASSP2023 Microsoft deep\necho cancellation challenge evaluation dataset, where our method outperforms\nsome of the end-to-end methods and other diffusion based echo cancellation\nmethods.\n",
        "title": "FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for\n  Acoustic Echo Cancellation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04285",
        "abstract_url": "http://arxiv.org/abs/2401.04285",
        "authors": [
            {
                "last_name": "Southworth",
                "first_name": "Ben S."
            },
            {
                "last_name": "Olivier",
                "first_name": "Samuel S."
            },
            {
                "last_name": "Park",
                "first_name": "HyeongKae"
            },
            {
                "last_name": "Buvoli",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Thermal radiation transport (TRT) is a time dependent, high dimensional\npartial integro-differential equation. In practical applications such as\ninertial confinement fusion, TRT is coupled to other physics such as\nhydrodynamics, plasmas, etc., and the timescales one is interested in capturing\nare often much slower than the radiation timescale. As a result, TRT is treated\nimplicitly, and due to its stiffness and high dimensionality, is often a\ndominant computational cost in multiphysics simulations. Here we develop a new\napproach for implicit-explicit (IMEX) integration of gray TRT in the\ndeterministic S$_N$ setting, which requires only one sweep per stage, with the\nsimplest first-order method requiring only one sweep per time step. The\npartitioning of equations is done via a moment-based high-order low-order\nformulation of TRT, where the streaming operator and first two moments are used\nto capture the asymptotic stiff regimes of the streaming limit and diffusion\nlimit. Absorption-reemission is treated explicitly, and although stiff, is\nsufficiently damped by the implicit solve that we achieve stable accurate time\nintegration without incorporating the coupling of the high order and low order\nequations implicitly. Due to nonlinear coupling of the high-order and low-order\nequations through temperature-dependent opacities, to facilitate IMEX\npartitioning and higher-order methods, we use a semi-implicit integration\napproach amenable to nonlinear partitions. Results are demonstrated on thick\nMarshak and crooked pipe benchmark problems, demonstrating orders of magnitude\nimprovement in accuracy and wallclock compared with the standard first-order\nimplicit integration typically used.\n",
        "title": "One-sweep moment-based semi-implicit-explicit integration for gray\n  thermal radiation transport",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04286",
        "abstract_url": "http://arxiv.org/abs/2401.04286",
        "authors": [
            {
                "last_name": "Ko",
                "first_name": "Hyunouk"
            },
            {
                "last_name": "Huo",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  In this paper, we first extend the result of FL93 and prove universal\nconsistency for a classification rule based on wide and deep ReLU neural\nnetworks trained on the logistic loss. Unlike the approach in FL93 that\ndecomposes the estimation and empirical error, we directly analyze the\nclassification risk based on the observation that a realization of a neural\nnetwork that is wide enough is capable of interpolating an arbitrary number of\npoints. Secondly, we give sufficient conditions for a class of probability\nmeasures under which classifiers based on neural networks achieve minimax\noptimal rates of convergence. Our result is motivated from the practitioner's\nobservation that neural networks are often trained to achieve 0 training error,\nwhich is the case for our proposed neural network classifiers. Our proofs hinge\non recent developments in empirical risk minimization and on approximation\nrates of deep ReLU neural networks for various function classes of interest.\nApplications to classical function spaces of smoothness illustrate the\nusefulness of our result.\n",
        "title": "Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax\n  Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04287",
        "abstract_url": "http://arxiv.org/abs/2401.04287",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Proksch",
                "first_name": "Sebastian"
            },
            {
                "last_name": "German",
                "first_name": "Daniel M."
            },
            {
                "last_name": "Godfrey",
                "first_name": "Michael W."
            },
            {
                "last_name": "Li",
                "first_name": "Li"
            },
            {
                "last_name": "McIntosh",
                "first_name": "Shane"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  \"App stores\" are online software stores where end users may browse, purchase,\ndownload, and install software applications. By far, the best known app stores\nare associated with mobile platforms, such as Google Play for Android and\nApple's App Store for iOS. The ubiquity of smartphones has led to mobile app\nstores becoming a touchstone experience of modern living. However, most of app\nstore research has concentrated on properties of the apps rather than the\nstores themselves. Today, there is a rich diversity of app stores and these\nstores have largely been overlooked by researchers: app stores exist on many\ndistinctive platforms, are aimed at different classes of users, and have\ndifferent end-goals beyond simply selling a standalone app to a smartphone\nuser.\n  We survey and characterize the broader dimensionality of app stores, and\nexplore how and why they influence software development practices, such as\nsystem design and release management. We begin by collecting a set of app store\nexamples from web search queries. By analyzing and curating the results, we\nderive a set of features common to app stores. We then build a dimensional\nmodel of app stores based on these features, and we fit each app store from our\nweb search result set into this model. Next, we performed unsupervised\nclustering to the app stores to find their natural groupings. Our results\nsuggest that app stores have become an essential stakeholder in modern software\ndevelopment. They control the distribution channel to end users and ensure that\nthe applications are of suitable quality; in turn, this leads to developers\nadhering to various store guidelines when creating their applications. However,\nwe found the app stores operational model could vary widely between stores, and\nthis variability could in turn affect the generalizability of existing\nunderstanding of app stores.\n",
        "title": "What Is an App Store? The Software Engineering Perspective",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04289",
        "abstract_url": "http://arxiv.org/abs/2401.04289",
        "authors": [
            {
                "last_name": "Wood",
                "first_name": "Kenan"
            },
            {
                "last_name": "Herlihy",
                "first_name": "Maurice"
            },
            {
                "last_name": "Mendes",
                "first_name": "Hammurabi"
            },
            {
                "last_name": "Pulaj",
                "first_name": "Jonad"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "DC",
            "",
            ""
        ],
        "abstract": "  An automated market maker (AMM) is a state machine that manages pools of\nassets, allowing parties to buy and sell those assets according to a fixed\nmathematical formula. AMMs are typically implemented as smart contracts on\nblockchains, and its prices are kept in line with the overall market price by\narbitrage: if the AMM undervalues an asset with respect to the market, an\n\"arbitrageur\" can make a risk-free profit by buying just enough of that asset\nto bring the AMM's price back in line with the market.\n  AMMs, however, are not designed for assets that expire: that is, assets that\ncannot be produced or resold after a specified date. As assets approach\nexpiration, arbitrage may not be able to reconcile supply and demand, and the\nliquidity providers that funded the AMM may have excessive exposure to risk due\nto rapid price variations.\n  This paper formally describes the design of a decentralized exchange (DEX)\nfor assets that expire, combining aspects of AMMs and limit-order books. We\nensure liveness and market clearance, providing mechanisms for liquidity\nproviders to control their exposure to risk and adjust prices dynamically in\nresponse to situations where arbitrage may fail.\n",
        "title": "Expiring Assets in Automated Market Makers",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04290",
        "abstract_url": "http://arxiv.org/abs/2401.04290",
        "authors": [
            {
                "last_name": "Kulinski",
                "first_name": "Sean"
            },
            {
                "last_name": "Waytowich",
                "first_name": "Nicholas R."
            },
            {
                "last_name": "Hare",
                "first_name": "James Z."
            },
            {
                "last_name": "Inouye",
                "first_name": "David I."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "MA"
        ],
        "abstract": "  Spatial reasoning tasks in multi-agent environments such as event prediction,\nagent type identification, or missing data imputation are important for\nmultiple applications (e.g., autonomous surveillance over sensor networks and\nsubtasks for reinforcement learning (RL)). StarCraft II game replays encode\nintelligent (and adversarial) multi-agent behavior and could provide a testbed\nfor these tasks; however, extracting simple and standardized representations\nfor prototyping these tasks is laborious and hinders reproducibility. In\ncontrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled\nrapid prototyping and reproducibility of ML methods. Following the simplicity\nof these datasets, we construct a benchmark spatial reasoning dataset based on\nStarCraft II replays that exhibit complex multi-agent behaviors, while still\nbeing as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize\na window of 255 consecutive game states to create 3.6 million summary images\nfrom 60,000 replays, including all relevant metadata such as game outcome and\nplayer races. We develop three formats of decreasing complexity: Hyperspectral\nimages that include one channel for every unit type (similar to multispectral\ngeospatial images), RGB images that mimic CIFAR10, and grayscale images that\nmimic MNIST. We show how this dataset can be used for prototyping spatial\nreasoning methods. All datasets, code for extraction, and code for dataset\nloading can be found at https://starcraftdata.davidinouye.com\n",
        "title": "StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For\n  Multi-Agent Environments",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04301",
        "abstract_url": "http://arxiv.org/abs/2401.04301",
        "authors": [
            {
                "last_name": "Dovonon",
                "first_name": "Gb\u00e8tondji J-S"
            },
            {
                "last_name": "Bronstein",
                "first_name": "Michael M."
            },
            {
                "last_name": "Kusner",
                "first_name": "Matt J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Transformer-based models have recently become wildly successful across a\ndiverse set of domains. At the same time, recent work has shown that\nTransformers are inherently low-pass filters that gradually oversmooth the\ninputs, reducing the expressivity of their representations. A natural question\nis: How can Transformers achieve these successes given this shortcoming? In\nthis work we show that in fact Transformers are not inherently low-pass\nfilters. Instead, whether Transformers oversmooth or not depends on the\neigenspectrum of their update equations. Our analysis extends prior work in\noversmoothing and in the closely-related phenomenon of rank collapse. We show\nthat many successful Transformer models have attention and weights which\nsatisfy conditions that avoid oversmoothing. Based on this analysis, we derive\na simple way to parameterize the weights of the Transformer update equations\nthat allows for control over its spectrum, ensuring that oversmoothing does not\noccur. Compared to a recent solution for oversmoothing, our approach improves\ngeneralization, even when training with more layers, fewer datapoints, and data\nthat is corrupted.\n",
        "title": "Setting the Record Straight on Transformer Oversmoothing",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04302",
        "abstract_url": "http://arxiv.org/abs/2401.04302",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Hang"
            },
            {
                "last_name": "Baloian",
                "first_name": "Artiom"
            },
            {
                "last_name": "Janak",
                "first_name": "Jan"
            },
            {
                "last_name": "Schulzrinne",
                "first_name": "Henning"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  eSIM(embedded SIM) is an advanced alternative to traditional physical SIM\ncards initially developed by the GSM Association(GSMA) in 2013 [1][2]. The eSIM\ntechnology has been deployed in many commercial products such as mobile\ndevices. However, the application of the eSIM technology in IoT devices has yet\nto start being primarily deployed. Understanding the eSIM architecture and the\nbasic ideas of the eSIM provisioning and operations is very important for\nengineers to promote eSIM technology deployment in more areas, both academics\nand industries.\n  The report focuses on the eSIM technology in the IoT architecture and two\nmajor operations of Remote SIM Provisioning(RSP) procedure: the Common Mutual\nAuthentication procedure, a process used to authenticate eSIM trusted\ncommunication parties over the public internet, and the Profile Downloading\nprocedure, the way to download the Profile from the operator SM-DP+ server and\neventually remotely provision the end-user devices.\n",
        "title": "eSIM Technology in IoT Architecture",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04305",
        "abstract_url": "http://arxiv.org/abs/2401.04305",
        "authors": [
            {
                "last_name": "Kirsch",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT"
        ],
        "abstract": "  At its core, this thesis aims to enhance the practicality of deep learning by\nimproving the label and training efficiency of deep learning models. To this\nend, we investigate data subset selection techniques, specifically active\nlearning and active sampling, grounded in information-theoretic principles.\nActive learning improves label efficiency, while active sampling enhances\ntraining efficiency. Supervised deep learning models often require extensive\ntraining with labeled data. Label acquisition can be expensive and\ntime-consuming, and training large models is resource-intensive, hindering the\nadoption outside academic research and ``big tech.'' Existing methods for data\nsubset selection in deep learning often rely on heuristics or lack a principled\ninformation-theoretic foundation. In contrast, this thesis examines several\nobjectives for data subset selection and their applications within deep\nlearning, striving for a more principled approach inspired by information\ntheory. We begin by disentangling epistemic and aleatoric uncertainty in single\nforward-pass deep neural networks, which provides helpful intuitions and\ninsights into different forms of uncertainty and their relevance for data\nsubset selection. We then propose and investigate various approaches for active\nlearning and data subset selection in (Bayesian) deep learning. Finally, we\nrelate various existing and proposed approaches to approximations of\ninformation quantities in weight or prediction space. Underpinning this work is\na principled and practical notation for information-theoretic quantities that\nincludes both random variables and observed outcomes. This thesis demonstrates\nthe benefits of working from a unified perspective and highlights the potential\nimpact of our contributions to the practical application of deep learning.\n",
        "title": "Advancing Deep Active Learning & Data Subset Selection: Unifying\n  Principles with Information-Theory Intuitions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04311",
        "abstract_url": "http://arxiv.org/abs/2401.04311",
        "authors": [
            {
                "last_name": "Stemmer",
                "first_name": "Uri"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Private Everlasting Prediction (PEP), recently introduced by Naor et al.\n[2023], is a model for differentially private learning in which the learner\nnever publicly releases a hypothesis. Instead, it provides black-box access to\na \"prediction oracle\" that can predict the labels of an endless stream of\nunlabeled examples drawn from the underlying distribution. Importantly, PEP\nprovides privacy both for the initial training set and for the endless stream\nof classification queries. We present two conceptual modifications to the\ndefinition of PEP, as well as new constructions exhibiting significant\nimprovements over prior work. Specifically,\n  (1) Robustness: PEP only guarantees accuracy provided that all the\nclassification queries are drawn from the correct underlying distribution. A\nfew out-of-distribution queries might break the validity of the prediction\noracle for future queries, even for future queries which are sampled from the\ncorrect distribution. We incorporate robustness against such poisoning attacks\ninto the definition of PEP, and show how to obtain it.\n  (2) Dependence of the privacy parameter $\\delta$ in the time horizon: We\npresent a relaxed privacy definition, suitable for PEP, that allows us to\ndisconnect the privacy parameter $\\delta$ from the number of total time steps\n$T$. This allows us to obtain algorithms for PEP whose sample complexity is\nindependent from $T$, thereby making them \"truly everlasting\". This is in\ncontrast to prior work where the sample complexity grows with $polylog(T)$.\n  (3) New constructions: Prior constructions for PEP exhibit sample complexity\nthat is quadratic in the VC dimension of the target class. We present new\nconstructions of PEP for axis-aligned rectangles and for decision-stumps that\nexhibit sample complexity linear in the dimension (instead of quadratic). We\nshow that our constructions satisfy very strong robustness properties.\n",
        "title": "Private Truly-Everlasting Robust-Prediction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04312",
        "abstract_url": "http://arxiv.org/abs/2401.04312",
        "authors": [
            {
                "last_name": "Dong",
                "first_name": "Xue"
            },
            {
                "last_name": "Song",
                "first_name": "Xuemeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Tongliang"
            },
            {
                "last_name": "Guan",
                "first_name": "Weili"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Multi-interest learning method for sequential recommendation aims to predict\nthe next item according to user multi-faceted interests given the user\nhistorical interactions. Existing methods mainly consist of two modules: the\nmulti-interest extraction module that learns user multi-interest embeddings to\ncapture the user multi-interests, and the multi-interest weight prediction\nmodule that learns the weight of each interest for aggregating the learned\nmulti-interest embeddings to derive the user embedding, used for predicting the\nuser rating to an item. Despite their effectiveness, existing methods have two\nkey limitations: 1) they directly feed the user interactions into the two\nmodules, while ignoring their different learning objectives, and 2) they merely\nconsider the centrality of the user interactions to learn the user\nmulti-interests, while overlooking their dispersion. To tackle these\nlimitations, we propose a prompt-based multi-interest learning method (PoMRec),\nwhere specific prompts are inserted into user interactions to make them\nadaptive to different learning objectives of the two modules. Moreover, we\nutilize both the mean and variance embeddings of user interactions to derive\nthe user multi-interest embeddings for comprehensively model the user\nmulti-interests. We conduct extensive experiments on two public datasets, and\nthe results verify that our proposed PoMRec outperforms the state-of-the-art\nmulti-interest learning methods.\n",
        "title": "Prompt-based Multi-interest Learning Method for Sequential\n  Recommendation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04316",
        "abstract_url": "http://arxiv.org/abs/2401.04316",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Dai",
                "first_name": "Bo"
            },
            {
                "last_name": "Gu",
                "first_name": "Feng"
            },
            {
                "last_name": "Han",
                "first_name": "Jianda"
            },
            {
                "last_name": "Liu",
                "first_name": "Guangjun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Aerial manipulator, which is composed of an UAV (Unmanned Aerial Vehicle) and\na multi-link manipulator and can perform aerial manipulation, has shown great\npotential of applications. However, dynamic coupling between the UAV and the\nmanipulator makes it difficult to control the aerial manipulator with high\nperformance. In this paper, system modeling and control problem of the aerial\nmanipulator are studied. Firstly, an UAV dynamic model is proposed with\nconsideration of the dynamic coupling from an attached manipulator, which is\ntreated as disturbance for the UAV. In the dynamic model, the disturbance is\naffected by the variable inertia parameters of the aerial manipulator system.\nThen, based on the proposed dynamic model, a disturbance compensation robust\n$H_{\\infty}$ controller is designed to stabilize flight of the UAV while the\nmanipulator is in operation. Finally, experiments are conducted and the\nexperimental results demonstrate the feasibility and validity of the proposed\ncontrol scheme.\n",
        "title": "Robust Control of An Aerial Manipulator Based on A Variable Inertia\n  Parameters Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04317",
        "abstract_url": "http://arxiv.org/abs/2401.04317",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Jianyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bowen"
            },
            {
                "last_name": "Dubey",
                "first_name": "Amartansh"
            },
            {
                "last_name": "Murch",
                "first_name": "Ross"
            },
            {
                "last_name": "Jing",
                "first_name": "Liwen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Indoor imaging is a critical task for robotics and internet-of-things. WiFi\nas an omnipresent signal is a promising candidate for carrying out passive\nimaging and synchronizing the up-to-date information to all connected devices.\nThis is the first research work to consider WiFi indoor imaging as a\nmulti-modal image generation task that converts the measured WiFi power into a\nhigh-resolution indoor image. Our proposed WiFi-GEN network achieves a shape\nreconstruction accuracy that is 275% of that achieved by physical model-based\ninversion methods. Additionally, the Frechet Inception Distance score has been\nsignificantly reduced by 82%. To examine the effectiveness of models for this\ntask, the first large-scale dataset is released containing 80,000 pairs of WiFi\nsignal and imaging target. Our model absorbs challenges for the model-based\nmethods including the non-linearity, ill-posedness and non-certainty into\nmassive parameters of our generative AI network. The network is also designed\nto best fit measured WiFi signals and the desired imaging output. For\nreproducibility, we will release the data and code upon acceptance.\n",
        "title": "Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04318",
        "abstract_url": "http://arxiv.org/abs/2401.04318",
        "authors": [
            {
                "last_name": "Kawase",
                "first_name": "Yasushi"
            },
            {
                "last_name": "Roy",
                "first_name": "Bodhayan"
            },
            {
                "last_name": "Sanpui",
                "first_name": "Mohammad Azharuddin"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We study the problem of allocating indivisible items on a path among agents.\nThe objective is to find a fair and efficient allocation in which each agent's\nbundle forms a contiguous block on the line. We demonstrate that, even when the\nvaluations are binary additive, deciding whether every item can be allocated to\nan agent who wants it is NP-complete. Consequently, we provide two\nfixed-parameter tractable (FPT) algorithms for maximizing utilitarian social\nwelfare, with respect to the number of agents and the number of items.\nAdditionally, we present a 2-approximation algorithm for the special case when\nthe valuations are binary additive and the maximum utility is equal to the\nnumber of items. Furthermore, we establish that deciding whether the maximum\negalitarian social welfare is at least 2 or at most 1 is NP-complete, even when\nthe valuations are binary additive. We also explore the case where the order of\nthe blocks of items allocated to the agents is predetermined. In this case, we\nshow that both maximum utilitarian social welfare and egalitarian social\nwelfare can be computed in polynomial time. However, we determine that checking\nthe existence of an EF1 allocation is NP-complete, even when the valuations are\nbinary additive.\n",
        "title": "Contiguous Allocation of Indivisible Items on a Path",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04319",
        "abstract_url": "http://arxiv.org/abs/2401.04319",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Yang",
                "first_name": "Dan"
            },
            {
                "last_name": "Hu",
                "first_name": "Binbin"
            },
            {
                "last_name": "Shen",
                "first_name": "Yue"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziqi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wen"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinjie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhiqiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In this paper, we explore a new way for user targeting, where non-expert\nmarketers could select their target users solely given demands in natural\nlanguage form. The key to this issue is how to transform natural languages into\npractical structured logical languages, i.e., the structured understanding of\nmarketer demands. Considering the impressive natural language processing\nability of large language models (LLMs), we try to leverage LLMs to solve this\nissue. Past research indicates that the reasoning ability of LLMs can be\neffectively enhanced through chain-of-thought (CoT) prompting. But existing\nmethods still have some limitations: (1) Previous methods either use simple\n\"Let's think step by step\" spells or provide fixed examples in demonstrations\nwithout considering compatibility between prompts and questions, making LLMs\nineffective in some complex reasoning tasks such as structured language\ntransformation. (2) Previous methods are often implemented in closed-source\nmodels or excessively large models, which is not suitable in industrial\npractical scenarios. Based on these, we propose ARALLM (i.e., Analogical\nReasoning Augmented Large Language Models) consisting of two modules:\nAnalogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model\nDistillation.\n",
        "title": "Know Your Needs Better: Towards Structured Understanding of Marketer\n  Demands with Analogical Reasoning Augmented LLMs",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04320",
        "abstract_url": "http://arxiv.org/abs/2401.04320",
        "authors": [
            {
                "last_name": "Kutzke",
                "first_name": "Demetrious T."
            },
            {
                "last_name": "Wariar",
                "first_name": "Ashwin"
            },
            {
                "last_name": "Sattar",
                "first_name": "Junaed"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The use of autonomous underwater vehicles (AUVs) to accomplish traditionally\nchallenging and dangerous tasks has proliferated thanks to advances in sensing,\nnavigation, manipulation, and on-board computing technologies. Utilizing AUVs\nin underwater human-robot interaction (UHRI) has witnessed comparatively\nsmaller levels of growth due to limitations in bi-directional communication and\nsignificant technical hurdles to bridge the gap between analogies with\nterrestrial interaction strategies and those that are possible in the\nunderwater domain. A necessary component to support UHRI is establishing a\nsystem for safe robotic-diver approach to establish face-to-face communication\nthat considers non-standard human body pose. In this work, we introduce a\nstereo vision system for enhancing UHRI that utilizes three-dimensional\nreconstruction from stereo image pairs and machine learning for localizing\nhuman joint estimates. We then establish a convention for a coordinate system\nthat encodes the direction the human is facing with respect to the camera\ncoordinate frame. This allows automatic setpoint computation that preserves\nhuman body scale and can be used as input to an image-based visual servo\ncontrol scheme. We show that our setpoint computations tend to agree both\nquantitatively and qualitatively with experimental setpoint baselines. The\nmethodology introduced shows promise for enhancing UHRI by improving robotic\nperception of human orientation underwater.\n",
        "title": "Autonomous robotic re-alignment for face-to-face underwater human-robot\n  interaction",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04323",
        "abstract_url": "http://arxiv.org/abs/2401.04323",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Chenxing"
            },
            {
                "last_name": "Guo",
                "first_name": "Qingyue"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL",
            "IR",
            "",
            ""
        ],
        "abstract": "  This paper investigates differences in characteristics across publication\ntypes for aging-related genetic research. We utilized bibliometric data for\nfive model species retrieved from authoritative databases including PubMed.\nPublications are classified into types according to PubMed. Results indicate\nsubstantial divergence across publication types in attention paid to\naging-related research, scopes of studied genes, and topical preferences. For\ninstance, comparative studies and meta-analyses show a greater focus on aging\nthan validation studies. Reviews concentrate more on cell biology while\nclinical studies emphasize translational topics. Publication types also\nmanifest variations in highly studied genes, like APOE for reviews versus GH1\nfor clinical studies. Despite differences, top genes like insulin are\nuniversally emphasized. Publication types demonstrate similar levels of\nimbalance in research efforts to genes. Differences also exist in bibliometrics\nlike authorship numbers, citation counts, etc. Publication types show distinct\npreferences for journals of certain topical specialties and scope of\nreadership. Overall, findings showcase distinct characteristics of publication\ntypes in studying aging-related genetics, owing to their unique nature and\nobjectives. This study is the first endeavor to systematically depict the\ninherent structure of a biomedical research field from the perspective of\npublication types and provides insights into knowledge production and\nevaluation patterns across biomedical communities.\n",
        "title": "Divergent Characteristics of Biomedical Research across Publication\n  Types: A Quantitative Analysis on the Aging-related Research",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04325",
        "abstract_url": "http://arxiv.org/abs/2401.04325",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Han"
            },
            {
                "last_name": "Ma",
                "first_name": "Yukai"
            },
            {
                "last_name": "Gu",
                "first_name": "Yaqing"
            },
            {
                "last_name": "Hu",
                "first_name": "Kewei"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            },
            {
                "last_name": "Zuo",
                "first_name": "Xingxing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a novel approach for metric dense depth estimation based on the\nfusion of a single-view image and a sparse, noisy Radar point cloud. The direct\nfusion of heterogeneous Radar and image data, or their encodings, tends to\nyield dense depth maps with significant artifacts, blurred boundaries, and\nsuboptimal accuracy. To circumvent this issue, we learn to augment versatile\nand robust monocular depth prediction with the dense metric scale induced from\nsparse and noisy Radar data. We propose a Radar-Camera framework for highly\naccurate and fine-detailed dense depth estimation with four stages, including\nmonocular depth prediction, global scale alignment of monocular depth with\nsparse Radar points, quasi-dense scale estimation through learning the\nassociation between Radar points and image patches, and local scale refinement\nof dense depth using a scale map learner. Our proposed method significantly\noutperforms the state-of-the-art Radar-Camera depth estimation methods by\nreducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2%\non the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam\ndataset, respectively.\n",
        "title": "RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned\n  Metric Scale",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04328",
        "abstract_url": "http://arxiv.org/abs/2401.04328",
        "authors": [
            {
                "last_name": "Venn",
                "first_name": "Daniel R."
            },
            {
                "last_name": "Ruuth",
                "first_name": "Steven J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose and analyze a class of meshfree, super-algebraically convergent\nmethods for partial differential equations (PDEs) on surfaces using Fourier\nextensions minimizing a measure of non-smoothness (such as a Sobolev norm).\nCurrent spectral methods for surface PDEs are primarily limited to a small\nclass of surfaces, such as subdomains of spheres. Other high order methods for\nsurface PDEs typically use radial basis functions (RBFs). Many of these methods\nare not well-understood analytically for surface PDEs and are highly\nill-conditioned. Our methods work by extending a surface PDE into a box-shaped\ndomain so that differential operators of the extended function agree with the\nsurface differential operators, as in the Closest Point Method. The methods can\nbe proven to converge super-algebraically for certain well-posed linear PDEs,\nand spectral convergence to machine error has been observed numerically for a\nvariety of problems. Our approach works on arbitrary smooth surfaces (closed or\nnon-closed) defined by point clouds with minimal conditions.\n",
        "title": "Underdetermined Fourier Extensions for Partial Differential Equations on\n  Surfaces",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04330",
        "abstract_url": "http://arxiv.org/abs/2401.04330",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Yonghui"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaolong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yishu"
            },
            {
                "last_name": "Ai",
                "first_name": "Jinquan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The purpose of remote sensing image change detection (RSCD) is to detect\ndifferences between bi-temporal images taken at the same place. Deep learning\nhas been extensively used to RSCD tasks, yielding significant results in terms\nof result recognition. However, due to the shooting angle of the satellite, the\nimpacts of thin clouds, and certain lighting conditions, the problem of fuzzy\nedges in the change region in some remote sensing photographs cannot be\nproperly handled using current RSCD algorithms. To solve this issue, we\nproposed a Body Decouple Multi-Scale by fearure Aggregation change detection\n(BD-MSA), a novel model that collects both global and local feature map\ninformation in the channel and space dimensions of the feature map during the\ntraining and prediction phases. This approach allows us to successfully extract\nthe change region's boundary information while also divorcing the change\nregion's main body from its boundary. Numerous studies have shown that the\nassessment metrics and evaluation effects of the model described in this paper\non the publicly available datasets DSIFN-CD and S2Looking are the best when\ncompared to other models.\n",
        "title": "BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method\n  guided by multi-scale feature information aggregation",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04331",
        "abstract_url": "http://arxiv.org/abs/2401.04331",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Qiyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Kai"
            },
            {
                "last_name": "Song",
                "first_name": "Yang"
            },
            {
                "last_name": "Xie",
                "first_name": "Yihang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yanan"
            },
            {
                "last_name": "Wang",
                "first_name": "Sijie"
            },
            {
                "last_name": "She",
                "first_name": "Rui"
            },
            {
                "last_name": "Tay",
                "first_name": "Wee Peng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In this work, we rigorously investigate the robustness of graph neural\nfractional-order differential equation (FDE) models. This framework extends\nbeyond traditional graph neural (integer-order) ordinary differential equation\n(ODE) models by implementing the time-fractional Caputo derivative. Utilizing\nfractional calculus allows our model to consider long-term memory during the\nfeature updating process, diverging from the memoryless Markovian updates seen\nin traditional graph neural ODE models. The superiority of graph neural FDE\nmodels over graph neural ODE models has been established in environments free\nfrom attacks or perturbations. While traditional graph neural ODE models have\nbeen verified to possess a degree of stability and resilience in the presence\nof adversarial attacks in existing literature, the robustness of graph neural\nFDE models, especially under adversarial conditions, remains largely\nunexplored. This paper undertakes a detailed assessment of the robustness of\ngraph neural FDE models. We establish a theoretical foundation outlining the\nrobustness characteristics of graph neural FDE models, highlighting that they\nmaintain more stringent output perturbation bounds in the face of input and\ngraph topology disturbances, compared to their integer-order counterparts. Our\nempirical evaluations further confirm the enhanced robustness of graph neural\nFDE models, highlighting their potential in adversarially robust applications.\n",
        "title": "Coupling Graph Neural Networks with Fractional Order Continuous\n  Dynamics: A Robustness Study",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04332",
        "abstract_url": "http://arxiv.org/abs/2401.04332",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Hou",
                "first_name": "Bingzhe"
            },
            {
                "last_name": "Wu",
                "first_name": "Tieru"
            },
            {
                "last_name": "Xin",
                "first_name": "Yue"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Two important problems in the field of Topological Data Analysis are defining\npractical multifiltrations on objects and showing ability of TDA to detect the\ngeometry. Motivated by the problems, we constuct three multifiltrations named\nmulti-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the\ninterleaving distance and multiparameter persistence landscape of multi-GENEO\nwith respect to the pseudometric of the subspace of bounded functions. We also\ngive the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we\nprovide experiment results on MNIST dataset to demonstrate our bifiltrations\nhave ability to detect geometric and topological differences of digital images.\n",
        "title": "Mix-GENEO: A flexible filtration for multiparameter persistent homology\n  detects digital images",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04334",
        "abstract_url": "http://arxiv.org/abs/2401.04334",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Wu",
                "first_name": "Zihao"
            },
            {
                "last_name": "Li",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Hanqi"
            },
            {
                "last_name": "Shu",
                "first_name": "Peng"
            },
            {
                "last_name": "Shi",
                "first_name": "Enze"
            },
            {
                "last_name": "Hu",
                "first_name": "Huawen"
            },
            {
                "last_name": "Ma",
                "first_name": "Chong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuhui"
            },
            {
                "last_name": "Yao",
                "first_name": "Yincheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Xuan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Huaqin"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengliang"
            },
            {
                "last_name": "Dai",
                "first_name": "Haixing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Lin"
            },
            {
                "last_name": "Ge",
                "first_name": "Bao"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shu"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Large language models (LLMs) have undergone significant expansion and have\nbeen increasingly integrated across various domains. Notably, in the realm of\nrobot task planning, LLMs harness their advanced reasoning and language\ncomprehension capabilities to formulate precise and efficient action plans\nbased on natural language instructions. However, for embodied tasks, where\nrobots interact with complex environments, text-only LLMs often face challenges\ndue to a lack of compatibility with robotic visual perception. This study\nprovides a comprehensive overview of the emerging integration of LLMs and\nmultimodal LLMs into various robotic tasks. Additionally, we propose a\nframework that utilizes multimodal GPT-4V to enhance embodied task planning\nthrough the combination of natural language instructions and robot visual\nperceptions. Our results, based on diverse datasets, indicate that GPT-4V\neffectively enhances robot performance in embodied tasks. This extensive survey\nand evaluation of LLMs and multimodal LLMs across a variety of robotic tasks\nenriches the understanding of LLM-centric embodied intelligence and provides\nforward-looking insights toward bridging the gap in Human-Robot-Environment\ninteraction.\n",
        "title": "Large Language Models for Robotics: Opportunities, Challenges, and\n  Perspectives",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04338",
        "abstract_url": "http://arxiv.org/abs/2401.04338",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Youshao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shangchun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhenglei"
            },
            {
                "last_name": "Huan",
                "first_name": "Zhaoxin"
            },
            {
                "last_name": "Ju",
                "first_name": "Lin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaolu"
            },
            {
                "last_name": "Wang",
                "first_name": "Lin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC",
            "IR"
        ],
        "abstract": "  Recently, a new paradigm, meta learning, has been widely applied to Deep\nLearning Recommendation Models (DLRM) and significantly improves statistical\nperformance, especially in cold-start scenarios. However, the existing systems\nare not tailored for meta learning based DLRM models and have critical problems\nregarding efficiency in distributed training in the GPU cluster. It is because\nthe conventional deep learning pipeline is not optimized for two task-specific\ndatasets and two update loops in meta learning. This paper provides a\nhigh-performance framework for large-scale training for Optimization-based Meta\nDLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta. Firstly,\nG-Meta utilizes both data parallelism and model parallelism with careful\norchestration regarding computation and communication efficiency, to enable\nhigh-speed distributed training. Secondly, it proposes a Meta-IO pipeline for\nefficient data ingestion to alleviate the I/O bottleneck. Various experimental\nresults show that G-Meta achieves notable training speed without loss of\nstatistical performance. Since early 2022, G-Meta has been deployed in Alipay's\ncore advertising and recommender system, shrinking the continuous delivery of\nmodels by four times. It also obtains 6.48\\% improvement in Conversion Rate\n(CVR) and 1.06\\% increase in CPM (Cost Per Mille) in Alipay's homepage display\nadvertising, with the benefit of larger training samples and tasks.\n",
        "title": "G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale\n  Recommender Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04339",
        "abstract_url": "http://arxiv.org/abs/2401.04339",
        "authors": [
            {
                "last_name": "Ryu",
                "first_name": "Hyogon"
            },
            {
                "last_name": "Lim",
                "first_name": "Seohyun"
            },
            {
                "last_name": "Shim",
                "first_name": "Hyunjung"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The rise of billion-parameter diffusion models like Stable Diffusion XL,\nImagen, and Dall-E3 markedly advances the field of generative AI. However,\ntheir large-scale nature poses challenges in fine-tuning and deployment due to\nhigh resource demands and slow inference speed. This paper ventures into the\nrelatively unexplored yet promising realm of fine-tuning quantized diffusion\nmodels. We establish a strong baseline by customizing three models: PEQA for\nfine-tuning quantization parameters, Q-Diffusion for post-training\nquantization, and DreamBooth for personalization. Our analysis reveals a\nnotable trade-off between subject and prompt fidelity within the baseline\nmodel. To address these issues, we introduce two strategies, inspired by the\ndistinct roles of different timesteps in diffusion models: S1 optimizing a\nsingle set of fine-tuning parameters exclusively at selected intervals, and S2\ncreating multiple fine-tuning parameter sets, each specialized for different\ntimestep intervals. Our approach not only enhances personalization but also\nupholds prompt fidelity and image quality, significantly outperforming the\nbaseline qualitatively and quantitatively. The code will be made publicly\navailable.\n",
        "title": "Memory-Efficient Personalization using Quantized Diffusion Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04340",
        "abstract_url": "http://arxiv.org/abs/2401.04340",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jianyi"
            },
            {
                "last_name": "Li",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Islam",
                "first_name": "Mohammad Jaminur"
            },
            {
                "last_name": "Ren",
                "first_name": "Shaolei"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "PF"
        ],
        "abstract": "  This paper studies online resource allocation with replenishable budgets,\nwhere budgets can be replenished on top of the initial budget and an agent\nsequentially chooses online allocation decisions without violating the\navailable budget constraint at each round. We propose a novel online algorithm,\ncalled OACP (Opportunistic Allocation with Conservative Pricing), that\nconservatively adjusts dual variables while opportunistically utilizing\navailable resources. OACP achieves a bounded asymptotic competitive ratio in\nadversarial settings as the number of decision rounds T gets large.\nImportantly, the asymptotic competitive ratio of OACP is optimal in the absence\nof additional assumptions on budget replenishment. To further improve the\ncompetitive ratio, we make a mild assumption that there is budget replenishment\nevery T^* >= 1 decision rounds and propose OACP+ to dynamically adjust the\ntotal budget assignment for online allocation. Next, we move beyond the\nworst-case and propose LA-OACP (Learning-Augmented OACP/OACP+), a novel\nlearning-augmented algorithm for online allocation with replenishable budgets.\nWe prove that LA-OACP can improve the average utility compared to OACP/OACP+\nwhen the ML predictor is properly trained, while still offering worst-case\nutility guarantees when the ML predictions are arbitrarily wrong. Finally, we\nrun simulation studies of sustainable AI inference powered by renewables,\nvalidating our analysis and demonstrating the empirical benefits of LA-OACP.\n",
        "title": "Online Allocation with Replenishable Budgets: Worst Case and Beyond",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04343",
        "abstract_url": "http://arxiv.org/abs/2401.04343",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Panda",
                "first_name": "Ashwinee"
            },
            {
                "last_name": "Nasr",
                "first_name": "Milad"
            },
            {
                "last_name": "Mahloujifar",
                "first_name": "Saeed"
            },
            {
                "last_name": "Mittal",
                "first_name": "Prateek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "CR"
        ],
        "abstract": "  Fine-tuning large pretrained models on private datasets may run the risk of\nviolating privacy. Differential privacy is a framework for mitigating privacy\nrisks by enforcing algorithmic stability. DP-SGD enables training models with\nprivate data in a privacy-preserving manner, but raises new obstacles in the\nform of performance loss and significant engineering challenges. We introduce\nDP-ZO, a new method for fine-tuning large language models that preserves the\nprivacy of training data by privatizing zeroth-order optimization. A key\ninsight into the design of our method is that the direction of the gradient in\nSPSA, the zeroth-order algorithm we use, is always random and the only\ninformation that depends on private data is the step size, i.e., a scalar.\nTherefore, we only need to privatize the scalar step size, which is\nmemory-efficient. DP-ZO, which can be instantiated with either Laplace or\nGaussian noise, provides a strong privacy-utility trade-off across different\ntasks, and model sizes, under conservative privacy budgets. One noteworthy\nresult is that DP-ZO exhibits just $1.86\\%$ performance degradation due to\nprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples\nfrom SQuAD.\n",
        "title": "Private Fine-tuning of Large Language Models with Zeroth-order\n  Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04345",
        "abstract_url": "http://arxiv.org/abs/2401.04345",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Hualie"
            },
            {
                "last_name": "Xu",
                "first_name": "Rui"
            },
            {
                "last_name": "Tan",
                "first_name": "Minglang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Wenjie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Omnidirectional stereo matching (OSM) is an essential and reliable means for\n$360^{\\circ}$ depth sensing. However, following earlier works on conventional\nstereo matching, prior state-of-the-art (SOTA) methods rely on a 3D\nencoder-decoder block to regularize the cost volume, causing the whole system\ncomplicated and sub-optimal results. Recently, the Recurrent All-pairs Field\nTransforms (RAFT) based approach employs the recurrent update in 2D and has\nefficiently improved image-matching tasks, \\ie, optical flow, and stereo\nmatching. To bridge the gap between OSM and RAFT, we mainly propose an opposite\nadaptive weighting scheme to seamlessly transform the outputs of spherical\nsweeping of OSM into the required inputs for the recurrent update, thus\ncreating a recurrent omnidirectional stereo matching (RomniStereo) algorithm.\nFurthermore, we introduce two techniques, \\ie, grid embedding and adaptive\ncontext feature generation, which also contribute to RomniStereo's performance.\nOur best model improves the average MAE metric by 40.7\\% over the previous SOTA\nbaseline across five datasets. When visualizing the results, our models\ndemonstrate clear advantages on both synthetic and realistic examples. The code\nis available at \\url{https://github.com/HalleyJiang/RomniStereo}.\n",
        "title": "RomniStereo: Recurrent Omnidirectional Stereo Matching",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04348",
        "abstract_url": "http://arxiv.org/abs/2401.04348",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Khoi M."
            },
            {
                "last_name": "Pham",
                "first_name": "Trinh"
            },
            {
                "last_name": "Quan",
                "first_name": "Tho"
            },
            {
                "last_name": "Luu",
                "first_name": "Anh Tuan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Paraphrases are texts that convey the same meaning while using different\nwords or sentence structures. It can be used as an automatic data augmentation\ntool for many Natural Language Processing tasks, especially when dealing with\nlow-resource languages, where data shortage is a significant problem. To\ngenerate a paraphrase in multilingual settings, previous studies have leveraged\nthe knowledge from the machine translation field, i.e., forming a paraphrase\nthrough zero-shot machine translation in the same language. Despite good\nperformance on human evaluation, those methods still require parallel\ntranslation datasets, thus making them inapplicable to languages that do not\nhave parallel corpora. To mitigate that problem, we proposed the first\nunsupervised multilingual paraphrasing model, LAMPAT ($\\textbf{L}$ow-rank\n$\\textbf{A}$daptation for $\\textbf{M}$ultilingual $\\textbf{P}$araphrasing using\n$\\textbf{A}$dversarial $\\textbf{T}$raining), by which monolingual dataset is\nsufficient enough to generate a human-like and diverse sentence. Throughout the\nexperiments, we found out that our method not only works well for English but\ncan generalize on unseen languages as well. Data and code are available at\nhttps://github.com/phkhanhtrinh23/LAMPAT.\n",
        "title": "LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using\n  Adversarial Training",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04349",
        "abstract_url": "http://arxiv.org/abs/2401.04349",
        "authors": [
            {
                "last_name": "Ferguson",
                "first_name": "Ethan"
            },
            {
                "last_name": "Wilson",
                "first_name": "Adam"
            },
            {
                "last_name": "Naghibijouybari",
                "first_name": "Hoda"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "AR"
        ],
        "abstract": "  Microarchitectural attacks on CPU structures have been studied in native\napplications, as well as in web browsers. These attacks continue to be a\nsubstantial threat to computing systems at all scales.\n  With the proliferation of heterogeneous systems and integration of hardware\naccelerators in every computing system, modern web browsers provide the support\nof GPU-based acceleration for the graphics and rendering processes. Emerging\nweb standards also support the GPU acceleration of general-purpose computation\nwithin web browsers.\n  In this paper, we present a new attack vector for microarchitectural attacks\nin web browsers. We use emerging GPU accelerating APIs in modern browsers\n(specifically WebGPU) to launch a GPU-based cache side channel attack on the\ncompute stack of the GPU that spies on victim activities on the graphics\n(rendering) stack of the GPU. Unlike prior works that rely on JavaScript APIs\nor software interfaces to build timing primitives, we build the timer using GPU\nhardware resources and develop a cache side channel attack on Intel's\nintegrated GPUs. We leverage the GPU's inherent parallelism at different levels\nto develop high-resolution parallel attacks. We demonstrate that GPU-based\ncache attacks can achieve a precision of 90 for website fingerprinting of 100\ntop websites. We also discuss potential countermeasures against the proposed\nattack to secure the systems at a critical time when these web standards are\nbeing developed and before they are widely deployed.\n",
        "title": "WebGPU-SPY: Finding Fingerprints in the Sandbox through GPU Cache\n  Attacks",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04350",
        "abstract_url": "http://arxiv.org/abs/2401.04350",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Sibo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Shan",
                "first_name": "Shiguang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Large-scale pre-trained vision-language models like CLIP have demonstrated\nimpressive performance across various tasks, and exhibit remarkable zero-shot\ngeneralization capability, while they are also vulnerable to imperceptible\nadversarial examples. Existing works typically employ adversarial training\n(fine-tuning) as a defense method against adversarial examples. However, direct\napplication to the CLIP model may result in overfitting, compromising the\nmodel's capacity for generalization. In this paper, we propose Pre-trained\nModel Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages\nsupervision from the original pre-trained model by carefully designing an\nauxiliary branch, to enhance the model's zero-shot adversarial robustness.\nSpecifically, PMG-AFT minimizes the distance between the features of\nadversarial examples in the target model and those in the pre-trained model,\naiming to preserve the generalization features already captured by the\npre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate\nthat PMG-AFT significantly outperforms the state-of-the-art method, improving\nthe top-1 robust accuracy by an average of 4.99%. Furthermore, our approach\nconsistently improves clean accuracy by an average of 8.72%.\n",
        "title": "Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial\n  Robustness",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04351",
        "abstract_url": "http://arxiv.org/abs/2401.04351",
        "authors": [
            {
                "last_name": "Arunan",
                "first_name": "Anushiya"
            },
            {
                "last_name": "Qin",
                "first_name": "Yan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaoli"
            },
            {
                "last_name": "Yuen",
                "first_name": "Chau"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  By informing the onset of the degradation process, health status evaluation\nserves as a significant preliminary step for reliable remaining useful life\n(RUL) estimation of complex equipment. This paper proposes a novel temporal\ndynamics learning-based model for detecting change points of individual\ndevices, even under variable operating conditions, and utilises the learnt\nchange points to improve the RUL estimation accuracy. During offline model\ndevelopment, the multivariate sensor data are decomposed to learn fused\ntemporal correlation features that are generalisable and representative of\nnormal operation dynamics across multiple operating conditions. Monitoring\nstatistics and control limit thresholds for normal behaviour are dynamically\nconstructed from these learnt temporal features for the unsupervised detection\nof device-level change points. The detected change points then inform the\ndegradation data labelling for training a long short-term memory (LSTM)-based\nRUL estimation model. During online monitoring, the temporal correlation\ndynamics of a query device is monitored for breach of the control limit derived\nin offline training. If a change point is detected, the device's RUL is\nestimated with the well-trained offline model for early preventive action.\nUsing C-MAPSS turbofan engines as the case study, the proposed method improved\nthe accuracy by 5.6\\% and 7.5\\% for two scenarios with six operating\nconditions, when compared to existing LSTM-based RUL estimation models that do\nnot consider heterogeneous change points.\n",
        "title": "A Change Point Detection Integrated Remaining Useful Life Estimation\n  Model under Variable Operating Conditions",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04354",
        "abstract_url": "http://arxiv.org/abs/2401.04354",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Xuzheng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Chen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Gan",
                "first_name": "Tian"
            },
            {
                "last_name": "Chao",
                "first_name": "Linlin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jianan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yuan"
            },
            {
                "last_name": "Guo",
                "first_name": "Qingpei"
            },
            {
                "last_name": "Chu",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method.\n",
        "title": "Knowledge-enhanced Multi-perspective Video Representation Learning for\n  Scene Recognition",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04357",
        "abstract_url": "http://arxiv.org/abs/2401.04357",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Yifan"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyu"
            },
            {
                "last_name": "Li",
                "first_name": "Shiqi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jihua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  As a fundamental problem in computer vision, point cloud registration aims to\nseek the optimal transformation for aligning a pair of point clouds. In most\nexisting methods, the information flows are usually forward transferring, thus\nlacking the guidance from high-level information to low-level information.\nBesides, excessive high-level information may be overly redundant, and directly\nusing it may conflict with the original low-level information. In this paper,\nwe propose a novel Iterative Feedback Network (IFNet) for unsupervised point\ncloud registration, in which the representation of low-level features is\nefficiently enriched by rerouting subsequent high-level features. Specifically,\nour IFNet is built upon a series of Feedback Registration Block (FRB) modules,\nwith each module responsible for generating the feedforward rigid\ntransformation and feedback high-level features. These FRB modules are cascaded\nand recurrently unfolded over time. Further, the Feedback Transformer is\ndesigned to efficiently select relevant information from feedback high-level\nfeatures, which is utilized to refine the low-level features. What's more, we\nincorporate a geometry-awareness descriptor to empower the network for making\nfull use of most geometric information, which leads to more precise\nregistration results. Extensive experiments on various benchmark datasets\ndemonstrate the superior registration performance of our IFNet.\n",
        "title": "Iterative Feedback Network for Unsupervised Point Cloud Registration",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04358",
        "abstract_url": "http://arxiv.org/abs/2401.04358",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yun"
            },
            {
                "last_name": "Ji",
                "first_name": "Fei"
            },
            {
                "last_name": "Wen",
                "first_name": "Miaowen"
            },
            {
                "last_name": "Qing",
                "first_name": "Hua"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "",
            ""
        ],
        "abstract": "  As a new candidate waveform for the next generation wireless communications,\northogonal chirp division multiplexing (OCDM) has attracted growing attention\nfor its ability to achieve full diversity in uncoded transmission, and its\nrobustness to narrow-band interference or impulsive noise. Under high mobility\nchannels with multiple lags and multiple Doppler-shifts (MLMD), the signal\nsuffers doubly selective (DS) fadings in time and frequency domain, and data\nsymbols modulated on orthogonal chirps are interfered by each other. To address\nthe problem of symbol detection of OCDM over MLMD channel, under the assumption\nthat path attenuation factors, delays, and Doppler shifts of the channel are\navailable, we first derive the closed-form channel matrix in Fresnel domain,\nand then propose a low-complexity method to approximate it as a sparse matrix.\nBased on the approximated Fresnel-domain channel, we propose a message passing\n(MP) based detector to estimate the transmit symbols iteratively. Finally,\nunder two MLMD channels (an underspread channel for terrestrial vehicular\ncommunication, and an overspread channel for narrow-band underwater acoustic\ncommunications), Monte Carlo simulation results and analysis are provided to\nvalidate its advantages as a promising detector for OCDM.\n",
        "title": "Message-Passing Receiver for OCDM over Multi-Lag Multi-Doppler Channels",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04360",
        "abstract_url": "http://arxiv.org/abs/2401.04360",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shixin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we study a class of special linear codes involving their\nparameters, weight distributions, and self-orthogonal properties. On one hand,\nwe prove that such codes must be maximum distance separable (MDS) or near MDS\n(NMDS) codes and completely determine their weight distributions with the help\nof the solutions to some subset sum problems. Based on the well-known Schur\nmethod, we also show that such codes are non-equivalent to generalized\nReed-Solomon codes. On the other hand, a sufficient and necessary condition for\nsuch codes to be self-orthogonal is characterized. Based on this condition, we\nfurther deduce that there are no self-dual codes in this class of linear codes\nand explicitly construct two classes of almost self-dual codes.\n",
        "title": "New non-GRS type MDS codes and NMDS codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04361",
        "abstract_url": "http://arxiv.org/abs/2401.04361",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaan"
            },
            {
                "last_name": "Qu",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Kexin"
            },
            {
                "last_name": "Li",
                "first_name": "Zhixu"
            },
            {
                "last_name": "Hua",
                "first_name": "Wen"
            },
            {
                "last_name": "Li",
                "first_name": "Ximing"
            },
            {
                "last_name": "Liu",
                "first_name": "An"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Knowledge-grounded dialogue (KGD) learns to generate an informative response\nbased on a given dialogue context and external knowledge (\\emph{e.g.},\nknowledge graphs; KGs). Recently, the emergence of large language models (LLMs)\nand pre-training techniques has brought great success to knowledge-grounded\ndialogue. However, when building KGD systems in real applications, there are\nvarious real-world noises that are inevitable to face. For example, the\ndialogue context might involve perturbations such as misspellings and\nabbreviations. In addition, KGs typically suffer from incompletion and also\nmight contain erroneous and outdated facts. Such real-world noises pose a\nchallenge to the robustness of KGD systems and hinder their applications in the\nreal world. In this paper, we propose an entity-based contrastive learning\nframework for improving the robustness of KGD. Specifically, we make use of the\nentity information in a KGD sample to create both its positive and negative\nsamples which involve semantic-irrelevant and semantic-relevant perturbations,\nrespectively. The contrastive learning framework ensures the KGD model is aware\nof these two types of perturbations, thus generating informative responses with\nthe potentially noisy inputs in real applications. Experimental results on\nthree benchmark datasets show that our method achieves new state-of-the-art\nperformance in terms of automatic evaluation scores, verifying its\neffectiveness and potentiality. Furthermore, we show that our method can\ngenerate better responses than comparison models in both the noisy and the\nfew-shot settings.\n",
        "title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive\n  Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04362",
        "abstract_url": "http://arxiv.org/abs/2401.04362",
        "authors": [
            {
                "last_name": "Yun",
                "first_name": "Kwan"
            },
            {
                "last_name": "Kim",
                "first_name": "Youngseo"
            },
            {
                "last_name": "Seo",
                "first_name": "Kwanggyoon"
            },
            {
                "last_name": "Seo",
                "first_name": "Chang Wook"
            },
            {
                "last_name": "Noh",
                "first_name": "Junyong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "GR",
            "",
            ""
        ],
        "abstract": "  We introduce DiffSketch, a method for generating a variety of stylized\nsketches from images. Our approach focuses on selecting representative features\nfrom the rich semantics of deep features within a pretrained diffusion model.\nThis novel sketch generation method can be trained with one manual drawing.\nFurthermore, efficient sketch extraction is ensured by distilling a trained\ngenerator into a streamlined extractor. We select denoising diffusion features\nthrough analysis and integrate these selected features with VAE features to\nproduce sketches. Additionally, we propose a sampling scheme for training\nmodels using a conditional generative approach. Through a series of\ncomparisons, we verify that distilled DiffSketch not only outperforms existing\nstate-of-the-art sketch extraction methods but also surpasses diffusion-based\nstylization methods in the task of extracting sketches.\n",
        "title": "Representative Feature Extraction During Diffusion Process for Sketch\n  Extraction with One Example",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04364",
        "abstract_url": "http://arxiv.org/abs/2401.04364",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Binh M."
            },
            {
                "last_name": "Kim",
                "first_name": "Jiwon"
            },
            {
                "last_name": "Tariq",
                "first_name": "Shahroz"
            },
            {
                "last_name": "Moore",
                "first_name": "Kristen"
            },
            {
                "last_name": "Abuadbba",
                "first_name": "Alsharif"
            },
            {
                "last_name": "Woo",
                "first_name": "Simon S."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR",
            "LG"
        ],
        "abstract": "  Deepfakes have rapidly emerged as a profound and serious threat to society,\nprimarily due to their ease of creation and dissemination. This situation has\ntriggered an accelerated development of deepfake detection technologies.\nHowever, many existing detectors rely heavily on lab-generated datasets for\nvalidation, which may not effectively prepare them for novel, emerging, and\nreal-world deepfake techniques. In this paper, we conduct an extensive and\ncomprehensive review and analysis of the latest state-of-the-art deepfake\ndetectors, evaluating them against several critical criteria. These criteria\nfacilitate the categorization of these detectors into 4 high-level groups and\n13 fine-grained sub-groups, all aligned with a unified standard conceptual\nframework. This classification and framework offer deep and practical insights\ninto the factors that affect detector efficacy. We assess the generalizability\nof 16 leading detectors across various standard attack scenarios, including\nblack-box, white-box, and gray-box settings. Our systematized analysis and\nexperimentation lay the groundwork for a deeper understanding of deepfake\ndetectors and their generalizability, paving the way for future research\nfocused on creating detectors adept at countering various attack scenarios.\nAdditionally, this work offers insights for developing more proactive defenses\nagainst deepfakes.\n",
        "title": "SoK: Facial Deepfake Detectors",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04367",
        "abstract_url": "http://arxiv.org/abs/2401.04367",
        "authors": [
            {
                "last_name": "Murray",
                "first_name": "Curtis"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Lewis"
            },
            {
                "last_name": "Tuke",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Mackay",
                "first_name": "Mark"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This study introduces a novel methodology for modelling patient emotions from\nonline patient experience narratives. We employed metadata network topic\nmodelling to analyse patient-reported experiences from Care Opinion, revealing\nkey emotional themes linked to patient-caregiver interactions and clinical\noutcomes. We develop a probabilistic, context-specific emotion recommender\nsystem capable of predicting both multilabel emotions and binary sentiments\nusing a naive Bayes classifier using contextually meaningful topics as\npredictors. The superior performance of our predicted emotions under this model\ncompared to baseline models was assessed using the information retrieval\nmetrics nDCG and Q-measure, and our predicted sentiments achieved an F1 score\nof 0.921, significantly outperforming standard sentiment lexicons. This method\noffers a transparent, cost-effective way to understand patient feedback,\nenhancing traditional collection methods and informing individualised patient\ncare. Our findings are accessible via an R package and interactive dashboard,\nproviding valuable tools for healthcare researchers and practitioners.\n",
        "title": "Probabilistic emotion and sentiment modelling of patient-reported\n  experiences",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04368",
        "abstract_url": "http://arxiv.org/abs/2401.04368",
        "authors": [
            {
                "last_name": "Manalu",
                "first_name": "Gabriel D. M."
            },
            {
                "last_name": "Christian",
                "first_name": "Mulomba Mukendi"
            },
            {
                "last_name": "You",
                "first_name": "Songhee"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The relationship between acute kidney injury (AKI) prediction and nephrotoxic\ndrugs, or drugs that adversely affect kidney function, is one that has yet to\nbe explored in the critical care setting. One contributing factor to this gap\nin research is the limited investigation of drug modalities in the intensive\ncare unit (ICU) context, due to the challenges of processing prescription data\ninto the corresponding drug representations and a lack in the comprehensive\nunderstanding of these drug representations. This study addresses this gap by\nproposing a novel approach that leverages patient prescription data as a\nmodality to improve existing models for AKI prediction. We base our research on\nElectronic Health Record (EHR) data, extracting the relevant patient\nprescription information and converting it into the selected drug\nrepresentation for our research, the extended-connectivity fingerprint (ECFP).\nFurthermore, we adopt a unique multimodal approach, developing machine learning\nmodels and 1D Convolutional Neural Networks (CNN) applied to clinical drug\nrepresentations, establishing a procedure which has not been used by any\nprevious studies predicting AKI. The findings showcase a notable improvement in\nAKI prediction through the integration of drug embeddings and other patient\ncohort features. By using drug features represented as ECFP molecular\nfingerprints along with common cohort features such as demographics and lab\ntest values, we achieved a considerable improvement in model performance for\nthe AKI prediction task over the baseline model which does not include the drug\nrepresentations as features, indicating that our distinct approach enhances\nexisting baseline techniques and highlights the relevance of drug data in\npredicting AKI in the ICU setting\n",
        "title": "Enhancing Acute Kidney Injury Prediction through Integration of Drug\n  Features in Intensive Care Units",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04369",
        "abstract_url": "http://arxiv.org/abs/2401.04369",
        "authors": [
            {
                "last_name": "Christian",
                "first_name": "Mulomba Mukendi"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Air pollution stands as the fourth leading cause of death globally. While\nextensive research has been conducted in this domain, most approaches rely on\nlarge datasets when it comes to prediction. This limits their applicability in\nlow-resource settings though more vulnerable. This study addresses this gap by\nproposing a novel machine learning approach for accurate air quality prediction\nusing two months of air quality data. By leveraging the World Weather\nRepository, the meteorological, air pollutant, and Air Quality Index features\nfrom 197 capital cities were considered to predict air quality for the next\nday. The evaluation of several machine learning models demonstrates the\neffectiveness of the Random Forest algorithm in generating reliable\npredictions, particularly when applied to classification rather than\nregression, approach which enhances the model's generalizability by 42%,\nachieving a cross-validation score of 0.38 for regression and 0.89 for\nclassification. To instill confidence in the predictions, interpretable machine\nlearning was considered. Finally, a cost estimation comparing the\nimplementation of this solution in high-resource and low-resource settings is\npresented including a tentative of technology licensing business model. This\nresearch highlights the potential for resource-limited countries to\nindependently predict air quality while awaiting larger datasets to further\nrefine their predictions.\n",
        "title": "Air Quality Forecasting Using Machine Learning: A Global perspective\n  with Relevance to Low-Resource Settings",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04371",
        "abstract_url": "http://arxiv.org/abs/2401.04371",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Kazi Ashik"
            },
            {
                "last_name": "Chen",
                "first_name": "Da Qi"
            },
            {
                "last_name": "Marathe",
                "first_name": "Madhav"
            },
            {
                "last_name": "Mortveit",
                "first_name": "Henning"
            },
            {
                "last_name": "Swarup",
                "first_name": "Samarth"
            },
            {
                "last_name": "Vullikanti",
                "first_name": "Anil"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Evacuation planning is an essential part of disaster management where the\ngoal is to relocate people under imminent danger to safety. Although government\nauthorities may prescribe routes and a schedule, evacuees generally behave as\nself-interested agents and may choose their action according to their own\nselfish interests. It is crucial to understand the degree of inefficiency this\ncan cause to the evacuation process. However, existing research has mainly\nfocused on selfish routing, i.e., they consider route selection as the only\nstrategic action. In this paper, we present a strategic routing and scheduling\ngame, named the Evacuation Planning Game (EPG), where evacuees choose both\ntheir route and the time of departure. We focus on confluent evacuation plans,\nwhere, if two routes meet at a node then their remaining portion is identical.\nWe also use dynamic flows to model the time-varying traffic on roads during\nevacuation. We show that every instance of EPG has at least one pure strategy\nNash equilibrium. We then present a polynomial time algorithm, the Sequential\nAction Algorithm (SAA), for finding equilibria in a given instance.\nAdditionally, we provide bounds on how bad an equilibrium state can be compared\nto a socially optimal state. Finally, We use Harris County of Houston, Texas as\nour study area and construct a game instance for it. Our results show that, by\nutilizing SAA, we can efficiently find equilibria in this instance that have\nsocial objective close to the optimal value.\n",
        "title": "Strategic Routing and Scheduling for Evacuations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04372",
        "abstract_url": "http://arxiv.org/abs/2401.04372",
        "authors": [
            {
                "last_name": "Gottwald",
                "first_name": "Georg"
            },
            {
                "last_name": "Li",
                "first_name": "Fengyi"
            },
            {
                "last_name": "Marzouk",
                "first_name": "Youssef"
            },
            {
                "last_name": "Reich",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We consider the problem of sampling from an unknown distribution for which\nonly a sufficiently large number of training samples are available. Such\nsettings have recently drawn considerable interest in the context of generative\nmodelling. In this paper, we propose a generative model combining diffusion\nmaps and Langevin dynamics. Diffusion maps are used to approximate the drift\nterm from the available training samples, which is then implemented in a\ndiscrete-time Langevin sampler to generate new samples. By setting the kernel\nbandwidth to match the time step size used in the unadjusted Langevin\nalgorithm, our method effectively circumvents any stability issues typically\nassociated with time-stepping stiff stochastic differential equations. More\nprecisely, we introduce a novel split-step scheme, ensuring that the generated\nsamples remain within the convex hull of the training samples. Our framework\ncan be naturally extended to generate conditional samples. We demonstrate the\nperformance of our proposed scheme through experiments on synthetic datasets\nwith increasing dimensions and on a stochastic subgrid-scale parametrization\nconditional sampling problem.\n",
        "title": "Stable generative modeling using diffusion maps",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04374",
        "abstract_url": "http://arxiv.org/abs/2401.04374",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Haoyi"
            },
            {
                "last_name": "L",
                "first_name": "Xuhong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiamin"
            },
            {
                "last_name": "Sun",
                "first_name": "Xinhao"
            },
            {
                "last_name": "Li",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Sun",
                "first_name": "Zeyi"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Given the complexity and lack of transparency in deep neural networks (DNNs),\nextensive efforts have been made to make these systems more interpretable or\nexplain their behaviors in accessible terms. Unlike most reviews, which focus\non algorithmic and model-centric perspectives, this work takes a \"data-centric\"\nview, examining how data collection, processing, and analysis contribute to\nexplainable AI (XAI). We categorize existing work into three categories subject\nto their purposes: interpretations of deep models, referring to feature\nattributions and reasoning processes that correlate data points with model\noutputs; influences of training data, examining the impact of training data\nnuances, such as data valuation and sample anomalies, on decision-making\nprocesses; and insights of domain knowledge, discovering latent patterns and\nfostering new knowledge from data and models to advance social values and\nscientific discovery. Specifically, we distill XAI methodologies into data\nmining operations on training and testing data across modalities, such as\nimages, text, and tabular data, as well as on training logs, checkpoints,\nmodels and other DNN behavior descriptors. In this way, our study offers a\ncomprehensive, data-centric examination of XAI from a lens of data mining\nmethods and applications.\n",
        "title": "Towards Explainable Artificial Intelligence (XAI): A Data Mining\n  Perspective",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04377",
        "abstract_url": "http://arxiv.org/abs/2401.04377",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jingtao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaonan"
            },
            {
                "last_name": "Wang",
                "first_name": "Danwei"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Tracking the object 6-DoF pose is crucial for various downstream robot tasks\nand real-world applications. In this paper, we investigate the real-world robot\ntask of aerial vision guidance for aerial robotics manipulation, utilizing\ncategory-level 6-DoF pose tracking. Aerial conditions inevitably introduce\nspecial challenges, such as rapid viewpoint changes in pitch and roll. To\nsupport this task and challenge, we firstly introduce a robust category-level\n6-DoF pose tracker (Robust6DoF). This tracker leverages shape and temporal\nprior knowledge to explore optimal inter-frame keypoint pairs, generated under\na priori structural adaptive supervision in a coarse-to-fine manner. Notably,\nour Robust6DoF employs a Spatial-Temporal Augmentation module to deal with the\nproblems of the inter-frame differences and intra-class shape variations\nthrough both temporal dynamic filtering and shape-similarity filtering. We\nfurther present a Pose-Aware Discrete Servo strategy (PAD-Servo), serving as a\ndecoupling approach to implement the final aerial vision guidance task. It\ncontains two servo action policies to better accommodate the structural\nproperties of aerial robotics manipulation. Exhaustive experiments on four\nwell-known public benchmarks demonstrate the superiority of our Robust6DoF.\nReal-world tests directly verify that our Robust6DoF along with PAD-Servo can\nbe readily used in real-world aerial robotic applications.\n",
        "title": "Towards Real-World Aerial Vision Guidance with Categorical 6D Pose\n  Tracker",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04378",
        "abstract_url": "http://arxiv.org/abs/2401.04378",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Zan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lianzeng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  In this paper, we propose a new efficient method for calculating the\nGerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function\nusually satisfies a class of integro-differential equation. We introduce the\nphysics-informed neural networks (PINN) which embed a differential equation\ninto the loss of the neural network using automatic differentiation. In\naddition, PINN is more free to set boundary conditions and does not rely on the\ndetermination of the initial value. This gives us an idea to calculate more\ngeneral Gerber-Shiu functions. Numerical examples are provided to illustrate\nthe very good performance of our approximation.\n",
        "title": "Computing the Gerber-Shiu function with interest and a constant dividend\n  barrier by physics-informed neural networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04385",
        "abstract_url": "http://arxiv.org/abs/2401.04385",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Tang",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Li",
                "first_name": "Kenli"
            },
            {
                "last_name": "Datta",
                "first_name": "Anwitaman"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.\n",
        "title": "Machine unlearning through fine-grained model parameters perturbation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04389",
        "abstract_url": "http://arxiv.org/abs/2401.04389",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mingshuai"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuangqi"
            },
            {
                "last_name": "Yan",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Lv",
                "first_name": "Yuanjun"
            },
            {
                "last_name": "Xia",
                "first_name": "Xianjun"
            },
            {
                "last_name": "Huang",
                "first_name": "Chuanzeng"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yijian"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  This paper introduces our repairing and denoising network (RaD-Net) for the\nICASSP 2024 Speech Signal Improvement (SSI) Challenge. We extend our previous\nframework based on a two-stage network and propose an upgraded model.\nSpecifically, we replace the repairing network with COM-Net from TEA-PSE. In\naddition, multi-resolution discriminators and multi-band discriminators are\nadopted in the training stage. Finally, we use a three-step training strategy\nto optimize our model. We submit two models with different sets of parameters\nto meet the RTF requirement of the two tracks. According to the official\nresults, the proposed systems rank 2nd in track 1 and 3rd in track 2.\n",
        "title": "RaD-Net: A Repairing and Denoising Network for Speech Signal Improvement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04390",
        "abstract_url": "http://arxiv.org/abs/2401.04390",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Heewon"
            },
            {
                "last_name": "Chang",
                "first_name": "Hyun Sung"
            },
            {
                "last_name": "Cho",
                "first_name": "Kiho"
            },
            {
                "last_name": "Lee",
                "first_name": "Jaeyun"
            },
            {
                "last_name": "Han",
                "first_name": "Bohyung"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Labor-intensive labeling becomes a bottleneck in developing computer vision\nalgorithms based on deep learning. For this reason, dealing with imperfect\nlabels has increasingly gained attention and has become an active field of\nstudy. We address learning with noisy labels (LNL) problem, which is formalized\nas a task of finding a structured manifold in the midst of noisy data. In this\nframework, we provide a proper objective function and an optimization algorithm\nbased on two expectation-maximization (EM) cycles. The separate networks\nassociated with the two EM cycles collaborate to optimize the objective\nfunction, where one model is for distinguishing clean labels from corrupted\nones while the other is for refurbishing the corrupted labels. This approach\nresults in a non-collapsing LNL-flywheel model in the end. Experiments show\nthat our algorithm achieves state-of-the-art performance in multiple standard\nbenchmarks with substantial margins under various types of label noise.\n",
        "title": "Learning with Noisy Labels: Interconnection of Two\n  Expectation-Maximizations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04394",
        "abstract_url": "http://arxiv.org/abs/2401.04394",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhifeng"
            },
            {
                "last_name": "Yu",
                "first_name": "Shengye"
            },
            {
                "last_name": "Li",
                "first_name": "Mengtian"
            },
            {
                "last_name": "He",
                "first_name": "Qile"
            },
            {
                "last_name": "Chen",
                "first_name": "Chaofeng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yu-Gang"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "SD",
            ""
        ],
        "abstract": "  There has been a growing interest in the task of generating sound for silent\nvideos, primarily because of its practicality in streamlining video\npost-production. However, existing methods for video-sound generation attempt\nto directly create sound from visual representations, which can be challenging\ndue to the difficulty of aligning visual representations with audio\nrepresentations. In this paper, we present SonicVisionLM, a novel framework\naimed at generating a wide range of sound effects by leveraging vision language\nmodels. Instead of generating audio directly from video, we use the\ncapabilities of powerful vision language models (VLMs). When provided with a\nsilent video, our approach first identifies events within the video using a VLM\nto suggest possible sounds that match the video content. This shift in approach\ntransforms the challenging task of aligning image and audio into more\nwell-studied sub-problems of aligning image-to-text and text-to-audio through\nthe popular diffusion models. To improve the quality of audio recommendations\nwith LLMs, we have collected an extensive dataset that maps text descriptions\nto specific sound effects and developed temporally controlled audio adapters.\nOur approach surpasses current state-of-the-art methods for converting video to\naudio, resulting in enhanced synchronization with the visuals and improved\nalignment between audio and video components. Project page:\nhttps://yusiissy.github.io/SonicVisionLM.github.io/\n",
        "title": "SonicVisionLM: Playing Sound with Vision Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04397",
        "abstract_url": "http://arxiv.org/abs/2401.04397",
        "authors": [
            {
                "last_name": "Keurulainen",
                "first_name": "Oskar"
            },
            {
                "last_name": "Alcan",
                "first_name": "Gokhan"
            },
            {
                "last_name": "Kyrki",
                "first_name": "Ville"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  Building machines capable of efficiently collaborating with humans has been a\nlongstanding goal in artificial intelligence. Especially in the presence of\nuncertainties, optimal cooperation often requires that humans and artificial\nagents model each other's behavior and use these models to infer underlying\ngoals, beliefs or intentions, potentially involving multiple levels of\nrecursion. Empirical evidence for such higher-order cognition in human behavior\nis also provided by previous works in cognitive science, linguistics, and\nrobotics. We advocate for a new paradigm for active learning for human feedback\nthat utilises humans as active data sources while accounting for their higher\nlevels of agency. In particular, we discuss how increasing level of agency\nresults in qualitatively different forms of rational communication between an\nactive learning system and a teacher. Additionally, we provide a practical\nexample of active learning using a higher-order cognitive model. This is\naccompanied by a computational study that underscores the unique behaviors that\nthis model produces.\n",
        "title": "The Role of Higher-Order Cognitive Models in Active Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04398",
        "abstract_url": "http://arxiv.org/abs/2401.04398",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zilong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Chun-Liang"
            },
            {
                "last_name": "Eisenschlos",
                "first_name": "Julian Martin"
            },
            {
                "last_name": "Perot",
                "first_name": "Vincent"
            },
            {
                "last_name": "Wang",
                "first_name": "Zifeng"
            },
            {
                "last_name": "Miculicich",
                "first_name": "Lesly"
            },
            {
                "last_name": "Fujii",
                "first_name": "Yasuhisa"
            },
            {
                "last_name": "Shang",
                "first_name": "Jingbo"
            },
            {
                "last_name": "Lee",
                "first_name": "Chen-Yu"
            },
            {
                "last_name": "Pfister",
                "first_name": "Tomas"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.\n",
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table\n  Understanding",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04402",
        "abstract_url": "http://arxiv.org/abs/2401.04402",
        "authors": [
            {
                "last_name": "Ghosheh",
                "first_name": "Ghadeer O."
            },
            {
                "last_name": "Li",
                "first_name": "Jin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Tingting"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n",
        "title": "IGNITE: Individualized GeNeration of Imputations in Time-series\n  Electronic health records",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04403",
        "abstract_url": "http://arxiv.org/abs/2401.04403",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Long"
            },
            {
                "last_name": "Li",
                "first_name": "Shanghong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yongquan"
            },
            {
                "last_name": "Luo",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the field of Industrial Informatics, interactive segmentation has gained\nsignificant attention for its application in human-computer interaction and\ndata annotation. Existing algorithms, however, face challenges in balancing the\nsegmentation accuracy between large and small targets, often leading to an\nincreased number of user interactions. To tackle this, a novel multi-scale\ntoken adaptation algorithm, leveraging token similarity, has been devised to\nenhance segmentation across varying target sizes. This algorithm utilizes a\ndifferentiable top-k tokens selection mechanism, allowing for fewer tokens to\nbe used while maintaining efficient multi-scale token interaction. Furthermore,\na contrastive loss is introduced to better discriminate between target and\nbackground tokens, improving the correctness and robustness of the tokens\nsimilar to the target. Extensive benchmarking shows that the algorithm achieves\nstate-of-the-art (SOTA) performance compared to current methods. An interactive\ndemo and all reproducible codes will be released at\nhttps://github.com/hahamyt/mst.\n",
        "title": "MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04405",
        "abstract_url": "http://arxiv.org/abs/2401.04405",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jinhai"
            },
            {
                "last_name": "Guo",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shijie"
            },
            {
                "last_name": "Li",
                "first_name": "Junlin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Li"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "",
            "CV",
            ""
        ],
        "abstract": "  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n",
        "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title\n  Bitrate Ladder Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04406",
        "abstract_url": "http://arxiv.org/abs/2401.04406",
        "authors": [
            {
                "last_name": "Jyhne",
                "first_name": "Sander Riis\u00f8en"
            },
            {
                "last_name": "Goodwin",
                "first_name": "Morten"
            },
            {
                "last_name": "Andersen",
                "first_name": "Per Arne"
            },
            {
                "last_name": "Oveland",
                "first_name": "Ivar"
            },
            {
                "last_name": "Nossum",
                "first_name": "Alexander Salveson"
            },
            {
                "last_name": "Ormseth",
                "first_name": "Karianne"
            },
            {
                "last_name": "\u00d8rstavik",
                "first_name": "Mathilde"
            },
            {
                "last_name": "Flatman",
                "first_name": "Andrew C."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  MapAI: Precision in Building Segmentation is a competition arranged with the\nNorwegian Artificial Intelligence Research Consortium (NORA) in collaboration\nwith Centre for Artificial Intelligence Research at the University of Agder\n(CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency\nfor Data Supply and Infrastructure. The competition will be held in the fall of\n2022. It will be concluded at the Northern Lights Deep Learning conference\nfocusing on the segmentation of buildings using aerial images and laser data.\nWe propose two different tasks to segment buildings, where the first task can\nonly utilize aerial images, while the second must use laser data (LiDAR) with\nor without aerial images. Furthermore, we use IoU and Boundary IoU to properly\nevaluate the precision of the models, with the latter being an IoU measure that\nevaluates the results' boundaries. We provide the participants with a training\ndataset and keep a test dataset for evaluation.\n",
        "title": "MapAI: Precision in Building Segmentation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04408",
        "abstract_url": "http://arxiv.org/abs/2401.04408",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Qinyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Penghan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Lai",
                "first_name": "Fan"
            },
            {
                "last_name": "Mao",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Wei",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Song",
                "first_name": "Jun"
            },
            {
                "last_name": "Tsai",
                "first_name": "Wei-Yu"
            },
            {
                "last_name": "Yang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Hu",
                "first_name": "Yuxi"
            },
            {
                "last_name": "Qian",
                "first_name": "Xuehai"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG",
            "",
            ""
        ],
        "abstract": "  Huge embedding tables in modern Deep Learning Recommender Models (DLRM)\nrequire prohibitively large memory during training and inference. Aiming to\nreduce the memory footprint of training, this paper proposes FIne-grained\nIn-Training Embedding Dimension optimization (FIITED). Given the observation\nthat embedding vectors are not equally important, FIITED adjusts the dimension\nof each individual embedding vector continuously during training, assigning\nlonger dimensions to more important embeddings while adapting to dynamic\nchanges in data. A novel embedding storage system based on virtually-hashed\nphysically-indexed hash tables is designed to efficiently implement the\nembedding dimension adjustment and effectively enable memory saving.\nExperiments on two industry models show that FIITED is able to reduce the size\nof embeddings by more than 65% while maintaining the trained model's quality,\nsaving significantly more memory than a state-of-the-art in-training embedding\npruning method. On public click-through rate prediction datasets, FIITED is\nable to prune up to 93.75%-99.75% embeddings without significant accuracy loss.\n",
        "title": "Fine-Grained Embedding Dimension Optimization During Training for\n  Recommender Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04411",
        "abstract_url": "http://arxiv.org/abs/2401.04411",
        "authors": [
            {
                "last_name": "Ferdaus",
                "first_name": "Farah"
            },
            {
                "last_name": "Talukder",
                "first_name": "B. M. S. Bahar"
            },
            {
                "last_name": "Rahman",
                "first_name": "Md Tauhidur"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "CR"
        ],
        "abstract": "  This article introduces a novel, low-cost technique for hiding data in\ncommercially available resistive-RAM (ReRAM) chips. The data is kept hidden in\nReRAM cells by manipulating its analog physical properties through switching\n($\\textit{set/reset}$) operations. This hidden data, later, is retrieved by\nsensing the changes in cells' physical properties (i.e., $\\textit{set/reset}$\ntime of the memory cells). The proposed system-level hiding technique does not\naffect the normal memory operations and does not require any hardware\nmodifications. Furthermore, the proposed hiding approach is robust against\ntemperature variations and the aging of the devices through normal read/write\noperation. The silicon results show that our proposed data hiding technique is\nacceptably fast with ${\\sim}0.4bit/min$ of encoding and ${\\sim}15.625bits/s$ of\nretrieval rates, and the hidden message is unrecoverable without the knowledge\nof the secret key, which is used to enhance the security of hidden information.\n",
        "title": "Hiding Information for Secure and Covert Data Storage in Commercial\n  ReRAM Chips",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04422",
        "abstract_url": "http://arxiv.org/abs/2401.04422",
        "authors": [
            {
                "last_name": "der Br\u00fcck",
                "first_name": "Tim vor"
            },
            {
                "last_name": "Pouly",
                "first_name": "Marc"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings\nenjoy great success in the semantic representation of words, sentences, and\nwhole documents as well as for semantic similarity estimation. However, they\nhave the shortcoming that they are directly extracted from a surface\nrepresentation, which does not adequately represent human thought processes and\nalso performs poorly for highly ambiguous words. Therefore, we propose Semantic\nConcept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism,\nwhich addresses both shortcomings. The evaluation on a marketing target group\ndistribution task showed that the accuracy of predicted target groups can be\nincreased by combining traditional word embeddings with semantic CEs.\n",
        "title": "Estimating Text Similarity based on Semantic Concept Embeddings",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04423",
        "abstract_url": "http://arxiv.org/abs/2401.04423",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wei"
            },
            {
                "last_name": "Lin",
                "first_name": "Yujie"
            },
            {
                "last_name": "Ren",
                "first_name": "Pengjie"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhumin"
            },
            {
                "last_name": "Mine",
                "first_name": "Tsunenori"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jianli"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Moyan"
            },
            {
                "last_name": "Ben",
                "first_name": "Xianye"
            },
            {
                "last_name": "Li",
                "first_name": "Yujun"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Sequential recommendation has attracted a lot of attention from both academia\nand industry, however the privacy risks associated to gathering and\ntransferring users' personal interaction data are often underestimated or\nignored. Existing privacy-preserving studies are mainly applied to traditional\ncollaborative filtering or matrix factorization rather than sequential\nrecommendation. Moreover, these studies are mostly based on differential\nprivacy or federated learning, which often leads to significant performance\ndegradation, or has high requirements for communication. In this work, we\naddress privacy-preserving from a different perspective. Unlike existing\nresearch, we capture collaborative signals of neighbor interaction sequences\nand directly inject indistinguishable items into the target sequence before the\nrecommendation process begins, thereby increasing the perplexity of the target\nsequence. Even if the target interaction sequence is obtained by attackers, it\nis difficult to discern which ones are the actual user interaction records. To\nachieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,\nnamely CLOUD, which incorporates a collaborative confusion mechanism to edit\nthe raw interaction sequences before conducting recommendation. Specifically,\nCLOUD first calculates the similarity between the target interaction sequence\nand other neighbor sequences to find similar sequences. Then, CLOUD considers\nthe shared representation of the target sequence and similar sequences to\ndetermine the operation to be performed: keep, delete, or insert. We design a\ncopy mechanism to make items from similar sequences have a higher probability\nto be inserted into the target sequence. Finally, the modified sequence is used\nto train the recommender and predict the next item.\n",
        "title": "Privacy-Preserving Sequential Recommendation with Collaborative\n  Confusion",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04425",
        "abstract_url": "http://arxiv.org/abs/2401.04425",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Kosmas",
                "first_name": "Panagiotis"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Domain generalization is a popular machine learning technique that enables\nmodels to perform well on the unseen target domain, by learning from multiple\nsource domains. Domain generalization is useful in cases where data is limited,\ndifficult, or expensive to collect, such as in object recognition and\nbiomedicine. In this paper, we propose a novel domain generalization algorithm\ncalled \"meta-forests\", which builds upon the basic random forests model by\nincorporating the meta-learning strategy and maximum mean discrepancy measure.\nThe aim of meta-forests is to enhance the generalization ability of classifiers\nby reducing the correlation among trees and increasing their strength. More\nspecifically, meta-forests conducts meta-learning optimization during each\nmeta-task, while also utilizing the maximum mean discrepancy as a\nregularization term to penalize poor generalization performance in the\nmeta-test process. To evaluate the effectiveness of our algorithm, we test it\non two publicly object recognition datasets and a glucose monitoring dataset\nthat we have used in a previous study. Our results show that meta-forests\noutperforms state-of-the-art approaches in terms of generalization performance\non both object recognition and glucose monitoring datasets.\n",
        "title": "Meta-forests: Domain generalization on random forests with meta-learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04429",
        "abstract_url": "http://arxiv.org/abs/2401.04429",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Peiyan"
            },
            {
                "last_name": "Song",
                "first_name": "Qiyuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Wanyuan"
            },
            {
                "last_name": "Wu",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wencan"
            },
            {
                "last_name": "Gao",
                "first_name": "Guanyu"
            },
            {
                "last_name": "Lyu",
                "first_name": "Yan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA"
        ],
        "abstract": "  Ride-hailing platforms have been facing the challenge of balancing demand and\nsupply. Existing vehicle reposition techniques often treat drivers as\nhomogeneous agents and relocate them deterministically, assuming compliance\nwith the reposition. In this paper, we consider a more realistic and\ndriver-centric scenario where drivers have unique cruising preferences and can\ndecide whether to take the recommendation or not on their own. We propose\ni-Rebalance, a personalized vehicle reposition technique with deep\nreinforcement learning (DRL). i-Rebalance estimates drivers' decisions on\naccepting reposition recommendations through an on-field user study involving\n99 real drivers. To optimize supply-demand balance and enhance preference\nsatisfaction simultaneously, i-Rebalance has a sequential reposition strategy\nwith dual DRL agents: Grid Agent to determine the reposition order of idle\nvehicles, and Vehicle Agent to provide personalized recommendations to each\nvehicle in the pre-defined order. This sequential learning strategy facilitates\nmore effective policy training within a smaller action space compared to\ntraditional joint-action methods. Evaluation of real-world trajectory data\nshows that i-Rebalance improves driver acceptance rate by 38.07% and total\ndriver income by 9.97%.\n",
        "title": "i-Rebalance: Personalized Vehicle Repositioning for Supply Demand\n  Balance",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04430",
        "abstract_url": "http://arxiv.org/abs/2401.04430",
        "authors": [
            {
                "last_name": "Dogukan",
                "first_name": "Ali Tugberk"
            },
            {
                "last_name": "Arslan",
                "first_name": "Emre"
            },
            {
                "last_name": "Basar",
                "first_name": "Ertugrul"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  Reconfigurable intelligent surfaces (RISs) bring great potential to the\nadvancement of 6G and beyond wireless communication technologies. RISs\nintroduce a great degree of flexibility, allowing some sort of virtual control\nover the wireless channel. Exploiting the flexibility introduced by RISs, we\npropose a novel RIS-enabled downlink (DL) non-orthogonal multiple access (NOMA)\nscheme where NOMA is enabled over-the-air rather than at the base station (BS)\nor the receiver (Rx). Here, the RIS is partitioned into distinctive groups\nwhere each part of the RIS serves a different user equipment (UE) to perform\nmultiple accessing. The BS transmits an unmodulated signal to the RIS, and each\npartition modulates the impinging signal over-the-air by introducing a phase\nshift according to the incoming information bits to serve the corresponding UE.\nFirst, the end-to-end system model for the proposed system is presented.\nFurthermore, outage probability calculations, theoretical error probability\nanalysis, and bit error rate (BER) derivations are discussed and reinforced\nwith comprehensive computer simulation results.\n",
        "title": "Reconfigurable Intelligent Surface-Enabled Downlink NOMA",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04431",
        "abstract_url": "http://arxiv.org/abs/2401.04431",
        "authors": [
            {
                "last_name": "Iafolla",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Fiorenza",
                "first_name": "Emiliano"
            },
            {
                "last_name": "Chiappini",
                "first_name": "Massimo"
            },
            {
                "last_name": "Carmisciano",
                "first_name": "Cosmo"
            },
            {
                "last_name": "Iafolla",
                "first_name": "Valerio Antonio"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Sea wave monitoring is key in many applications in oceanography such as the\nvalidation of weather and wave models. Conventional in situ solutions are based\non moored buoys whose measurements are often recognized as a standard. However,\nbeing exposed to a harsh environment, they are not reliable, need frequent\nmaintenance, and the datasets feature many gaps. To overcome the previous\nlimitations, we propose a system including a buoy, a micro-seismic measuring\nstation, and a machine learning algorithm. The working principle is based on\nmeasuring the micro-seismic signals generated by the sea waves. Thus, the\nmachine learning algorithm will be trained to reconstruct the missing buoy data\nfrom the micro-seismic data. As the micro-seismic station can be installed\nindoor, it assures high reliability while the machine learning algorithm\nprovides accurate reconstruction of the missing buoy data. In this work, we\npresent the methods to process the data, develop and train the machine learning\nalgorithm, and assess the reconstruction accuracy. As a case of study, we used\nexperimental data collected in 2014 from the Northern Tyrrhenian Sea\ndemonstrating that the data reconstruction can be done both for significant\nwave height and wave period. The proposed approach was inspired from Data\nScience, whose methods were the foundation for the new solutions presented in\nthis work. For example, estimating the period of the sea waves, often not\ndiscussed in previous works, was relatively simple with machine learning. In\nconclusion, the experimental results demonstrated that the new system can\novercome the reliability issues of the buoy keeping the same accuracy.\n",
        "title": "Sea wave data reconstruction using micro-seismic measurements and\n  machine learning methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04435",
        "abstract_url": "http://arxiv.org/abs/2401.04435",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Kuo"
            },
            {
                "last_name": "Li",
                "first_name": "Duo"
            },
            {
                "last_name": "Hu",
                "first_name": "Menghan"
            },
            {
                "last_name": "Zhai",
                "first_name": "Guangtao"
            },
            {
                "last_name": "Yang",
                "first_name": "Xiaokang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao-Ping"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  For semi-supervised learning with imbalance classes, the long-tailed\ndistribution of data will increase the model prediction bias toward dominant\nclasses, undermining performance on less frequent classes. Existing methods\nalso face challenges in ensuring the selection of sufficiently reliable\npseudo-labels for model training and there is a lack of mechanisms to adjust\nthe selection of more reliable pseudo-labels based on different training\nstages. To mitigate this issue, we introduce uncertainty into the modeling\nprocess for pseudo-label sampling, taking into account that the model\nperformance on the tailed classes varies over different training stages. For\nexample, at the early stage of model training, the limited predictive accuracy\nof model results in a higher rate of uncertain pseudo-labels. To counter this,\nwe propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach.\nThis approach allows the model to perceive the uncertainty of pseudo-labels at\ndifferent training stages, thereby adaptively adjusting the selection\nthresholds for different classes. Compared to other methods such as the\nbaseline method FixMatch, UDTS achieves an increase in accuracy of at least\napproximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image\ndatasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset\nTissueMNIST, respectively. The source code of UDTS is publicly available at:\nhttps://github.com/yangk/UDTS.\n",
        "title": "Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04436",
        "abstract_url": "http://arxiv.org/abs/2401.04436",
        "authors": [
            {
                "last_name": "van Dissel",
                "first_name": "Mauritz Cartier"
            },
            {
                "last_name": "Gora",
                "first_name": "Pawe\u0142"
            },
            {
                "last_name": "Manea",
                "first_name": "Drago\u015f"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Urban road transport is a major civilisational and economic challenge,\naffecting quality of life and economic activity. Addressing these challenges\nrequires a multidisciplinary approach and sustainable urban planning strategies\nto mitigate the negative effects of traffic in cities. In this paper, we will\nintroduce an extension of one of the most popular macroscopic traffic\nsimulation models, the Payne-Whitham model. We will investigate how this model,\noriginally designed to model highway traffic on straight road segments, can be\nadapted to more realistic conditions with arbitrary road network graphs and\nmultiple intersections with traffic signals. Furthermore, we will showcase the\npractical application of this extension in experiments aimed at optimising\ntraffic signal settings. For computational reasons, these experiments involve\nthe adoption of surrogate models for approximating our extended Payne-Whitham\nmodel, and subsequently, we utilise various optimisation algorithms, e.g.\nSLSQP, resulting in the identification of traffic signal settings that enhance\nthe average speed of cars, thereby facilitating smoother traffic flow.\n",
        "title": "A Payne-Whitham model of urban traffic networks in the presence of\n  traffic lights and its application to traffic optimisation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04437",
        "abstract_url": "http://arxiv.org/abs/2401.04437",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Dongeon"
            },
            {
                "last_name": "Park",
                "first_name": "YeongHyeon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recent studies try to use hyperspectral imaging (HSI) to detect foreign\nmatters in products because it enables to visualize the invisible wavelengths\nincluding ultraviolet and infrared. Considering the enormous image channels of\nthe HSI, several dimension reduction methods-e.g., PCA or UMAP-can be\nconsidered to reduce but those cannot ease the fundamental limitations, as\nfollows: (1) latency of HSI capturing. (2) less explanation ability of the\nimportant channels. In this paper, to circumvent the aforementioned methods,\none of the ways to channel reduction, on anomaly detection proposed HSI.\nDifferent from feature extraction methods (i.e., PCA or UMAP), feature\nselection can sort the feature by impact and show better explainability so we\nmight redesign the task-optimized and cost-effective spectroscopic camera. Via\nthe extensive experiment results with synthesized MVTec AD dataset, we confirm\nthat the feature selection method shows 6.90x faster at the inference phase\ncompared with feature extraction-based approaches while preserving anomaly\ndetection performance. Ultimately, we conclude the advantage of feature\nselection which is effective yet fast.\n",
        "title": "Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using\n  Dimension Reduction Methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04441",
        "abstract_url": "http://arxiv.org/abs/2401.04441",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Yishuang"
            },
            {
                "last_name": "Wang",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.\n",
        "title": "Image classification network enhancement methods based on knowledge\n  injection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04446",
        "abstract_url": "http://arxiv.org/abs/2401.04446",
        "authors": [
            {
                "last_name": "Schneider",
                "first_name": "Simon"
            },
            {
                "last_name": "Ferreyra",
                "first_name": "Nicol\u00e1s E. D\u00edaz"
            },
            {
                "last_name": "Qu\u00e9val",
                "first_name": "Pierre-Jean"
            },
            {
                "last_name": "Simhandl",
                "first_name": "Georg"
            },
            {
                "last_name": "Zdun",
                "first_name": "Uwe"
            },
            {
                "last_name": "Scandariato",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Models of software systems are used throughout the software development\nlifecycle. Dataflow diagrams (DFDs), in particular, are well-established\nresources for security analysis. Many techniques, such as threat modelling, are\nbased on DFDs of the analysed application. However, their impact on the\nperformance of analysts in a security analysis setting has not been explored\nbefore. In this paper, we present the findings of an empirical experiment\nconducted to investigate this effect. Following a within-groups design,\nparticipants were asked to solve security-relevant tasks for a given\nmicroservice application. In the control condition, the participants had to\nexamine the source code manually. In the model-supported condition, they were\nadditionally provided a DFD of the analysed application and traceability\ninformation linking model items to artefacts in source code. We found that the\nparticipants (n = 24) performed significantly better in answering the analysis\ntasks correctly in the model-supported condition (41% increase in analysis\ncorrectness). Further, participants who reported using the provided\ntraceability information performed better in giving evidence for their answers\n(315% increase in correctness of evidence). Finally, we identified three open\nchallenges of using DFDs for security analysis based on the insights gained in\nthe experiment.\n",
        "title": "How Dataflow Diagrams Impact Software Security Analysis: an Empirical\n  Experiment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04447",
        "abstract_url": "http://arxiv.org/abs/2401.04447",
        "authors": [
            {
                "last_name": "Mulimani",
                "first_name": "Manjunath"
            },
            {
                "last_name": "Mesaros",
                "first_name": "Annamaria"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  In this paper, we propose a method for class-incremental learning of\npotentially overlapping sounds for solving a sequence of multi-label audio\nclassification tasks. We design an incremental learner that learns new classes\nindependently of the old classes. To preserve knowledge about the old classes,\nwe propose a cosine similarity-based distillation loss that minimizes\ndiscrepancy in the feature representations of subsequent learners, and use it\nalong with a Kullback-Leibler divergence-based distillation loss that minimizes\ndiscrepancy in their respective outputs. Experiments are performed on a dataset\nwith 50 sound classes, with an initial classification task containing 30 base\nclasses and 4 incremental phases of 5 classes each. After each phase, the\nsystem is tested for multi-label classification with the entire set of classes\nlearned so far. The proposed method obtains an average F1-score of 40.9% over\nthe five phases, ranging from 45.2% in phase 0 on 30 classes, to 36.3% in phase\n4 on 50 classes. Average performance degradation over incremental phases is\nonly 0.7 percentage points from the initial F1-score of 45.2%.\n",
        "title": "Class-Incremental Learning for Multi-Label Audio Classification",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04448",
        "abstract_url": "http://arxiv.org/abs/2401.04448",
        "authors": [
            {
                "last_name": "Breci",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Guarnera",
                "first_name": "Luca"
            },
            {
                "last_name": "Battiato",
                "first_name": "Sebastiano"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Forensic handwriting examination is a branch of Forensic Science that aims to\nexamine handwritten documents in order to properly define or hypothesize the\nmanuscript's author. These analysis involves comparing two or more (digitized)\ndocuments through a comprehensive comparison of intrinsic local and global\nfeatures. If a correlation exists and specific best practices are satisfied,\nthen it will be possible to affirm that the documents under analysis were\nwritten by the same individual. The need to create sophisticated tools capable\nof extracting and comparing significant features has led to the development of\ncutting-edge software with almost entirely automated processes, improving the\nforensic examination of handwriting and achieving increasingly objective\nevaluations. This is made possible by algorithmic solutions based on purely\nmathematical concepts. Machine Learning and Deep Learning models trained with\nspecific datasets could turn out to be the key elements to best solve the task\nat hand. In this paper, we proposed a new and challenging dataset consisting of\ntwo subsets: the first consists of 21 documents written either by the classic\n``pen and paper\" approach (and later digitized) and directly acquired on common\ndevices such as tablets; the second consists of 362 handwritten manuscripts by\n124 different people, acquired following a specific pipeline. Our study\npioneered a comparison between traditionally handwritten documents and those\nproduced with digital tools (e.g., tablets). Preliminary results on the\nproposed datasets show that 90% classification accuracy can be achieved on the\nfirst subset (documents written on both paper and pen and later digitized and\non tablets) and 96% on the second portion of the data. The datasets are\navailable at\nhttps://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/.\n",
        "title": "A Novel Dataset for Non-Destructive Inspection of Handwritten Documents",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04452",
        "abstract_url": "http://arxiv.org/abs/2401.04452",
        "authors": [
            {
                "last_name": "Richard",
                "first_name": "Magali"
            },
            {
                "last_name": "Blum",
                "first_name": "Yuna"
            },
            {
                "last_name": "Guinney",
                "first_name": "Justin"
            },
            {
                "last_name": "Stolovitzky",
                "first_name": "Gustavo"
            },
            {
                "last_name": "Pav\u00e3o",
                "first_name": "Adrien"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This chapter provides a comprehensive overview of the pragmatic aspects\ninvolved in organizing AI competitions. We begin by discussing strategies to\nincentivize participation, touching upon effective communication techniques,\naligning with trending topics in the field, structuring awards, potential\nrecruitment opportunities, and more. We then shift to the essence of community\nengagement, and into organizational best practices and effective means of\ndisseminating challenge outputs. Lastly, the chapter addresses the logistics,\nexposing on costs, required manpower, and resource allocation for effectively\nmanaging and executing a challenge. By examining these practical problems,\nreaders will gain actionable insights to navigate the multifaceted landscape of\nAI competition organization, from inception to completion.\n",
        "title": "AI Competitions and Benchmarks, Practical issues: Proposals, grant\n  money, sponsors, prizes, dissemination, publicity",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04454",
        "abstract_url": "http://arxiv.org/abs/2401.04454",
        "authors": [
            {
                "last_name": "Bezuidenhout",
                "first_name": "Louise"
            },
            {
                "last_name": "Ratti",
                "first_name": "Emanuele"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  In this chapter, we propose a non-traditional RCR training in data science\nthat is grounded into a virtue theory framework. First, we delineate the\napproach in more theoretical detail, by discussing how the goal of RCR training\nis to foster the cultivation of certain moral abilities. We specify the nature\nof these abilities: while the ideal is the cultivation of virtues, the limited\nspace allowed by RCR modules can only facilitate the cultivation of superficial\nabilities or proto-virtues, which help students to familiarize with moral and\npolitical issues in the data science environment. Third, we operationalize our\napproach by stressing that (proto-)virtue acquisition (like skill acquisition)\noccurs through the technical and social tasks of daily data science activities,\nwhere these repetitive tasks provide the opportunities to develop\n(proto-)virtue capacity and to support the development of ethically robust data\nsystems. Finally, we discuss a concrete example of how this approach has been\nimplemented. In particular, we describe how this method is applied to teach\ndata ethics to students participating in the CODATA-RDA Data Science Summer\nSchools.\n",
        "title": "Character comes from practice: longitudinal practice-based ethics\n  training in data science",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04456",
        "abstract_url": "http://arxiv.org/abs/2401.04456",
        "authors": [
            {
                "last_name": "Di Pietro",
                "first_name": "Daniele A."
            },
            {
                "last_name": "Droniou",
                "first_name": "Jerome"
            },
            {
                "last_name": "Qian",
                "first_name": "Jia Jia"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this work we design and analyse a Discrete de Rham (DDR) method for the\nincompressible Navier-Stokes equations. Our focus is, more specifically, on the\nSDDR variant, where a reduction in the number of unknowns is obtained using\nserendipity techniques. The main features of the DDR approach are the support\nof general meshes and arbitrary approximation orders. The method we develop is\nbased on the curl-curl formulation of the momentum equation and, through\ncompatibility with the Helmholtz-Hodge decomposition, delivers pressure-robust\nerror estimates for the velocity. It also enables non-standard boundary\nconditions, such as imposing the value of the pressure on the boundary.\nIn-depth numerical validation on a complete panel of tests including general\npolyhedral meshes is provided. The paper also contains an appendix where bounds\non DDR potential reconstructions and differential operators are proved in the\nmore general framework of Polytopal Exterior Calculus.\n",
        "title": "A pressure-robust Discrete de Rham scheme for the Navier-Stokes\n  equations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04461",
        "abstract_url": "http://arxiv.org/abs/2401.04461",
        "authors": [
            {
                "last_name": "Klein",
                "first_name": "C."
            },
            {
                "last_name": "Stoilov",
                "first_name": "N."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose a method to numerically compute fractional derivatives (or the\nfractional Laplacian) on the whole real line via Riesz fractional integrals.\nThe compactified real line is divided into a number of intervals, thus\namounting to a multi-domain approach; after transformations in accordance with\nthe underlying $Z_{q}$ curve ensuring analyticity of the respective integrands,\nthe integrals over the different domains are computed with a Clenshaw-Curtis\nalgorithm. As an example, we consider solitary waves for fractional Korteweg-de\nVries equations and compare these to results obtained with a discrete Fourier\ntransform.\n",
        "title": "Multi-domain spectral approach to rational-order fractional derivatives",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04463",
        "abstract_url": "http://arxiv.org/abs/2401.04463",
        "authors": [
            {
                "last_name": "Tebbe",
                "first_name": "Justin"
            },
            {
                "last_name": "Tayyub",
                "first_name": "Jawad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diffusion models have found valuable applications in anomaly detection by\ncapturing the nominal data distribution and identifying anomalies via\nreconstruction. Despite their merits, they struggle to localize anomalies of\nvarying scales, especially larger anomalies like entire missing components.\nAddressing this, we present a novel framework that enhances the capability of\ndiffusion models, by extending the previous introduced implicit conditioning\napproach Meng et al. (2022) in three significant ways. First, we incorporate a\ndynamic step size computation that allows for variable noising steps in the\nforward process guided by an initial anomaly prediction. Second, we demonstrate\nthat denoising an only scaled input, without any added noise, outperforms\nconventional denoising process. Third, we project images in a latent space to\nabstract away from fine details that interfere with reconstruction of large\nmissing components. Additionally, we propose a fine-tuning mechanism that\nfacilitates the model to effectively grasp the nuances of the target domain.\nOur method undergoes rigorous evaluation on two prominent anomaly detection\ndatasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our\nframework effectively localizes anomalies regardless of their scale, marking a\npivotal advancement in diffusion-based anomaly detection.\n",
        "title": "D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly\n  Detection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04464",
        "abstract_url": "http://arxiv.org/abs/2401.04464",
        "authors": [
            {
                "last_name": "Fibaek",
                "first_name": "Casper"
            },
            {
                "last_name": "Camilleri",
                "first_name": "Luke"
            },
            {
                "last_name": "Luyts",
                "first_name": "Andreas"
            },
            {
                "last_name": "Dionelis",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "Saux",
                "first_name": "Bertrand Le"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Massive amounts of unlabelled data are captured by Earth Observation (EO)\nsatellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.\nThis makes Remote Sensing a data-rich domain well suited to Machine Learning\n(ML) solutions. However, a bottleneck in applying ML models to EO is the lack\nof annotated data as annotation is a labour-intensive and costly process. As a\nresult, research in this domain has focused on Self-Supervised Learning and\nFoundation Model approaches. This paper addresses the need to evaluate\ndifferent Foundation Models on a fair and uniform benchmark by introducing the\nPhilEO Bench, a novel evaluation framework for EO Foundation Models. The\nframework comprises of a testbed and a novel 400 GB Sentinel-2 dataset\ncontaining labels for three downstream tasks, building density estimation, road\nsegmentation, and land cover classification. We present experiments using our\nframework evaluating different Foundation Models, including Prithvi and SatMAE,\nat multiple n-shots and convergence rates.\n",
        "title": "PhilEO Bench: Evaluating Geo-Spatial Foundation Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04468",
        "abstract_url": "http://arxiv.org/abs/2401.04468",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weimin"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhijie"
            },
            {
                "last_name": "Yan",
                "first_name": "Jiangqiao"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuo"
            },
            {
                "last_name": "Low",
                "first_name": "Chetwin"
            },
            {
                "last_name": "Hoang",
                "first_name": "Tuyen"
            },
            {
                "last_name": "Wu",
                "first_name": "Jie"
            },
            {
                "last_name": "Liew",
                "first_name": "Jun Hao"
            },
            {
                "last_name": "Yan",
                "first_name": "Hanshu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Daquan"
            },
            {
                "last_name": "Feng",
                "first_name": "Jiashi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The growing demand for high-fidelity video generation from textual\ndescriptions has catalyzed significant research in this field. In this work, we\nintroduce MagicVideo-V2 that integrates the text-to-image model, video motion\ngenerator, reference image embedding module and frame interpolation module into\nan end-to-end video generation pipeline. Benefiting from these architecture\ndesigns, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution\nvideo with remarkable fidelity and smoothness. It demonstrates superior\nperformance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,\nMoon Valley and Stable Video Diffusion model via user evaluation at large\nscale.\n",
        "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04471",
        "abstract_url": "http://arxiv.org/abs/2401.04471",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xue"
            },
            {
                "last_name": "Shi",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Lou",
                "first_name": "Xinyue"
            },
            {
                "last_name": "Qi",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Yufeng"
            },
            {
                "last_name": "Xu",
                "first_name": "Jinan"
            },
            {
                "last_name": "Han",
                "first_name": "Wenjuan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) and multimodal large language models (MLLMs)\nhave shown excellent general capabilities, even exhibiting adaptability in many\nprofessional domains such as law, economics, transportation, and medicine.\nCurrently, many domain-specific benchmarks have been proposed to verify the\nperformance of (M)LLMs in specific fields. Among various domains,\ntransportation plays a crucial role in modern society as it impacts the\neconomy, the environment, and the quality of life for billions of people.\nHowever, it is unclear how much traffic knowledge (M)LLMs possess and whether\nthey can reliably perform transportation-related tasks. To address this gap, we\npropose TransportationGames, a carefully designed and thorough evaluation\nbenchmark for assessing (M)LLMs in the transportation domain. By\ncomprehensively considering the applications in real-world scenarios and\nreferring to the first three levels in Bloom's Taxonomy, we test the\nperformance of various (M)LLMs in memorizing, understanding, and applying\ntransportation knowledge by the selected tasks. The experimental results show\nthat although some models perform well in some tasks, there is still much room\nfor improvement overall. We hope the release of TransportationGames can serve\nas a foundation for future research, thereby accelerating the implementation\nand application of (M)LLMs in the transportation domain.\n",
        "title": "TransportationGames: Benchmarking Transportation Knowledge of\n  (Multimodal) Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04472",
        "abstract_url": "http://arxiv.org/abs/2401.04472",
        "authors": [
            {
                "last_name": "Woisetschl\u00e4ger",
                "first_name": "Herbert"
            },
            {
                "last_name": "Isenko",
                "first_name": "Alexander"
            },
            {
                "last_name": "Wang",
                "first_name": "Shiqiang"
            },
            {
                "last_name": "Mayer",
                "first_name": "Ruben"
            },
            {
                "last_name": "Jacobsen",
                "first_name": "Hans-Arno"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DC",
            "",
            ""
        ],
        "abstract": "  Federated Learning (FL) has become an established technique to facilitate\nprivacy-preserving collaborative training. However, new approaches to FL often\ndiscuss their contributions involving small deep-learning models only. With the\ntremendous success of transformer models, the following question arises: What\nis necessary to operationalize foundation models in an FL application? Knowing\nthat computation and communication often take up similar amounts of time in FL,\nwe introduce a novel taxonomy focused on computational and communication\nefficiency methods in FL applications. This said, these methods aim to optimize\nthe training time and reduce communication between clients and the server. We\nalso look at the current state of widely used FL frameworks and discuss future\nresearch potentials based on existing approaches in FL research and beyond.\n",
        "title": "A Survey on Efficient Federated Learning Methods for Foundation Model\n  Training",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04474",
        "abstract_url": "http://arxiv.org/abs/2401.04474",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Ngoc Luyen"
            },
            {
                "last_name": "Abel",
                "first_name": "Marie-H\u00e9l\u00e8ne"
            },
            {
                "last_name": "Gouspillou",
                "first_name": "Philippe"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            ""
        ],
        "abstract": "  In today's data-rich environment, recommender systems play a crucial role in\ndecision support systems. They provide to users personalized recommendations\nand explanations about these recommendations. Embedding-based models, despite\ntheir widespread use, often suffer from a lack of interpretability, which can\nundermine trust and user engagement. This paper presents an approach that\ncombines embedding-based and semantic-based models to generate post-hoc\nexplanations in recommender systems, leveraging ontology-based knowledge graphs\nto improve interpretability and explainability. By organizing data within a\nstructured framework, ontologies enable the modeling of intricate relationships\nbetween entities, which is essential for generating explanations. By combining\nembedding-based and semantic based models for post-hoc explanations in\nrecommender systems, the framework we defined aims at producing meaningful and\neasy-to-understand explanations, enhancing user trust and satisfaction, and\npotentially promoting the adoption of recommender systems across the e-commerce\nsector.\n",
        "title": "Combining Embedding-Based and Semantic-Based Models for Post-hoc\n  Explanations in Recommender Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04478",
        "abstract_url": "http://arxiv.org/abs/2401.04478",
        "authors": [
            {
                "last_name": "Schuh",
                "first_name": "Maximilian G."
            },
            {
                "last_name": "Boldini",
                "first_name": "Davide"
            },
            {
                "last_name": "Sieber",
                "first_name": "Stephan A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  The success of drug discovery and development relies on the precise\nprediction of molecular activities and properties. While in silico molecular\nproperty prediction has shown remarkable potential, its use has been limited so\nfar to assays for which large amounts of data are available. In this study, we\nuse a fine-tuned large language model to integrate biological assays based on\ntheir textual information, coupled with Barlow Twins, a Siamese neural network\nusing a novel self-supervised learning approach. This architecture uses both\nassay information and molecular fingerprints to extract the true molecular\ninformation. TwinBooster enables the prediction of properties of unseen\nbioassays and molecules by providing state-of-the-art zero-shot learning tasks.\nRemarkably, our artificial intelligence pipeline shows excellent performance on\nthe FS-Mol benchmark. This breakthrough demonstrates the application of deep\nlearning to critical property prediction tasks where data is typically scarce.\nBy accelerating the early identification of active molecules in drug discovery\nand development, this method has the potential to help streamline the\nidentification of novel therapeutics.\n",
        "title": "TwinBooster: Synergising Large Language Models with Barlow Twins and\n  Gradient Boosting for Enhanced Molecular Property Prediction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04481",
        "abstract_url": "http://arxiv.org/abs/2401.04481",
        "authors": [
            {
                "last_name": "Satapara",
                "first_name": "Shrey"
            },
            {
                "last_name": "Mehta",
                "first_name": "Parth"
            },
            {
                "last_name": "Ganguly",
                "first_name": "Debasis"
            },
            {
                "last_name": "Modha",
                "first_name": "Sandip"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The recent success in language generation capabilities of large language\nmodels (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns\nabout their possible misuse in inducing mass agitation and communal hatred via\ngenerating fake news and spreading misinformation. Traditional means of\ndeveloping a misinformation ground-truth dataset does not scale well because of\nthe extensive manual effort required to annotate the data. In this paper, we\npropose an LLM-based approach of creating silver-standard ground-truth datasets\nfor identifying misinformation. Specifically speaking, given a trusted news\narticle, our proposed approach involves prompting LLMs to automatically\ngenerate a summarised version of the original article. The prompts in our\nproposed approach act as a controlling mechanism to generate specific types of\nfactual incorrectness in the generated summaries, e.g., incorrect quantities,\nfalse attributions etc. To investigate the usefulness of this dataset, we\nconduct a set of experiments where we train a range of supervised models for\nthe task of misinformation detection.\n",
        "title": "Fighting Fire with Fire: Adversarial Prompting to Generate a\n  Misinformation Detection Dataset",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04482",
        "abstract_url": "http://arxiv.org/abs/2401.04482",
        "authors": [
            {
                "last_name": "Huber",
                "first_name": "Christian"
            },
            {
                "last_name": "Waibel",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities and\ndomain-specific special words for which little or no data is available. To\naddress the problem of recognizing these words, we propose an self-supervised\ncontinual learning approach. Given the audio of a lecture talk with\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from previous work. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation dataset. Continual learning is then performed on this\nset by adapting low-rank matrix weights added to each weight matrix of the\nmodel. The whole procedure is iterated for many talks. We show that with this\napproach, we obtain increasing performance on the new words when they occur\nmore frequently (more than 80% recall) while preserving the general performance\nof the model.\n",
        "title": "Continuously Learning New Words in Automatic Speech Recognition",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04485",
        "abstract_url": "http://arxiv.org/abs/2401.04485",
        "authors": [
            {
                "last_name": "Alzaben",
                "first_name": "Linda"
            },
            {
                "last_name": "Boffi",
                "first_name": "Daniele"
            },
            {
                "last_name": "Dedner",
                "first_name": "Andreas"
            },
            {
                "last_name": "Gastaldi",
                "first_name": "Lucia"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  In this paper we introduce an abstract setting for the convergence analysis\nof the virtual element approximation of an acoustic vibration problem. We\ndiscuss the effect of the stabilization parameters and remark that in some\ncases it is possible to achieve optimal convergence without the need of any\nstabilization. This statement is rigorously proved for lowest order triangular\nelement and supported by several numerical experiments.\n",
        "title": "On the stabilization of a virtual element method for an acoustic\n  vibration problem",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04486",
        "abstract_url": "http://arxiv.org/abs/2401.04486",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Yufei"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuanpei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The Spiking Neural Network (SNN) is a biologically inspired neural network\ninfrastructure that has recently garnered significant attention. It utilizes\nbinary spike activations to transmit information, thereby replacing\nmultiplications with additions and resulting in high energy efficiency.\nHowever, training an SNN directly poses a challenge due to the undefined\ngradient of the firing spike process. Although prior works have employed\nvarious surrogate gradient training methods that use an alternative function to\nreplace the firing process during back-propagation, these approaches ignore an\nintrinsic problem: gradient vanishing. To address this issue, we propose a\nshortcut back-propagation method in our paper, which advocates for transmitting\nthe gradient directly from the loss to the shallow layers. This enables us to\npresent the gradient to the shallow layers directly, thereby significantly\nmitigating the gradient vanishing problem. Additionally, this method does not\nintroduce any burden during the inference phase. To strike a balance between\nfinal accuracy and ease of training, we also propose an evolutionary training\nframework and implement it by inducing a balance coefficient that dynamically\nchanges with the training epoch, which further improves the network's\nperformance. Extensive experiments conducted over static and dynamic datasets\nusing several popular network structures reveal that our method consistently\noutperforms state-of-the-art methods.\n",
        "title": "Take A Shortcut Back: Mitigating the Gradient Vanishing for Training\n  Spiking Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04487",
        "abstract_url": "http://arxiv.org/abs/2401.04487",
        "authors": [
            {
                "last_name": "Nonhoff",
                "first_name": "Marko"
            },
            {
                "last_name": "Dall'Anese",
                "first_name": "Emiliano"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Matthias A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This article investigates the problem of controlling linear time-invariant\nsystems subject to time-varying and a priori unknown cost functions, state and\ninput constraints, and exogenous disturbances. We combine the online convex\noptimization framework with tools from robust model predictive control to\npropose an algorithm that is able to guarantee robust constraint satisfaction.\nThe performance of the closed loop emerging from application of our framework\nis studied in terms of its dynamic regret, which is proven to be bounded\nlinearly by the variation of the cost functions and the magnitude of the\ndisturbances. We corroborate our theoretical findings and illustrate\nimplementational aspects of the proposed algorithm by a numerical case study of\na tracking control problem of an autonomous vehicle.\n",
        "title": "Online convex optimization for robust control of constrained dynamical\n  systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04489",
        "abstract_url": "http://arxiv.org/abs/2401.04489",
        "authors": [
            {
                "last_name": "Huisman",
                "first_name": "Tim"
            },
            {
                "last_name": "van der Linden",
                "first_name": "Jacobus G. M."
            },
            {
                "last_name": "Demirovi\u0107",
                "first_name": "Emir"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DS"
        ],
        "abstract": "  Survival analysis studies and predicts the time of death, or other singular\nunrepeated events, based on historical data, while the true time of death for\nsome instances is unknown. Survival trees enable the discovery of complex\nnonlinear relations in a compact human comprehensible model, by recursively\nsplitting the population and predicting a distinct survival distribution in\neach leaf node. We use dynamic programming to provide the first survival tree\nmethod with optimality guarantees, enabling the assessment of the optimality\ngap of heuristics. We improve the scalability of our method through a special\nalgorithm for computing trees up to depth two. The experiments show that our\nmethod's run time even outperforms some heuristics for realistic cases while\nobtaining similar out-of-sample performance with the state-of-the-art.\n",
        "title": "Optimal Survival Trees: A Dynamic Programming Approach",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04491",
        "abstract_url": "http://arxiv.org/abs/2401.04491",
        "authors": [
            {
                "last_name": "Gonzalez",
                "first_name": "Hector A."
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Kelber",
                "first_name": "Florian"
            },
            {
                "last_name": "Nazeer",
                "first_name": "Khaleelulla Khan"
            },
            {
                "last_name": "Langer",
                "first_name": "Tim"
            },
            {
                "last_name": "Liu",
                "first_name": "Chen"
            },
            {
                "last_name": "Lohrmann",
                "first_name": "Matthias"
            },
            {
                "last_name": "Rostami",
                "first_name": "Amirhossein"
            },
            {
                "last_name": "Sch\u00f6ne",
                "first_name": "Mark"
            },
            {
                "last_name": "Vogginger",
                "first_name": "Bernhard"
            },
            {
                "last_name": "Wunderlich",
                "first_name": "Timo C."
            },
            {
                "last_name": "Yan",
                "first_name": "Yexin"
            },
            {
                "last_name": "Akl",
                "first_name": "Mahmoud"
            },
            {
                "last_name": "Mayr",
                "first_name": "Christian"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG",
            "NE"
        ],
        "abstract": "  The joint progress of artificial neural networks (ANNs) and domain specific\nhardware accelerators such as GPUs and TPUs took over many domains of machine\nlearning research. This development is accompanied by a rapid growth of the\nrequired computational demands for larger models and more data. Concurrently,\nemerging properties of foundation models such as in-context learning drive new\nopportunities for machine learning applications. However, the computational\ncost of such applications is a limiting factor of the technology in data\ncenters, and more importantly in mobile devices and edge systems. To mediate\nthe energy footprint and non-trivial latency of contemporary systems,\nneuromorphic computing systems deeply integrate computational principles of\nneurobiological systems by leveraging low-power analog and digital\ntechnologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable\nmachine learning. The event-based and asynchronous design of SpiNNaker2 allows\nthe composition of large-scale systems involving thousands of chips. This work\nfeatures the operating principles of SpiNNaker2 systems, outlining the\nprototype of novel machine learning applications. These applications range from\nANNs over bio-inspired spiking neural networks to generalized event-based\nneural networks. With the successful development and deployment of SpiNNaker2,\nwe aim to facilitate the advancement of event-based and asynchronous algorithms\nfor future generations of machine learning systems.\n",
        "title": "SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and\n  Asynchronous Machine Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04492",
        "abstract_url": "http://arxiv.org/abs/2401.04492",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Junling"
            },
            {
                "last_name": "Pecorella",
                "first_name": "Matteo"
            },
            {
                "last_name": "Iovene",
                "first_name": "Elisa"
            },
            {
                "last_name": "Palumbo",
                "first_name": "Maria Chiara"
            },
            {
                "last_name": "Rota",
                "first_name": "Alberto"
            },
            {
                "last_name": "Redaelli",
                "first_name": "Alberto"
            },
            {
                "last_name": "Ferrigno",
                "first_name": "Giancarlo"
            },
            {
                "last_name": "De Momi",
                "first_name": "Elena"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  During Percutaneous Nephrolithotomy (PCNL) operations, the surgeon is\nrequired to define the incision point on the patient's back, align the needle\nto a pre-planned path, and perform puncture operations afterward. The procedure\nis currently performed manually using ultrasound or fluoroscopy imaging for\nneedle orientation, which, however, implies limited accuracy and low\nreproducibility. This work incorporates Augmented Reality (AR) visualization\nwith an optical see-through head-mounted display (OST-HMD) and Human-Robot\nCollaboration (HRC) framework to empower the surgeon's task completion\nperformance. In detail, Eye-to-Hand calibration, system registration, and\nhologram model registration are performed to realize visual guidance. A\nCartesian impedance controller is used to guide the operator during the needle\npuncture task execution. Experiments are conducted to verify the system\nperformance compared with conventional manual puncture procedures and a 2D\nmonitor-based visualisation interface. The results showed that the proposed\nframework achieves the lowest median and standard deviation error across all\nthe experimental groups, respectively. Furthermore, the NASA-TLX user\nevaluation results indicate that the proposed framework requires the lowest\nworkload score for task completion compared to other experimental setups. The\nproposed framework exhibits significant potential for clinical application in\nthe PCNL task, as it enhances the surgeon's perception capability, facilitates\ncollision-free needle insertion path planning, and minimises errors in task\ncompletion.\n",
        "title": "Augmented Reality and Human-Robot Collaboration Framework for\n  Percutaneous Nephrolithotomy",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04494",
        "abstract_url": "http://arxiv.org/abs/2401.04494",
        "authors": [
            {
                "last_name": "Fernandesa",
                "first_name": "Jo\u00e3o B."
            },
            {
                "last_name": "de Assis",
                "first_name": "\u00cdtalo A. S."
            },
            {
                "last_name": "Martins",
                "first_name": "Idalmis M. S."
            },
            {
                "last_name": "Barros",
                "first_name": "Tiago"
            },
            {
                "last_name": "Xavier-de-Souza",
                "first_name": "Samuel"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Supercomputers have revolutionized how industries and scientific fields\nprocess large amounts of data. These machines group hundreds or thousands of\ncomputing nodes working together to execute time-consuming programs that\nrequire a large amount of computational resources. Over the years,\nsupercomputers have expanded to include new and different technologies\ncharacterizing them as heterogeneous. However, executing a program in a\nheterogeneous environment requires attention to a specific aspect of\nperformance degradation: load imbalance. In this research, we address the\nchallenges associated with load imbalance when scheduling many homogeneous\ntasks in a heterogeneous environment. To address this issue, we introduce the\nconcept of adaptive asynchronous work-stealing. This approach collects\ninformation about the nodes and utilizes it to improve work-stealing aspects,\nsuch as victim selection and task offloading. Additionally, the proposed\napproach eliminates the need for extra threads to communicate information,\nthereby reducing overhead when implementing a fully asynchronous approach. Our\nexperimental results demonstrate a performance improvement of approximately\n10.1\\% compared to other conventional and state-of-the-art implementations.\n",
        "title": "Adaptive Asynchronous Work-Stealing for distributed load-balancing in\n  heterogeneous systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04495",
        "abstract_url": "http://arxiv.org/abs/2401.04495",
        "authors": [
            {
                "last_name": "Calderini",
                "first_name": "Marco"
            },
            {
                "last_name": "Civino",
                "first_name": "Roberto"
            },
            {
                "last_name": "Invernizzi",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "IT",
            "",
            ""
        ],
        "abstract": "  The use of alternative operations in differential cryptanalysis, or\nalternative notions of differentials, are lately receiving increasing\nattention. Recently, Civino et al. managed to design a block cipher which is\nsecure w.r.t. classical differential cryptanalysis performed using\nXOR-differentials, but weaker with respect to the attack based on an\nalternative difference operation acting on the first s-box of the block. We\nextend this result to parallel alternative operations, i.e. acting on each\ns-box of the block. First, we recall the mathematical framework needed to\ndefine and use such operations. After that, we perform some differential\nexperiments against a toy cipher and compare the effectiveness of the attack\nw.r.t. the one that uses XOR-differentials.\n",
        "title": "Differential experiments using parallel alternative operations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04507",
        "abstract_url": "http://arxiv.org/abs/2401.04507",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Chang",
                "first_name": "Yuying"
            },
            {
                "last_name": "Li",
                "first_name": "Zhong"
            },
            {
                "last_name": "An",
                "first_name": "Ning"
            },
            {
                "last_name": "Ma",
                "first_name": "Qi"
            },
            {
                "last_name": "Hei",
                "first_name": "Lei"
            },
            {
                "last_name": "Luo",
                "first_name": "Haibo"
            },
            {
                "last_name": "Lu",
                "first_name": "Yifei"
            },
            {
                "last_name": "Ren",
                "first_name": "Feiliang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models have exhibited robust performance across diverse\nnatural language processing tasks. This report introduces TechGPT-2.0, a\nproject designed to enhance the capabilities of large language models\nspecifically in knowledge graph construction tasks, including named entity\nrecognition (NER) and relationship triple extraction (RTE) tasks in NLP\napplications. Additionally, it serves as a LLM accessible for research within\nthe Chinese open-source model community. We offer two 7B large language model\nweights and a QLoRA weight specialized for processing lengthy texts.Notably,\nTechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all\nfunctionalities from TechGPT-1.0, it exhibits robust text processing\ncapabilities, particularly in the domains of medicine and law. Furthermore, we\nintroduce new capabilities to the model, enabling it to process texts in\nvarious domains such as geographical areas, transportation, organizations,\nliterary works, biology, natural sciences, astronomical objects, and\narchitecture. These enhancements also fortified the model's adeptness in\nhandling hallucinations, unanswerable queries, and lengthy texts. This report\nprovides a comprehensive and detailed introduction to the full fine-tuning\nprocess on Huawei's Ascend servers, encompassing experiences in Ascend server\ndebugging, instruction fine-tuning data processing, and model training. Our\ncode is available at https://github.com/neukg/TechGPT-2.0\n",
        "title": "TechGPT-2.0: A large language model project to solve the task of\n  knowledge graph construction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04508",
        "abstract_url": "http://arxiv.org/abs/2401.04508",
        "authors": [
            {
                "last_name": "Schulze",
                "first_name": "Jan C."
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We use Koopman theory for data-driven model reduction of nonlinear dynamical\nsystems with controls. We propose generic model structures combining\ndelay-coordinate encoding of measurements and full-state decoding to integrate\nreduced Koopman modeling and state estimation. We present a deep-learning\napproach to train the proposed models. A case study demonstrates that our\napproach provides accurate control models and enables real-time capable\nnonlinear model predictive control of a high-purity cryogenic distillation\ncolumn.\n",
        "title": "Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated\n  Control Form and NMPC Case Study",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04509",
        "abstract_url": "http://arxiv.org/abs/2401.04509",
        "authors": [
            {
                "last_name": "Inenaga",
                "first_name": "Shunsuke"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "FL"
        ],
        "abstract": "  The linear-size suffix tries (LSTries) [Crochemore et al., TCS 2016] are a\nversion of suffix trees in which the edge labels are single characters, yet are\nable to perform pattern matching queries in optimal time. Instead of explicitly\nstoring the input text, LSTries have some extra non-branching internal nodes\ncalled type-2 nodes. The extended techniques are then used in the linear-size\ncompact directed acyclic word graphs (LCDAWGs) [Takagi et al. SPIRE 2017],\nwhich can be stored with $O(el(T)+er(T))$ space (i.e. without the text), where\n$el(T)$ and $er(T)$ are the numbers of left- and right-extensions of the\nmaximal repeats in the input text string $T$, respectively. In this paper, we\npresent simpler alternatives to the aforementioned indexing structures, called\nthe simplified LSTries (simLSTries) and the simplified LCDAWGs (simLCDAWGs), in\nwhich most of the type-2 nodes are removed. In particular, our simLCDAWGs\nrequire only $O(er(T))$ space and work on a weaker model of computation (i.e.\nthe pointer machine). This contrasts the $O(er(T))$-space CDAWG representation\nof [Belazzougui \\& Cunial, SPIRE 2017], which works on the word RAM.\n",
        "title": "Linear-size Suffix Tries and Linear-size CDAWGs Simplified and Improved",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04511",
        "abstract_url": "http://arxiv.org/abs/2401.04511",
        "authors": [
            {
                "last_name": "Dutta",
                "first_name": "Soumya"
            },
            {
                "last_name": "Ganapathy",
                "first_name": "Sriram"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "SD"
        ],
        "abstract": "  The problem of audio-to-audio (A2A) style transfer involves replacing the\nstyle features of the source audio with those from the target audio while\npreserving the content related attributes of the source audio. In this paper,\nwe propose an efficient approach, termed as Zero-shot Emotion Style Transfer\n(ZEST), that allows the transfer of emotional content present in the given\nsource audio with the one embedded in the target audio while retaining the\nspeaker and speech content from the source. The proposed system builds upon\ndecomposing speech into semantic tokens, speaker representations and emotion\nembeddings. Using these factors, we propose a framework to reconstruct the\npitch contour of the given speech signal and train a decoder that reconstructs\nthe speech signal. The model is trained using a self-supervision based\nreconstruction loss. During conversion, the emotion embedding is alone derived\nfrom the target audio, while rest of the factors are derived from the source\naudio. In our experiments, we show that, even without using parallel training\ndata or labels from the source or target audio, we illustrate zero shot emotion\ntransfer capabilities of the proposed ZEST model using objective and subjective\nquality evaluations.\n",
        "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04514",
        "abstract_url": "http://arxiv.org/abs/2401.04514",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Haochen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xin"
            },
            {
                "last_name": "Shen",
                "first_name": "Zhiqi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "IR",
            "LG"
        ],
        "abstract": "  In code search, the Generation-Augmented Retrieval (GAR) framework, which\ngenerates exemplar code snippets to augment queries, has emerged as a promising\nstrategy to address the principal challenge of modality misalignment between\ncode snippets and natural language queries, particularly with the demonstrated\ncode generation capabilities of Large Language Models (LLMs). Nevertheless, our\npreliminary investigations indicate that the improvements conferred by such an\nLLM-augmented framework are somewhat constrained. This limitation could\npotentially be ascribed to the fact that the generated codes, albeit\nfunctionally accurate, frequently display a pronounced stylistic deviation from\nthe ground truth code in the codebase. In this paper, we extend the\nfoundational GAR framework and propose a simple yet effective method that\nadditionally Rewrites the Code (ReCo) within the codebase for style\nnormalization. Experimental results demonstrate that ReCo significantly boosts\nretrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),\nand fine-tuned dense (up to 23.6%) retrieval settings in diverse search\nscenarios. To further elucidate the advantages of ReCo and stimulate research\nin code style normalization, we introduce Code Style Similarity, the first\nmetric tailored to quantify stylistic similarities in code. Notably, our\nempirical findings reveal the inadequacy of existing metrics in capturing\nstylistic nuances.\n",
        "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented\n  Code Search",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04515",
        "abstract_url": "http://arxiv.org/abs/2401.04515",
        "authors": [
            {
                "last_name": "Tikhomirov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Loukachevitch",
                "first_name": "Natalia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This article investigates a zero-shot approach to hypernymy prediction using\nlarge language models (LLMs). The study employs a method based on text\nprobability calculation, applying it to various generated prompts. The\nexperiments demonstrate a strong correlation between the effectiveness of\nlanguage model prompts and classic patterns, indicating that preliminary prompt\nselection can be carried out using smaller models before moving to larger ones.\nWe also explore prompts for predicting co-hyponyms and improving hypernymy\npredictions by augmenting prompts with additional information through\nautomatically identified co-hyponyms. An iterative approach is developed for\npredicting higher-level concepts, which further improves the quality on the\nBLESS dataset (MAP = 0.8).\n",
        "title": "Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with\n  Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04518",
        "abstract_url": "http://arxiv.org/abs/2401.04518",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Shichao"
            },
            {
                "last_name": "Li",
                "first_name": "Junlong"
            },
            {
                "last_name": "Yuan",
                "first_name": "Weizhe"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ruifeng"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Liu",
                "first_name": "Pengfei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Critique, as a natural language description for assessing the quality of\nmodel-generated content, has been proven to play an essential role in the\ntraining, evaluation, and refinement of Large Language Models (LLMs). However,\nthere is a lack of principled understanding in evaluating the quality of the\ncritique itself. In this paper, we pioneer the critique of critique, termed\nMetaCritique, which is a framework to evaluate the critique from two aspects,\ni.e., factuality as precision score and comprehensiveness as recall score. We\ncalculate the harmonic mean of precision and recall as the overall rating\ncalled F1 score. To obtain a reliable evaluation outcome, we propose Atomic\nInformation Units (AIUs), which describe the critique in a more fine-grained\nmanner. MetaCritique takes each AIU into account and aggregates each AIU's\njudgment for the overall score. Moreover, given the evaluation process involves\nintricate reasoning, our MetaCritique provides a natural language rationale to\nsupport each judgment. We construct a meta-evaluation dataset containing 300\ncritiques (2653 AIUs) across four tasks (question answering, reasoning,\nentailment, and summarization), and we conduct a comparative study to\ndemonstrate the feasibility and effectiveness. Experiments also show superior\ncritique judged by MetaCritique leads to better refinement, indicating\ngenerative artificial intelligence indeed has the potential to be significantly\nadvanced with our MetaCritique. We will release relevant code and\nmeta-evaluation datasets at https://github.com/GAIR-NLP/MetaCritique.\n",
        "title": "The Critique of Critique",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04519",
        "abstract_url": "http://arxiv.org/abs/2401.04519",
        "authors": [
            {
                "last_name": "Gallistl",
                "first_name": "Dietmar"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  It is shown how mixed finite element methods for symmetric positive definite\neigenvalue problems related to partial differential operators can provide\nguaranteed lower eigenvalue bounds. The method is based on a classical\ncompatibility condition (inclusion of kernels) of the mixed scheme and on local\nconstants related to compact embeddings, which are often known explicitly.\nApplications include scalar second-order elliptic operators, linear elasticity,\nand the Steklov eigenvalue problem.\n",
        "title": "Mixed methods and lower eigenvalue bounds",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04522",
        "abstract_url": "http://arxiv.org/abs/2401.04522",
        "authors": [
            {
                "last_name": "Saidov",
                "first_name": "Marat"
            },
            {
                "last_name": "Bakalova",
                "first_name": "Aleksandra"
            },
            {
                "last_name": "Taktasheva",
                "first_name": "Ekaterina"
            },
            {
                "last_name": "Mikhailov",
                "first_name": "Vladislav"
            },
            {
                "last_name": "Artemova",
                "first_name": "Ekaterina"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The evaluation of Natural Language Generation (NLG) models has gained\nincreased attention, urging the development of metrics that evaluate various\naspects of generated text. LUNA addresses this challenge by introducing a\nunified interface for 20 NLG evaluation metrics. These metrics are categorized\nbased on their reference-dependence and the type of text representation they\nemploy, from string-based n-gram overlap to the utilization of static\nembeddings and pre-trained language models.\n  The straightforward design of LUNA allows for easy extension with novel\nmetrics, requiring just a few lines of code. LUNA offers a user-friendly tool\nfor evaluating generated texts.\n",
        "title": "LUNA: A Framework for Language Understanding and Naturalness Assessment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04524",
        "abstract_url": "http://arxiv.org/abs/2401.04524",
        "authors": [
            {
                "last_name": "Litvinov",
                "first_name": "Oleg"
            },
            {
                "last_name": "Sekuli\u0107",
                "first_name": "Ivan"
            },
            {
                "last_name": "Aliannejadi",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Crestani",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Clarifying user's information needs is an essential component of modern\nsearch systems. While most of the approaches for constructing clarifying\nprompts rely on query facets, the impact of the quality of the facets is\nrelatively unexplored. In this work, we concentrate on facet quality through\nthe notion of facet coherency and assess its importance for overall usefulness\nfor clarification in search. We find that existing evaluation procedures do not\naccount for facet coherency, as evident by the poor correlation of coherency\nwith automated metrics. Moreover, we propose a coherency classifier and assess\nthe prevalence of incoherent facets in a well-established dataset on\nclarification. Our findings can serve as motivation for future work on the\ntopic.\n",
        "title": "Analyzing Coherency in Facet-based Clarification Prompt Generation for\n  Search",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04531",
        "abstract_url": "http://arxiv.org/abs/2401.04531",
        "authors": [
            {
                "last_name": "Fenogenova",
                "first_name": "Alena"
            },
            {
                "last_name": "Chervyakov",
                "first_name": "Artem"
            },
            {
                "last_name": "Martynov",
                "first_name": "Nikita"
            },
            {
                "last_name": "Kozlova",
                "first_name": "Anastasia"
            },
            {
                "last_name": "Tikhonova",
                "first_name": "Maria"
            },
            {
                "last_name": "Akhmetgareeva",
                "first_name": "Albina"
            },
            {
                "last_name": "Emelyanov",
                "first_name": "Anton"
            },
            {
                "last_name": "Shevelev",
                "first_name": "Denis"
            },
            {
                "last_name": "Lebedev",
                "first_name": "Pavel"
            },
            {
                "last_name": "Sinev",
                "first_name": "Leonid"
            },
            {
                "last_name": "Isaeva",
                "first_name": "Ulyana"
            },
            {
                "last_name": "Kolomeytseva",
                "first_name": "Katerina"
            },
            {
                "last_name": "Moskovskiy",
                "first_name": "Daniil"
            },
            {
                "last_name": "Goncharova",
                "first_name": "Elizaveta"
            },
            {
                "last_name": "Savushkin",
                "first_name": "Nikita"
            },
            {
                "last_name": "Mikhailova",
                "first_name": "Polina"
            },
            {
                "last_name": "Dimitrov",
                "first_name": "Denis"
            },
            {
                "last_name": "Panchenko",
                "first_name": "Alexander"
            },
            {
                "last_name": "Markov",
                "first_name": "Sergei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.\n",
        "title": "MERA: A Comprehensive LLM Evaluation in Russian",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04534",
        "abstract_url": "http://arxiv.org/abs/2401.04534",
        "authors": [
            {
                "last_name": "Kaszuba",
                "first_name": "Sara"
            },
            {
                "last_name": "Sabbella",
                "first_name": "Sandeep Reddy"
            },
            {
                "last_name": "Leotta",
                "first_name": "Francesco"
            },
            {
                "last_name": "Serrarens",
                "first_name": "Pascal"
            },
            {
                "last_name": "Nardi",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  In recent years, an increasing number of Human-Robot Interaction (HRI)\napproaches have been implemented and evaluated in Virtual Reality (VR), as it\nallows to speed-up design iterations and makes it safer for the final user to\nevaluate and master the HRI primitives. However, identifying the most suitable\nVR experience is not straightforward. In this work, we evaluate how, in a smart\nagriculture scenario, immersive and non-immersive VR are perceived by users\nwith respect to a speech act understanding task. In particular, we collect\nopinions and suggestions from the 81 participants involved in both experiments\nto highlight the strengths and weaknesses of these different experiences.\n",
        "title": "Testing Human-Robot Interaction in Virtual Reality: Experience from a\n  Study on Speech Act Classification",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04535",
        "abstract_url": "http://arxiv.org/abs/2401.04535",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Zhao"
            },
            {
                "last_name": "Duan",
                "first_name": "Chenguang"
            },
            {
                "last_name": "Jiao",
                "first_name": "Yuling"
            },
            {
                "last_name": "Yang",
                "first_name": "Jerry Zhijian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  We propose SDORE, a semi-supervised deep Sobolev regressor, for the\nnonparametric estimation of the underlying regression function and its\ngradient. SDORE employs deep neural networks to minimize empirical risk with\ngradient norm regularization, allowing computation of the gradient norm on\nunlabeled data. We conduct a comprehensive analysis of the convergence rates of\nSDORE and establish a minimax optimal rate for the regression function.\nCrucially, we also derive a convergence rate for the associated plug-in\ngradient estimator, even in the presence of significant domain shift. These\ntheoretical findings offer valuable prior guidance for selecting regularization\nparameters and determining the size of the neural network, while showcasing the\nprovable advantage of leveraging unlabeled data in semi-supervised learning. To\nthe best of our knowledge, SDORE is the first provable neural network-based\napproach that simultaneously estimates the regression function and its\ngradient, with diverse applications including nonparametric variable selection\nand inverse problems. The effectiveness of SDORE is validated through an\nextensive range of numerical simulations and real data analysis.\n",
        "title": "Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection\n  and Beyond",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04536",
        "abstract_url": "http://arxiv.org/abs/2401.04536",
        "authors": [
            {
                "last_name": "Davidson",
                "first_name": "Tim R."
            },
            {
                "last_name": "Veselovsky",
                "first_name": "Veniamin"
            },
            {
                "last_name": "Josifoski",
                "first_name": "Martin"
            },
            {
                "last_name": "Peyrard",
                "first_name": "Maxime"
            },
            {
                "last_name": "Bosselut",
                "first_name": "Antoine"
            },
            {
                "last_name": "Kosinski",
                "first_name": "Michal"
            },
            {
                "last_name": "West",
                "first_name": "Robert"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Companies, organizations, and governments increasingly exploit Language\nModels' (LM) remarkable capability to display agent-like behavior. As LMs are\nadopted to perform tasks with growing autonomy, there exists an urgent need for\nreliable and scalable evaluation benchmarks. Current, predominantly static LM\nbenchmarks are ill-suited to evaluate such dynamic applications. Thus, we\npropose jointly evaluating LM performance and alignment through the lenses of\nnegotiation games. We argue that this common task better reflects real-world\ndeployment conditions while offering insights into LMs' decision-making\nprocesses. Crucially, negotiation games allow us to study multi-turn, and\ncross-model interactions, modulate complexity, and side-step accidental data\nleakage in evaluation. We report results for six publicly accessible LMs from\nseveral major providers on a variety of negotiation games, evaluating both\nself-play and cross-play performance. Noteworthy findings include: (i)\nopen-source models are currently unable to complete these tasks; (ii)\ncooperative bargaining games prove challenging; and (iii) the most powerful\nmodels do not always \"win\".\n",
        "title": "Evaluating Language Model Agency through Negotiations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04538",
        "abstract_url": "http://arxiv.org/abs/2401.04538",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shaohua"
            },
            {
                "last_name": "Su",
                "first_name": "Zhendong"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "PL",
            "SE"
        ],
        "abstract": "  In this paper, we propose a testing framework for validating sanitizer\nimplementations in compilers. Our core components are (1) a program generator\nspecifically designed for producing programs containing undefined behavior\n(UB), and (2) a novel test oracle for sanitizer testing. The program generator\nemploys Shadow Statement Insertion, a general and effective approach for\nintroducing UB into a valid seed program. The generated UB programs are\nsubsequently utilized for differential testing of multiple sanitizer\nimplementations. Nevertheless, discrepant sanitizer reports may stem from\neither compiler optimization or sanitizer bugs. To accurately determine if a\ndiscrepancy is caused by sanitizer bugs, we introduce a new test oracle called\ncrash-site mapping. We have incorporated our techniques into UBfuzz, a\npractical tool for testing sanitizers. Over a five-month testing period, UBfuzz\nsuccessfully found 31 bugs in both GCC and LLVM sanitizers. These bugs reveal\nthe serious false negative problems in sanitizers, where certain UBs in\nprograms went unreported. This research paves the way for further investigation\nin this crucial area of study.\n",
        "title": "UBfuzz: Finding Bugs in Sanitizer Implementations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04539",
        "abstract_url": "http://arxiv.org/abs/2401.04539",
        "authors": [
            {
                "last_name": "Mei",
                "first_name": "Haoran"
            },
            {
                "last_name": "Peng",
                "first_name": "Limei"
            },
            {
                "last_name": "Ho",
                "first_name": "Pin-Han"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  This article introduces a novel framework of multi-user detection (MUD) for\nK-repetition grant-free non-orthogonal multiple access (K-GF-NOMA), called\n$\\alpha$ iterative interference cancellation diversity slotted aloha\n($\\alpha$-IIC-DSA). The proposed framework targets at a simple yet effective\ndecoding process where the AP can intelligently exploit the correlation among\nsignals received at different resource blocks (RBs) so as to generate required\nmulti-access interference (MAI) for realizing the signal-interference\ncancellation (SIC) based MUD. By keeping all operation and hardware complexity\nat the access point (AP), the proposed framework is applicable to the scenarios\nwith random and uncoordinated access by numerous miniature mMTC devices\n(MTCDs). Numerical experiments are conducted to gain deep understanding on the\nperformance of launching the proposed framework for K-GF-NOMA.\n",
        "title": "A Novel Framework of K-repetition Grant-free Access via Diversity\n  Slotted Aloha (DSA)",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04545",
        "abstract_url": "http://arxiv.org/abs/2401.04545",
        "authors": [
            {
                "last_name": "Sabbella",
                "first_name": "Sandeep Reddy"
            },
            {
                "last_name": "Kaszuba",
                "first_name": "Sara"
            },
            {
                "last_name": "Leotta",
                "first_name": "Francesco"
            },
            {
                "last_name": "Serrarens",
                "first_name": "Pascal"
            },
            {
                "last_name": "Nardi",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  Human-Robot Interaction (HRI) has become increasingly important as robots are\nbeing integrated into various aspects of daily life. One key aspect of HRI is\ngesture recognition, which allows robots to interpret and respond to human\ngestures in real-time. Gesture recognition plays an important role in\nnon-verbal communication in HRI. To this aim, there is ongoing research on how\nsuch non-verbal communication can strengthen verbal communication and improve\nthe system's overall efficiency, thereby enhancing the user experience with the\nrobot. However, several challenges need to be addressed in gesture recognition\nsystems, which include data generation, transferability, scalability,\ngeneralizability, standardization, and lack of benchmarking of the gestural\nsystems. In this preliminary paper, we want to address the challenges of data\ngeneration using virtual reality simulations and standardization issues by\npresenting gestures to some commands that can be used as a standard in ground\nrobots.\n",
        "title": "Evaluating Gesture Recognition in Virtual Reality",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04550",
        "abstract_url": "http://arxiv.org/abs/2401.04550",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shengli"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Lin",
                "first_name": "Sen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Although deep convolutional neural networks have achieved remarkable success\nin removing synthetic fog, it is essential to be able to process images taken\nin complex foggy conditions, such as dense or non-homogeneous fog, in the real\nworld. However, the haze distribution in the real world is complex, and\ndownsampling can lead to color distortion or loss of detail in the output\nresults as the resolution of a feature map or image resolution decreases. In\naddition to the challenges of obtaining sufficient training data, overfitting\ncan also arise in deep learning techniques for foggy image processing, which\ncan limit the generalization abilities of the model, posing challenges for its\npractical applications in real-world scenarios. Considering these issues, this\npaper proposes a Transformer-based wavelet network (WaveletFormerNet) for\nreal-world foggy image recovery. We embed the discrete wavelet transform into\nthe Vision Transformer by proposing the WaveletFormer and IWaveletFormer\nblocks, aiming to alleviate texture detail loss and color distortion in the\nimage due to downsampling. We introduce parallel convolution in the Transformer\nblock, which allows for the capture of multi-frequency information in a\nlightweight mechanism. Additionally, we have implemented a feature aggregation\nmodule (FAM) to maintain image resolution and enhance the feature extraction\ncapacity of our model, further contributing to its impressive performance in\nreal-world foggy image recovery tasks. Extensive experiments demonstrate that\nour WaveletFormerNet performs better than state-of-the-art methods, as shown\nthrough quantitative and qualitative evaluations of minor model complexity.\nAdditionally, our satisfactory results on real-world dust removal and\napplication tests showcase the superior generalization ability and improved\nperformance of WaveletFormerNet in computer vision-related applications.\n",
        "title": "WaveletFormerNet: A Transformer-based Wavelet Network for Real-world\n  Non-homogeneous and Dense Fog Removal",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04552",
        "abstract_url": "http://arxiv.org/abs/2401.04552",
        "authors": [
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            },
            {
                "last_name": "Copik",
                "first_name": "Marcin"
            },
            {
                "last_name": "Beckman",
                "first_name": "Pete"
            },
            {
                "last_name": "Jones",
                "first_name": "Andrew"
            },
            {
                "last_name": "Foster",
                "first_name": "Ian"
            },
            {
                "last_name": "Parashar",
                "first_name": "Manish"
            },
            {
                "last_name": "Reed",
                "first_name": "Daniel"
            },
            {
                "last_name": "Troyer",
                "first_name": "Matthias"
            },
            {
                "last_name": "Schulthess",
                "first_name": "Thomas"
            },
            {
                "last_name": "Ernst",
                "first_name": "Dan"
            },
            {
                "last_name": "Dongarra",
                "first_name": "Jack"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  HPC and Cloud have evolved independently, specializing their innovations into\nperformance or productivity. Acceleration as a Service (XaaS) is a recipe to\nempower both fields with a shared execution platform that provides transparent\naccess to computing resources, regardless of the underlying cloud or HPC\nservice provider. Bridging HPC and cloud advancements, XaaS presents a unified\narchitecture built on performance-portable containers. Our converged model\nconcentrates on low-overhead, high-performance communication and computing,\ntargeting resource-intensive workloads from climate simulations to machine\nlearning. XaaS lifts the restricted allocation model of Function-as-a-Service\n(FaaS), allowing users to benefit from the flexibility and efficient resource\nutilization of serverless while supporting long-running and\nperformance-sensitive workloads from HPC.\n",
        "title": "XaaS: Acceleration as a Service to Enable Productive High-Performance\n  Cloud Computing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04553",
        "abstract_url": "http://arxiv.org/abs/2401.04553",
        "authors": [
            {
                "last_name": "Radhakrishnan",
                "first_name": "Adityanarayanan"
            },
            {
                "last_name": "Belkin",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Drusvyatskiy",
                "first_name": "Dmitriy"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  A fundamental problem in machine learning is to understand how neural\nnetworks make accurate predictions, while seemingly bypassing the curse of\ndimensionality. A possible explanation is that common training algorithms for\nneural networks implicitly perform dimensionality reduction - a process called\nfeature learning. Recent work posited that the effects of feature learning can\nbe elicited from a classical statistical estimator called the average gradient\nouter product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as\nan algorithm that explicitly performs feature learning by alternating between\n(1) reweighting the feature vectors by the AGOP and (2) learning the prediction\nfunction in the transformed space. In this work, we develop the first\ntheoretical guarantees for how RFM performs dimensionality reduction by\nfocusing on the class of overparametrized problems arising in sparse linear\nregression and low-rank matrix recovery. Specifically, we show that RFM\nrestricted to linear models (lin-RFM) generalizes the well-studied Iteratively\nReweighted Least Squares (IRLS) algorithm. Our results shed light on the\nconnection between feature learning in neural networks and classical sparse\nrecovery algorithms. In addition, we provide an implementation of lin-RFM that\nscales to matrices with millions of missing entries. Our implementation is\nfaster than the standard IRLS algorithm as it is SVD-free. It also outperforms\ndeep linear networks for sparse linear regression and low-rank matrix\ncompletion.\n",
        "title": "Linear Recursive Feature Machines provably recover low-rank matrices",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04554",
        "abstract_url": "http://arxiv.org/abs/2401.04554",
        "authors": [
            {
                "last_name": "Goedgebeur",
                "first_name": "Jan"
            },
            {
                "last_name": "Noguchi",
                "first_name": "Kenta"
            },
            {
                "last_name": "Renders",
                "first_name": "Jarne"
            },
            {
                "last_name": "Zamfirescu",
                "first_name": "Carol T."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM"
        ],
        "abstract": "  In a given graph, a HIST is a spanning tree without $2$-valent vertices.\nMotivated by developing a better understanding of HIST-free graphs, i.e. graphs\ncontaining no HIST, in this article's first part we study HIST-critical graphs,\ni.e. HIST-free graphs in which every vertex-deleted subgraph does contain a\nHIST (e.g. a triangle). We give an almost complete characterisation of the\norders for which these graphs exist and present an infinite family of planar\nexamples which are $3$-connected and in which nearly all vertices are\n$4$-valent. This leads naturally to the second part in which we investigate\nplanar $4$-regular graphs with and without HISTs, motivated by a conjecture of\nMalkevitch, which we computationally verify up to order $22$. First we\nenumerate HISTs in antiprisms, whereafter we present planar $4$-regular graphs\nwith and without HISTs, obtained via line graphs.\n",
        "title": "HIST-Critical Graphs and Malkevitch's Conjecture",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04556",
        "abstract_url": "http://arxiv.org/abs/2401.04556",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Qiulin"
            },
            {
                "last_name": "Ishii",
                "first_name": "Hideaki"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper studies novel epidemic spreading problems influenced by opinion\nevolution in social networks, where the opinions reflect the public health\nconcerns. A coupled bilayer network is proposed, where the epidemics spread\nover several communities through a physical network layer while the opinions\nevolve over the same communities through a social network layer. The epidemic\nspreading process is described by a susceptible-infected-vigilant (SIV) model,\nwhich introduces opinion-dependent epidemic vigilance state compared with the\nclassical epidemic models. The opinion process is modeled by a polar opinion\ndynamics model, which includes infection prevalence and human stubbornness into\nthe opinion evolution. By introducing an opinion-dependent reproduction number,\nwe analyze the stability of disease-free and endemic equilibria and derive\nsufficient conditions for their global asymptotic stability. We also discuss\nthe mutual effects between epidemic eradication and opinion consensus, and the\npossibility of suppressing epidemic by intervening in the opinions or\nimplementing public health strategies. Simulations are conducted to verify the\ntheoretical results and demonstrate the feasibility of epidemic suppression.\n",
        "title": "On a Discrete-Time Networked SIV Epidemic Model with Polar Opinion\n  Dynamics",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04558",
        "abstract_url": "http://arxiv.org/abs/2401.04558",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Akama",
                "first_name": "Taketo"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            ""
        ],
        "abstract": "  GANStrument, exploiting GANs with a pitch-invariant feature extractor and\ninstance conditioning technique, has shown remarkable capabilities in\nsynthesizing realistic instrument sounds. To further improve the reconstruction\nability and pitch accuracy to enhance the editability of user-provided sound,\nwe propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to\nmodulate the weights of a pre-trained GANStrument generator, given a one-shot\nsound as input. The hypernetwork modulation provides feedback for the generator\nin the reconstruction of the input sound. In addition, we take advantage of an\nadversarial fine-tuning scheme for the hypernetwork to improve the\nreconstruction fidelity and generation diversity of the generator. Experimental\nresults show that the proposed model not only enhances the generation\ncapability of GANStrument but also significantly improves the editability of\nsynthesized sounds. Audio examples are available at the online demo page.\n",
        "title": "HyperGANStrument: Instrument Sound Synthesis and Editing with\n  Pitch-Invariant Hypernetworks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04560",
        "abstract_url": "http://arxiv.org/abs/2401.04560",
        "authors": [
            {
                "last_name": "Hwang",
                "first_name": "Gyutae"
            },
            {
                "last_name": "Lee",
                "first_name": "Sang Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively.\n",
        "title": "Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04567",
        "abstract_url": "http://arxiv.org/abs/2401.04567",
        "authors": [
            {
                "last_name": "Mariot",
                "first_name": "Luca"
            },
            {
                "last_name": "Leporati",
                "first_name": "Alberto"
            },
            {
                "last_name": "Manzoni",
                "first_name": "Luca"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "CR"
        ],
        "abstract": "  A Particle Swarm Optimizer for the search of balanced Boolean functions with\ngood cryptographic properties is proposed in this paper. The algorithm is a\nmodified version of the permutation PSO by Hu, Eberhart and Shi which preserves\nthe Hamming weight of the particles positions, coupled with the Hill Climbing\nmethod devised by Millan, Clark and Dawson to improve the nonlinearity and\ndeviation from correlation immunity of Boolean functions. The parameters for\nthe PSO velocity equation are tuned by means of two meta-optimization\ntechniques, namely Local Unimodal Sampling (LUS) and Continuous Genetic\nAlgorithms (CGA), finding that CGA produces better results. Using the\nCGA-evolved parameters, the PSO algorithm is then run on the spaces of Boolean\nfunctions from $n=7$ to $n=12$ variables. The results of the experiments are\nreported, observing that this new PSO algorithm generates Boolean functions\nfeaturing similar or better combinations of nonlinearity, correlation immunity\nand propagation criterion with respect to the ones obtained by other\noptimization methods.\n",
        "title": "A Discrete Particle Swarm Optimizer for the Design of Cryptographic\n  Boolean Functions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04570",
        "abstract_url": "http://arxiv.org/abs/2401.04570",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Weijin"
            },
            {
                "last_name": "Sha",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Yang",
                "first_name": "Huihua"
            },
            {
                "last_name": "Jiang",
                "first_name": "Rongcai"
            },
            {
                "last_name": "Li",
                "first_name": "Zhanying"
            },
            {
                "last_name": "Liu",
                "first_name": "Wentao"
            },
            {
                "last_name": "Su",
                "first_name": "Ruisheng"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Hemorrhagic Stroke (HS) has a rapid onset and is a serious condition that\nposes a great health threat. Promptly and accurately delineating the bleeding\nregion and estimating the volume of bleeding in Computer Tomography (CT) images\ncan assist clinicians in treatment planning, leading to improved treatment\noutcomes for patients. In this paper, a cascaded 3D model is constructed based\non UNet to perform a two-stage segmentation of the hemorrhage area in CT images\nfrom rough to fine, and the hemorrhage volume is automatically calculated from\nthe segmented area. On a dataset with 341 cases of hemorrhagic stroke CT scans,\nthe proposed model provides high-quality segmentation outcome with higher\naccuracy (DSC 85.66%) and better computation efficiency (6.2 second per sample)\nwhen compared to the traditional Tada formula with respect to hemorrhage volume\nestimation.\n",
        "title": "An Automatic Cascaded Model for Hemorrhagic Stroke Segmentation and\n  Hemorrhagic Volume Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04572",
        "abstract_url": "http://arxiv.org/abs/2401.04572",
        "authors": [
            {
                "last_name": "Amadori",
                "first_name": "Pierluigi Vito"
            },
            {
                "last_name": "Bradley",
                "first_name": "Timothy"
            },
            {
                "last_name": "Spick",
                "first_name": "Ryan"
            },
            {
                "last_name": "Moss",
                "first_name": "Guy"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Game development is a long process that involves many stages before a product\nis ready for the market. Human play testing is among the most time consuming,\nas testers are required to repeatedly perform tasks in the search for errors in\nthe code. Therefore, automated testing is seen as a key technology for the\ngaming industry, as it would dramatically improve development costs and\nefficiency. Toward this end, we propose EVOLUTE, a novel imitation\nlearning-based architecture that combines behavioural cloning (BC) with energy\nbased models (EBMs). EVOLUTE is a two-stream ensemble model that splits the\naction space of autonomous agents into continuous and discrete tasks. The EBM\nstream handles the continuous tasks, to have a more refined and adaptive\ncontrol, while the BC stream handles discrete actions, to ease training. We\nevaluate the performance of EVOLUTE in a shooting-and-driving game, where the\nagent is required to navigate and continuously identify targets to attack. The\nproposed model has higher generalisation capabilities than standard BC\napproaches, showing a wider range of behaviours and higher performances. Also,\nEVOLUTE is easier to train than a pure end-to-end EBM model, as discrete tasks\ncan be quite sparse in the dataset and cause model training to explore a much\nwider set of possible actions while training.\n",
        "title": "Robust Imitation Learning for Automated Game Testing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04575",
        "abstract_url": "http://arxiv.org/abs/2401.04575",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Yatong"
            },
            {
                "last_name": "Garg",
                "first_name": "Utsav"
            },
            {
                "last_name": "Shanker",
                "first_name": "Apaar"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haoming"
            },
            {
                "last_name": "Parajuli",
                "first_name": "Samyak"
            },
            {
                "last_name": "Bas",
                "first_name": "Erhan"
            },
            {
                "last_name": "Filipovic",
                "first_name": "Isidora"
            },
            {
                "last_name": "Chu",
                "first_name": "Amelia N."
            },
            {
                "last_name": "Fomitcheva",
                "first_name": "Eugenia D"
            },
            {
                "last_name": "Branson",
                "first_name": "Elliot"
            },
            {
                "last_name": "Kim",
                "first_name": "Aerin"
            },
            {
                "last_name": "Sojoudi",
                "first_name": "Somayeh"
            },
            {
                "last_name": "Cho",
                "first_name": "Kyunghyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Vision and vision-language applications of neural networks, such as image\nclassification and captioning, rely on large-scale annotated datasets that\nrequire non-trivial data-collecting processes. This time-consuming endeavor\nhinders the emergence of large-scale datasets, limiting researchers and\npractitioners to a small number of choices. Therefore, we seek more efficient\nways to collect and annotate images. Previous initiatives have gathered\ncaptions from HTML alt-texts and crawled social media postings, but these data\nsources suffer from noise, sparsity, or subjectivity. For this reason, we turn\nto commercial shopping websites whose data meet three criteria: cleanliness,\ninformativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset,\na large-scale public dataset with 15 million image-caption pairs from publicly\navailable e-commerce websites. When compared with existing general-domain\ndatasets, the LGS images focus on the foreground object and have less complex\nbackgrounds. Our experiments on LGS show that the classifiers trained on\nexisting benchmark datasets do not readily generalize to e-commerce data, while\nspecific self-supervised visual feature extractors can better generalize.\nFurthermore, LGS's high-quality e-commerce-focused images and bimodal nature\nmake it advantageous for vision-language bi-modal tasks: LGS enables\nimage-captioning models to generate richer captions and helps text-to-image\ngeneration models achieve e-commerce style transfer.\n",
        "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual\n  Concept Understanding",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04577",
        "abstract_url": "http://arxiv.org/abs/2401.04577",
        "authors": [
            {
                "last_name": "Ziv",
                "first_name": "Alon"
            },
            {
                "last_name": "Gat",
                "first_name": "Itai"
            },
            {
                "last_name": "Lan",
                "first_name": "Gael Le"
            },
            {
                "last_name": "Remez",
                "first_name": "Tal"
            },
            {
                "last_name": "Kreuk",
                "first_name": "Felix"
            },
            {
                "last_name": "D\u00e9fossez",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Copet",
                "first_name": "Jade"
            },
            {
                "last_name": "Synnaeve",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Adi",
                "first_name": "Yossi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "LG",
            ""
        ],
        "abstract": "  We introduce MAGNeT, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work,\nMAGNeT is comprised of a single-stage, non-autoregressive transformer. During\ntraining, we predict spans of masked tokens obtained from a masking scheduler,\nwhile during inference we gradually construct the output sequence using several\ndecoding steps. To further enhance the quality of the generated audio, we\nintroduce a novel rescoring method in which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNeT, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of\nMAGNeT, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel. We demonstrate the efficiency of\nMAGNeT for the task of text-to-music and text-to-audio generation and conduct\nan extensive empirical evaluation, considering both objective metrics and human\nstudies. The proposed approach is comparable to the evaluated baselines, while\nbeing significantly faster (x7 faster than the autoregressive baseline).\nThrough ablation studies and analysis, we shed light on the importance of each\nof the components comprising MAGNeT, together with pointing to the trade-offs\nbetween autoregressive and non-autoregressive modeling, considering latency,\nthroughput, and generation quality. Samples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.\n",
        "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04578",
        "abstract_url": "http://arxiv.org/abs/2401.04578",
        "authors": [
            {
                "last_name": "Abbas",
                "first_name": "Amro"
            },
            {
                "last_name": "Rusak",
                "first_name": "Evgenia"
            },
            {
                "last_name": "Tirumala",
                "first_name": "Kushal"
            },
            {
                "last_name": "Brendel",
                "first_name": "Wieland"
            },
            {
                "last_name": "Chaudhuri",
                "first_name": "Kamalika"
            },
            {
                "last_name": "Morcos",
                "first_name": "Ari S."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Utilizing massive web-scale datasets has led to unprecedented performance\ngains in machine learning models, but also imposes outlandish compute\nrequirements for their training. In order to improve training and data\nefficiency, we here push the limits of pruning large-scale multimodal datasets\nfor training CLIP-style models. Today's most effective pruning method on\nImageNet clusters data samples into separate concepts according to their\nembedding and prunes away the most prototypical samples. We scale this approach\nto LAION and improve it by noting that the pruning rate should be\nconcept-specific and adapted to the complexity of the concept. Using a simple\nand intuitive complexity measure, we are able to reduce the training cost to a\nquarter of regular training. By filtering from the LAION dataset, we find that\ntraining on a smaller set of high-quality data can lead to higher performance\nwith significantly lower training costs. More specifically, we are able to\noutperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot\naccuracy by 1.1p.p. while only using 27.7% of the data and training compute.\nDespite a strong reduction in training cost, we also see improvements on\nImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium\nbenchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a\ncompetitive average zero-shot accuracy on 38 evaluation tasks.\n",
        "title": "Effective pruning of web-scale datasets based on complexity of concept\n  clusters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04579",
        "abstract_url": "http://arxiv.org/abs/2401.04579",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuqian"
            },
            {
                "last_name": "Xue",
                "first_name": "Tengfei"
            },
            {
                "last_name": "Zekelman",
                "first_name": "Leo"
            },
            {
                "last_name": "Makris",
                "first_name": "Nikos"
            },
            {
                "last_name": "Rathi",
                "first_name": "Yogesh"
            },
            {
                "last_name": "Cai",
                "first_name": "Weidong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Donnell",
                "first_name": "Lauren J. O'"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Large datasets often contain multiple distinct feature sets, or views, that\noffer complementary information that can be exploited by multi-view learning\nmethods to improve results. We investigate anatomical multi-view data, where\neach brain anatomical structure is described with multiple feature sets. In\nparticular, we focus on sets of white matter microstructure and connectivity\nfeatures from diffusion MRI, as well as sets of gray matter area and thickness\nfeatures from structural MRI. We investigate machine learning methodology that\napplies multi-view approaches to improve the prediction of non-imaging\nphenotypes, including demographics (age), motor (strength), and cognition\n(picture vocabulary). We present an explainable multi-view network (EMV-Net)\nthat can use different anatomical views to improve prediction performance. In\nthis network, each individual anatomical view is processed by a view-specific\nfeature extractor and the extracted information from each view is fused using a\nlearnable weight. This is followed by a wavelet transform-based module to\nobtain complementary information across views which is then applied to\ncalibrate the view-specific information. Additionally, the calibrator produces\nan attention-based calibration score to indicate anatomical structures'\nimportance for interpretation.\n",
        "title": "A Deep Network for Explainable Prediction of Non-Imaging Phenotypes\n  using Anatomical Multi-View Data",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04585",
        "abstract_url": "http://arxiv.org/abs/2401.04585",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xuewen"
            },
            {
                "last_name": "Li",
                "first_name": "Zhikai"
            },
            {
                "last_name": "Xiao",
                "first_name": "Junrui"
            },
            {
                "last_name": "Gu",
                "first_name": "Qingyi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion models have achieved great success in image generation tasks\nthrough iterative noise estimation. However, the heavy denoising process and\ncomplex neural networks hinder their low-latency applications in real-world\nscenarios. Quantization can effectively reduce model complexity, and\npost-training quantization (PTQ), which does not require fine-tuning, is highly\npromising in accelerating the denoising process. Unfortunately, we find that\ndue to the highly dynamic distribution of activations in different denoising\nsteps, existing PTQ methods for diffusion models suffer from distribution\nmismatch issues at both calibration sample level and reconstruction output\nlevel, which makes the performance far from satisfactory, especially in low-bit\ncases. In this paper, we propose Enhanced Distribution Alignment for\nPost-Training Quantization of Diffusion Models (EDA-DM) to address the above\nissues. Specifically, at the calibration sample level, we select calibration\nsamples based on the density and diversity in the latent space, thus\nfacilitating the alignment of their distribution with the overall samples; and\nat the reconstruction output level, we propose Fine-grained Block\nReconstruction, which can align the outputs of the quantized model and the\nfull-precision model at different network granularity. Extensive experiments\ndemonstrate that EDA-DM outperforms the existing post-training quantization\nframeworks in both unconditional and conditional generation scenarios. At\nlow-bit precision, the quantized models with our method even outperform the\nfull-precision models on most datasets.\n",
        "title": "Enhanced Distribution Alignment for Post-Training Quantization of\n  Diffusion Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04592",
        "abstract_url": "http://arxiv.org/abs/2401.04592",
        "authors": [
            {
                "last_name": "Arcan",
                "first_name": "Mihael"
            },
            {
                "last_name": "Niland",
                "first_name": "Paul-David"
            },
            {
                "last_name": "Delahunty",
                "first_name": "Fionn"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Mental health challenges pose considerable global burdens on individuals and\ncommunities. Recent data indicates that more than 20% of adults may encounter\nat least one mental disorder in their lifetime. On the one hand, the\nadvancements in large language models have facilitated diverse applications,\nyet a significant research gap persists in understanding and enhancing the\npotential of large language models within the domain of mental health. On the\nother hand, across various applications, an outstanding question involves the\ncapacity of large language models to comprehend expressions of human mental\nhealth conditions in natural language. This study presents an initial\nevaluation of large language models in addressing this gap. Due to this, we\ncompare the performance of Llama-2 and ChatGPT with classical Machine as well\nas Deep learning models. Our results on the DAIC-WOZ dataset show that\ntransformer-based models, like BERT or XLNet, outperform the large language\nmodels.\n",
        "title": "An Assessment on Comprehending Mental Health through Large Language\n  Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04594",
        "abstract_url": "http://arxiv.org/abs/2401.04594",
        "authors": [
            {
                "last_name": "Zilberstein",
                "first_name": "Noam"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL"
        ],
        "abstract": "  Starting with Hoare Logic over 50 years ago, numerous sound and relatively\ncomplete program logics have been devised to reason about the diverse programs\nencountered in the real world. This includes reasoning about computational\neffects, particularly those effects that cause the program execution to branch\ninto multiple paths due to, e.g., nondeterministic or probabilistic choice.\n  The recently introduced Outcome Logic reimagines Hoare Logic with effects at\nits core, using an algebraic representation of choice to capture a variety of\neffects. In this paper, we give the first relatively complete proof system for\nOutcome Logic, handling general purpose looping for the first time. We also\nshow that this proof system applies to programs with various effects and that\nit facilitates the reuse of proof fragments across different kinds of\nspecifications.\n",
        "title": "A Relatively Complete Program Logic for Effectful Branching",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04595",
        "abstract_url": "http://arxiv.org/abs/2401.04595",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Sha",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Feitian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Underwater target localization uses real-time sensory measurements to\nestimate the position of underwater objects of interest, providing critical\nfeedback information for underwater robots. While acoustic sensing is the most\nacknowledged method in underwater robots and possibly the only effective\napproach for long-range underwater target localization, such a sensing modality\ngenerally suffers from low resolution, high cost and high energy consumption,\nthus leading to a mediocre performance when applied to close-range underwater\ntarget localization. On the other hand, optical sensing has attracted\nincreasing attention in the underwater robotics community for its advantages of\nhigh resolution and low cost, holding a great potential particularly in\nclose-range underwater target localization. However, most existing studies in\nunderwater optical sensing are restricted to specific types of targets due to\nthe limited training data available. In addition, these studies typically focus\non the design of estimation algorithms and ignore the influence of illumination\nconditions on the sensing performance, thus hindering wider applications in the\nreal world. To address the aforementioned issues, this paper proposes a novel\ntarget localization method that assimilates both optical and acoustic sensory\nmeasurements to estimate the 3D positions of close-range underwater targets. A\ntest platform with controllable illumination conditions is designed and\ndeveloped to experimentally investigate the proposed multi-modal sensing\napproach. A large vision model is applied to process the optical imaging\nmeasurements, eliminating the requirement for training data acquisition, thus\nsignificantly expanding the scope of potential applications. Extensive\nexperiments are conducted, the results of which validate the effectiveness of\nthe proposed underwater target localization method.\n",
        "title": "A Multi-Modal Approach Based on Large Vision Model for Close-Range\n  Underwater Target Localization",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04601",
        "abstract_url": "http://arxiv.org/abs/2401.04601",
        "authors": [
            {
                "last_name": "MacNeil",
                "first_name": "Stephen"
            },
            {
                "last_name": "Spurlock",
                "first_name": "Scott"
            },
            {
                "last_name": "Applebaum",
                "first_name": "Ian"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In the contemporary landscape of computing education, the ubiquity of\nGenerative Artificial Intelligence has significantly disrupted traditional\nassessment methods, rendering them obsolete and prompting educators to seek\ninnovative alternatives. This research paper explores the challenges posed by\nGenerative AI in the assessment domain and the persistent attempts to\ncircumvent its impact. Despite various efforts to devise workarounds, the\nacademic community is yet to find a comprehensive solution. Amidst this\nstruggle, ungrading emerges as a potential yet under-appreciated solution to\nthe assessment dilemma. Ungrading, a pedagogical approach that involves moving\naway from traditional grading systems, has faced resistance due to its\nperceived complexity and the reluctance of educators to depart from\nconventional assessment practices. However, as the inadequacies of current\nassessment methods become increasingly evident in the face of Generative AI,\nthe time is ripe to reconsider and embrace ungrading.\n",
        "title": "Imagining Computing Education Assessment after Generative AI",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04606",
        "abstract_url": "http://arxiv.org/abs/2401.04606",
        "authors": [
            {
                "last_name": "Grohe",
                "first_name": "Martin"
            },
            {
                "last_name": "Kimelfeld",
                "first_name": "Benny"
            },
            {
                "last_name": "Lindner",
                "first_name": "Peter"
            },
            {
                "last_name": "Standke",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  We propose and study a framework for quantifying the importance of the\nchoices of parameter values to the result of a query over a database. These\nparameters occur as constants in logical queries, such as conjunctive queries.\nIn our framework, the importance of a parameter is its SHAP score. This score\nis a popular instantiation of the game-theoretic Shapley value to measuring the\nimportance of feature values in machine learning models. We make the case for\nthe rationale of using this score by explaining the intuition behind SHAP, and\nby showing that we arrive at this score in two different, apparently opposing,\napproaches to quantifying the contribution of a parameter.\n  The application of the SHAP score requires two components in addition to the\nquery and the database: (a) a probability distribution over the combinations of\nparameter values, and (b) a utility function that measures the similarity\nbetween the result for the original parameters and the result for hypothetical\nparameters. The main question addressed in the paper is the complexity of\ncalculating the SHAP score for different distributions and similarity measures.\nWe first address the case of probabilistically independent parameters. The\nproblem is hard if we consider a fragment of queries that is hard to evaluate\n(as one would expect), and even for the fragment of acyclic conjunctive\nqueries. In some cases, though, one can efficiently list all relevant parameter\ncombinations, and then the SHAP score can be computed in polynomial time under\nreasonable general conditions. Also tractable is the case of full acyclic\nconjunctive queries for certain (natural) similarity functions. We extend our\nresults to conjunctive queries with inequalities between variables and\nparameters. Finally, we discuss a simple approximation technique for the case\nof correlated parameters.\n",
        "title": "The Importance of Parameters in Database Queries",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04608",
        "abstract_url": "http://arxiv.org/abs/2401.04608",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jingyuan"
            },
            {
                "last_name": "Feng",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Huang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent years have witnessed remarkable progress in image generation task,\nwhere users can create visually astonishing images with high-quality. However,\nexisting text-to-image diffusion models are proficient in generating concrete\nconcepts (dogs) but encounter challenges with more abstract ones (emotions).\nSeveral efforts have been made to modify image emotions with color and style\nadjustments, facing limitations in effectively conveying emotions with fixed\nimage contents. In this work, we introduce Emotional Image Content Generation\n(EICG), a new task to generate semantic-clear and emotion-faithful images given\nemotion categories. Specifically, we propose an emotion space and construct a\nmapping network to align it with the powerful Contrastive Language-Image\nPre-training (CLIP) space, providing a concrete interpretation of abstract\nemotions. Attribute loss and emotion confidence are further proposed to ensure\nthe semantic diversity and emotion fidelity of the generated images. Our method\noutperforms the state-of-the-art text-to-image approaches both quantitatively\nand qualitatively, where we derive three custom metrics, i.e., emotion\naccuracy, semantic clarity and semantic diversity. In addition to generation,\nour method can help emotion understanding and inspire emotional art design.\n",
        "title": "EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion\n  Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04609",
        "abstract_url": "http://arxiv.org/abs/2401.04609",
        "authors": [
            {
                "last_name": "Kraus",
                "first_name": "Johannes"
            },
            {
                "last_name": "Lymbery",
                "first_name": "Maria"
            },
            {
                "last_name": "Osthues",
                "first_name": "Kevin"
            },
            {
                "last_name": "Philo",
                "first_name": "Fadi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We consider the dynamic Biot model describing the interaction between fluid\nflow and solid deformation including wave propagation phenomena in both the\nliquid and solid phases of a saturated porous medium. The model couples a\nhyperbolic equation for momentum balance to a second-order in time dynamic\nDarcy law and a parabolic equation for the balance of mass and is here\nconsidered in three-field formulation with the displacement of the elastic\nmatrix, the fluid velocity, and the fluid pressure being the physical fields of\ninterest. A family of variational space-time finite element methods is proposed\nthat combines a continuous-in-time Galerkin ansatz of arbitrary polynomial\ndegree with inf-sup stable $H(\\rm{div})$-conforming approximations of\ndiscontinuous Galerkin (DG) type in case of the displacement and a mixed\napproximation of the flux, its time derivative and the pressure field. We prove\nerror estimates in a combined energy norm as well as $L^2$~error estimates in\nspace for the individual fields for both maximum and $L^2$ norm in time which\nare optimal for the displacement and pressure approximations.\n",
        "title": "Analysis of a family of time-continuous strongly conservative space-time\n  finite element methods for the dynamic Biot model",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04612",
        "abstract_url": "http://arxiv.org/abs/2401.04612",
        "authors": [
            {
                "last_name": "Dheur",
                "first_name": "Victor"
            },
            {
                "last_name": "Bosser",
                "first_name": "Tanguy"
            },
            {
                "last_name": "Izbicki",
                "first_name": "Rafael"
            },
            {
                "last_name": "Taieb",
                "first_name": "Souhaib Ben"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Sequences of labeled events observed at irregular intervals in continuous\ntime are ubiquitous across various fields. Temporal Point Processes (TPPs)\nprovide a mathematical framework for modeling these sequences, enabling\ninferences such as predicting the arrival time of future events and their\nassociated label, called mark. However, due to model misspecification or lack\nof training data, these probabilistic models may provide a poor approximation\nof the true, unknown underlying process, with prediction regions extracted from\nthem being unreliable estimates of the underlying uncertainty. This paper\ndevelops more reliable methods for uncertainty quantification in neural TPP\nmodels via the framework of conformal prediction. A primary objective is to\ngenerate a distribution-free joint prediction region for the arrival time and\nmark, with a finite-sample marginal coverage guarantee. A key challenge is to\nhandle both a strictly positive, continuous response and a categorical\nresponse, without distributional assumptions. We first consider a simple but\noverly conservative approach that combines individual prediction regions for\nthe event arrival time and mark. Then, we introduce a more effective method\nbased on bivariate highest density regions derived from the joint predictive\ndensity of event arrival time and mark. By leveraging the dependencies between\nthese two variables, this method exclude unlikely combinations of the two,\nresulting in sharper prediction regions while still attaining the pre-specified\ncoverage level. We also explore the generation of individual univariate\nprediction regions for arrival times and marks through conformal regression and\nclassification techniques. Moreover, we investigate the stronger notion of\nconditional coverage. Finally, through extensive experimentation on both\nsimulated and real-world datasets, we assess the validity and efficiency of\nthese methods.\n",
        "title": "Distribution-Free Conformal Joint Prediction Regions for Neural Marked\n  Temporal Point Processes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04614",
        "abstract_url": "http://arxiv.org/abs/2401.04614",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Ziyue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingming"
            },
            {
                "last_name": "Gong",
                "first_name": "Yuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Qingjie"
            },
            {
                "last_name": "Wang",
                "first_name": "Yunhong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning models are essential for scene classification, change\ndetection, land cover segmentation, and other remote sensing image\nunderstanding tasks. Most backbones of existing remote sensing deep learning\nmodels are typically initialized by pre-trained weights obtained from ImageNet\npre-training (IMP). However, domain gaps exist between remote sensing images\nand natural images (e.g., ImageNet), making deep learning models initialized by\npre-trained weights of IMP perform poorly for remote sensing image\nunderstanding. Although some pre-training methods are studied in the remote\nsensing community, current remote sensing pre-training methods face the problem\nof vague generalization by only using remote sensing images. In this paper, we\npropose a novel remote sensing pre-training framework, Generic Knowledge\nBoosted Remote Sensing Pre-training (GeRSP), to learn robust representations\nfrom remote sensing and natural images for remote sensing understanding tasks.\nGeRSP contains two pre-training branches: (1) A self-supervised pre-training\nbranch is adopted to learn domain-related representations from unlabeled remote\nsensing images. (2) A supervised pre-training branch is integrated into GeRSP\nfor general knowledge learning from labeled natural images. Moreover, GeRSP\ncombines two pre-training branches using a teacher-student architecture to\nsimultaneously learn representations with general and special knowledge, which\ngenerates a powerful pre-trained model for deep learning model initialization.\nFinally, we evaluate GeRSP and other remote sensing pre-training methods on\nthree downstream tasks, i.e., object detection, semantic segmentation, and\nscene classification. The extensive experimental results consistently\ndemonstrate that GeRSP can effectively learn robust representations in a\nunified manner, improving the performance of remote sensing downstream tasks.\n",
        "title": "Generic Knowledge Boosted Pre-training For Remote Sensing Images",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04619",
        "abstract_url": "http://arxiv.org/abs/2401.04619",
        "authors": [
            {
                "last_name": "S",
                "first_name": "Selva Kumar"
            },
            {
                "last_name": "Khan",
                "first_name": "Afifah Khan Mohammed Ajmal"
            },
            {
                "last_name": "Manjeshwar",
                "first_name": "Chirag"
            },
            {
                "last_name": "Banday",
                "first_name": "Imadh Ajaz"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            ""
        ],
        "abstract": "  In the contemporary digital era, the Internet functions as an unparalleled\ncatalyst, dismantling geographical and linguistic barriers particularly evident\nin texting. This evolution facilitates global communication, transcending\nphysical distances and fostering dynamic cultural exchange. A notable trend is\nthe widespread use of transliteration, where the English alphabet is employed\nto convey messages in native languages, posing a unique challenge for language\ntechnology in accurately detecting the source language. This paper addresses\nthis challenge through a dataset of phone text messages in Hindi and Russian\ntransliterated into English utilizing BERT for language classification and\nGoogle Translate API for transliteration conversion. The research pioneers\ninnovative approaches to identify and convert transliterated text, navigating\nchallenges in the diverse linguistic landscape of digital communication.\nEmphasizing the pivotal role of comprehensive datasets for training Large\nLanguage Models LLMs like BERT, our model showcases exceptional proficiency in\naccurately identifying and classifying languages from transliterated text. With\na validation accuracy of 99% our models robust performance underscores its\nreliability. The comprehensive exploration of transliteration dynamics\nsupported by innovative approaches and cutting edge technologies like BERT,\npositions our research at the forefront of addressing unique challenges in the\nlinguistic landscape of digital communication. Beyond contributing to language\nidentification and transliteration capabilities this work holds promise for\napplications in content moderation, analytics and fostering a globally\nconnected community engaged in meaningful dialogue.\n",
        "title": "Language Detection for Transliterated Content",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04626",
        "abstract_url": "http://arxiv.org/abs/2401.04626",
        "authors": [
            {
                "last_name": "Feraudo",
                "first_name": "Angelo"
            },
            {
                "last_name": "Calvio",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Bellavista",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Vehicular cloud computing is gaining popularity thanks to the rapid\nadvancements in next generation wireless communication networks. Similarly,\nEdge Computing, along with its standard proposals such as European\nTelecommunications Standards Institute (ETSI) Multi-access Edge Computing\n(MEC), will play a vital role in these scenarios, by enabling the execution of\ncloud-based services at the edge of the network. Together, these solutions have\nthe potential to create real micro-datacenters at the network edge, favoring\nseveral benefits like minimal latency, real-time data processing, and data\nlocality. However, the research community has not yet the opportunity to use\nintegrated simulation frameworks for the easy testing of applications that\nexploit both the vehicular cloud paradigm and MEC-compliant 5G deployment\nenvironments. In this paper, we present our simulation tool as a platform for\nresearchers and engineers to design, test, and enhance applications utilizing\nthe concepts of vehicular and edge cloud. Our platform significantly extends\nOMNet++ and Simu5G, and implements our ETSI MEC-compliant architecture that\nleverages resources provided by far-edge nodes. In addition, the paper analyzes\nand reports performance results for our simulation platform, as well as\nprovides a use case where our simulator is used to support the design, test,\nand validation of an algorithm to distribute MEC application components on\nvehicular cloud resources.\n",
        "title": "A Novel OMNeT++-based Simulation Tool for Vehicular Cloud Computing in\n  ETSI MEC-compliant 5G Environments",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04628",
        "abstract_url": "http://arxiv.org/abs/2401.04628",
        "authors": [
            {
                "last_name": "Lynch",
                "first_name": "Nancy A."
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "DS"
        ],
        "abstract": "  We describe how hierarchical concepts can be represented in three types of\nlayered neural networks. The aim is to support recognition of the concepts when\npartial information about the concepts is presented, and also when some of the\nneurons in the network might fail. Our failure model involves initial random\nfailures. The three types of networks are: feed-forward networks with high\nconnectivity, feed-forward networks with low connectivity, and layered networks\nwith low connectivity and with both forward edges and \"lateral\" edges within\nlayers. In order to achieve fault-tolerance, the representations all use\nmultiple representative neurons for each concept. We show how recognition can\nwork in all three of these settings, and quantify how the probability of\ncorrect recognition depends on several parameters, including the number of\nrepresentatives and the neuron failure probability. We also discuss how these\nrepresentations might be learned, in all three types of networks. For the\nfeed-forward networks, the learning algorithms are similar to ones used in [4],\nwhereas for networks with lateral edges, the algorithms are generally inspired\nby work on the assembly calculus [3, 6, 7].\n",
        "title": "Multi-Neuron Representations of Hierarchical Concepts in Spiking Neural\n  Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04631",
        "abstract_url": "http://arxiv.org/abs/2401.04631",
        "authors": [
            {
                "last_name": "Luis",
                "first_name": "Samuel Yanes"
            },
            {
                "last_name": "Shutin",
                "first_name": "Dmitriy"
            },
            {
                "last_name": "G\u00f3mez",
                "first_name": "Juan Marchal"
            },
            {
                "last_name": "Reina",
                "first_name": "Daniel Guti\u00e9rrez"
            },
            {
                "last_name": "Mar\u00edn",
                "first_name": "Sergio Toral"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The conservation of hydrological resources involves continuously monitoring\ntheir contamination. A multi-agent system composed of autonomous surface\nvehicles is proposed in this paper to efficiently monitor the water quality. To\nachieve a safe control of the fleet, the fleet policy should be able to act\nbased on measurements and to the the fleet state. It is proposed to use Local\nGaussian Processes and Deep Reinforcement Learning to jointly obtain effective\nmonitoring policies. Local Gaussian processes, unlike classical global Gaussian\nprocesses, can accurately model the information in a dissimilar spatial\ncorrelation which captures more accurately the water quality information. A\nDeep convolutional policy is proposed, that bases the decisions on the\nobservation on the mean and variance of this model, by means of an information\ngain reward. Using a Double Deep Q-Learning algorithm, agents are trained to\nminimize the estimation error in a safe manner thanks to a Consensus-based\nheuristic. Simulation results indicate an improvement of up to 24% in terms of\nthe mean absolute error with the proposed models. Also, training results with\n1-3 agents indicate that our proposed approach returns 20% and 24% smaller\naverage estimation errors for, respectively, monitoring water quality variables\nand monitoring algae blooms, as compared to state-of-the-art approaches\n",
        "title": "Deep Reinforcement Multi-agent Learning framework for Information\n  Gathering with Local Gaussian Processes for Water Monitoring",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04632",
        "abstract_url": "http://arxiv.org/abs/2401.04632",
        "authors": [
            {
                "last_name": "Kycia",
                "first_name": "Rados\u0142aw"
            },
            {
                "last_name": "Niemczynowicz",
                "first_name": "Agnieszka"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  The three classes of architectures for time series prediction were tested.\nThey differ by input layers which contain either convolutional, LSTM, or dense\nhypercomplex layers for 4D algebras. The input was four related Stock Market\ntime series, and the prediction of one of them is expected. The optimization of\nhyperparameters related to the classes of architectures was performed in order\nto compare the best neural networks within the class. The results show that in\nmost cases, the architecture with a hypercomplex dense layer provides similar\nMAE accuracy to other architectures, however, with considerably less trainable\nparameters. Thanks to it, hypercomplex neural networks can be learned and\nprocess data faster than the other tested architectures. Moreover, the order of\nthe input time series has an impact on effectively.\n",
        "title": "Hypercomplex neural network in time series forecasting of stock data",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04636",
        "abstract_url": "http://arxiv.org/abs/2401.04636",
        "authors": [
            {
                "last_name": "Sabu",
                "first_name": "Nithin V."
            },
            {
                "last_name": "Gupta",
                "first_name": "Abhishek K."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A network of nanomachines (NMs) can be used to build a target detection\nsystem for a variety of promising applications. They have the potential to\ndetect toxic chemicals, infectious bacteria, and biomarkers of dangerous\ndiseases such as cancer within the human body. Many diseases and health\ndisorders can be detected early and efficiently treated in the future by\nutilizing these systems. To fully grasp the potential of these systems,\nmathematical analysis is required. This paper describes an analytical framework\nfor modeling and analyzing the performance of target detection systems composed\nof multiple mobile nanomachines of varying sizes with passive/absorbing\nboundaries. We consider both direct contact detection, in which NMs must\nphysically contact the target to detect it, and indirect sensing, in which NMs\nmust detect the marker molecules emitted by the target. The detection\nperformance of such systems is calculated for degradable and non-degradable\ntargets, as well as mobile and stationary targets. The derived expressions\nprovide various insights, such as the effect of NM density and target\ndegradation on detection probability.\n",
        "title": "On the Target Detection Performance of a Molecular Communication Network\n  with Multiple Mobile Nanomachines",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04637",
        "abstract_url": "http://arxiv.org/abs/2401.04637",
        "authors": [
            {
                "last_name": "Aracena",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Luster",
                "first_name": "Kyle"
            },
            {
                "last_name": "Santos",
                "first_name": "Fabio"
            },
            {
                "last_name": "Steinmacher",
                "first_name": "Igor"
            },
            {
                "last_name": "Gerosa",
                "first_name": "Marco A."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  Effective prioritization of issue reports is crucial in software engineering\nto optimize resource allocation and address critical problems promptly.\nHowever, the manual classification of issue reports for prioritization is\nlaborious and lacks scalability. Alternatively, many open source software (OSS)\nprojects employ automated processes for this task, albeit relying on\nsubstantial datasets for adequate training. This research seeks to devise an\nautomated approach that ensures reliability in issue prioritization, even when\ntrained on smaller datasets. Our proposed methodology harnesses the power of\nGenerative Pre-trained Transformers (GPT), recognizing their potential to\nefficiently handle this task. By leveraging the capabilities of such models, we\naim to develop a robust system for prioritizing issue reports accurately,\nmitigating the necessity for extensive training data while maintaining\nreliability. In our research, we have developed a reliable GPT-based approach\nto accurately label and prioritize issue reports with a reduced training\ndataset. By reducing reliance on massive data requirements and focusing on\nfew-shot fine-tuning, our methodology offers a more accessible and efficient\nsolution for issue prioritization in software engineering. Our model predicted\nissue types in individual projects up to 93.2% in precision, 95% in recall, and\n89.3% in F1-score.\n",
        "title": "Applying Large Language Models API to Issue Classification Problem",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04638",
        "abstract_url": "http://arxiv.org/abs/2401.04638",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Wenkai"
            },
            {
                "last_name": "Dinitz",
                "first_name": "Michael"
            },
            {
                "last_name": "Foerster",
                "first_name": "Klaus-Tycho"
            },
            {
                "last_name": "Luo",
                "first_name": "Long"
            },
            {
                "last_name": "Schmid",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF",
            "DM"
        ],
        "abstract": "  Emerging reconfigurable optical communication technologies allow to enhance\ndatacenter topologies with demand-aware links optimized towards traffic\npatterns. This paper studies the algorithmic problem of jointly optimizing\ntopology and routing in such demand-aware networks to minimize congestion,\nalong two dimensions: (1) splittable or unsplittable flows, and (2) whether\nrouting is segregated, i.e., whether routes can or cannot combine both\ndemand-aware and demand-oblivious (static) links.\n  For splittable and segregated routing, we show that the problem is generally\n$2$-approximable, but APX-hard even for uniform demands induced by a bipartite\ndemand graph. For unsplittable and segregated routing, we establish upper and\nlower bounds of $O\\left(\\log m/ \\log\\log m \\right)$ and $\\Omega\\left(\\log m/\n\\log\\log m \\right)$, respectively, for polynomial-time approximation\nalgorithms, where $m$ is the number of static links. We further reveal that\nunder un-/splittable and non-segregated routing, even for demands of a single\nsource (resp., destination), the problem cannot be approximated better than\n$\\Omega\\left(\\frac{c_{\\max}}{c_{\\min}} \\right)$ unless P=NP, where $c_{\\max}$\n(resp., $c_{\\min}$) denotes the maximum (resp., minimum) capacity. It remains\nNP-hard for uniform capacities, but is tractable for a single commodity and\nuniform capacities.\n  Our trace-driven simulations show a significant reduction in network\ncongestion compared to existing solutions.\n",
        "title": "Approximation Algorithms for Minimizing Congestion in Demand-Aware\n  Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04647",
        "abstract_url": "http://arxiv.org/abs/2401.04647",
        "authors": [
            {
                "last_name": "Garg",
                "first_name": "Tanmay"
            },
            {
                "last_name": "Vemuri",
                "first_name": "Deepika"
            },
            {
                "last_name": "Balasubramanian",
                "first_name": "Vineeth N"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  This paper presents a novel concept learning framework for enhancing model\ninterpretability and performance in visual classification tasks. Our approach\nappends an unsupervised explanation generator to the primary classifier network\nand makes use of adversarial training. During training, the explanation module\nis optimized to extract visual concepts from the classifier's latent\nrepresentations, while the GAN-based module aims to discriminate images\ngenerated from concepts, from true images. This joint training scheme enables\nthe model to implicitly align its internally learned concepts with\nhuman-interpretable visual properties. Comprehensive experiments demonstrate\nthe robustness of our approach, while producing coherent concept activations.\nWe analyse the learned concepts, showing their semantic concordance with object\nparts and visual attributes. We also study how perturbations in the adversarial\ntraining protocol impact both classification and concept acquisition. In\nsummary, this work presents a significant step towards building inherently\ninterpretable deep vision models with task-aligned concept representations - a\nkey enabler for developing trustworthy AI for real-world perception tasks.\n",
        "title": "Advancing Ante-Hoc Explainable Models through Generative Adversarial\n  Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04648",
        "abstract_url": "http://arxiv.org/abs/2401.04648",
        "authors": [
            {
                "last_name": "Kag",
                "first_name": "Vijay"
            },
            {
                "last_name": "Pal",
                "first_name": "Birupaksha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Modelling of systems where the full system information is unknown is an oft\nencountered problem for various engineering and industrial applications, as\nit's either impossible to consider all the complex physics involved or simpler\nmodels are considered to keep within the limits of the available resources.\nRecent advances in greybox modelling like the deep hidden physics models\naddress this space by combining data and physics. However, for most real-life\napplications, model generalizability is a key issue, as retraining a model for\nevery small change in system inputs and parameters or modification in domain\nconfiguration can render the model economically unviable. In this work we\npresent a novel enhancement to the idea of hidden physics models which can\ngeneralize for changes in system inputs, parameters and domains. We also show\nthat this approach holds promise in system discovery as well and helps learn\nthe hidden physics for the changed system inputs, parameters and domain\nconfiguration.\n",
        "title": "A novel framework for generalization of deep hidden physics models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04649",
        "abstract_url": "http://arxiv.org/abs/2401.04649",
        "authors": [
            {
                "last_name": "Nawratil",
                "first_name": "Georg"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            "RO"
        ],
        "abstract": "  We give a full classification of continuous flexible discrete axial\ncone-nets, which are called axial C-hedra. The obtained result can also be used\nto construct their semi-discrete analogs. Moreover, we identify a novel\nsubclass within the determined class of (semi-)discrete axial cone-nets, whose\nmembers are named axial P-nets as they fulfill the proportion (P) of the\nintercept theorem. Known special cases of these axial P-nets are the smooth and\ndiscrete conic crease patterns with reflecting rule lines. By using a\nparallelism operation one can even generalize axial P-nets. The resulting\ngeneral P-nets constitute a rich novel class of continuous flexible\n(semi-)discrete surfaces, which allow direct access to their spatial shapes by\nthree control polylines. This intuitive method makes them suitable for\ntransformable design tasks using interactive tools.\n",
        "title": "From axial C-hedra to general P-nets",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04650",
        "abstract_url": "http://arxiv.org/abs/2401.04650",
        "authors": [
            {
                "last_name": "Mensah",
                "first_name": "Immanuel Ampomah"
            },
            {
                "last_name": "Healey",
                "first_name": "Jessica"
            },
            {
                "last_name": "Wu",
                "first_name": "Celina"
            },
            {
                "last_name": "Lacunza",
                "first_name": "Andrea"
            },
            {
                "last_name": "Hanson",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Dorsey",
                "first_name": "Kristen L."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  An underdeveloped capability in soft robotics is proprioceptive feedback\ncontrol, where soft actuators can be sensed and controlled using only sensors\non the robot's body. Additionally, soft actuators are often unable to support\nhuman-scale loads due to the extremely compliant materials in use. Developing\nboth feedback control and the ability to actuate under large loads (e.g. 500 N)\nare key capacities required to move soft robotics into everyday applications.\nIn this work, we independently demonstrate these key factors towards\ncontrolling and actuating human-scale loads: proprioceptive (embodied) feedback\ncontrol of a soft, pneumatically-actuated origami robot; and actuation of these\norigami origami robots under a person's weight in an open-loop configuration.\nIn both demonstrations, the actuators are controlled by internal fluidic\npressure. Capacitive sensors patterned onto the robot provide position\nestimation and serve as input to a feedback controller. We demonstrate position\ncontrol of a single actuator during stepped setpoints and sinusoidal trajectory\nfollowing, with root mean square error (RMSE) below 4 mm. We also showcase the\nactuator's potential towards human-scale robotics as an \"origami balance board\"\nby joining three actuators into an open-loop controlled system with a platform\nthat varies its height, roll, and pitch. This work contributes to the field of\nsoft robotics by demonstrating closed-loop feedback position control without\nvisual tracking as an input and lightweight, soft actuators that can support a\nperson's weight. The project repository, including videos, CAD files, and ROS\ncode, is available at https://parses-lab.github.io/kresling_control.\n",
        "title": "Hold 'em and Fold 'em: Towards Human-scale, Feedback-Controlled Soft\n  Origami Robots",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04651",
        "abstract_url": "http://arxiv.org/abs/2401.04651",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Jiang",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingyi"
            },
            {
                "last_name": "Qiu",
                "first_name": "Han"
            },
            {
                "last_name": "Lu",
                "first_name": "Lewei"
            },
            {
                "last_name": "Lu",
                "first_name": "Shijian"
            },
            {
                "last_name": "Xing",
                "first_name": "Eric"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great\npotential in learning to segment anything. The core design of SAMs lies with\nPromptable Segmentation, which takes a handcrafted prompt as input and returns\nthe expected segmentation mask. SAMs work with two types of prompts including\nspatial prompts (e.g., points) and semantic prompts (e.g., texts), which work\ntogether to prompt SAMs to segment anything on downstream datasets. Despite the\nimportant role of prompts, how to acquire suitable prompts for SAMs is largely\nunder-explored. In this work, we examine the architecture of SAMs and identify\ntwo challenges for learning effective prompts for SAMs. To this end, we propose\nspatial-semantic prompt learning (SSPrompt) that learns effective semantic and\nspatial prompts for better SAMs. Specifically, SSPrompt introduces spatial\nprompt learning and semantic prompt learning, which optimize spatial prompts\nand semantic prompts directly over the embedding space and selectively leverage\nthe knowledge encoded in pre-trained prompt encoders. Extensive experiments\nshow that SSPrompt achieves superior image segmentation performance\nconsistently across multiple widely adopted datasets.\n",
        "title": "Learning to Prompt Segment Anything Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04653",
        "abstract_url": "http://arxiv.org/abs/2401.04653",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Liang"
            },
            {
                "last_name": "Ganko",
                "first_name": "Krystian"
            },
            {
                "last_name": "Braatz",
                "first_name": "Richard D."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Determining solving-time certificates of nonlinear model predictive control\n(NMPC) implementations is a pressing requirement when deploying NMPC in\nproduction environments. Such a certificate guarantees that the NMPC controller\nreturns a solution before the next sampling time. However, NMPC formulations\nproduce nonlinear programs (NLPs) for which it is very difficult to derive\ntheir solving-time certificates. Our previous work, Wu and Braatz (2023),\nchallenged this limitation with a proposed input-constrained MPC algorithm\nhaving exact iteration complexity but was restricted to linear MPC\nformulations. This work extends the algorithm to solve input-constrained NMPC\nproblems, by using the Koopman operator and a condensing MPC technique. We\nillustrate the algorithm performance on a high-dimensional, nonlinear partial\ndifferential equation (PDE) control case study, in which we theoretically and\nnumerically certify the solving time to be less than the sampling time.\n",
        "title": "Time-certified Input-constrained NMPC via Koopman Operator",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04655",
        "abstract_url": "http://arxiv.org/abs/2401.04655",
        "authors": [
            {
                "last_name": "Rahman",
                "first_name": "Abu Bakar Siddiqur"
            },
            {
                "last_name": "Ta",
                "first_name": "Hoang-Thang"
            },
            {
                "last_name": "Najjar",
                "first_name": "Lotfollah"
            },
            {
                "last_name": "Azadmanesh",
                "first_name": "Azad"
            },
            {
                "last_name": "G\u00f6n\u00fcl",
                "first_name": "Ali Saffet"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Emotions are integral to human social interactions, with diverse responses\nelicited by various situational contexts. Particularly, the prevalence of\nnegative emotional states has been correlated with negative outcomes for mental\nhealth, necessitating a comprehensive analysis of their occurrence and impact\non individuals. In this paper, we introduce a novel dataset named DepressionEmo\ndesigned to detect 8 emotions associated with depression by 6037 examples of\nlong Reddit user posts. This dataset was created through a majority vote over\ninputs by zero-shot classifications from pre-trained models and validating the\nquality by annotators and ChatGPT, exhibiting an acceptable level of interrater\nreliability between annotators. The correlation between emotions, their\ndistribution over time, and linguistic analysis are conducted on DepressionEmo.\nBesides, we provide several text classification methods classified into two\ngroups: machine learning methods such as SVM, XGBoost, and Light GBM; and deep\nlearning methods such as BERT, GAN-BERT, and BART. The pretrained BART model,\nbart-base allows us to obtain the highest F1- Macro of 0.76, showing its\noutperformance compared to other methods evaluated in our analysis. Across all\nemotions, the highest F1-Macro value is achieved by suicide intent, indicating\na certain value of our dataset in identifying emotions in individuals with\ndepression symptoms through text analysis. The curated dataset is publicly\navailable at: https://github.com/abuBakarSiddiqurRahman/DepressionEmo.\n",
        "title": "DepressionEmo: A novel dataset for multilabel classification of\n  depression emotions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04658",
        "abstract_url": "http://arxiv.org/abs/2401.04658",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Zhen"
            },
            {
                "last_name": "Sun",
                "first_name": "Weigao"
            },
            {
                "last_name": "Li",
                "first_name": "Dong"
            },
            {
                "last_name": "Shen",
                "first_name": "Xuyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Weixuan"
            },
            {
                "last_name": "Zhong",
                "first_name": "Yiran"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.\n",
        "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence\n  Lengths in Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04660",
        "abstract_url": "http://arxiv.org/abs/2401.04660",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Yuzhou"
            },
            {
                "last_name": "Disar\u00f2",
                "first_name": "Giorgia"
            },
            {
                "last_name": "Liu",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Sun",
                "first_name": "Jian"
            },
            {
                "last_name": "Valcher",
                "first_name": "Maria Elena"
            },
            {
                "last_name": "Wang",
                "first_name": "Gang"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Unknown inputs related to, e.g., sensor aging, modeling errors, or device\nbias, represent a major concern in wireless sensor networks, as they degrade\nthe state estimation performance. To improve the performance, unknown-input\nobservers (UIOs) have been proposed. Most of the results available to design\nUIOs are based on explicit system models, which can be difficult or impossible\nto obtain in real-world applications. Data-driven techniques, on the other\nhand, have become a viable alternative for the design and analysis of unknown\nsystems using only data. In this context, a novel data-driven distributed\nunknown-input observer (D-DUIO) for an unknown linear system is developed,\nwhich leverages solely some data collected offline, without any prior knowledge\nof the system matrices. In the paper, first, the design of a DUIO is\ninvestigated by resorting to a traditional model-based approach. By resorting\nto a Lyapunov equation, it is proved that under some conditions, the state\nestimates at all nodes of the DUIO achieve consensus and collectively converge\nto the state of the system. Moving to a data-driven approach, it is shown that\nthe input/output/state trajectories of the system are compatible with the\nequations of a D-DUIO, and this allows, under suitable assumptions, to express\nthe matrices of a possible DUIO in terms of the matrices of pre-collected data.\nThen, necessary and sufficient conditions for the existence of the proposed\nD-DUIO are given. Finally, the efficacy of the D-DUIO is illustrated by means\nof numerical examples.\n",
        "title": "Distributed Data-driven Unknown-input Observers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04662",
        "abstract_url": "http://arxiv.org/abs/2401.04662",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhou"
            },
            {
                "last_name": "Wang",
                "first_name": "Kailong"
            },
            {
                "last_name": "Ma",
                "first_name": "Kai"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuo"
            },
            {
                "last_name": "Luo",
                "first_name": "Xiapu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yajin"
            },
            {
                "last_name": "Wu",
                "first_name": "Lei"
            },
            {
                "last_name": "Bai",
                "first_name": "Guangdong"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoyu"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The dark web has emerged as the state-of-the-art solution for enhanced\nanonymity. Just like a double-edged sword, it also inadvertently becomes the\nsafety net and breeding ground for illicit activities. Among them,\ncryptocurrencies have been prevalently abused to receive illicit income while\nevading regulations. Despite the continuing efforts to combat illicit\nactivities, there is still a lack of an in-depth understanding regarding the\ncharacteristics and dynamics of cryptocurrency abuses on the dark web. In this\nwork, we conduct a multi-dimensional and systematic study to track\ncryptocurrency-related illicit activities and campaigns on the dark web. We\nfirst harvest a dataset of 4,923 cryptocurrency-related onion sites with over\n130K pages. Then, we detect and extract the illicit blockchain transactions to\ncharacterize the cryptocurrency abuses, targeting features from\nsingle/clustered addresses and illicit campaigns. Throughout our study, we have\nidentified 2,564 illicit sites with 1,189 illicit blockchain addresses, which\naccount for 90.8 BTC in revenue. Based on their inner connections, we further\nidentify 66 campaigns behind them. Our exploration suggests that illicit\nactivities on the dark web have strong correlations, which can guide us to\nidentify new illicit blockchain addresses and onions, and raise alarms at the\nearly stage of their deployment.\n",
        "title": "The Devil Behind the Mirror: Tracking the Campaigns of Cryptocurrency\n  Abuses on the Dark Web",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04666",
        "abstract_url": "http://arxiv.org/abs/2401.04666",
        "authors": [
            {
                "last_name": "Himel",
                "first_name": "Galib Muhammad Shahriar"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Masudul"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  As the most basic application and implementation of deep learning, image\nclassification has grown in popularity. Various datasets are provided by\nrenowned data science communities for benchmarking machine learning algorithms\nand pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is\nbeing used in this research for its overall acceptance and benchmark standards.\nA comparison of various pre-trained models is demonstrated by using different\ntypes of optimizers and loss functions. Hyper-parameters are changed to gain\nthe best result from a model. By applying this approach, we have got higher\naccuracy without major changes in the training model. To run the experiment, we\nused three different computer architectures: a laptop equipped with NVIDIA\nGeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a\ndesktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate\nsupremacy in terms of accuracy over the previously done experiments on this\ndataset. From this experiment, the highest accuracy which is 99.65% is gained\nusing the NASNet Large.\n",
        "title": "Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA\n  Cats and Dogs Dataset",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04669",
        "abstract_url": "http://arxiv.org/abs/2401.04669",
        "authors": [
            {
                "last_name": "Randall",
                "first_name": "Thomas"
            },
            {
                "last_name": "Koo",
                "first_name": "Jaehoon"
            },
            {
                "last_name": "Videau",
                "first_name": "Brice"
            },
            {
                "last_name": "Kruse",
                "first_name": "Michael"
            },
            {
                "last_name": "Wu",
                "first_name": "Xingfu"
            },
            {
                "last_name": "Hovland",
                "first_name": "Paul"
            },
            {
                "last_name": "Hall",
                "first_name": "Mary"
            },
            {
                "last_name": "Ge",
                "first_name": "Rong"
            },
            {
                "last_name": "Balaprakash",
                "first_name": "Prasanna"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            ""
        ],
        "abstract": "  As diverse high-performance computing (HPC) systems are built, many\nopportunities arise for applications to solve larger problems than ever before.\nGiven the significantly increased complexity of these HPC systems and\napplication tuning, empirical performance tuning, such as autotuning, has\nemerged as a promising approach in recent years. Despite its effectiveness,\nautotuning is often a computationally expensive approach. Transfer learning\n(TL)-based autotuning seeks to address this issue by leveraging the data from\nprior tuning. Current TL methods for autotuning spend significant time modeling\nthe relationship between parameter configurations and performance, which is\nineffective for few-shot (that is, few empirical evaluations) tuning on new\ntasks. We introduce the first generative TL-based autotuning approach based on\nthe Gaussian copula (GC) to model the high-performing regions of the search\nspace from prior data and then generate high-performing configurations for new\ntasks. This allows a sampling-based approach that maximizes few-shot\nperformance and provides the first probabilistic estimation of the few-shot\nbudget for effective TL-based autotuning. We compare our generative TL approach\nwith state-of-the-art autotuning techniques on several benchmarks. We find that\nthe GC is capable of achieving 64.37% of peak few-shot performance in its first\nevaluation. Furthermore, the GC model can determine a few-shot transfer budget\nthat yields up to 33.39$\\times$ speedup, a dramatic improvement over the\n20.58$\\times$ speedup using prior techniques.\n",
        "title": "Transfer-Learning-Based Autotuning Using Gaussian Copula",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04670",
        "abstract_url": "http://arxiv.org/abs/2401.04670",
        "authors": [
            {
                "last_name": "Karim",
                "first_name": "Ramin Goudarzi"
            },
            {
                "last_name": "Dulal",
                "first_name": "Dipak"
            },
            {
                "last_name": "Navasca",
                "first_name": "Carmeliza"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper explores a new version of the Levenberg-Marquardt algorithm used\nfor Tensor Canonical Polyadic (CP) decomposition with an emphasis on image\ncompression and reconstruction. Tensor computation, especially CP\ndecomposition, holds significant applications in data compression and analysis.\nIn this study, we formulate CP as a nonlinear least squares optimization\nproblem. Then, we present an iterative Levenberg-Marquardt (LM) based algorithm\nfor computing the CP decomposition. Ultimately, we test the algorithm on\nvarious datasets, including randomly generated tensors and RGB images. The\nproposed method proves to be both efficient and effective, offering a reduced\ncomputational burden when compared to the traditional Levenberg-Marquardt\ntechnique.\n",
        "title": "Modified Levenberg-Marquardt Algorithm For Tensor CP Decomposition in\n  Image Compression",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04674",
        "abstract_url": "http://arxiv.org/abs/2401.04674",
        "authors": [
            {
                "last_name": "Epstein",
                "first_name": "Charles L."
            },
            {
                "last_name": "Mazzeo",
                "first_name": "Rafe"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  This paper continues the analysis of the scattering problem for a network of\nopen wave-guides started in [arXiv:2302.04353, arXiv:2310.05816]. In this part\nwe present explicit, physically motivated radiation conditions that ensure\nuniqueness of the solution to the scattering problem. These conditions stem\nfrom a 2000 paper of A. Vasy on 3-body Schrodinger operators; we discuss\nclosely related conditions from a 1994 paper of H. Isozaki. Vasy's paper also\nproves the existence of the limiting absorption resolvents, and that the\nlimiting solutions satisfy the radiation conditions. The statements of these\nresults require a calculus of pseudodifferential operators, called the 3-body\nscattering calculus, which is briefly introduced here. We show that the\nsolutions to the model problem obtained in arXiv:2302.04353 satisfy these\nradiation conditions, which makes it possible to prove uniqueness, and\ntherefore existence, for the system of Fredholm integral equations introduced\nin that paper.\n",
        "title": "Solving the Scattering Problem for Open Wave-Guides, III: Radiation\n  Conditions and Uniqueness",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04675",
        "abstract_url": "http://arxiv.org/abs/2401.04675",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Schwartz",
                "first_name": "Moshe"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Motivated by applications in DNA storage, we study a setting in which strings\nare affected by tandem-duplication errors. In particular, we look at two\nsettings: disjoint tandem-duplication errors, and equal-length\ntandem-duplication errors. We construct codes, with positive asymptotic rate,\nfor the two settings, as well as for their combination. Our constructions are\nduplication-free codes, comprising codewords that do not contain tandem\nduplications of specific lengths. Additionally, our codes generalize previous\nconstructions, containing them as special cases.\n",
        "title": "On Duplication-Free Codes for Disjoint or Equal-Length Errors",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04680",
        "abstract_url": "http://arxiv.org/abs/2401.04680",
        "authors": [
            {
                "last_name": "Howard",
                "first_name": "Sunny"
            },
            {
                "last_name": "Norreys",
                "first_name": "Peter"
            },
            {
                "last_name": "D\u00f6pp",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Optical imaging systems are inherently limited in their resolution due to the\npoint spread function (PSF), which applies a static, yet spatially-varying,\nconvolution to the image. This degradation can be addressed via Convolutional\nNeural Networks (CNNs), particularly through deblurring techniques. However,\ncurrent solutions face certain limitations in efficiently computing\nspatially-varying convolutions. In this paper we propose CoordGate, a novel\nlightweight module that uses a multiplicative gate and a coordinate encoding\nnetwork to enable efficient computation of spatially-varying convolutions in\nCNNs. CoordGate allows for selective amplification or attenuation of filters\nbased on their spatial position, effectively acting like a locally connected\nneural network. The effectiveness of the CoordGate solution is demonstrated\nwithin the context of U-Nets and applied to the challenging problem of image\ndeblurring. The experimental results show that CoordGate outperforms\nconventional approaches, offering a more robust and spatially aware solution\nfor CNNs in various computer vision applications.\n",
        "title": "CoordGate: Efficiently Computing Spatially-Varying Convolutions in\n  Convolutional Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04682",
        "abstract_url": "http://arxiv.org/abs/2401.04682",
        "authors": [
            {
                "last_name": "De Santiago",
                "first_name": "Kylliann"
            },
            {
                "last_name": "Szafranski",
                "first_name": "Marie"
            },
            {
                "last_name": "Ambroise",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  In this work, we propose an original method for aggregating multiple\nclustering coming from different sources of information. Each partition is\nencoded by a co-membership matrix between observations. Our approach uses a\nmixture of multilayer Stochastic Block Models (SBM) to group co-membership\nmatrices with similar information into components and to partition observations\ninto different clusters, taking into account their specificities within the\ncomponents. The identifiability of the model parameters is established and a\nvariational Bayesian EM algorithm is proposed for the estimation of these\nparameters. The Bayesian framework allows for selecting an optimal number of\nclusters and components. The proposed approach is compared using synthetic data\nwith consensus clustering and tensor-based algorithms for community detection\nin large-scale complex networks. Finally, the method is utilized to analyze\nglobal food trading networks, leading to structures of interest.\n",
        "title": "Mixture of multilayer stochastic block models for multiview clustering",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04691",
        "abstract_url": "http://arxiv.org/abs/2401.04691",
        "authors": [
            {
                "last_name": "Estopinan",
                "first_name": "Joaquim"
            },
            {
                "last_name": "Servajean",
                "first_name": "Maximilien"
            },
            {
                "last_name": "Bonnet",
                "first_name": "Pierre"
            },
            {
                "last_name": "Joly",
                "first_name": "Alexis"
            },
            {
                "last_name": "Munoz",
                "first_name": "Fran\u00e7ois"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Although increasing threats on biodiversity are now widely recognised, there\nare no accurate global maps showing whether and where species assemblages are\nat risk. We hereby assess and map at kilometre resolution the conservation\nstatus of the iconic orchid family, and discuss the insights conveyed at\nmultiple scales. We introduce a new Deep Species Distribution Model trained on\n1M occurrences of 14K orchid species to predict their assemblages at global\nscale and at kilometre resolution. We propose two main indicators of the\nconservation status of the assemblages: (i) the proportion of threatened\nspecies, and (ii) the status of the most threatened species in the assemblage.\nWe show and analyze the variation of these indicators at World scale and in\nrelation to currently protected areas in Sumatra island. Global and interactive\nmaps available online show the indicators of conservation status of orchid\nassemblages, with sharp spatial variations at all scales. The highest level of\nthreat is found at Madagascar and the neighbouring islands. In Sumatra, we\nfound good correspondence of protected areas with our indicators, but\nsupplementing current IUCN assessments with status predictions results in\nalarming levels of species threat across the island. Recent advances in deep\nlearning enable reliable mapping of the conservation status of species\nassemblages on a global scale. As an umbrella taxon, orchid family provides a\nreference for identifying vulnerable ecosystems worldwide, and prioritising\nconservation actions both at international and local levels.\n",
        "title": "AI-based Mapping of the Conservation Status of Orchid Assemblages at\n  Global Scale",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04692",
        "abstract_url": "http://arxiv.org/abs/2401.04692",
        "authors": [
            {
                "last_name": "Rodrigues",
                "first_name": "Nils"
            },
            {
                "last_name": "Dennig",
                "first_name": "Frederik L."
            },
            {
                "last_name": "Brandt",
                "first_name": "Vincent"
            },
            {
                "last_name": "Keim",
                "first_name": "Daniel A."
            },
            {
                "last_name": "Weiskopf",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Scatter plots are popular for displaying 2D data, but in practice, many data\nsets have more than two dimensions. For the analysis of such multivariate data,\nit is often necessary to switch between scatter plots of different dimension\npairs, e.g., in a scatter plot matrix (SPLOM). Alternative approaches include a\n\"grand tour\" for an overview of the entire data set or creating artificial axes\nfrom dimensionality reduction (DR). A cross-cutting concern in all techniques\nis the ability of viewers to find correspondence between data points in\ndifferent views. Previous work proposed animations to preserve the mental map\nbetween view changes and to trace points as well as clusters between scatter\nplots of the same underlying data set. In this paper, we evaluate a variety of\nspline- and rotation-based view transitions in a crowdsourced user study\nfocusing on ecological validity. Using the study results, we assess each\nanimation's suitability for tracing points and clusters across view changes. We\nevaluate whether the order of horizontal and vertical rotation is relevant for\ntask accuracy. The results show that rotations with an orthographic camera or\nstaged expansion of a depth axis significantly outperform all other animation\ntechniques for the traceability of individual points. Further, we provide a\nranking of the animated transition techniques for traceability of individual\npoints. However, we could not find any significant differences for the\ntraceability of clusters. Furthermore, we identified differences by animation\ndirection that could guide further studies to determine potential confounds for\nthese differences. We publish the study data for reuse and provide the\nanimation framework as a D3.js plug-in.\n",
        "title": "Comparative Evaluation of Animated Scatter Plot Transitions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04694",
        "abstract_url": "http://arxiv.org/abs/2401.04694",
        "authors": [
            {
                "last_name": "Pashazad",
                "first_name": "Hossein"
            },
            {
                "last_name": "Song",
                "first_name": "Xiaoyu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Dynamic crack branching in unsaturated porous media holds significant\nrelevance in various fields, including geotechnical engineering, geosciences,\nand petroleum engineering. This article presents a numerical investigation into\ndynamic crack branching in unsaturated porous media using a recently developed\ncoupled micro-periporomechanics paradigm. This paradigm extends the\nperiporomechanics model by incorporating the micro-rotation of the solid\nskeleton. Within this framework, each material point is equipped with three\ndegrees of freedom: displacement, micro-rotation, and fluid pressure.\nConsistent with the Cosserat continuum theory, a length scale associated with\nthe micro-rotation of material points is inherently integrated into the model.\nThis study encompasses several key aspects: (1) Validation of the coupled\nmicro-periporomechanics paradigm for effectively modeling crack branching in\ndeformable porous media, (2) Examination of the transition from a single branch\nto multiple branches in porous media under drained conditions, (3) Simulation\nof single crack branching in unsaturated porous media under dynamic loading\nconditions, and (4) Investigation of multiple crack branching in unsaturated\nporous media under dynamic loading conditions. The numerical results obtained\nin this study are systematically analyzed to elucidate the factors that\ninfluence dynamic crack branching in porous media subjected to dynamic loading.\nFurthermore, the comprehensive numerical findings underscore the efficacy and\nrobustness of the coupled micro-periporomechanics paradigm in accurately\nmodeling dynamic crack branching in variably saturated porous media.\n",
        "title": "Modeling dynamic crack branching in unsaturated porous media through\n  multi-phase micro-periporomechanics",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04695",
        "abstract_url": "http://arxiv.org/abs/2401.04695",
        "authors": [
            {
                "last_name": "Yona",
                "first_name": "Gal"
            },
            {
                "last_name": "Aharoni",
                "first_name": "Roee"
            },
            {
                "last_name": "Geva",
                "first_name": "Mor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Factual questions typically can be answered correctly at different levels of\ngranularity. For example, both ``August 4, 1961'' and ``1961'' are correct\nanswers to the question ``When was Barack Obama born?''. Standard question\nanswering (QA) evaluation protocols, however, do not explicitly take this into\naccount and compare a predicted answer against answers of a single granularity\nlevel. In this work, we propose GRANOLA QA, a novel evaluation setting where a\npredicted answer is evaluated in terms of accuracy and informativeness against\na set of multi-granularity answers. We present a simple methodology for\nenriching existing datasets with multi-granularity answers, and create\nGRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We\nevaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,\ncalled Decoding with Response Aggregation (DRAG), that is geared towards\naligning the response granularity with the model's uncertainty. Our experiments\nshow that large language models with standard decoding tend to generate\nspecific answers, which are often incorrect. In contrast, when evaluated on\nmulti-granularity answers, DRAG yields a nearly 20 point increase in accuracy\non average, which further increases for rare entities. Overall, this reveals\nthat standard evaluation and decoding schemes may significantly underestimate\nthe knowledge encapsulated in LMs.\n",
        "title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering\n  with Multi-Granularity Answers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04700",
        "abstract_url": "http://arxiv.org/abs/2401.04700",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Jia-Chen"
            },
            {
                "last_name": "Xu",
                "first_name": "Hao-Xiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Jun-Yu"
            },
            {
                "last_name": "Lu",
                "first_name": "Pan"
            },
            {
                "last_name": "Ling",
                "first_name": "Zhen-Hua"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            },
            {
                "last_name": "Peng",
                "first_name": "Nanyun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advances in large language models (LLMs) have opened up new paradigms\nfor accessing the knowledge stored in their parameters. One critical challenge\nthat has emerged is the presence of hallucinations in LLM outputs due to false\nor outdated knowledge. Since retraining LLMs with updated information is\nresource-intensive, there has been a growing interest in model editing.\nHowever, many model editing methods, while effective in various scenarios, tend\nto overemphasize aspects such as efficacy, generalization, and locality in\nediting performance, often overlooking potential side effects on the general\nabilities of LLMs. In this paper, we raise concerns that the improvement of\nmodel factuality may come at the cost of a significant degradation of these\ngeneral abilities, which is not conducive to the sustainable development of\nLLMs. Systematically, we analyze side effects by evaluating four popular\nediting methods on two LLMs across eight representative task categories.\nExtensive empirical research reveals that model editing does improve model\nfactuality but at the expense of substantially impairing general abilities.\nTherefore, we advocate for more research efforts to minimize the loss of\ngeneral abilities acquired during LLM pre-training and to ultimately preserve\nthem during model editing.\n",
        "title": "Model Editing Can Hurt General Abilities of Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04701",
        "abstract_url": "http://arxiv.org/abs/2401.04701",
        "authors": [
            {
                "last_name": "Jacobson",
                "first_name": "John"
            },
            {
                "last_name": "Burtscher",
                "first_name": "Martin"
            },
            {
                "last_name": "Gopalakrishnan",
                "first_name": "Ganesh"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Data races are egregious parallel programming bugs on CPUs. They are even\nworse on GPUs due to the hierarchical thread and memory structure, which makes\nit possible to write code that is correctly synchronized within a thread group\nwhile not being correct across groups. Thus far, all major data-race checkers\nfor GPUs suffer from at least one of the following problems: they do not check\nraces in global memory, do not work on recent GPUs, scale poorly, have not been\nextensively tested, miss simple data races, or are not dependable without\ndetailed knowledge of the compiler.\n  Our new data-race detection tool, HiRace, overcomes these limitations. Its\nkey novelty is an innovative parallel finite-state machine that condenses an\narbitrarily long access history into a constant-length state, thus allowing it\nto handle large and long-running programs. HiRace is a dynamic tool that checks\nfor thread-group shared memory and global device memory races. It utilizes\nsource-code instrumentation, thus avoiding driver, compiler, and hardware\ndependencies. We evaluate it on a modern calibrated data-race benchmark suite.\nOn the 580 tested CUDA kernels, 346 of which contain data races, HiRace finds\nraces missed by other tools without false alarms and is more than 10 times\nfaster on average than the current state of the art, while incurring only half\nthe memory overhead.\n",
        "title": "HiRace: Accurate and Fast Source-Level Race Checking of GPU Programs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04705",
        "abstract_url": "http://arxiv.org/abs/2401.04705",
        "authors": [
            {
                "last_name": "Balogun",
                "first_name": "Emmanuel"
            },
            {
                "last_name": "Buechler",
                "first_name": "Elizabeth"
            },
            {
                "last_name": "Bhela",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Onori",
                "first_name": "Simona"
            },
            {
                "last_name": "Rajagopal",
                "first_name": "Ram"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA",
            "SE",
            ""
        ],
        "abstract": "  To enable the electrification of transportation systems, it is important to\nunderstand how technologies such as grid storage, solar photovoltaic systems,\nand control strategies can aid the deployment of electric vehicle charging at\nscale. In this work, we present EV-EcoSim, a co-simulation platform that\ncouples electric vehicle charging, battery systems, solar photovoltaic systems,\ngrid transformers, control strategies, and power distribution systems, to\nperform cost quantification and analyze the impacts of electric vehicle\ncharging on the grid. This python-based platform can run a receding horizon\ncontrol scheme for real-time operation and a one-shot control scheme for\nplanning problems, with multi-timescale dynamics for different systems to\nsimulate realistic scenarios. We demonstrate the utility of EV-EcoSim through a\ncase study focused on economic evaluation of battery size to reduce electricity\ncosts while considering impacts of fast charging on the power distribution\ngrid. We present qualitative and quantitative evaluations on the battery size\nin tabulated results. The tabulated results delineate the trade-offs between\ncandidate battery sizing solutions, providing comprehensive insights for\ndecision-making under uncertainty. Additionally, we demonstrate the\nimplications of the battery controller model fidelity on the system costs and\nshow that the fidelity of the battery controller can completely change\ndecisions made when planning an electric vehicle charging site.\n",
        "title": "EV-EcoSim: A grid-aware co-simulation platform for the design and\n  optimization of electric vehicle charging infrastructure",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04707",
        "abstract_url": "http://arxiv.org/abs/2401.04707",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Muhammad Shahbaz"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Jawad"
            },
            {
                "last_name": "Al-Dubai",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Ghaleb",
                "first_name": "Baraq"
            },
            {
                "last_name": "Pitropakis",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "Buchanan",
                "first_name": "William J."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Given the security concerns of Internet of Things (IoT) networks and limited\ncomputational resources of IoT devices, this paper presents RNA-TransCrypt, a\nnovel image encryption scheme that is not only highly secure but also efficient\nand lightweight. RNA-TransCrypt integrates the biocryptographic properties of\nRNA encoding with the non-linearity and unpredictability of chaos theory. This\nscheme introduces three novel contributions: 1) the two-base RNA encoding\nmethod, which transforms the image into RNA strands-like sequence, ensuring\nefficient scrambling; 2) the transformative substitution technique, which\ntransforms the s-box values before replacing the pixel values, and is\nresponsible for making the scheme lightweight; and 3) three mathematical\ncryptographic operations designed especially for image encryption that ensure\nthe effective transformation of the s-box values, resulting in a new outcome\neven for the same input values. These modules are key-dependent, utilizing\nchaotic keys generated by the De Jong Fractal Map and the Van der Pol\nOscillator. Extensive security analysis, including histogram analysis,\ncorrelation analysis, and the results of the statistical security parameters\nobtained from the Gray-Level Co-occurrence Matrix (GLCM) validate the efficacy\nof the proposed scheme in encrypting input images with close-to-ideal results\nof 7.997 entropy and 0.0006 correlation.\n",
        "title": "RNA-TransCrypt: Image Encryption Using Chaotic RNA Encoding, Novel\n  Transformative Substitution, and Tailored Cryptographic Operations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04714",
        "abstract_url": "http://arxiv.org/abs/2401.04714",
        "authors": [
            {
                "last_name": "Hebbar",
                "first_name": "Anish"
            },
            {
                "last_name": "Khan",
                "first_name": "Arindam"
            },
            {
                "last_name": "Sreenivas",
                "first_name": "K. V. N."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Best-Fit is one of the most prominent and practically used algorithms for the\nbin packing problem, where a set of items with associated sizes needs to be\npacked in the minimum number of unit-capacity bins. Kenyon [SODA '96] studied\nonline bin packing under random-order arrival, where the adversary chooses the\nlist of items, but the items arrive one by one according to an arrival order\ndrawn uniformly randomly from the set of all permutations of the items.\nKenyon's seminal result established an upper bound of $1.5$ and a lower bound\nof $1.08$ on the random-order ratio of Best-Fit, and it was conjectured that\nthe true ratio is $\\approx 1.15$. The conjecture, if true, will also imply that\nBest-Fit (on randomly permuted input) has the best performance guarantee among\nall the widely-used simple algorithms for (offline) bin packing. This\nconjecture has remained one of the major open problems in the area, as\nhighlighted in the recent survey on random-order models by Gupta and Singla\n[Beyond the Worst-Case Analysis of Algorithms '20]. Recently, Albers et al.\n[Algorithmica '21] improved the upper bound to $1.25$ for the special case when\nall the item sizes are greater than $1/3$, and they improve the lower bound to\n$1.1$. Ayyadevara et al. [ICALP '22] obtained an improved result for the\nspecial case when all the item sizes lie in $(1/4, 1/2]$, which corresponds to\nthe $3$-partition problem. The upper bound of $3/2$ for the general case,\nhowever, has remained unimproved.\n  In this paper, we make the first progress towards the conjecture, by showing\nthat Best-Fit achieves a random-order ratio of at most $1.5 - \\varepsilon$, for\na small constant $\\varepsilon>0$. Furthermore, we establish an improved lower\nbound of $1.144$ on the random-order ratio of Best-Fit, nearly reaching the\nconjectured ratio.\n",
        "title": "Bin Packing under Random-Order: Breaking the Barrier of 3/2",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04720",
        "abstract_url": "http://arxiv.org/abs/2401.04720",
        "authors": [
            {
                "last_name": "Roth",
                "first_name": "Benedikt"
            },
            {
                "last_name": "Koch",
                "first_name": "Valentin"
            },
            {
                "last_name": "Wagner",
                "first_name": "Sophia J."
            },
            {
                "last_name": "Schnabel",
                "first_name": "Julia A."
            },
            {
                "last_name": "Marr",
                "first_name": "Carsten"
            },
            {
                "last_name": "Peng",
                "first_name": "Tingying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To handle the large scale of whole slide images in computational pathology,\nmost approaches first tessellate the images into smaller patches, extract\nfeatures from these patches, and finally aggregate the feature vectors with\nweakly-supervised learning. The performance of this workflow strongly depends\non the quality of the extracted features. Recently, foundation models in\ncomputer vision showed that leveraging huge amounts of data through supervised\nor self-supervised learning improves feature quality and generalizability for a\nvariety of tasks. In this study, we benchmark the most popular vision\nfoundation models as feature extractors for histopathology data. We evaluate\nthe models in two settings: slide-level classification and patch-level\nclassification. We show that foundation models are a strong baseline. Our\nexperiments demonstrate that by finetuning a foundation model on a single GPU\nfor only two hours or three days depending on the dataset, we can match or\noutperform state-of-the-art feature extractors for computational pathology.\nThese findings imply that even with little resources one can finetune a feature\nextractor tailored towards a specific downstream task and dataset. This is a\nconsiderable shift from the current state, where only few institutions with\nlarge amounts of resources and datasets are able to train a feature extractor.\nWe publish all code used for training and evaluation as well as the finetuned\nmodels.\n",
        "title": "Low-resource finetuning of foundation models beats state-of-the-art in\n  histopathology",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04722",
        "abstract_url": "http://arxiv.org/abs/2401.04722",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Jun"
            },
            {
                "last_name": "Li",
                "first_name": "Feifei"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Convolutional Neural Networks (CNNs) and Transformers have been the most\npopular architectures for biomedical image segmentation, but both of them have\nlimited ability to handle long-range dependencies because of inherent locality\nor computational complexity. To address this challenge, we introduce U-Mamba, a\ngeneral-purpose network for biomedical image segmentation. Inspired by the\nState Space Sequence Models (SSMs), a new family of deep sequence models known\nfor their strong capability in handling long sequences, we design a hybrid\nCNN-SSM block that integrates the local feature extraction power of\nconvolutional layers with the abilities of SSMs for capturing the long-range\ndependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it\nto automatically adapt to various datasets without manual intervention. We\nconduct extensive experiments on four diverse tasks, including the 3D abdominal\norgan segmentation in CT and MR images, instrument segmentation in endoscopy\nimages, and cell segmentation in microscopy images. The results reveal that\nU-Mamba outperforms state-of-the-art CNN-based and Transformer-based\nsegmentation networks across all tasks. This opens new avenues for efficient\nlong-range dependency modeling in biomedical image analysis. The code, models,\nand data are publicly available at https://wanglab.ai/u-mamba.html.\n",
        "title": "U-Mamba: Enhancing Long-range Dependency for Biomedical Image\n  Segmentation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04726",
        "abstract_url": "http://arxiv.org/abs/2401.04726",
        "authors": [
            {
                "last_name": "Batagelj",
                "first_name": "Vladimir"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DS",
            "",
            ""
        ],
        "abstract": "  Large bibliographic networks are sparse -- the average node degree is small.\nThis is not necessarily true for their product -- in some cases, it can\n``explode'' (it is not sparse, increases in time and space complexity). An\napproach in such cases is to reduce the complexity of the problem by limiting\nour attention to a selected subset of important nodes and computing with\ncorresponding truncated networks. The nodes can be selected by different\ncriteria. An option is to consider the most important nodes in the derived\nnetwork -- nodes with the largest weighted degree. It turns out that the\nweighted degrees in the derived network can be computed efficiently without\ncomputing the derived network itself.\n",
        "title": "Weighted degrees and truncated derived bibliographic networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04727",
        "abstract_url": "http://arxiv.org/abs/2401.04727",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Li",
                "first_name": "Xianhang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongru"
            },
            {
                "last_name": "Xie",
                "first_name": "Cihang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The machine learning community has witnessed a drastic change in the training\npipeline, pivoted by those ''foundation models'' with unprecedented scales.\nHowever, the field of adversarial training is lagging behind, predominantly\ncentered around small model sizes like ResNet-50, and tiny and low-resolution\ndatasets like CIFAR-10. To bridge this transformation gap, this paper provides\na modern re-examination with adversarial training, investigating its potential\nbenefits when applied at scale. Additionally, we introduce an efficient and\neffective training strategy to enable adversarial training with giant models\nand web-scale data at an affordable computing cost. We denote this newly\nintroduced framework as AdvXL.\n  Empirical results demonstrate that AdvXL establishes new state-of-the-art\nrobust accuracy records under AutoAttack on ImageNet-1K. For example, by\ntraining on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to\nsubstantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and\n$l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.\nThis achievement posits AdvXL as a pioneering approach, charting a new\ntrajectory for the efficient training of robust visual representations at\nsignificantly larger scales. Our code is available at\nhttps://github.com/UCSC-VLAA/AdvXL.\n",
        "title": "Revisiting Adversarial Training at Scale",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04728",
        "abstract_url": "http://arxiv.org/abs/2401.04728",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xiyi"
            },
            {
                "last_name": "Mihajlovic",
                "first_name": "Marko"
            },
            {
                "last_name": "Wang",
                "first_name": "Shaofei"
            },
            {
                "last_name": "Prokudin",
                "first_name": "Sergey"
            },
            {
                "last_name": "Tang",
                "first_name": "Siyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multiview-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks.\n",
        "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar\n  Creation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04729",
        "abstract_url": "http://arxiv.org/abs/2401.04729",
        "authors": [
            {
                "last_name": "Spitzer",
                "first_name": "Philipp"
            },
            {
                "last_name": "Holstein",
                "first_name": "Joshua"
            },
            {
                "last_name": "Hemmer",
                "first_name": "Patrick"
            },
            {
                "last_name": "V\u00f6ssing",
                "first_name": "Michael"
            },
            {
                "last_name": "K\u00fchl",
                "first_name": "Niklas"
            },
            {
                "last_name": "Martin",
                "first_name": "Dominik"
            },
            {
                "last_name": "Satzger",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  The constantly increasing capabilities of artificial intelligence (AI) open\nnew possibilities for human-AI collaboration. One promising approach to\nleverage existing complementary capabilities is allowing humans to delegate\nindividual instances to the AI. However, enabling humans to delegate instances\neffectively requires them to assess both their own and the AI's capabilities in\nthe context of the given task. In this work, we explore the effects of\nproviding contextual information on human decisions to delegate instances to an\nAI. We find that providing participants with contextual information\nsignificantly improves the human-AI team performance. Additionally, we show\nthat the delegation behavior changes significantly when participants receive\nvarying types of contextual information. Overall, this research advances the\nunderstanding of human-AI interaction in human delegation and provides\nactionable insights for designing more effective collaborative systems.\n",
        "title": "On the Effect of Contextual Information on Human Delegation Behavior in\n  Human-AI collaboration",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04730",
        "abstract_url": "http://arxiv.org/abs/2401.04730",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Ronglai"
            },
            {
                "last_name": "Wei",
                "first_name": "Fangyun"
            },
            {
                "last_name": "Chen",
                "first_name": "Zenggui"
            },
            {
                "last_name": "Mak",
                "first_name": "Brian"
            },
            {
                "last_name": "Yang",
                "first_name": "Jiaolong"
            },
            {
                "last_name": "Tong",
                "first_name": "Xin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The objective of this paper is to develop a functional system for translating\nspoken languages into sign languages, referred to as Spoken2Sign translation.\nThe Spoken2Sign task is orthogonal and complementary to traditional sign\nlanguage to spoken language (Sign2Spoken) translation. To enable Spoken2Sign\ntranslation, we present a simple baseline consisting of three steps: 1)\ncreating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2)\nestimating a 3D sign for each sign video in the dictionary; 3) training a\nSpoken2Sign model, which is composed of a Text2Gloss translator, a sign\nconnector, and a rendering module, with the aid of the yielded gloss-3D sign\ndictionary. The translation results are then displayed through a sign avatar.\nAs far as we know, we are the first to present the Spoken2Sign task in an\noutput format of 3D signs. In addition to its capability of Spoken2Sign\ntranslation, we also demonstrate that two by-products of our approach-3D\nkeypoint augmentation and multi-view understanding-can assist in keypoint-based\nsign language understanding. Code and models will be available at\nhttps://github.com/FangyunWei/SLRT\n",
        "title": "A Simple Baseline for Spoken Language to Sign Language Translation with\n  3D Avatars",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04732",
        "abstract_url": "http://arxiv.org/abs/2401.04732",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Manpreet"
            },
            {
                "last_name": "Pasricha",
                "first_name": "Ravdeep"
            },
            {
                "last_name": "Singh",
                "first_name": "Nitish"
            },
            {
                "last_name": "Kondapalli",
                "first_name": "Ravi Prasad"
            },
            {
                "last_name": "R",
                "first_name": "Manoj"
            },
            {
                "last_name": "R",
                "first_name": "Kiran"
            },
            {
                "last_name": "Bou\u00e9",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "",
            "LG"
        ],
        "abstract": "  In this paper, we design a real-time question-answering system specifically\ntargeted for helping sellers get relevant material/documentation they can share\nlive with their customers or refer to during a call. Taking the Seismic content\nrepository as a relatively large scale example of a diverse dataset of sales\nmaterial, we demonstrate how LLM embeddings of sellers' queries can be matched\nwith the relevant content. We achieve this by engineering prompts in an\nelaborate fashion that makes use of the rich set of meta-features available for\ndocuments and sellers. Using a bi-encoder with cross-encoder re-ranker\narchitecture, we show how the solution returns the most relevant content\nrecommendations in just a few seconds even for large datasets. Our recommender\nsystem is deployed as an AML endpoint for real-time inferencing and has been\nintegrated into a Copilot interface that is now deployed in the production\nversion of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.\n",
        "title": "A case study of Generative AI in MSX Sales Copilot: Improving seller\n  productivity with a real-time question-answering system for content\n  recommendation",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04734",
        "abstract_url": "http://arxiv.org/abs/2401.04734",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Xiaofan"
            },
            {
                "last_name": "Khan",
                "first_name": "Muhammad Aadil"
            },
            {
                "last_name": "Onori",
                "first_name": "Simona"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  A key challenge that is currently hindering the widespread deployment and use\nof retired electric vehicle (EV) batteries for second-life (SL) applications is\nthe ability to accurately estimate and monitor their state of health (SOH).\nSecond-life battery systems can be sourced from different battery packs with a\nlack of knowledge of their historical usage.\n  To facilitate the on-the-field use of SL batteries, this paper introduces an\nonline adaptive health estimation strategy with guaranteed stability. This\nmethod relies exclusively on operational data that can be accessed in real-time\nfrom SL batteries. The adaptation algorithm is designed to ensure\nbounded-input-bounded-output (BIBO) stability. The effectiveness of the\nproposed approach is shown on a laboratory-aged experimental data set of\nretired EV batteries. The estimator gains are dynamically adapted to\naccommodate the distinct characteristics of each individual cell, making it a\npromising candidate for future SL battery management systems (BMS2).\n",
        "title": "Online Adaptive Data-driven State-of-health Estimation for Second-life\n  Batteries with BIBO Stability Guarantees",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04736",
        "abstract_url": "http://arxiv.org/abs/2401.04736",
        "authors": [
            {
                "last_name": "Choudhury",
                "first_name": "Tashfique Hasnine"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  The extensive use of distributed vehicle platoon controllers has resulted in\nseveral benefits for transportation systems, such as increased traffic flow,\nfuel efficiency, and decreased pollution. The rising reliance on interconnected\nsystems and communication networks, on the other hand, exposes these\ncontrollers to potential cyber-attacks, which may compromise their safety and\nfunctionality. This thesis aims to improve the security of distributed vehicle\nplatoon controllers by investigating attack scenarios and assessing their\ninfluence on system performance. Various attack techniques, including\nman-in-the-middle (MITM) and false data injection (FDI), are simulated using\nModel Predictive Control (MPC) controller to identify vulnerabilities and\nweaknesses of the platoon controller. Countermeasures are offered and tested,\nthat includes attack analysis and reinforced communication protocols using\nMachine Learning techniques for detection. The findings emphasize the\nsignificance of integrating security issues into their design and\nimplementation, which helps to construct safe and resilient distributed platoon\ncontrollers.\n",
        "title": "Exploring Attack Resilience in Distributed Platoon Controllers with\n  Model Predictive Control",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04737",
        "abstract_url": "http://arxiv.org/abs/2401.04737",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Yigang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  In recent years, various well-designed algorithms have empowered music\nplatforms to provide content based on one's preferences. Music genres are\ndefined through various aspects, including acoustic features and cultural\nconsiderations. Music genre classification works well with content-based\nfiltering, which recommends content based on music similarity to users. Given a\nconsiderable dataset, one premise is automatic annotation using machine\nlearning or deep learning methods that can effectively classify audio files.\nThe effectiveness of systems largely depends on feature and model selection, as\ndifferent architectures and features can facilitate each other and yield\ndifferent results. In this study, we conduct a comparative study investigating\nthe performances of three models: a proposed convolutional neural network\n(CNN), the VGG16 with fully connected layers (FC), and an eXtreme Gradient\nBoosting (XGBoost) approach on different features: 30-second Mel spectrogram\nand 3-second Mel-frequency cepstral coefficients (MFCCs). The results show that\nthe MFCC XGBoost model outperformed the others. Furthermore, applying data\nsegmentation in the data preprocessing phase can significantly enhance the\nperformance of the CNNs.\n",
        "title": "Music Genre Classification: A Comparative Analysis of CNN and XGBoost\n  Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04739",
        "abstract_url": "http://arxiv.org/abs/2401.04739",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            },
            {
                "last_name": "Gao",
                "first_name": "Eryang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In recent years, the recognition of free-hand sketches has remained a popular\ntask. However, in some special fields such as the military field, free-hand\nsketches are difficult to sample on a large scale. Common data augmentation and\nimage generation techniques are difficult to produce images with various\nfree-hand sketching styles. Therefore, the recognition and segmentation tasks\nin related fields are limited. In this paper, we propose a novel adversarial\ngenerative network that can accurately generate realistic free-hand sketches\nwith various styles. We explore the performance of the model, including using\nstyles randomly sampled from a prior normal distribution to generate images\nwith various free-hand sketching styles, disentangling the painters' styles\nfrom known free-hand sketches to generate images with specific styles, and\ngenerating images of unknown classes that are not in the training set. We\nfurther demonstrate with qualitative and quantitative evaluations our\nadvantages in visual quality, content accuracy, and style imitation on\nSketchIME.\n",
        "title": "Content-Conditioned Generation of Stylized Free hand Sketches",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04740",
        "abstract_url": "http://arxiv.org/abs/2401.04740",
        "authors": [
            {
                "last_name": "Chenna",
                "first_name": "Dwith"
            },
            {
                "last_name": "Bhogawar",
                "first_name": "Suyash"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Brain extraction and removal of skull artifacts from magnetic resonance\nimages (MRI) is an important preprocessing step in neuroimaging analysis. There\nare many tools developed to handle human fMRI images, which could involve\nmanual steps for verifying results from brain segmentation that makes it time\nconsuming and inefficient. In this study, we will use the segment anything\nmodel (SAM), a freely available neural network released by Meta[4], which has\nshown promising results in many generic segmentation applications. We will\nanalyze the efficiency of SAM for neuroimaging brain segmentation by removing\nskull artifacts. The results of the experiments showed promising results that\nexplore using automated segmentation algorithms for neuroimaging without the\nneed to train on custom medical imaging dataset.\n",
        "title": "Segment anything model (SAM) for brain extraction in fMRI studies",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04741",
        "abstract_url": "http://arxiv.org/abs/2401.04741",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yuanchi"
            },
            {
                "last_name": "He",
                "first_name": "Hui"
            },
            {
                "last_name": "Lei",
                "first_name": "Zhongxiang"
            },
            {
                "last_name": "Niu",
                "first_name": "Zhendong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph clustering algorithms with autoencoder structures have recently gained\npopularity due to their efficient performance and low training cost. However,\nfor existing graph autoencoder clustering algorithms based on GCN or GAT, not\nonly do they lack good generalization ability, but also the number of clusters\nclustered by such autoencoder models is difficult to determine automatically.\nTo solve this problem, we propose a new framework called Graph Clustering with\nMasked Autoencoders (GCMA). It employs our designed fusion autoencoder based on\nthe graph masking method for the fusion coding of graph. It introduces our\nimproved density-based clustering algorithm as a second decoder while decoding\nwith multi-target reconstruction. By decoding the mask embedding, our model can\ncapture more generalized and comprehensive knowledge. The number of clusters\nand clustering results can be output end-to-end while improving the\ngeneralization ability. As a nonparametric class method, extensive experiments\ndemonstrate the superiority of \\textit{GCMA} over state-of-the-art baselines.\n",
        "title": "Masked AutoEncoder for Graph Clustering without Pre-defined Cluster\n  Number k",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04744",
        "abstract_url": "http://arxiv.org/abs/2401.04744",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            },
            {
                "last_name": "Hefenbrock",
                "first_name": "Michael"
            },
            {
                "last_name": "Prenat",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Anghel",
                "first_name": "Lorena"
            },
            {
                "last_name": "Tahoori",
                "first_name": "Mehdi B."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "AR",
            "LG"
        ],
        "abstract": "  Bayesian Neural Networks (BayNNs) can inherently estimate predictive\nuncertainty, facilitating informed decision-making. Dropout-based BayNNs are\nincreasingly implemented in spintronics-based computation-in-memory\narchitectures for resource-constrained yet high-performance safety-critical\napplications. Although uncertainty estimation is important, the reliability of\nDropout generation and BayNN computation is equally important for target\napplications but is overlooked in existing works. However, testing BayNNs is\nsignificantly more challenging compared to conventional NNs, due to their\nstochastic nature. In this paper, we present for the first time the model of\nthe non-idealities of the spintronics-based Dropout module and analyze their\nimpact on uncertainty estimates and accuracy. Furthermore, we propose a testing\nframework based on repeatability ranking for Dropout-based BayNN with up to\n$100\\%$ fault coverage while using only $0.2\\%$ of training data as test\nvectors.\n",
        "title": "Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian\n  Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04746",
        "abstract_url": "http://arxiv.org/abs/2401.04746",
        "authors": [
            {
                "last_name": "Himel",
                "first_name": "Galib Muhammad Shahriar"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Masudul"
            },
            {
                "last_name": "Al-Aff",
                "first_name": "Kh Abdullah"
            },
            {
                "last_name": "Karim",
                "first_name": "Shams Ibne"
            },
            {
                "last_name": "Sikder",
                "first_name": "Md. Kabir Uddin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Skin cancer is a global health concern, necessitating early and accurate\ndiagnosis for improved patient outcomes. This study introduces a groundbreaking\napproach to skin cancer classification, employing the Vision Transformer, a\nstate-of-the-art deep learning architecture renowned for its success in diverse\nimage analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously\nannotated skin lesion images, the model undergoes preprocessing for enhanced\nrobustness. The Vision Transformer, adapted to the skin cancer classification\ntask, leverages the self-attention mechanism to capture intricate spatial\ndependencies, achieving superior performance over traditional deep learning\narchitectures. Segment Anything Model aids in precise segmentation of cancerous\nareas, attaining high IOU and Dice Coefficient. Extensive experiments highlight\nthe model's supremacy, particularly the Google-based ViT patch-32 variant,\nwhich achieves 96.15% accuracy and showcases potential as an effective tool for\ndermatologists in skin cancer diagnosis, contributing to advancements in\ndermatological practices.\n",
        "title": "Skin Cancer Segmentation and Classification Using Vision Transformer for\n  Automatic Analysis in Dermatoscopy-based Non-invasive Digital System",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04747",
        "abstract_url": "http://arxiv.org/abs/2401.04747",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junming"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Ailing"
            },
            {
                "last_name": "Li",
                "first_name": "Yu"
            },
            {
                "last_name": "Chen",
                "first_name": "Qifeng"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            "CV",
            "GR",
            ""
        ],
        "abstract": "  We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D\nExpression and Gesture generation with arbitrary length. While previous works\nfocused on co-speech gesture or expression generation individually, the joint\ngeneration of synchronized expressions and gestures remains barely explored. To\naddress this, our diffusion-based co-speech motion generation transformer\nenables uni-directional information flow from expression to gesture,\nfacilitating improved matching of joint expression-gesture distributions.\nFurthermore, we introduce an outpainting-based sampling strategy for arbitrary\nlong sequence generation in diffusion models, offering flexibility and\ncomputational efficiency. Our method provides a practical solution that\nproduces high-quality synchronized expression and gesture generation driven by\nspeech. Evaluated on two public datasets, our approach achieves\nstate-of-the-art performance both quantitatively and qualitatively.\nAdditionally, a user study confirms the superiority of DiffSHEG over prior\napproaches. By enabling the real-time generation of expressive and synchronized\nmotions, DiffSHEG showcases its potential for various applications in the\ndevelopment of digital humans and embodied agents.\n",
        "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven\n  Holistic 3D Expression and Gesture Generation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04748",
        "abstract_url": "http://arxiv.org/abs/2401.04748",
        "authors": [
            {
                "last_name": "Olisah",
                "first_name": "Chollette C."
            },
            {
                "last_name": "Trewhella",
                "first_name": "Ben"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            },
            {
                "last_name": "Smith",
                "first_name": "Melvyn L."
            },
            {
                "last_name": "Winstone",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Whitfield",
                "first_name": "E. Charles"
            },
            {
                "last_name": "Fern\u00e1ndez",
                "first_name": "Felicidad Fern\u00e1ndez"
            },
            {
                "last_name": "Duncalfe",
                "first_name": "Harriet"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Fruit ripeness estimation models have for decades depended on spectral index\nfeatures or colour-based features, such as mean, standard deviation, skewness,\ncolour moments, and/or histograms for learning traits of fruit ripeness.\nRecently, few studies have explored the use of deep learning techniques to\nextract features from images of fruits with visible ripeness cues. However, the\nblackberry (Rubus fruticosus) fruit does not show obvious and reliable visible\ntraits of ripeness when mature and therefore poses great difficulty to fruit\npickers. The mature blackberry, to the human eye, is black before, during, and\npost-ripening. To address this engineering application challenge, this paper\nproposes a novel multi-input convolutional neural network (CNN) ensemble\nclassifier for detecting subtle traits of ripeness in blackberry fruits. The\nmulti-input CNN was created from a pre-trained visual geometry group 16-layer\ndeep convolutional network (VGG16) model trained on the ImageNet dataset. The\nfully connected layers were optimized for learning traits of ripeness of mature\nblackberry fruits. The resulting model served as the base for building\nhomogeneous ensemble learners that were ensemble using the stack generalization\nensemble (SGE) framework. The input to the network is images acquired with a\nstereo sensor using visible and near-infrared (VIS-NIR) spectral filters at\nwavelengths of 700 nm and 770 nm. Through experiments, the proposed model\nachieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-field\nconditions. Further experiments reveal that machine sensory is highly and\npositively correlated to human sensory over blackberry fruit skin texture.\n",
        "title": "Convolutional Neural Network Ensemble Learning for Hyperspectral\n  Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm\n  Environment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04749",
        "abstract_url": "http://arxiv.org/abs/2401.04749",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieqiao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bo"
            },
            {
                "last_name": "peng",
                "first_name": "Junran"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "SE"
        ],
        "abstract": "  Log anomaly detection is a key component in the field of artificial\nintelligence for IT operations (AIOps). Considering log data of variant\ndomains, retraining the whole network for unknown domains is inefficient in\nreal industrial scenarios. However, previous deep models merely focused on\nextracting the semantics of log sequences in the same domain, leading to poor\ngeneralization on multi-domain logs. To alleviate this issue, we propose a\nunified Transformer-based framework for Log anomaly detection (LogFormer) to\nimprove the generalization ability across different domains, where we establish\na two-stage process including the pre-training and adapter-based tuning stage.\nSpecifically, our model is first pre-trained on the source domain to obtain\nshared semantic knowledge of log data. Then, we transfer such knowledge to the\ntarget domain via shared parameters. Besides, the Log-Attention module is\nproposed to supplement the information ignored by the log-paring. The proposed\nmethod is evaluated on three public and one real-world datasets. Experimental\nresults on multiple benchmarks demonstrate the effectiveness of our LogFormer\nwith fewer trainable parameters and lower training costs.\n",
        "title": "LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04750",
        "abstract_url": "http://arxiv.org/abs/2401.04750",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shengli"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Lin",
                "first_name": "Sen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While dust significantly affects the environmental perception of automated\nagricultural machines, the existing deep learning-based methods for dust\nremoval require further research and improvement in this area to improve the\nperformance and reliability of automated agricultural machines in agriculture.\nWe propose an end-to-end trainable learning network (DedustNet) to solve the\nreal-world agricultural dust removal task. To our knowledge, DedustNet is the\nfirst time Swin Transformer-based units have been used in wavelet networks for\nagricultural image dusting. Specifically, we present the frequency-dominated\nblock (DWTFormer block and IDWTFormer block) by adding a spatial features\naggregation scheme (SFAS) to the Swin Transformer and combining it with the\nwavelet transform, the DWTFormer block and IDWTFormer block, alleviating the\nlimitation of the global receptive field of Swin Transformer when dealing with\ncomplex dusty backgrounds. Furthermore, We propose a cross-level information\nfusion module to fuse different levels of features and effectively capture\nglobal and long-range feature relationships. In addition, we present a dilated\nconvolution module to capture contextual information guided by wavelet\ntransform at multiple scales, which combines the advantages of wavelet\ntransform and dilated convolution. Our algorithm leverages deep learning\ntechniques to effectively remove dust from images while preserving the original\nstructural and textural features. Compared to existing state-of-the-art\nmethods, DedustNet achieves superior performance and more reliable results in\nagricultural image dedusting, providing strong support for the application of\nagricultural machinery in dusty environments. Additionally, the impressive\nperformance on real-world hazy datasets and application tests highlights\nDedustNet superior generalization ability and computer vision-related\napplication performance.\n",
        "title": "DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network\n  for Agricultural Dust Removal",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04751",
        "abstract_url": "http://arxiv.org/abs/2401.04751",
        "authors": [
            {
                "last_name": "Howard",
                "first_name": "Daniel Anthony"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "PF",
            ""
        ],
        "abstract": "  Improving energy efficiency in industrial production processes is crucial for\ncompetitiveness, and compliance with climate policies. This paper introduces a\ndata-driven approach to identify optimal melting patterns in induction\nfurnaces. Through time-series K-means clustering the melting patterns could be\nclassified into distinct clusters based on temperature profiles. Using the\nelbow method, 12 clusters were identified, representing the range of melting\npatterns. Performance parameters such as melting time, energy-specific\nperformance, and carbon cost were established for each cluster, indicating\nfurnace efficiency and environmental impact. Multiple criteria decision-making\nmethods including Simple Additive Weighting, Multiplicative Exponential\nWeighting, Technique for Order of Preference by Similarity to Ideal Solution,\nmodified TOPSIS, and VlseKriterijumska Optimizacija I Kompromisno Resenje were\nutilized to determine the best-practice cluster. The study successfully\nidentified the cluster with the best performance. Implementing the best\npractice operation resulted in an 8.6 % reduction in electricity costs,\nhighlighting the potential energy savings in the foundry.\n",
        "title": "Identifying Best Practice Melting Patterns in Induction Furnaces: A\n  Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria\n  Decision Making",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04757",
        "abstract_url": "http://arxiv.org/abs/2401.04757",
        "authors": [
            {
                "last_name": "Owen",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  We investigate large language model performance across five orders of\nmagnitude of compute scaling in eleven recent model architectures. We show that\naverage benchmark performance, aggregating over many individual tasks and\nevaluations as in the commonly-used BIG-Bench dataset, is decently predictable\nas a function of training compute scale. Specifically, when extrapolating\nBIG-Bench Hard performance across one order of magnitude in compute, we observe\naverage absolute errors of 6 percentage points (pp). By contrast, extrapolation\nfor individual BIG-Bench tasks across an order of magnitude in compute yields\nhigher average errors of 18pp. Nonetheless, individual task performance remains\nsignificantly more predictable than chance. Overall, our work suggests compute\nscaling provides a promising basis to forecast AI capabilities in diverse\nbenchmarks, though predicting performance in specific tasks poses challenges.\n",
        "title": "How predictable is language model benchmark performance?",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04758",
        "abstract_url": "http://arxiv.org/abs/2401.04758",
        "authors": [
            {
                "last_name": "Gatterbauer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Dunne",
                "first_name": "Cody"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "HC",
            "LO"
        ],
        "abstract": "  Comparing relational languages by their logical expressiveness is well\nunderstood. Less well understood is how to compare relational languages by\ntheir ability to represent relational query patterns. Indeed, what are query\npatterns other than \"a certain way of writing a query\"? And how can query\npatterns be defined across procedural and declarative languages, irrespective\nof their syntax? To the best of our knowledge, we provide the first semantic\ndefinition of relational query patterns by using a variant of\nstructure-preserving mappings between the relational tables of queries. This\nformalism allows us to analyze the relative pattern expressiveness of\nrelational language fragments and create a hierarchy of languages with equal\nlogical expressiveness yet different pattern expressiveness. Notably, for the\nnon-disjunctive language fragment, we show that relational calculus can express\na larger class of patterns than the basic operators of relational algebra.\n  Our language-independent definition of query patterns opens novel paths for\nassisting database users. For example, these patterns could be leveraged to\ncreate visual query representations that faithfully represent query patterns,\nspeed up interpretation, and provide visual feedback during query editing. As a\nconcrete example, we propose Relational Diagrams, a complete and sound\ndiagrammatic representation of safe relational calculus that is provably (i)\nunambiguous, (ii) relationally complete, and (iii) able to represent all query\npatterns for unions of non-disjunctive queries. Among all diagrammatic\nrepresentations for relational queries that we are aware of, ours is the only\none with these three properties. Furthermore, our anonymously preregistered\nuser study shows that Relational Diagrams allow users to recognize patterns\nmeaningfully faster and more accurately than SQL.\n",
        "title": "On The Reasonable Effectiveness of Relational Diagrams: Explaining\n  Relational Query Patterns and the Pattern Expressiveness of Relational\n  Languages",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04771",
        "abstract_url": "http://arxiv.org/abs/2401.04771",
        "authors": [
            {
                "last_name": "Smiley",
                "first_name": "Octavious"
            },
            {
                "last_name": "Hoffmann",
                "first_name": "Till"
            },
            {
                "last_name": "Onnela",
                "first_name": "Jukka-Pekka"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            ""
        ],
        "abstract": "  Network science explores intricate connections among objects, employed in\ndiverse domains like social interactions, fraud detection, and disease spread.\nVisualization of networks facilitates conceptualizing research questions and\nforming scientific hypotheses. Networks, as mathematical high-dimensional\nobjects, require dimensionality reduction for (planar) visualization.\nVisualizing empirical networks present additional challenges. They often\ncontain false positive (spurious) and false negative (missing) edges.\nTraditional visualization methods don't account for errors in observation,\npotentially biasing interpretations. Moreover, contemporary network data\nincludes rich nodal attributes. However, traditional methods neglect these\nattributes when computing node locations. Our visualization approach aims to\nleverage nodal attribute richness to compensate for network data limitations.\nWe employ a statistical model estimating the probability of edge connections\nbetween nodes based on their covariates. We enhance the Fruchterman-Reingold\nalgorithm to incorporate estimated dyad connection probabilities, allowing\npractitioners to balance reliance on observed versus estimated edges. We\nexplore optimal smoothing levels, offering a natural way to include relevant\nnodal information in layouts. Results demonstrate the effectiveness of our\nmethod in achieving robust network visualization, providing insights for\nimproved analysis.\n",
        "title": "Network Layout Algorithm with Covariate Smoothing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04778",
        "abstract_url": "http://arxiv.org/abs/2401.04778",
        "authors": [
            {
                "last_name": "Br\u00fcck",
                "first_name": "Florian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  In this work, we provide a simulation algorithm to simulate from a\n(multivariate) characteristic function, which is only accessible in a black-box\nformat. We construct a generative neural network, whose loss function exploits\na specific representation of the Maximum-Mean-Discrepancy metric to directly\nincorporate the targeted characteristic function. The construction is universal\nin the sense that it is independent of the dimension and that it does not\nrequire any assumptions on the given characteristic function. Furthermore,\nfinite sample guarantees on the approximation quality in terms of the\nMaximum-Mean Discrepancy metric are derived. The method is illustrated in a\nshort simulation study.\n",
        "title": "Generative neural networks for characteristic functions",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04781",
        "abstract_url": "http://arxiv.org/abs/2401.04781",
        "authors": [
            {
                "last_name": "Mazaev",
                "first_name": "A. V."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In the development of materials for structural purposes, the main focus is on\nthe advantageous combination of mechanical and volume-mass properties. Due to\nthe development of production, solid plates are increasingly being replaced by\nmodern composite materials with improved properties, one of the varieties of\nwhich is layered composites with a honeycomb core. The most widespread are\nthree-layer plates with solid face layers and a hexagonal honeycomb core.\nHowever, with the development of new technologies, including 3D printing,\nhoneycombs with new geometries are gaining popularity, the mechanical\nproperties of which make it possible to obtain layered composites with improved\nfeatures. In this paper, three-layer plates with solid face layers and a\ntetrachiral honeycomb core are investigated. The influence of discretization\n(number of unit cells), relative density and thickness of the honeycomb core on\nthe stress state of three-layer composites subjected to static bending under\nvarious boundary conditions is studied. Mathematical modeling is carried out\nwithin the framework of the theory of elasticity by the finite element method\nvia three-dimensional modeling in the Comsol Multiphysics system, as well as\nusing algorithms developed by the author for analyzing the stress state of\nmultilayer plates with tetrachiral honeycombs by solving a plane problem of the\ntheory of elasticity. As a result, good agreement is shown between the\nnumerical results obtained using algorithms for solving a plane problem and via\nthree-dimensional finite element modeling in the Comsol Multiphysics system,\nwhile the numerical results are qualitatively consistent with laboratory test\ndata.\n",
        "title": "Mathematical modeling of mechanical behavior of three-layer plates with\n  tetrachiral honeycomb core",
        "date": "2023-11-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04783",
        "abstract_url": "http://arxiv.org/abs/2401.04783",
        "authors": [
            {
                "last_name": "Christlieb",
                "first_name": "Andrew J."
            },
            {
                "last_name": "Ding",
                "first_name": "Mingchang"
            },
            {
                "last_name": "Huang",
                "first_name": "Juntao"
            },
            {
                "last_name": "Krupansky",
                "first_name": "Nicholas A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We introduce a hyperbolic closure for the Grad moment expansion of the\nBhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained\non BGK's moment data. This closure is motivated by the exact closure for the\nfree streaming limit that we derived in our paper on closures in transport\n\\cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest\nmoment to the gradient of four lower moments. As with our past work, the model\npresented here learns the gradient of the highest moment in terms of the\ncoefficients of gradients for all lower ones. By necessity, this means that the\nresulting hyperbolic system is not conservative in the highest moment. For\nstability, the output layers of the NN are designed to enforce hyperbolicity\nand Galilean invariance. This ensures the model can be run outside of the\ntraining window of the NN. Unlike our previous work on radiation transport that\ndealt with linear models, the BGK model's nonlinearity demanded advanced\ntraining tools. These comprised an optimal learning rate discovery, one cycle\ntraining, batch normalization in each neural layer, and the use of the\n\\texttt{AdamW} optimizer. To address the non-conservative structure of the\nhyperbolic model, we adopt the FORCE numerical method to achieve robust\nsolutions. This results in a comprehensive computing model combining learned\nclosures with methods for solving hyperbolic models. The proposed model can\ncapture accurate moment solutions across a broad spectrum of Knudsen numbers.\nOur paper details the multi-scale model construction and is run on a range of\ntest problems.\n",
        "title": "Hyperbolic Machine Learning Moment Closures for the BGK Equations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04787",
        "abstract_url": "http://arxiv.org/abs/2401.04787",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Shih-Chi"
            },
            {
                "last_name": "Heide",
                "first_name": "A. Leonid"
            },
            {
                "last_name": "Hemati",
                "first_name": "Maziar S."
            },
            {
                "last_name": "Seiler",
                "first_name": "Peter J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Quadratic systems with lossless quadratic terms arise in many applications,\nincluding models of atmosphere and incompressible fluid flows. Such systems\nhave a trapping region if all trajectories eventually converge to and stay\nwithin a bounded set. Conditions for the existence and characterization of\ntrapping regions have been established in prior works for boundedness analysis.\nHowever, prior solutions have used non-convex optimization methods, resulting\nin conservative estimates. In this paper, we build on this prior work and\nprovide a convex semidefinite programming condition for the existence of a\ntrapping region. The condition allows precise verification or falsification of\nthe existence of a trapping region. If a trapping region exists, then we\nprovide a second semidefinite program to compute the least conservative\ntrapping region in the form of a ball. Two low-dimensional systems are provided\nas examples to illustrate the results. A third high-dimensional example is also\nincluded to demonstrate that the computation required for the analysis can be\nscaled to systems of up to $\\sim O(100)$ states. The proposed method provides a\nprecise and computationally efficient numerical approach for computing trapping\nregions. We anticipate this work will benefit future studies on modeling and\ncontrol of lossless quadratic dynamical systems.\n",
        "title": "A Convex Optimization Approach to Compute Trapping Regions for Lossless\n  Quadratic Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04791",
        "abstract_url": "http://arxiv.org/abs/2401.04791",
        "authors": [
            {
                "last_name": "Kinnari",
                "first_name": "Jouko"
            },
            {
                "last_name": "Thomas",
                "first_name": "Annika"
            },
            {
                "last_name": "Lusk",
                "first_name": "Parker"
            },
            {
                "last_name": "Kondo",
                "first_name": "Kota"
            },
            {
                "last_name": "How",
                "first_name": "Jonathan P."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  We present a novel framework for open-set Simultaneous Localization and\nMapping (SLAM) in unstructured environments that uses segmentation to create a\nmap of objects and geometric relationships between objects for localization.\nOur system consists of 1) a front-end mapping pipeline using a zero-shot\nsegmentation model to extract object masks from images and track them across\nframes to generate an object-based map and 2) a frame alignment pipeline that\nuses the geometric consistency of objects to efficiently localize within maps\ntaken in a variety of conditions. This approach is shown to be more robust to\nchanges in lighting and appearance than traditional feature-based SLAM systems\nor global descriptor methods. This is established by evaluating SOS-SLAM on the\nBatvik seasonal dataset which includes drone flights collected over a coastal\nplot of southern Finland during different seasons and lighting conditions.\nAcross flights during varying environmental conditions, our approach achieves\nhigher recall than benchmark methods with precision of 1.0. SOS-SLAM localizes\nwithin a reference map up to 14x faster than other feature based approaches and\nhas a map size less than 0.4% the size of the most compact other maps. When\nconsidering localization performance from varying viewpoints, our approach\noutperforms all benchmarks from the same viewpoint and most benchmarks from\ndifferent viewpoints. SOS-SLAM is a promising new approach for SLAM in\nunstructured environments that is robust to changes in lighting and appearance\nand is more computationally efficient than other approaches. We release our\ncode and datasets: https://acl.mit.edu/SOS-SLAM/.\n",
        "title": "SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04792",
        "abstract_url": "http://arxiv.org/abs/2401.04792",
        "authors": [
            {
                "last_name": "Hamad",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Finkenzeller",
                "first_name": "Andreas"
            },
            {
                "last_name": "K\u00fchr",
                "first_name": "Michael"
            },
            {
                "last_name": "Roberts",
                "first_name": "Andrew"
            },
            {
                "last_name": "Maennel",
                "first_name": "Olaf"
            },
            {
                "last_name": "Prevelakis",
                "first_name": "Vassilis"
            },
            {
                "last_name": "Steinhorst",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Autonomous and connected vehicles are rapidly evolving, integrating numerous\ntechnologies and software. This progress, however, has made them appealing\ntargets for cybersecurity attacks. As the risk of cyber threats escalates with\nthis advancement, the focus is shifting from solely preventing these attacks to\nalso mitigating their impact. Current solutions rely on vehicle security\noperation centers, where attack information is analyzed before deciding on a\nresponse strategy. However, this process can be time-consuming and faces\nscalability challenges, along with other issues stemming from vehicle\nconnectivity. This paper proposes a dynamic intrusion response system\nintegrated within the vehicle. This system enables the vehicle to respond to a\nvariety of incidents almost instantly, thereby reducing the need for\ninteraction with the vehicle security operation center. The system offers a\ncomprehensive list of potential responses, a methodology for response\nevaluation, and various response selection methods. The proposed solution was\nimplemented on an embedded platform. Two distinct cyberattack use cases served\nas the basis for evaluating the system. The evaluation highlights the system's\nadaptability, its ability to respond swiftly, its minimal memory footprint, and\nits capacity for dynamic system parameter adjustments. The proposed solution\nunderscores the necessity and feasibility of incorporating dynamic response\nmechanisms in smart vehicles. This is a crucial factor in ensuring the safety\nand resilience of future smart mobility.\n",
        "title": "REACT: Autonomous Intrusion Response System for Intelligent Vehicles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04795",
        "abstract_url": "http://arxiv.org/abs/2401.04795",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Gauri"
            },
            {
                "last_name": "Kapila",
                "first_name": "Ritvik"
            },
            {
                "last_name": "Chopra",
                "first_name": "Ayush"
            },
            {
                "last_name": "Raskar",
                "first_name": "Ramesh"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "LG",
            "SI",
            ""
        ],
        "abstract": "  Pandemics, notably the recent COVID-19 outbreak, have impacted both public\nhealth and the global economy. A profound understanding of disease progression\nand efficient response strategies is thus needed to prepare for potential\nfuture outbreaks. In this paper, we emphasize the potential of Agent-Based\nModels (ABM) in capturing complex infection dynamics and understanding the\nimpact of interventions. We simulate realistic pharmaceutical, behavioral, and\ndigital interventions that mirror challenges in real-world policy adoption and\nsuggest a holistic combination of these interventions for pandemic response.\nUsing these simulations, we study the trends of emergent behavior on a\nlarge-scale population based on real-world socio-demographic and geo-census\ndata from Kings County in Washington. Our analysis reveals the pivotal role of\nthe initial 100 days in dictating a pandemic's course, emphasizing the\nimportance of quick decision-making and efficient policy development. Further,\nwe highlight that investing in behavioral and digital interventions can reduce\nthe burden on pharmaceutical interventions by reducing the total number of\ninfections and hospitalizations, and by delaying the pandemic's peak. We also\ninfer that allocating the same amount of dollars towards extensive testing with\ncontact tracing and self-quarantine offers greater cost efficiency compared to\nspending the entire budget on vaccinations.\n",
        "title": "First 100 days of pandemic; an interplay of pharmaceutical, behavioral\n  and digital interventions -- A study using agent based modeling",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04799",
        "abstract_url": "http://arxiv.org/abs/2401.04799",
        "authors": [
            {
                "last_name": "Owonikoko",
                "first_name": "Waheed"
            },
            {
                "last_name": "Elsaadany",
                "first_name": "Mazen"
            },
            {
                "last_name": "Pandey",
                "first_name": "Amritanshu"
            },
            {
                "last_name": "Almassalkhi",
                "first_name": "Mads R."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  High penetration of renewable resources results in a power system with lower\ninertia and higher frequency sensitivity to power imbalances. Such systems are\nbecoming increasingly susceptible to frequency collapse during extreme\ndisturbances. Under-Frequency Load Shedding (UFLS) is a last-resort protection\nscheme and acts as an emergency brake by shedding load to arrest frequency\ndecline. Current and emerging efforts to optimize UFLS settings and frequency\nthresholds are mostly network agnostic, ignoring network spatial information.\nWith the prevalence of Distributed Energy Resources (DERs) in the\nhigh-renewable paradigm, the power grid is becoming more bidirectional, making\nsome locations in the network less effective for UFLS action than others. This\nwork proposes a Mixed Integer Linear Program that optimizes the UFLS setpoints\n(prioritizing one location over another) to minimize frequency deviation and\nload-shed for a given disturbance. The formulation considers system information\nand DER generation mix at different network locations, increasing model\nfidelity. The formulation also captures the discrete nature and practical time\ndelays and deadbands associated with UFLS using a minimal set of binary\nvariables, reducing problem complexity. We empirically validate the\noptimization approach on the dynamic IEEE 39-bus system for performance\nmetrics, including frequency nadir, steady-state frequency and total load shed.\n",
        "title": "Optimization-based Framework for Selecting Under-frequency Load Shedding\n  Parameters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04801",
        "abstract_url": "http://arxiv.org/abs/2401.04801",
        "authors": [
            {
                "last_name": "Vance",
                "first_name": "Nathan"
            },
            {
                "last_name": "Flynn",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Model architecture refinement is a challenging task in deep learning research\nfields such as remote photoplethysmography (rPPG). One architectural\nconsideration, the depth of the model, can have significant consequences on the\nresulting performance. In rPPG models that are overprovisioned with more layers\nthan necessary, redundancies exist, the removal of which can result in faster\ntraining and reduced computational load at inference time. With too few layers\nthe models may exhibit sub-optimal error rates. We apply Centered Kernel\nAlignment (CKA) to an array of rPPG architectures of differing depths,\ndemonstrating that shallower models do not learn the same representations as\ndeeper models, and that after a certain depth, redundant layers are added\nwithout significantly increased functionality. An empirical study confirms\nthese findings and shows how this method could be used to refine rPPG\narchitectures.\n",
        "title": "Refining Remote Photoplethysmography Architectures using CKA and\n  Empirical Methods",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04805",
        "abstract_url": "http://arxiv.org/abs/2401.04805",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Clifton Paul"
            },
            {
                "last_name": "Uvaydov",
                "first_name": "Daniel"
            },
            {
                "last_name": "D'Oro",
                "first_name": "Salvatore"
            },
            {
                "last_name": "Melodia",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Spectrum sensing is an essential component of modern wireless networks as it\noffers a tool to characterize spectrum usage and better utilize it. Deep\nLearning (DL) has become one of the most used techniques to perform spectrum\nsensing as they are capable of delivering high accuracy and reliability.\nHowever, current techniques suffer from ad-hoc implementations and high\ncomplexity, which makes them unsuited for practical deployment on wireless\nsystems where flexibility and fast inference time are necessary to support\nreal-time spectrum sensing. In this paper, we introduce DeepSweep, a novel\nDL-based transceiver design that allows scalable, accurate, and fast spectrum\nsensing while maintaining a high level of customizability to adapt its design\nto a broad range of application scenarios and use cases. DeepSweep is designed\nto be seamlessly integrated with well-established transceiver designs and\nleverages shallow convolutional neural network (CNN) to \"sweep\" the spectrum\nand process captured IQ samples fast and reliably without interrupting ongoing\ndemodulation and decoding operations. DeepSweep reduces training and inference\ntimes by more than 2 times and 10 times respectively, achieves up to 98 percent\naccuracy in locating spectrum activity, and produces outputs in less than 1 ms,\nthus showing that DeepSweep can be used for a broad range of spectrum sensing\napplications and scenarios.\n",
        "title": "DeepSweep: Parallel and Scalable Spectrum Sensing via Convolutional\n  Neural Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04810",
        "abstract_url": "http://arxiv.org/abs/2401.04810",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Eugene"
            },
            {
                "last_name": "Lawrie",
                "first_name": "Dawn"
            },
            {
                "last_name": "Mayfield",
                "first_name": "James"
            },
            {
                "last_name": "Oard",
                "first_name": "Douglas W."
            },
            {
                "last_name": "Miller",
                "first_name": "Scott"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Prior work on English monolingual retrieval has shown that a cross-encoder\ntrained using a large number of relevance judgments for query-document pairs\ncan be used as a teacher to train more efficient, but similarly effective,\ndual-encoder student models. Applying a similar knowledge distillation approach\nto training an efficient dual-encoder model for Cross-Language Information\nRetrieval (CLIR), where queries and documents are in different languages, is\nchallenging due to the lack of a sufficiently large training collection when\nthe query and document languages differ. The state of the art for CLIR thus\nrelies on translating queries, documents, or both from the large English MS\nMARCO training set, an approach called Translate-Train. This paper proposes an\nalternative, Translate-Distill, in which knowledge distillation from either a\nmonolingual cross-encoder or a CLIR cross-encoder is used to train a\ndual-encoder CLIR student model. This richer design space enables the teacher\nmodel to perform inference in an optimized setting, while training the student\nmodel directly for CLIR. Trained models and artifacts are publicly available on\nHuggingface.\n",
        "title": "Translate-Distill: Learning Cross-Language Dense Retrieval by\n  Translation and Distillation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04812",
        "abstract_url": "http://arxiv.org/abs/2401.04812",
        "authors": [
            {
                "last_name": "Zhai",
                "first_name": "Yaoguang"
            },
            {
                "last_name": "Qin",
                "first_name": "Zhizhen"
            },
            {
                "last_name": "Gao",
                "first_name": "Sicun"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Standard approaches for global optimization of non-convex functions, such as\nbranch-and-bound, maintain partition trees to systematically prune the domain.\nThe tree size grows exponentially in the number of dimensions. We propose new\nsampling-based methods for non-convex optimization that adapts Monte Carlo Tree\nSearch (MCTS) to improve efficiency. Instead of the standard use of visitation\ncount in Upper Confidence Bounds, we utilize numerical overapproximations of\nthe objective as an uncertainty metric, and also take into account of sampled\nestimates of first-order and second-order information. The Monte Carlo tree in\nour approach avoids the usual fixed combinatorial patterns in growing the tree,\nand aggressively zooms into the promising regions, while still balancing\nexploration and exploitation. We evaluate the proposed algorithms on\nhigh-dimensional non-convex optimization benchmarks against competitive\nbaselines and analyze the effects of the hyper parameters.\n",
        "title": "Sample-and-Bound for Non-Convex Optimization",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04820",
        "abstract_url": "http://arxiv.org/abs/2401.04820",
        "authors": [
            {
                "last_name": "\u00c7olhak",
                "first_name": "Furkan"
            },
            {
                "last_name": "Ecevit",
                "first_name": "Mert \u0130lhan"
            },
            {
                "last_name": "U\u00e7ar",
                "first_name": "Bilal Emir"
            },
            {
                "last_name": "Creutzburg",
                "first_name": "Reiner"
            },
            {
                "last_name": "Da\u011f",
                "first_name": "Hasan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The way we communicate and work has changed significantly with the rise of\nthe Internet. While it has opened up new opportunities, it has also brought\nabout an increase in cyber threats. One common and serious threat is phishing,\nwhere cybercriminals employ deceptive methods to steal sensitive\ninformation.This study addresses the pressing issue of phishing by introducing\nan advanced detection model that meticulously focuses on HTML content. Our\nproposed approach integrates a specialized Multi-Layer Perceptron (MLP) model\nfor structured tabular data and two pretrained Natural Language Processing\n(NLP) models for analyzing textual features such as page titles and content.\nThe embeddings from these models are harmoniously combined through a novel\nfusion process. The resulting fused embeddings are then input into a linear\nclassifier. Recognizing the scarcity of recent datasets for comprehensive\nphishing research, our contribution extends to the creation of an up-to-date\ndataset, which we openly share with the community. The dataset is meticulously\ncurated to reflect real-life phishing conditions, ensuring relevance and\napplicability. The research findings highlight the effectiveness of the\nproposed approach, with the CANINE demonstrating superior performance in\nanalyzing page titles and the RoBERTa excelling in evaluating page content. The\nfusion of two NLP and one MLP model,termed MultiText-LP, achieves impressive\nresults, yielding a 96.80 F1 score and a 97.18 accuracy score on our research\ndataset. Furthermore, our approach outperforms existing methods on the\nCatchPhish HTML dataset, showcasing its efficacies.\n",
        "title": "Phishing Website Detection through Multi-Model Analysis of HTML Content",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04821",
        "abstract_url": "http://arxiv.org/abs/2401.04821",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Haotian"
            },
            {
                "last_name": "Liu",
                "first_name": "Yihong"
            },
            {
                "last_name": "Ma",
                "first_name": "Chunlan"
            },
            {
                "last_name": "Sch\u00fctze",
                "first_name": "Hinrich"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Transformer-based pre-trained language models (PLMs) have achieved remarkable\nperformance in various natural language processing (NLP) tasks. However,\npre-training such models can take considerable resources that are almost only\navailable to high-resource languages. On the contrary, static word embeddings\nare easier to train in terms of computing resources and the amount of data\nrequired. In this paper, we introduce MoSECroT Model Stitching with Static Word\nEmbeddings for Crosslingual Zero-shot Transfer), a novel and challenging task\nthat is especially relevant to low-resource languages for which static word\nembeddings are available. To tackle the task, we present the first framework\nthat leverages relative representations to construct a common space for the\nembeddings of a source language PLM and the static word embeddings of a target\nlanguage. In this way, we can train the PLM on source-language training data\nand perform zero-shot transfer to the target language by simply swapping the\nembedding layer. However, through extensive experiments on two classification\ndatasets, we show that although our proposed framework is competitive with weak\nbaselines when addressing MoSECroT, it fails to achieve competitive results\ncompared with some strong baselines. In this paper, we attempt to explain this\nnegative result and provide several thoughts on possible improvement.\n",
        "title": "MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual\n  Zero-shot Transfer",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04823",
        "abstract_url": "http://arxiv.org/abs/2401.04823",
        "authors": [
            {
                "last_name": "\u0160petl\u00edk",
                "first_name": "Martin"
            },
            {
                "last_name": "B\u0159ezina",
                "first_name": "Jan"
            },
            {
                "last_name": "Laloy",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Simulating 2D flow in fractured crystalline rock requires 2D stochastic\ndiscrete-fracture matrix (DFM) models. To obtain the simulation statistics of\ninterest at an affordable computational cost, we aim to use the multilevel\nMonte Carlo method. To use this multiscale approach, one needs to upscale the\nhydraulic conductivity of the fractures by numerical homogenization. In this\nwork, we substitute numerical homogenization with a surrogate model to speed up\nthe computations. In particular, we resort to a deep convolutional neural\nnetwork (CNN) connected to a deep feed-forward neural network. The equivalent\nhydraulic conductivity tensor $K_{eq}$ is predicted based on an input spatial\nrandom field (SRF) of hydraulic conductivity tensors, cross-section, and\nhydraulic conductivity of fractures. Three independent surrogates with the same\narchitecture are trained using data from DFM models with three different ratios\nof hydraulic conductivities of fracture and bulk $K_f/K_b$. As the $K_f/K_b$\nratio increases, the multivariate $K_{eq}$ distribution becomes more complex,\nand thus, the prediction accuracy of the trained surrogates deteriorates.\nRegardless of $K_f/K_b$, however, an improvement in the prediction accuracy of\nthe trained surrogates is noted as the considered fracture density of the\nmodeling setup decreases. We also investigate prediction accuracy on input SRFs\nof different correlation lengths. Upscaling by numerical homogenization and by\nsurrogate modeling is compared on two practical problems: upscaling of the\nhydraulic conductivity tensor and groundwater flow through a given surface. We\nobtained equally accurate results for the equivalent hydraulic tensor\ncalculation of upscaled DFM models regardless of the upscaling method. For the\ngroundwater flow problem, the accuracy of quantity of interest imitates the\naccuracy of $K_{eq}$ predictions.\n",
        "title": "Deep learning surrogate for predicting hydraulic conductivity tensors\n  from stochastic discrete fracture-matrix models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04827",
        "abstract_url": "http://arxiv.org/abs/2401.04827",
        "authors": [
            {
                "last_name": "Barrett",
                "first_name": "Christopher"
            },
            {
                "last_name": "Bura",
                "first_name": "Andrei"
            },
            {
                "last_name": "Huang",
                "first_name": "Fenix"
            },
            {
                "last_name": "Reidys",
                "first_name": "Christian"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A new perspective is introduced regarding the analysis of Multiple Sequence\nAlignments (MSA), representing aligned data defined over a finite alphabet of\nsymbols. The framework is designed to produce a block decomposition of an MSA,\nwhere each block is comprised of sequences exhibiting a certain site-coherence.\nThe key component of this framework is an information theoretical potential\ndefined on pairs of sites (links) within the MSA. This potential quantifies the\nexpected drop in variation of information between the two constituent sites,\nwhere the expectation is taken with respect to all possible sub-alignments,\nobtained by removing a finite, fixed collection of rows. It is proved that the\npotential is zero for linked sites representing columns, whose symbols are in\nbijective correspondence and it is strictly positive, otherwise. It is\nfurthermore shown that the potential assumes its unique minimum for links at\nwhich each symbol pair appears with the same multiplicity. Finally, an\napplication is presented regarding anomaly detection in an MSA, composed of\ninverse fold solutions of a fixed tRNA secondary structure, where the anomalies\nare represented by inverse fold solutions of a different RNA structure.\n",
        "title": "The site linkage spectrum of data arrays",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04829",
        "abstract_url": "http://arxiv.org/abs/2401.04829",
        "authors": [
            {
                "last_name": "Akkas",
                "first_name": "Selahattin"
            },
            {
                "last_name": "Azad",
                "first_name": "Ariful"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Graph neural networks (GNNs) are popular machine learning models for graphs\nwith many applications across scientific domains. However, GNNs are considered\nblack box models, and it is challenging to understand how the model makes\npredictions. Game theory-based Shapley value approaches are popular explanation\nmethods in other domains but are not well-studied for graphs. Some studies have\nproposed Shapley value-based GNN explanations, yet they have several\nlimitations: they consider limited samples to approximate Shapley values; some\nmainly focus on small and large coalition sizes, and they are an order of\nmagnitude slower than other explanation methods, making them inapplicable to\neven moderate-size graphs. In this work, we propose GNNShap, which provides\nexplanations for edges since they provide more natural explanations for graphs\nand more fine-grained explanations. We overcome the limitations by sampling\nfrom all coalition sizes, parallelizing the sampling on GPUs, and speeding up\nmodel predictions by batching. GNNShap gives better fidelity scores and faster\nexplanations than baselines on real-world datasets.\n",
        "title": "GNNShap: Fast and Accurate GNN Explanations using Shapley Values",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04831",
        "abstract_url": "http://arxiv.org/abs/2401.04831",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Jaeyoung"
            },
            {
                "last_name": "Achermann",
                "first_name": "Florian"
            },
            {
                "last_name": "Girod",
                "first_name": "Rik"
            },
            {
                "last_name": "Lawrance",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Siegwart",
                "first_name": "Roland"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Fixed-wing aerial vehicles provide an efficient way to navigate long\ndistances or cover large areas for environmental monitoring applications. By\ndesign, they also require large open spaces due to limited maneuverability.\nHowever, strict regulatory and safety altitude limits constrain the available\nspace. Especially in complex, confined, or steep terrain, ensuring the vehicle\ndoes not enter an inevitable collision state(ICS) can be challenging. In this\nwork, we propose a strategy to find safe paths that do not enter an ICS while\nnavigating within tight altitude constraints. The method uses periodic paths to\nefficiently classify ICSs. A sampling-based planner creates collision-free and\nkinematically feasible paths that begin and end in safe periodic (circular)\npaths. We show that, in realistic terrain, using circular periodic paths can\nsimplify the goal selection process by making it yaw agnostic and constraining\nyaw. We demonstrate our approach by dynamically planning safe paths in\nreal-time while navigating steep terrain on a flight test in complex alpine\nterrain.\n",
        "title": "Safe Low-Altitude Navigation in Steep Terrain with Fixed-Wing Aerial\n  Vehicles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04836",
        "abstract_url": "http://arxiv.org/abs/2401.04836",
        "authors": [
            {
                "last_name": "Raje",
                "first_name": "Saurabh"
            },
            {
                "last_name": "Xu",
                "first_name": "Yufan"
            },
            {
                "last_name": "Rountev",
                "first_name": "Atanas"
            },
            {
                "last_name": "Valeev",
                "first_name": "Edward F."
            },
            {
                "last_name": "Sadayappan",
                "first_name": "Saday"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "DC",
            "PF"
        ],
        "abstract": "  Sparse tensor networks are commonly used to represent contractions over\nsparse tensors. Tensor contractions are higher-order analogs of matrix\nmultiplication. Tensor networks arise commonly in many domains of scientific\ncomputing and data science. After a transformation into a tree of binary\ncontractions, the network is implemented as a sequence of individual\ncontractions. Several critical aspects must be considered in the generation of\nefficient code for a contraction tree, including sparse tensor layout mode\norder, loop fusion to reduce intermediate tensors, and the interdependence of\nloop order, mode order, and contraction order. We propose CoNST, a novel\napproach that considers these factors in an integrated manner using a single\nformulation. Our approach creates a constraint system that encodes these\ndecisions and their interdependence, while aiming to produce reduced-order\nintermediate tensors via fusion. The constraint system is solved by the Z3 SMT\nsolver and the result is used to create the desired fused loop structure and\ntensor mode layouts for the entire contraction tree. This structure is lowered\nto the IR of the TACO compiler, which is then used to generate executable code.\nOur experimental evaluation demonstrates very significant (sometimes orders of\nmagnitude) performance improvements over current state-of-the-art sparse tensor\ncompiler/library alternatives.\n",
        "title": "CoNST: Code Generator for Sparse Tensor Networks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04837",
        "abstract_url": "http://arxiv.org/abs/2401.04837",
        "authors": [
            {
                "last_name": "Belgiovine",
                "first_name": "Mauro"
            },
            {
                "last_name": "Groen",
                "first_name": "Joshua"
            },
            {
                "last_name": "Sirera",
                "first_name": "Miquel"
            },
            {
                "last_name": "Tassie",
                "first_name": "Chinenye"
            },
            {
                "last_name": "Yildiz",
                "first_name": "Ayberk Yarkin"
            },
            {
                "last_name": "Trudeau",
                "first_name": "Sage"
            },
            {
                "last_name": "Ioannidis",
                "first_name": "Stratis"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Kaushik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI",
            ""
        ],
        "abstract": "  Spectrum sharing allows different protocols of the same standard (e.g.,\n802.11 family) or different standards (e.g., LTE and DVB) to coexist in\noverlapping frequency bands. As this paradigm continues to spread, wireless\nsystems must also evolve to identify active transmitters and unauthorized\nwaveforms in real time under intentional distortion of preambles, extremely low\nsignal-to-noise ratios and challenging channel conditions. We overcome\nlimitations of correlation-based preamble matching methods in such conditions\nthrough the design of T-PRIME: a Transformer-based machine learning approach.\nT-PRIME learns the structural design of transmitted frames through its\nattention mechanism, looking at sequence patterns that go beyond the preamble\nalone. The paper makes three contributions: First, it compares Transformer\nmodels and demonstrates their superiority over traditional methods and\nstate-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's\nreal-time feasibility on DeepWave's AIR-T platform. Third, it utilizes an\nextensive 66 GB dataset of over-the-air (OTA) WiFi transmissions for training,\nwhich is released along with the code for community use. Results reveal nearly\nperfect (i.e. $>98\\%$) classification accuracy under simulated scenarios,\nshowing $100\\%$ detection improvement over legacy methods in low SNR ranges,\n$97\\%$ classification accuracy for OTA single-protocol transmissions and up to\n$75\\%$ double-protocol classification accuracy in interference scenarios.\n",
        "title": "T-PRIME: Transformer-based Protocol Identification for Machine-learning\n  at the Edge",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04842",
        "abstract_url": "http://arxiv.org/abs/2401.04842",
        "authors": [
            {
                "last_name": "Arabzadeh",
                "first_name": "Negar"
            },
            {
                "last_name": "Bigdeli",
                "first_name": "Amin"
            },
            {
                "last_name": "Clarke",
                "first_name": "Charles L. A."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large language models can now directly generate answers to many factual\nquestions without referencing external sources. Unfortunately, relatively\nlittle attention has been paid to methods for evaluating the quality and\ncorrectness of these answers, for comparing the performance of one model to\nanother, or for comparing one prompt to another. In addition, the quality of\ngenerated answers are rarely directly compared to the quality of retrieved\nanswers. As models evolve and prompts are modified, we have no systematic way\nto measure improvements without resorting to expensive human judgments. To\naddress this problem we adapt standard retrieval benchmarks to evaluate answers\ngenerated by large language models. Inspired by the BERTScore metric for\nsummarization, we explore two approaches. In the first, we base our evaluation\non the benchmark relevance judgments. We empirically run experiments on how\ninformation retrieval relevance judgments can be utilized as an anchor to\nevaluating the generated answers. In the second, we compare generated answers\nto the top results retrieved by a diverse set of retrieval models, ranging from\ntraditional approaches to advanced methods, allowing us to measure improvements\nwithout human judgments. In both cases, we measure the similarity between an\nembedded representation of the generated answer and an embedded representation\nof a known, or assumed, relevant passage from the retrieval benchmark.\n",
        "title": "Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04846",
        "abstract_url": "http://arxiv.org/abs/2401.04846",
        "authors": [
            {
                "last_name": "Glinsky",
                "first_name": "Michael E."
            },
            {
                "last_name": "Sievert",
                "first_name": "Sharon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper will examine what makes a being intelligent, whether that be a\nbiological being or an artificial silicon being on a computer. Special\nattention will be paid to the being having the ability to characterize and\ncontrol a collective system of many identical conservative sub-systems\nconservatively interacting. The essence of intelligence will be found to be the\ngolden rule -- \"the collective acts as one\" or \"knowing the global consequences\nof local actions\". The flow of the collective is a small set of twinkling\ntextures, that are governed by a puppeteer who is pulling a small number of\nstrings according to a geodesic motion of least action, determined by the\nsymmetries. Controlling collective conservative systems is difficult and has\nhistorically been done by adding significant viscosity to the system to\nstabilize the desirable meta stable equilibriums of maximum performance, but it\ndegrades or destroys them in the process. There is an alternative. Once the\noptimum twinkling textures of the meta stable equilibriums are identified by\nthe intelligent being (that is the collective system is characterized), the\ncollective system can be moved by the intelligent being to the optimum\ntwinkling textures, then quickly vibrated by the intelligent being according to\nthe textures so that the collective system remains at the meta stable\nequilibrium. Well educated intelligence knows the global consequences of its\nlocal actions so that it will not take short term actions that will lead to\npoor long term outcomes. In contrast, trained intelligence or trained stupidity\nwill optimize its short term actions, leading to poor long term outcomes. Well\neducated intelligence is inherently good, but trained stupidity is inherently\nevil and should be feared. Particular attention is paid to the control and\noptimization of economic and social collectives.\n",
        "title": "The inherent goodness of well educated intelligence",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04848",
        "abstract_url": "http://arxiv.org/abs/2401.04848",
        "authors": [
            {
                "last_name": "Skiredj",
                "first_name": "Abderrahman"
            },
            {
                "last_name": "Berrada",
                "first_name": "Ismail"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Automatic diacritization of Arabic text involves adding diacritical marks\n(diacritics) to the text. This task poses a significant challenge with\nnoteworthy implications for computational processing and comprehension. In this\npaper, we introduce PTCAD (Pre-FineTuned Token Classification for Arabic\nDiacritization, a novel two-phase approach for the Arabic Text Diacritization\ntask. PTCAD comprises a pre-finetuning phase and a finetuning phase, treating\nArabic Text Diacritization as a token classification task for pre-trained\nmodels. The effectiveness of PTCAD is demonstrated through evaluations on two\nbenchmark datasets derived from the Tashkeela dataset, where it achieves\nstate-of-the-art results, including a 20\\% reduction in Word Error Rate (WER)\ncompared to existing benchmarks and superior performance over GPT-4 in ATD\ntasks.\n",
        "title": "Arabic Text Diacritization In The Age Of Transfer Learning: Token\n  Classification Is All You Need",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04849",
        "abstract_url": "http://arxiv.org/abs/2401.04849",
        "authors": [
            {
                "last_name": "Hao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Existing Spatial Interaction Models (SIMs) are limited in capturing the\ncomplex and context-aware interactions between business clusters and trade\nareas. To address the limitation, we propose a SIM-GAT model to predict\nspatiotemporal visitation flows between community business clusters and their\ntrade areas. The model innovatively represents the integrated system of\nbusiness clusters, trade areas, and transportation infrastructure within an\nurban region using a connected graph. Then, a graph-based deep learning model,\ni.e., Graph AttenTion network (GAT), is used to capture the complexity and\ninterdependencies of business clusters. We developed this model with data\ncollected from the Miami metropolitan area in Florida. We then demonstrated its\neffectiveness in capturing varying attractiveness of business clusters to\ndifferent residential neighborhoods and across scenarios with an eXplainable AI\napproach. We contribute a novel method supplementing conventional SIMs to\npredict and analyze the dynamics of inter-connected community business\nclusters. The analysis results can inform data-evidenced and place-specific\nplanning strategies helping community business clusters better accommodate\ntheir customers across scenarios, and hence improve the resilience of community\nbusinesses.\n",
        "title": "A Deep Learning Representation of Spatial Interaction Model for\n  Resilient Spatial Planning of Community Business Clusters",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04850",
        "abstract_url": "http://arxiv.org/abs/2401.04850",
        "authors": [
            {
                "last_name": "Abdelmoniem",
                "first_name": "Ahmed M."
            },
            {
                "last_name": "Bensaou",
                "first_name": "Brahim"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The peculiar congestion patterns in data centers are caused by the bursty and\ncomposite nature of traffic, the small bandwidth-delay product, and the tiny\nswitch buffers. It is not practical to modify TCP to adapt to data centers,\nespecially in public clouds where multiple congestion control protocols\ncoexist. In this work, we design a switch-based method to address such\ncongestion issues; our approach does not require any modification to TCP, which\nenables easy and seamless deployment in public data centers via switch software\nupdate. We first present a simple analysis to demonstrate the stability and\neffectiveness of the scheme, and then we discuss a hardware NetFPGA\nswitch-based prototype. The experimental results from real deployments in a\nsmall testbed cluster show the effectiveness of our approach.\n",
        "title": "FairQ: Fair and Fast Rate Allocation in Data Centers",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04851",
        "abstract_url": "http://arxiv.org/abs/2401.04851",
        "authors": [
            {
                "last_name": "Paul",
                "first_name": "Steve"
            },
            {
                "last_name": "Witter",
                "first_name": "Jhoel"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Souma"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "LG"
        ],
        "abstract": "  This paper develops a graph reinforcement learning approach to online\nplanning of the schedule and destinations of electric aircraft that comprise an\nurban air mobility (UAM) fleet operating across multiple vertiports. This fleet\nscheduling problem is formulated to consider time-varying demand, constraints\nrelated to vertiport capacity, aircraft capacity and airspace safety\nguidelines, uncertainties related to take-off delay, weather-induced route\nclosures, and unanticipated aircraft downtime. Collectively, such a formulation\npresents greater complexity, and potentially increased realism, than in\nexisting UAM fleet planning implementations. To address these complexities, a\nnew policy architecture is constructed, primary components of which include:\ngraph capsule conv-nets for encoding vertiport and aircraft-fleet states both\nabstracted as graphs; transformer layers encoding time series information on\ndemand and passenger fare; and a Multi-head Attention-based decoder that uses\nthe encoded information to compute the probability of selecting each available\ndestination for an aircraft. Trained with Proximal Policy Optimization, this\npolicy architecture shows significantly better performance in terms of daily\naveraged profits on unseen test scenarios involving 8 vertiports and 40\naircraft, when compared to a random baseline and genetic algorithm-derived\noptimal solutions, while being nearly 1000 times faster in execution than the\nlatter.\n",
        "title": "Graph Learning-based Fleet Scheduling for Urban Air Mobility under\n  Operational Constraints, Varying Demand & Uncertainties",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04852",
        "abstract_url": "http://arxiv.org/abs/2401.04852",
        "authors": [
            {
                "last_name": "Askari",
                "first_name": "Arian"
            },
            {
                "last_name": "Yang",
                "first_name": "Zihui"
            },
            {
                "last_name": "Ren",
                "first_name": "Zhaochun"
            },
            {
                "last_name": "Verberne",
                "first_name": "Suzan"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The task of answer retrieval in the legal domain aims to help users to seek\nrelevant legal advice from massive amounts of professional responses. Two main\nchallenges hinder applying existing answer retrieval approaches in other\ndomains to the legal domain: (1) a huge knowledge gap between lawyers and\nnon-professionals; and (2) a mix of informal and formal content on legal QA\nwebsites. To tackle these challenges, we propose CE_FS, a novel cross-encoder\n(CE) re-ranker based on the fine-grained structured inputs. CE_FS uses\nadditional structured information in the CQA data to improve the effectiveness\nof cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world\nbenchmark dataset for evaluating answer retrieval in the legal domain.\nExperiments conducted on LegalQA show that our proposed method significantly\noutperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel\nfinding is that adding the question tags of each question besides the question\ndescription and title into the input of cross-encoder re-rankers structurally\nboosts the rankers' effectiveness. While we study our proposed method in the\nlegal domain, we believe that our method can be applied in similar applications\nin other domains.\n",
        "title": "Answer Retrieval in Legal Community Question Answering",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04853",
        "abstract_url": "http://arxiv.org/abs/2401.04853",
        "authors": [
            {
                "last_name": "Babaian",
                "first_name": "Tamara"
            },
            {
                "last_name": "Xu",
                "first_name": "Jennifer"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Extraction of concepts and entities of interest from non-formal texts such as\nsocial media posts and informal communication is an important capability for\ndecision support systems in many domains, including healthcare, customer\nrelationship management, and others. Despite the recent advances in training\nlarge language models for a variety of natural language processing tasks, the\ndeveloped models and techniques have mainly focused on formal texts and do not\nperform as well on colloquial data, which is characterized by a number of\ndistinct challenges. In our research, we focus on the healthcare domain and\ninvestigate the problem of symptom recognition from colloquial texts by\ndesigning and evaluating several training strategies for BERT-based model\nfine-tuning. These strategies are distinguished by the choice of the base\nmodel, the training corpora, and application of term perturbations in the\ntraining data. The best-performing models trained using these strategies\noutperform the state-of-the-art specialized symptom recognizer by a large\nmargin. Through a series of experiments, we have found specific patterns of\nmodel behavior associated with the training strategies we designed. We present\ndesign principles for training strategies for effective entity recognition in\ncolloquial texts based on our findings.\n",
        "title": "Entity Recognition from Colloquial Text",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04854",
        "abstract_url": "http://arxiv.org/abs/2401.04854",
        "authors": [
            {
                "last_name": "Lederman",
                "first_name": "Harvey"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Are LLMs cultural technologies like photocopiers or printing presses, which\ntransmit information but cannot create new content? A challenge for this idea,\nwhich we call bibliotechnism, is that LLMs often do generate entirely novel\ntext. We begin by defending bibliotechnism against this challenge, showing how\nnovel text may be meaningful only in a derivative sense, so that the content of\nthis generated text depends in an important sense on the content of original\nhuman text. We go on to present a different, novel challenge for\nbibliotechnism, stemming from examples in which LLMs generate \"novel\nreference\", using novel names to refer to novel entities. Such examples could\nbe smoothly explained if LLMs were not cultural technologies but possessed a\nlimited form of agency (beliefs, desires, and intentions). According to\ninterpretationism in the philosophy of mind, a system has beliefs, desires and\nintentions if and only if its behavior is well-explained by the hypothesis that\nit has such states. In line with this view, we argue that cases of novel\nreference provide evidence that LLMs do in fact have beliefs, desires, and\nintentions, and thus have a limited form of agency.\n",
        "title": "Are Language Models More Like Libraries or Like Librarians?\n  Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04855",
        "abstract_url": "http://arxiv.org/abs/2401.04855",
        "authors": [
            {
                "last_name": "Agarwal",
                "first_name": "Saurav"
            },
            {
                "last_name": "Muthukrishnan",
                "first_name": "Ramya"
            },
            {
                "last_name": "Gosrich",
                "first_name": "Walker"
            },
            {
                "last_name": "Ribeiro",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vijay"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Coverage control is the problem of navigating a robot swarm to\ncollaboratively monitor features or a phenomenon of interest not known a\npriori. The problem is challenging in decentralized settings with robots that\nhave limited communication and sensing capabilities. This paper proposes a\nlearnable Perception-Action-Communication (LPAC) architecture for the coverage\ncontrol problem. In the proposed solution, a convolution neural network (CNN)\nprocesses localized perception of the environment; a graph neural network (GNN)\nenables communication of relevant information between neighboring robots;\nfinally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN\nin the communication module enables collaboration in the robot swarm by\ncomputing what information to communicate with neighbors and how to use\nreceived information to take appropriate actions. We train models using\nimitation learning with a centralized clairvoyant algorithm that is aware of\nthe entire environment. Evaluations show that the LPAC models outperform\nstandard decentralized and centralized coverage control algorithms. The learned\npolicy generalizes to environments different from the training dataset,\ntransfers to larger environments with an increased number of robots, and is\nrobust to noisy position estimates. The results indicate that LPAC\narchitectures are well-suited for decentralized navigation in robot swarms to\nachieve collaborative behavior.\n",
        "title": "LPAC: Learnable Perception-Action-Communication Loops with Applications\n  to Coverage Control",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04856",
        "abstract_url": "http://arxiv.org/abs/2401.04856",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Sixu"
            },
            {
                "last_name": "Chen",
                "first_name": "Shi"
            },
            {
                "last_name": "Li",
                "first_name": "Qin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Score-based Generative Models (SGMs) is one leading method in generative\nmodeling, renowned for their ability to generate high-quality samples from\ncomplex, high-dimensional data distributions. The method enjoys empirical\nsuccess and is supported by rigorous theoretical convergence properties. In\nparticular, it has been shown that SGMs can generate samples from a\ndistribution that is close to the ground-truth if the underlying score function\nis learned well, suggesting the success of SGM as a generative model. We\nprovide a counter-example in this paper. Through the sample complexity\nargument, we provide one specific setting where the score function is learned\nwell. Yet, SGMs in this setting can only output samples that are Gaussian\nblurring of training data points, mimicking the effects of kernel density\nestimation. The finding resonates a series of recent finding that reveal that\nSGMs can demonstrate strong memorization effect and fail to generate.\n",
        "title": "A Good Score Does not Lead to A Good Generative Model",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04857",
        "abstract_url": "http://arxiv.org/abs/2401.04857",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Haotian"
            },
            {
                "last_name": "Jacobs",
                "first_name": "Tim"
            },
            {
                "last_name": "Kaminsky",
                "first_name": "Philip"
            },
            {
                "last_name": "Guo",
                "first_name": "Xin"
            },
            {
                "last_name": "Li",
                "first_name": "Xinyu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Currently, Amazon relies on third parties for transportation marketplace rate\nforecasts, despite the poor quality and lack of interpretability of these\nforecasts. While transportation marketplace rates are typically very\nchallenging to forecast accurately, we have developed a novel signature-based\nstatistical technique to address these challenges and built a predictive and\nadaptive model to forecast marketplace rates. This novel technique is based on\ntwo key properties of the signature transform. The first is its universal\nnonlinearity which linearizes the feature space and hence translates the\nforecasting problem into a linear regression analysis; the second is the\nsignature kernel which allows for comparing computationally efficiently\nsimilarities between time series data. Combined, these properties allow for\nefficient feature generation and more precise identification of seasonality and\nregime switching in the forecasting process. Preliminary result by the model\nshows that this new technique leads to far superior forecast accuracy versus\ncommercially available industry models with better interpretability, even\nduring the period of Covid-19 and with the sudden onset of the Ukraine war.\n",
        "title": "Transportation Market Rate Forecast Using Signature Transform",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04858",
        "abstract_url": "http://arxiv.org/abs/2401.04858",
        "authors": [
            {
                "last_name": "Doddapaneni",
                "first_name": "Sumanth"
            },
            {
                "last_name": "Sayana",
                "first_name": "Krishna"
            },
            {
                "last_name": "Jash",
                "first_name": "Ambarish"
            },
            {
                "last_name": "Sodhi",
                "first_name": "Sukhdeep"
            },
            {
                "last_name": "Kuzmin",
                "first_name": "Dima"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "IR",
            "LG"
        ],
        "abstract": "  Modeling long histories plays a pivotal role in enhancing recommendation\nsystems, allowing to capture user's evolving preferences, resulting in more\nprecise and personalized recommendations. In this study we tackle the\nchallenges of modeling long user histories for preference understanding in\nnatural language. Specifically, we introduce a new User Embedding Module (UEM)\nthat efficiently processes user history in free-form text by compressing and\nrepresenting them as embeddings, to use them as soft prompts to a LM. Our\nexperiments demonstrate the superior capability of this approach in handling\nsignificantly longer histories compared to conventional text based prompting\nmethods, yielding substantial improvements in predictive performance. The main\ncontribution of this research is to demonstrate the ability to bias language\nmodels with user signals represented as embeddings.\n",
        "title": "User Embedding Model for Personalized Language Prompting",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04859",
        "abstract_url": "http://arxiv.org/abs/2401.04859",
        "authors": [
            {
                "last_name": "Buvoli",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Southworth",
                "first_name": "Ben S."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This work introduces a new class of Runge-Kutta methods for solving\nnonlinearly partitioned initial value problems. These new methods, named\nnonlinearly partitioned Runge-Kutta (NPRK), generalize existing additive and\ncomponent-partitioned Runge-Kutta methods, and allow one to distribute\ndifferent types of implicitness within nonlinear terms. The paper introduces\nthe NPRK framework and discusses order conditions, linear stability, and the\nderivation of implicit-explicit and implicit-implicit NPRK integrators. The\npaper concludes with numerical experiments that demonstrate the utility of NPRK\nmethods for solving viscous Burger's and the gray thermal radiation transport\nequations.\n",
        "title": "A New Class of Runge-Kutta Methods for Nonlinearly Partitioned Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04860",
        "abstract_url": "http://arxiv.org/abs/2401.04860",
        "authors": [
            {
                "last_name": "Lyou",
                "first_name": "Eunyi"
            },
            {
                "last_name": "Lee",
                "first_name": "Doyeon"
            },
            {
                "last_name": "Kim",
                "first_name": "Jooeun"
            },
            {
                "last_name": "Lee",
                "first_name": "Joonseok"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Zero-shot learning offers an efficient solution for a machine learning model\nto treat unseen categories, avoiding exhaustive data collection. Zero-shot\nSketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it\nis hard and costly to collect paired sketch-photo samples. We propose a novel\nframework that indirectly aligns sketches and photos by contrasting them\nthrough texts, removing the necessity of access to sketch-photo pairs. With an\nexplicit modality encoding learned from data, our approach disentangles\nmodality-agnostic semantics from modality-specific information, bridging the\nmodality gap and enabling effective cross-modal content retrieval within a\njoint latent space. From comprehensive experiments, we verify the efficacy of\nthe proposed model on ZS-SBIR, and it can be also applied to generalized and\nfine-grained settings.\n",
        "title": "Modality-Aware Representation Learning for Zero-shot Sketch-based Image\n  Retrieval",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04861",
        "abstract_url": "http://arxiv.org/abs/2401.04861",
        "authors": [
            {
                "last_name": "Miao",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Bai",
                "first_name": "Yang"
            },
            {
                "last_name": "Duan",
                "first_name": "Haoran"
            },
            {
                "last_name": "Huang",
                "first_name": "Yawen"
            },
            {
                "last_name": "Wan",
                "first_name": "Fan"
            },
            {
                "last_name": "Long",
                "first_name": "Yang"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yefeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The goal of our work is to generate high-quality novel views from monocular\nvideos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have\nshown impressive performance by leveraging time-varying dynamic radiation\nfields. However, these methods have limitations when it comes to accurately\nmodeling the motion of complex objects, which can lead to inaccurate and blurry\nrenderings of details. To address this limitation, we propose a novel approach\nthat builds upon a recent generalization NeRF, which aggregates nearby views\nonto new viewpoints. However, such methods are typically only effective for\nstatic scenes. To overcome this challenge, we introduce a module that operates\nin both the time and frequency domains to aggregate the features of object\nmotion. This allows us to learn the relationship between frames and generate\nhigher-quality images. Our experiments demonstrate significant improvements\nover state-of-the-art methods on dynamic scene datasets. Specifically, our\napproach outperforms existing methods in terms of both the accuracy and visual\nquality of the synthesized views.\n",
        "title": "CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from\n  Monocular Video",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04864",
        "abstract_url": "http://arxiv.org/abs/2401.04864",
        "authors": [
            {
                "last_name": "Charleston",
                "first_name": "M. A."
            },
            {
                "last_name": "Chowdhury",
                "first_name": "S. M."
            },
            {
                "last_name": "Marashdeh",
                "first_name": "Q. M."
            },
            {
                "last_name": "Straiton",
                "first_name": "B. J."
            },
            {
                "last_name": "Teixeira",
                "first_name": "F. L."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  The use of capacitance sensors for fuel mass gauging has been in\nconsideration since the early days of manned space flight. However, certain\ndifficulties arise when considering tanks in microgravity environments. Surface\ntension effects lead to fluid wetting of the interior surface of the tank,\nleaving large interior voids, while thrust/settling effects can lead to\ndispersed two-phase mixtures. With the exception of Electrical Capacitance\nVolume Tomography (ECVT), few sensing technologies are well suited for\nmeasuring annular, stratified, and dispersed fluid configurations as well as\nhandling the additional complications of mechanical installation inside a\nspherical tank. To optimize the design of future ECVT based spherical tank mass\ngauging sensors, different electrode plate layouts are considered, and their\neffect on the performance of the sensor as a fuel mass gauge is analyzed\nthrough the use of imaging and averaging techniques.\n",
        "title": "Microgravity Mass Gauging with Capacitance Sensing: Sensor Design and\n  Experiment",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04867",
        "abstract_url": "http://arxiv.org/abs/2401.04867",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Koji"
            },
            {
                "last_name": "Lala",
                "first_name": "Divesh"
            },
            {
                "last_name": "Ochi",
                "first_name": "Keiko"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Skantze",
                "first_name": "Gabriel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC"
        ],
        "abstract": "  Establishing evaluation schemes for spoken dialogue systems is important, but\nit can also be challenging. While subjective evaluations are commonly used in\nuser experiments, objective evaluations are necessary for research comparison\nand reproducibility. To address this issue, we propose a framework for\nindirectly but objectively evaluating systems based on users' behaviours. In\nthis paper, to this end, we investigate the relationship between user\nbehaviours and subjective evaluation scores in social dialogue tasks: attentive\nlistening, job interview, and first-meeting conversation. The results reveal\nthat in dialogue tasks where user utterances are primary, such as attentive\nlistening and job interview, indicators like the number of utterances and words\nplay a significant role in evaluation. Observing disfluency also can indicate\nthe effectiveness of formal tasks, such as job interview. On the other hand, in\ndialogue tasks with high interactivity, such as first-meeting conversation,\nbehaviours related to turn-taking, like average switch pause length, become\nmore important. These findings suggest that selecting appropriate user\nbehaviours can provide valuable insights for objective evaluation in each\nsocial dialogue task.\n",
        "title": "An Analysis of User Behaviours for Objectively Evaluating Spoken\n  Dialogue Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04868",
        "abstract_url": "http://arxiv.org/abs/2401.04868",
        "authors": [
            {
                "last_name": "Inoue",
                "first_name": "Koji"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bing'er"
            },
            {
                "last_name": "Ekstedt",
                "first_name": "Erik"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            },
            {
                "last_name": "Skantze",
                "first_name": "Gabriel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC",
            "SD",
            ""
        ],
        "abstract": "  A demonstration of a real-time and continuous turn-taking prediction system\nis presented. The system is based on a voice activity projection (VAP) model,\nwhich directly maps dialogue stereo audio to future voice activities. The VAP\nmodel includes contrastive predictive coding (CPC) and self-attention\ntransformers, followed by a cross-attention transformer. We examine the effect\nof the input context audio length and demonstrate that the proposed system can\noperate in real-time with CPU settings, with minimal performance degradation.\n",
        "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity\n  Projection",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04872",
        "abstract_url": "http://arxiv.org/abs/2401.04872",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuexin"
            },
            {
                "last_name": "Li",
                "first_name": "Kunming"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yongliang"
            },
            {
                "last_name": "Worrall",
                "first_name": "Stewart"
            },
            {
                "last_name": "Li",
                "first_name": "You-Fu"
            },
            {
                "last_name": "Kong",
                "first_name": "He"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "RO"
        ],
        "abstract": "  Predicting pedestrian motion trajectories is crucial for path planning and\nmotion control of autonomous vehicles. Accurately forecasting crowd\ntrajectories is challenging due to the uncertain nature of human motions in\ndifferent environments. For training, recent deep learning-based prediction\napproaches mainly utilize information like trajectory history and interactions\nbetween pedestrians, among others. This can limit the prediction performance\nacross various scenarios since the discrepancies between training datasets have\nnot been properly incorporated. To overcome this limitation, this paper\nproposes a graph transformer structure to improve prediction performance,\ncapturing the differences between the various sites and scenarios contained in\nthe datasets. In particular, a self-attention mechanism and a domain adaption\nmodule have been designed to improve the generalization ability of the model.\nMoreover, an additional metric considering cross-dataset sequences is\nintroduced for training and performance evaluation purposes. The proposed\nframework is validated and compared against existing methods using popular\npublic datasets, i.e., ETH and UCY. Experimental results demonstrate the\nimproved performance of our proposed scheme.\n",
        "title": "Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04874",
        "abstract_url": "http://arxiv.org/abs/2401.04874",
        "authors": [
            {
                "last_name": "Mu",
                "first_name": "Xinying"
            },
            {
                "last_name": "Kon",
                "first_name": "Mark"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  A machine learning (ML) feature network is a graph that connects ML features\nin learning tasks based on their similarity. This network representation allows\nus to view feature vectors as functions on the network. By leveraging function\noperations from Fourier analysis and from functional analysis, one can easily\ngenerate new and novel features, making use of the graph structure imposed on\nthe feature vectors. Such network structures have previously been studied\nimplicitly in image processing and computational biology. We thus describe\nfeature networks as graph structures imposed on feature vectors, and provide\napplications in machine learning. One application involves graph-based\ngeneralizations of convolutional neural networks, involving structured deep\nlearning with hierarchical representations of features that have varying depth\nor complexity. This extends also to learning algorithms that are able to\ngenerate useful new multilevel features. Additionally, we discuss the use of\nfeature networks to engineer new features, which can enhance the expressiveness\nof the model. We give a specific example of a deep tree-structured feature\nnetwork, where hierarchical connections are formed through feature clustering\nand feed-forward learning. This results in low learning complexity and\ncomputational efficiency. Unlike \"standard\" neural features which are limited\nto modulated (thresholded) linear combinations of adjacent ones, feature\nnetworks offer more general feedforward dependencies among features. For\nexample, radial basis functions or graph structure-based dependencies between\nfeatures can be utilized.\n",
        "title": "Feature Network Methods in Machine Learning and Applications",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04875",
        "abstract_url": "http://arxiv.org/abs/2401.04875",
        "authors": [
            {
                "last_name": "Kobayashi",
                "first_name": "Tsutomu"
            },
            {
                "last_name": "Bondu",
                "first_name": "Martin"
            },
            {
                "last_name": "Ishikawa",
                "first_name": "Fuyuki"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring the safety of autonomous vehicles (AVs) is the key requisite for\ntheir acceptance in society. This complexity is the core challenge in formally\nproving their safety conditions with AI-based black-box controllers and\nsurrounding objects under various traffic scenarios. This paper describes our\nstrategy and experience in modelling, deriving, and proving the safety\nconditions of AVs with the Event-B refinement mechanism to reduce complexity.\nOur case study targets the state-of-the-art model of goal-aware\nresponsibility-sensitive safety to argue over interactions with surrounding\nvehicles. We also employ the Simplex architecture to involve advanced black-box\nAI controllers. Our experience has demonstrated that the refinement mechanism\ncan be effectively used to gradually develop the complex system over scenario\nvariations.\n",
        "title": "Formal Modelling of Safety Architecture for Responsibility-Aware\n  Autonomous Vehicle via Event-B Refinement",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04881",
        "abstract_url": "http://arxiv.org/abs/2401.04881",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Zi"
            },
            {
                "last_name": "Hua",
                "first_name": "Nan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As LLMs have become capable of processing more complex types of inputs,\nresearchers have recently studied how to efficiently and affordably process\npossibly arbitrarily long sequences. One effective approach is to use a FIFO\nmemory to store keys and values of an attention sublayer from past chunks to\nallow subsequent queries to attend. However, this approach requires a large\nmemory and/or takes into the consideration the specific LM architecture.\nMoreover, due to the causal nature between the key-values in prior context and\nthe queries at present, this approach cannot be extended to bidirectional\nattention such as in an encoder-decoder or PrefixLM decoder-only architecture.\nIn this paper, we propose to use eviction policies, such as LRA and LFA, to\nreduce the memory size and adapt to various architectures, and we also propose\nthe Attendre layer, a wait-to-attend mechanism by retrieving the key-value\nmemory (K/V memory) with evicted queries in the query memory (Q memory). As a\nfirst step, we evaluate this method in the context length extension setup using\nthe TriviaQA reading comprehension task, and show the effectiveness of the\napproach.\n",
        "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in\n  Memory-Based Transformers for Long Context Processing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04883",
        "abstract_url": "http://arxiv.org/abs/2401.04883",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Manqing"
            },
            {
                "last_name": "Ting",
                "first_name": "Paishun"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yijian"
            },
            {
                "last_name": "Xu",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Julia"
            },
            {
                "last_name": "Lin",
                "first_name": "Jianzhe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancements in large language models (LLMs) have provided a new\navenue for chatbot development, while most existing research has primarily\ncentered on single-user chatbots that focus on deciding \"What\" to answer after\nuser inputs. In this paper, we identified that multi-user chatbots have more\ncomplex 3W design dimensions -- \"What\" to say, \"When\" to respond, and \"Who\" to\nanswer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an\nLLM-based framework for chatbots specifically designed for group discussions.\nMUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and\nUtterance Strategies Arbitrator. These modules jointly determine suitable\nresponse contents, timings, and the appropriate recipients. To make the\noptimizing process for MUCA easier, we further propose an LLM-based Multi-User\nSimulator (MUS) that can mimic real user behavior. This enables faster\nsimulation of a conversation between the chatbot and simulated users, making\nthe early development of the chatbot framework much more efficient. MUCA\ndemonstrates effectiveness, including appropriate chime-in timing, relevant\ncontent, and positive user engagement, in goal-oriented conversations with a\nsmall to medium number of participants, as evidenced by case studies and\nexperimental results from user studies.\n",
        "title": "Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate\n  Group Conversations",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04884",
        "abstract_url": "http://arxiv.org/abs/2401.04884",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junren"
            },
            {
                "last_name": "Scarlett",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "DM",
            "",
            ""
        ],
        "abstract": "  In recent years, the mathematical limits and algorithmic bounds for\nprobabilistic group testing having become increasingly well-understood, with\nexact asymptotic thresholds now being known in general scaling regimes for the\nnoiseless setting. In the noisy setting where each test outcome is flipped with\nconstant probability, there have been similar developments, but the overall\nunderstanding has lagged significantly behind the noiseless setting. In this\npaper, we substantially narrow this gap by deriving exact asymptotic thresholds\nfor the noisy setting under two widely-studied random test designs: i.i.d.\nBernoulli and near-constant tests-per-item. These thresholds are established by\ncombining components of an existing information-theoretic threshold decoder\nwith a novel analysis of maximum-likelihood decoding (upper bounds), and\nderiving a novel set of impossibility results by analyzing certain failure\nevents for optimal maximum-likelihood decoding (lower bounds). Our results show\nthat existing algorithmic upper bounds for the noisy setting are strictly\nsuboptimal, and leave open the interesting question of whether our thresholds\ncan be attained using computationally efficient algorithms.\n",
        "title": "Exact Thresholds for Noisy Non-Adaptive Group Testing",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04885",
        "abstract_url": "http://arxiv.org/abs/2401.04885",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Xie",
                "first_name": "Conghui"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Ding",
                "first_name": "Cunsheng"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Sum-rank codes have known applications in the multishot network coding, the\ndistributed storage and the construction of space-time codes. U.\nMart\\'{\\i}nez-Pe\\~{n}as introduced the cyclic-skew-cyclic sum-rank codes and\nproposed the BCH bound on the cyclic-skew-cyclic sum-rank codes in his paper\npublished in IEEE Trans. Inf. Theory, vol. 67, no. 8, 2021. Afterwards, many\nsum-rank BCH codes with lower bounds on their dimensions and minimum sum-rank\ndistances were constructed. Sum-rank Hartmann-Tzeng bound and sum-rank Roos\nbound on cyclic-skew-cyclic codes were proposed and proved by G. N. Alfarano,\nF. J. Lobillo, A. Neri, and A. Wachter-Zeh in 2022. In this paper, cyclic,\nnegacyclic and constacyclic sum-rank codes are introduced and a direct\nconstruction of cyclic, negacyclic and constacyclic sum-rank codes of the\nmatrix size $m \\times m$ from cyclic, negacyclic and constacyclic codes over\n${\\bf F}_{q^m}$ in the Hamming metric is proposed. The cyclic-skew-cylic\nsum-rank codes are special cyclic sum-rank codes. In addition, BCH and\nHartmann-Tzeng bounds for a type of cyclic sum-rank codes are developed.\nSpecific constructions of cyclic, negacyclic and constacyclic sum-rank codes\nwith known dimensions and controllable minimum sum-rank distances are proposed.\nMoreover, many distance-optimal binary sum-rank codes and an infinite family of\ndistance-optimal binary cyclic sum-rank codes with minimum sum-rank distance\nfour are constructed. This is the first infinite family of distance-optimal\nsum-rank codes with minimum sum-rank distance four in the literature.\n",
        "title": "Cyclic and Negacyclic Sum-Rank Codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04887",
        "abstract_url": "http://arxiv.org/abs/2401.04887",
        "authors": [
            {
                "last_name": "Escamilla",
                "first_name": "Emily"
            },
            {
                "last_name": "Klein",
                "first_name": "Martin"
            },
            {
                "last_name": "Cooper",
                "first_name": "Talya"
            },
            {
                "last_name": "Rampin",
                "first_name": "Vicky"
            },
            {
                "last_name": "Weigle",
                "first_name": "Michele C."
            },
            {
                "last_name": "Nelson",
                "first_name": "Michael L."
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  One in five arXiv articles published in 2021 contained a URI to a Git Hosting\nPlatform (GHP), which demonstrates the growing prevalence of GHP URIs in\nscholarly publications. However, GHP URIs are vulnerable to the same reference\nrot that plagues the Web at large. The disappearance of software hosting\nplatforms, like Gitorious and Google Code, and the source code they contain\nthreatens research reproducibility. Archiving the source code and development\nhistory available in GHPs enables the long-term reproducibility of research.\nSoftware Heritage and Web archives contain archives of GHP URI resources.\nHowever, are the GHP URIs referenced by scholarly publications contained within\nthe Software Heritage and Web archive collections? We analyzed a dataset of GHP\nURIs extracted from scholarly publications to determine (1) is the URI still\npublicly available on the live Web?, (2) has the URI been archived by Software\nHeritage?, and (3) has the URI been archived by Web archives? Of all GHP URIs,\nwe found that 93.98% were still publicly available on the live Web, 68.39% had\nbeen archived by Software Heritage, and 81.43% had been archived by Web\narchives.\n",
        "title": "Cited But Not Archived: Analyzing the Status of Code References in\n  Scholarly Articles",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04890",
        "abstract_url": "http://arxiv.org/abs/2401.04890",
        "authors": [
            {
                "last_name": "Lachapelle",
                "first_name": "S\u00e9bastien"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Pau Rodr\u00edguez"
            },
            {
                "last_name": "Sharma",
                "first_name": "Yash"
            },
            {
                "last_name": "Everett",
                "first_name": "Katie"
            },
            {
                "last_name": "Priol",
                "first_name": "R\u00e9mi Le"
            },
            {
                "last_name": "Lacoste",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Lacoste-Julien",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  This work introduces a novel principle for disentanglement we call mechanism\nsparsity regularization, which applies when the latent factors of interest\ndepend sparsely on observed auxiliary variables and/or past latent factors. We\npropose a representation learning method that induces disentanglement by\nsimultaneously learning the latent factors and the sparse causal graphical\nmodel that explains them. We develop a nonparametric identifiability theory\nthat formalizes this principle and shows that the latent factors can be\nrecovered by regularizing the learned causal graph to be sparse. More\nprecisely, we show identifiablity up to a novel equivalence relation we call\n\"consistency\", which allows some latent factors to remain entangled (hence the\nterm partial disentanglement). To describe the structure of this entanglement,\nwe introduce the notions of entanglement graphs and graph preserving functions.\nWe further provide a graphical criterion which guarantees complete\ndisentanglement, that is identifiability up to permutations and element-wise\ntransformations. We demonstrate the scope of the mechanism sparsity principle\nas well as the assumptions it relies on with several worked out examples. For\ninstance, the framework shows how one can leverage multi-node interventions\nwith unknown targets on the latent factors to disentangle them. We further draw\nconnections between our nonparametric results and the now popular exponential\nfamily assumption. Lastly, we propose an estimation procedure based on\nvariational autoencoders and a sparsity constraint and demonstrate it on\nvarious synthetic datasets. This work is meant to be a significantly extended\nversion of Lachapelle et al. (2022).\n",
        "title": "Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse\n  Actions, Interventions and Sparse Temporal Dependencies",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04898",
        "abstract_url": "http://arxiv.org/abs/2401.04898",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Bingchao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Recently, various Large Language Models (LLMs) evaluation datasets have\nemerged, but most of them have issues with distorted rankings and difficulty in\nmodel capabilities analysis. Addressing these concerns, this paper introduces\nANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes\n\\textit{Keypoint} categorization standard for the first time, each question in\nANGO can correspond to multiple keypoints, effectively enhancing\ninterpretability of evaluation results. Base on performance of real humans, we\nbuild a quantifiable question difficulty standard and divide ANGO questions\ninto 9 difficulty levels, which provide more precise guidance for model\ntraining. To minimize data leakage impact and fully leverage ANGO's innovative\nfeatures, we have engineered exclusive sampling strategies and a new evaluation\nframework that support swift testset iteration. Our experiments demonstrate\nthat ANGO poses a stronger challenge to models and reveals more details in\nevaluation result compared to existing benchmarks.\n",
        "title": "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language\n  Models In Chinese Domain",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04900",
        "abstract_url": "http://arxiv.org/abs/2401.04900",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Mengmeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Fan"
            },
            {
                "last_name": "Bu",
                "first_name": "Yude"
            },
            {
                "last_name": "Li",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Yi",
                "first_name": "Zhenping"
            },
            {
                "last_name": "Liu",
                "first_name": "Meng"
            },
            {
                "last_name": "Kong",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  The age and mass of red giants are essential for understanding the structure\nand evolution of the Milky Way. Traditional isochrone methods for these\nestimations are inherently limited due to overlapping isochrones in the\nHertzsprung-Russell diagram, while asteroseismology, though more precise,\nrequires high-precision, long-term observations. In response to these\nchallenges, we developed a novel framework, Spectral Transformer (SPT), to\npredict the age and mass of red giants aligned with asteroseismology from their\nspectra. A key component of SPT, the Multi-head Hadamard Self-Attention\nmechanism, designed specifically for spectra, can capture complex relationships\nacross different wavelength. Further, we introduced a Mahalanobis\ndistance-based loss function to address scale imbalance and interaction mode\nloss, and incorporated Monte Carlo dropout for quantitative analysis of\nprediction uncertainty.Trained and tested on 3,880 red giant spectra from\nLAMOST, the SPT achieved remarkable age and mass estimations with average\npercentage errors of 17.64% and 6.61%, respectively, and provided uncertainties\nfor each corresponding prediction. The results significantly outperform those\nof traditional machine learning algorithms and demonstrate a high level of\nconsistency with asteroseismology methods and isochrone fitting techniques. In\nthe future, our work will leverage datasets from the Chinese Space Station\nTelescope and the Large Synoptic Survey Telescope to enhance the precision of\nthe model and broaden its applicability in the field of astronomy and\nastrophysics.\n",
        "title": "SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04903",
        "abstract_url": "http://arxiv.org/abs/2401.04903",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jianqiao"
            },
            {
                "last_name": "Su",
                "first_name": "Yudi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Zeng",
                "first_name": "Zequn"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhengjue"
            },
            {
                "last_name": "Chen",
                "first_name": "Bo"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Captioning (VC) is a challenging multi-modal task since it requires\ndescribing the scene in language by understanding various and complex videos.\nFor machines, the traditional VC follows the\n\"imaging-compression-decoding-and-then-captioning\" pipeline, where compression\nis pivot for storage and transmission. However, in such a pipeline, some\npotential shortcomings are inevitable, i.e., information redundancy resulting\nin low efficiency and information loss during the sampling process for\ncaptioning. To address these problems, in this paper, we propose a novel VC\npipeline to generate captions directly from the compressed measurement, which\ncan be captured by a snapshot compressive sensing camera and we dub our model\nSnapCap. To be more specific, benefiting from the signal simulation, we have\naccess to obtain abundant measurement-video-annotation data pairs for our\nmodel. Besides, to better extract language-related visual representations from\nthe compressed measurement, we propose to distill the knowledge from videos via\na pre-trained CLIP with plentiful language-vision associations to guide the\nlearning of our SnapCap. To demonstrate the effectiveness of SnapCap, we\nconduct experiments on two widely-used VC datasets. Both the qualitative and\nquantitative results verify the superiority of our pipeline over conventional\nVC pipelines. In particular, compared to the \"caption-after-reconstruction\"\nmethods, our SnapCap can run at least 3$\\times$ faster, and achieve better\ncaption results.\n",
        "title": "SnapCap: Efficient Snapshot Compressive Video Captioning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04904",
        "abstract_url": "http://arxiv.org/abs/2401.04904",
        "authors": [
            {
                "last_name": "Akar",
                "first_name": "Nail"
            },
            {
                "last_name": "Liyanaarachchi",
                "first_name": "Sahan"
            },
            {
                "last_name": "Ulukus",
                "first_name": "Sennur"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI",
            "PF"
        ],
        "abstract": "  We study cyclic scheduling for generate-at-will (GAW) multi-source status\nupdate systems with heterogeneous service times and packet drop probabilities,\nwith the aim of minimizing the weighted sum age of information (AoI), called\nsystem AoI, or the weighted sum peak AoI (PAoI), called system PAoI. In\nparticular, we obtain well-performing cyclic schedulers which can easily scale\nto thousands of information sources and which also have low online\nimplementation complexity. The proposed schedulers are comparatively studied\nagainst existing scheduling algorithms in terms of computational load and\nsystem AoI/PAoI performance, to validate their effectiveness.\n",
        "title": "Scalable Cyclic Schedulers for Age of Information Optimization in\n  Large-Scale Status Update Systems",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04906",
        "abstract_url": "http://arxiv.org/abs/2401.04906",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xinxin"
            },
            {
                "last_name": "Gao",
                "first_name": "Lei"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Device-to-device (D2D) technology is one of the key research areas in 5G/6G\nnetworks, and full-duplex (FD) D2D will further enhance its spectral efficiency\n(SE). In recent years, deep learning approaches have shown remarkable\nperformance in D2D resource allocation tasks. However, most schemes only model\nthe channel state information (CSI) as an independent feature, neglecting the\nspatial relationships among multiple channels and users within the scenario. In\nthis paper, we first design an objective function for FD D2D communication\nresource allocation, which aims to maximize the SE of D2D users while ensuring\nthe minimal required SE of cellular users. Then, considering the complex CSI\nconstituted by all the users in different channels as a three-dimensional\nvector, a centralized resource allocation model based on multi-dimensional\nspatial convolutional networks and attention mechanisms (SP-Conv-Att) is\nproposed. To alleviate the burden of base station, we develop two distributed\nmodels, Dist-Att and Dist-Att-Conv, to facilitate users to perform channel and\npower allocation locally, based on attention and multi-user convolutional\nnetworks respectively. Numerical results demonstrate that our models outperform\ntraditional schemes and recent deep neural network models, significantly\napproximating the optimal solution computed by exhaustive algorithm with\nextremely low latency.\n",
        "title": "Deep Learning Based Resource Allocation for Full-duplex Device-to-Device\n  Communication",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04908",
        "abstract_url": "http://arxiv.org/abs/2401.04908",
        "authors": [
            {
                "last_name": "Mei",
                "first_name": "Haoran"
            },
            {
                "last_name": "Peng",
                "first_name": "Limei"
            },
            {
                "last_name": "Ho",
                "first_name": "Pin-Han"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  Grant-free access (GFA) has been envisioned to play an active role in massive\nMachine Type Communication (mMTC) under 5G and Beyond mobile systems, which\ntargets at achieving significant reduction of signaling overhead and access\nlatency in the presence of sporadic traffic and small-size data. The paper\nfocuses on a novel K-repetition GFA (K-GFA) scheme by incorporating\nReed-Solomon (RS) code with the contention resolution diversity slotted ALOHA\n(CRDSA), aiming to achieve high-reliability and low-latency access in the\npresence of massive uncoordinated MTC devices (MTCDs). We firstly defines a MAC\nlayer transmission structure at each MTCD for supporting message-level RS\ncoding on a data message of $Q$ packets, where a RS code of $KQ$ packets is\ngenerated and sent in a super time frame (STF) that is composed of $Q$ time\nframes. The access point (AP) can recover the original $Q$ packets of the data\nmessage if at least $Q$ out of the $KQ$ packets of the RS code are successfully\nreceived. The AP buffers the received MTCD signals of each resource block (RB)\nwithin an STF and exercises the CRDSA based multi-user detection (MUD) by\nexploring signal-level inter-RB correlation via iterative interference\ncancellation (IIC). With the proposed CRDSA based K-GFA scheme, we provide the\ncomplexity analysis, and derive a closed-form analytical model on the access\nprobability for each MTCD as well as its simplified approximate form. Extensive\nnumerical experiments are conducted to validate its effectiveness on the\nproposed CRDSA based K-GFA scheme and gain deep understanding on its\nperformance regarding various key operational parameters.\n",
        "title": "On Achieving High-Fidelity Grant-free Non-Orthogonal Multiple Access",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04912",
        "abstract_url": "http://arxiv.org/abs/2401.04912",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhongyan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhifang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Node repair is a crucial problem in erasure-code-based distributed storage\nsystems. An important metric for repair efficiency is the I/O cost which equals\nthe total amount of data accessed at helper nodes to repair a failed node. In\nthis work, a general formula for computing the I/O cost of linear repair\nschemes is derived from a new perspective, i.e., by investigating the Hamming\nweight of a related linear space. Applying the formula to Reed-Solomon (RS)\ncodes, we obtain lower bounds on the I/O cost for full-length RS codes with two\nand three parities. Furthermore, we build linear repair schemes for the RS\ncodes with improved I/O cost. For full-length RS codes with two parities, our\nscheme meets the lower bound on the I/O cost.\n",
        "title": "A Formula for the I/O Cost of Linear Repair Schemes and Application to\n  Reed-Solomon Codes",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04914",
        "abstract_url": "http://arxiv.org/abs/2401.04914",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Guohui"
            },
            {
                "last_name": "Li",
                "first_name": "Jianjun"
            },
            {
                "last_name": "Wang",
                "first_name": "Chaoyang"
            },
            {
                "last_name": "Shi",
                "first_name": "Si"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Learning precise representations of users and items to fit observed\ninteraction data is the fundamental task of collaborative filtering. Existing\nstudies usually infer entangled representations to fit such interaction data,\nneglecting to model the diverse matching relationships between users and items\nbehind their interactions, leading to limited performance and weak\ninterpretability. To address this problem, we propose a Dual Disentangled\nVariational AutoEncoder (DualVAE) for collaborative recommendation, which\ncombines disentangled representation learning with variational inference to\nfacilitate the generation of implicit interaction data. Specifically, we first\nimplement the disentangling concept by unifying an attention-aware dual\ndisentanglement and disentangled variational autoencoder to infer the\ndisentangled latent representations of users and items. Further, to encourage\nthe correspondence and independence of disentangled representations of users\nand items, we design a neighborhood-enhanced representation constraint with a\ncustomized contrastive mechanism to improve the representation quality.\nExtensive experiments on three real-world benchmarks show that our proposed\nmodel significantly outperforms several recent state-of-the-art baselines.\nFurther empirical experimental results also illustrate the interpretability of\nthe disentangled representations learned by DualVAE.\n",
        "title": "DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04915",
        "abstract_url": "http://arxiv.org/abs/2401.04915",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Erica"
            },
            {
                "last_name": "Simek",
                "first_name": "Olga"
            },
            {
                "last_name": "Miller",
                "first_name": "Benjamin A."
            },
            {
                "last_name": "Sullivan-Pao",
                "first_name": "Danielle"
            },
            {
                "last_name": "Young",
                "first_name": "Evan"
            },
            {
                "last_name": "Smith",
                "first_name": "Christopher L."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  We propose a pipeline for identifying important entities from intelligence\nreports that constructs a knowledge graph, where nodes correspond to entities\nof fine-grained types (e.g. traffickers) extracted from the text and edges\ncorrespond to extracted relations between entities (e.g. cartel membership).\nThe important entities in intelligence reports then map to central nodes in the\nknowledge graph. We introduce a novel method that extracts fine-grained\nentities in a few-shot setting (few labeled examples), given limited resources\navailable to label the frequently changing entity types that intelligence\nanalysts are interested in. It outperforms other state-of-the-art methods.\nNext, we identify challenges facing previous evaluations of zero-shot (no\nlabeled examples) methods for extracting relations, affecting the step of\npopulating edges. Finally, we explore the utility of the pipeline: given the\ngoal of identifying important entities, we evaluate the impact of relation\nextraction errors on the identification of central nodes in several real and\nsynthetic networks. The impact of these errors varies significantly by graph\ntopology, suggesting that confidence in measurements based on automatically\nextracted relations should depend on observed network features.\n",
        "title": "From low resource information extraction to identifying influential\n  nodes in knowledge graphs",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04916",
        "abstract_url": "http://arxiv.org/abs/2401.04916",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Kan"
            },
            {
                "last_name": "Mei",
                "first_name": "Jie"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojun"
            },
            {
                "last_name": "Hou",
                "first_name": "Lu"
            },
            {
                "last_name": "Ma",
                "first_name": "Siwei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Vehicles are no longer isolated entities in traffic environments, thanks to\nthe development of IoV powered by 5G networks and their evolution into 6G.\nHowever, it is not enough for vehicles in a highly dynamic and complex traffic\nenvironment to make reliable and efficient decisions. As a result, this paper\nproposes a cloud-edge-end computing system with multi-streams for IoV, referred\nto as Vehicular Digital Retina (VDR). Local computing and edge computing are\neffectively integrated in the VDR system through the aid of\nvehicle-to-everything (V2X) networks, resulting in a heterogeneous computing\nenvironment that improves vehicles' perception and decision-making abilities\nwith collaborative strategies. Once the system framework is presented, various\nimportant functions in the VDR system are explained in detail, including\nV2X-aided collaborative perception, V2X-aided stream sharing for collaborative\nlearning, and V2X-aided secured collaboration. All of them enable the\ndevelopment of efficient mechanisms of data sharing and information interaction\nwith high security for collaborative intelligent driving. We also present a\ncase study with simulation results to demonstrate the effectiveness of the\nproposed VDR system.\n",
        "title": "Digital Retina for IoV Towards 6G: Architecture, Opportunities, and\n  Challenges",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04918",
        "abstract_url": "http://arxiv.org/abs/2401.04918",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Kaitao"
            },
            {
                "last_name": "Masouros",
                "first_name": "Christos"
            },
            {
                "last_name": "Chen",
                "first_name": "Guangji"
            },
            {
                "last_name": "Liu",
                "first_name": "Fan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this study, we explore integrated sensing and communication (ISAC)\nnetworks to strike a more effective balance between sensing and communication\n(S&C) performance at the network scale. We leverage stochastic geometry to\nanalyze the S&C performance, shedding light on critical cooperative\ndependencies of ISAC networks. According to the derived expressions of network\nperformance, we optimize the user/target loads and the cooperative base station\ncluster sizes for S&C to achieve a flexible trade-off between network-scale S&C\nperformance. It is observed that the optimal strategy emphasizes the full\nutilization of spatial resources to enhance multiplexing and diversity gain\nwhen maximizing communication ASE. In contrast, for sensing objectives, parts\nof spatial resources are allocated to cancel inter-cell sensing interference to\nmaximize sensing ASE. Simulation results validate that the proposed ISAC scheme\nrealizes a remarkable enhancement in overall S&C network performance.\n",
        "title": "BS Coordination Optimization in Integrated Sensing and Communication: A\n  Stochastic Geometric View",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04921",
        "abstract_url": "http://arxiv.org/abs/2401.04921",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Wang",
                "first_name": "Yong"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengyuan"
            },
            {
                "last_name": "Wu",
                "first_name": "Doudou"
            },
            {
                "last_name": "Liu",
                "first_name": "Peng"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xinlin"
            },
            {
                "last_name": "Yang",
                "first_name": "Wenming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed to\nenhance pose accuracy by generating multiple hypotheses. However, most of the\nhypotheses generated deviate substantially from the true pose. Compared to\ndeterministic models, the excessive uncertainty in probabilistic models leads\nto weaker performance in single-hypothesis prediction. To address these two\nchallenges, we propose a diffusion-based refinement framework called DRPose,\nwhich refines the output of deterministic models by reverse diffusion and\nachieves more suitable multi-hypothesis prediction for the current pose\nbenchmark by multi-step refinement with multiple noises. To this end, we\npropose a Scalable Graph Convolution Transformer (SGCT) and a Pose Refinement\nModule (PRM) for denoising and refining. Extensive experiments on Human3.6M and\nMPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-art\nperformance on both single and multi-hypothesis 3DHPE. Code is available at\nhttps://github.com/KHB1698/DRPose.\n",
        "title": "Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D\n  Human Pose Estimaiton",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04923",
        "abstract_url": "http://arxiv.org/abs/2401.04923",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Ruiyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Ouyang"
            },
            {
                "last_name": "Guo",
                "first_name": "Yunhui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Active learning is a commonly used approach that reduces the labeling effort\nrequired to train deep neural networks. However, the effectiveness of current\nactive learning methods is limited by their closed-world assumptions, which\nassume that all data in the unlabeled pool comes from a set of predefined known\nclasses. This assumption is often not valid in practical situations, as there\nmay be unknown classes in the unlabeled data, leading to the active open-set\nannotation problem. The presence of unknown classes in the data can\nsignificantly impact the performance of existing active learning methods due to\nthe uncertainty they introduce. To address this issue, we propose a novel\ndata-centric active learning method called NEAT that actively annotates\nopen-set data. NEAT is designed to label known classes data from a pool of both\nknown and unknown classes unlabeled data. It utilizes the clusterability of\nlabels to identify the known classes from the unlabeled pool and selects\ninformative samples from those classes based on a consistency criterion that\nmeasures inconsistencies between model predictions and local feature\ndistribution. Unlike the recently proposed learning-centric method for the same\nproblem, NEAT is much more computationally efficient and is a data-centric\nactive open-set annotation method. Our experiments demonstrate that NEAT\nachieves significantly better performance than state-of-the-art active learning\nmethods for active open-set annotation.\n",
        "title": "Inconsistency-Based Data-Centric Active Open-Set Annotation",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04925",
        "abstract_url": "http://arxiv.org/abs/2401.04925",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Yu",
                "first_name": "Qinkai"
            },
            {
                "last_name": "shu",
                "first_name": "Dong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Hua",
                "first_name": "Wenyue"
            },
            {
                "last_name": "Meng",
                "first_name": "Yanda"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongfeng"
            },
            {
                "last_name": "Du",
                "first_name": "Mengnan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations,\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences.\n",
        "title": "The Impact of Reasoning Step Length on Large Language Models",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04928",
        "abstract_url": "http://arxiv.org/abs/2401.04928",
        "authors": [
            {
                "last_name": "Seo",
                "first_name": "Seonguk"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinkyu"
            },
            {
                "last_name": "Kim",
                "first_name": "Geeho"
            },
            {
                "last_name": "Han",
                "first_name": "Bohyung"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a novel contrastive learning framework to effectively address the\nchallenges of data heterogeneity in federated learning. We first analyze the\ninconsistency of gradient updates across clients during local training and\nestablish its dependence on the distribution of feature representations,\nleading to the derivation of the supervised contrastive learning (SCL)\nobjective to mitigate local deviations. In addition, we show that a na\\\"ive\nadoption of SCL in federated learning leads to representation collapse,\nresulting in slow convergence and limited performance gains. To address this\nissue, we introduce a relaxed contrastive learning loss that imposes a\ndivergence penalty on excessively similar sample pairs within each class. This\nstrategy prevents collapsed representations and enhances feature\ntransferability, facilitating collaborative training and leading to significant\nperformance improvements. Our framework outperforms all existing federated\nlearning approaches by huge margins on the standard benchmarks through\nextensive experimental results.\n",
        "title": "Relaxed Contrastive Learning for Federated Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04929",
        "abstract_url": "http://arxiv.org/abs/2401.04929",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Haonan"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Tu"
            },
            {
                "last_name": "Wang",
                "first_name": "An"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG"
        ],
        "abstract": "  Machine learning models, in particular deep neural networks, are currently an\nintegral part of various applications, from healthcare to finance. However,\nusing sensitive data to train these models raises concerns about privacy and\nsecurity. One method that has emerged to verify if the trained models are\nprivacy-preserving is Membership Inference Attacks (MIA), which allows\nadversaries to determine whether a specific data point was part of a model's\ntraining dataset. While a series of MIAs have been proposed in the literature,\nonly a few can achieve high True Positive Rates (TPR) in the low False Positive\nRate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA\nto be practically useful in real-world settings. In this paper, we present a\nnovel approach to MIA that is aimed at significantly improving TPR at low FPRs.\nOur method, named learning-based difficulty calibration for MIA(LDC-MIA),\ncharacterizes data records by their hardness levels using a neural network\nclassifier to determine membership. The experiment results show that LDC-MIA\ncan improve TPR at low FPR by up to 4x compared to the other difficulty\ncalibration based MIAs. It also has the highest Area Under ROC curve (AUC)\nacross all datasets. Our method's cost is comparable with most of the existing\nMIAs, but is orders of magnitude more efficient than one of the\nstate-of-the-art methods, LiRA, while achieving similar performance.\n",
        "title": "Learning-Based Difficulty Calibration for Enhanced Membership Inference\n  Attacks",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04933",
        "abstract_url": "http://arxiv.org/abs/2401.04933",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Sicong"
            },
            {
                "last_name": "He",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Lui",
                "first_name": "Kry Yik Chau"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While likelihood is attractive in theory, its estimates by deep generative\nmodels (DGMs) are often broken in practice, and perform poorly for out of\ndistribution (OOD) Detection. Various recent works started to consider\nalternative scores and achieved better performances. However, such recipes do\nnot come with provable guarantees, nor is it clear that their choices extract\nsufficient information.\n  We attempt to change this by conducting a case study on variational\nautoencoders (VAEs). First, we introduce the likelihood path (LPath) principle,\ngeneralizing the likelihood principle. This narrows the search for informative\nsummary statistics down to the minimal sufficient statistics of VAEs'\nconditional likelihoods. Second, introducing new theoretic tools such as nearly\nessential support, essential distance and co-Lipschitzness, we obtain\nnon-asymptotic provable OOD detection guarantees for certain distillation of\nthe minimal sufficient statistics. The corresponding LPath algorithm\ndemonstrates SOTA performances, even using simple and small VAEs with poor\nlikelihood estimates. To our best knowledge, this is the first provable\nunsupervised OOD method that delivers excellent empirical results, better than\nany other VAEs based techniques. We use the same model as\n\\cite{xiao2020likelihood}, open sourced from:\nhttps://github.com/XavierXiao/Likelihood-Regret\n",
        "title": "Rethinking Test-time Likelihood: The Likelihood Path Principle and Its\n  Application to OOD Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04934",
        "abstract_url": "http://arxiv.org/abs/2401.04934",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Jiechuan"
            },
            {
                "last_name": "Su",
                "first_name": "Kefan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zongqing"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA",
            "",
            "LG"
        ],
        "abstract": "  Cooperative multi-agent reinforcement learning is a powerful tool to solve\nmany real-world cooperative tasks, but restrictions of real-world applications\nmay require training the agents in a fully decentralized manner. Due to the\nlack of information about other agents, it is challenging to derive algorithms\nthat can converge to the optimal joint policy in a fully decentralized setting.\nThus, this research area has not been thoroughly studied. In this paper, we\nseek to systematically review the fully decentralized methods in two settings:\nmaximizing a shared reward of all agents and maximizing the sum of individual\nrewards of all agents, and discuss open questions and future research\ndirections.\n",
        "title": "Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A\n  Survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04935",
        "abstract_url": "http://arxiv.org/abs/2401.04935",
        "authors": [
            {
                "last_name": "Vosoughi",
                "first_name": "Ali"
            },
            {
                "last_name": "Bondi",
                "first_name": "Luca"
            },
            {
                "last_name": "Wu",
                "first_name": "Ho-Hsiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenliang"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Conventional audio classification relied on predefined classes, lacking the\nability to learn from free-form text. Recent methods unlock learning joint\naudio-text embeddings from raw audio-text pairs describing audio in natural\nlanguage. Despite recent advancements, there is little exploration of\nsystematic methods to train models for recognizing sound events and sources in\nalternative scenarios, such as distinguishing fireworks from gunshots at\noutdoor events in similar situations. This study introduces causal reasoning\nand counterfactual analysis in the audio domain. We use counterfactual\ninstances and include them in our model across different aspects. Our model\nconsiders acoustic characteristics and sound source information from\nhuman-annotated reference texts. To validate the effectiveness of our model, we\nconducted pre-training utilizing multiple audio captioning datasets. We then\nevaluate with several common downstream tasks, demonstrating the merits of the\nproposed method as one of the first works leveraging counterfactual information\nin audio domain. Specifically, the top-1 accuracy in open-ended language-based\naudio retrieval task increased by more than 43%.\n",
        "title": "Learning Audio Concepts from Counterfactual Natural Language",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04936",
        "abstract_url": "http://arxiv.org/abs/2401.04936",
        "authors": [
            {
                "last_name": "De Sterck",
                "first_name": "H."
            },
            {
                "last_name": "Falgout",
                "first_name": "R. D."
            },
            {
                "last_name": "Krzysik",
                "first_name": "O. A."
            },
            {
                "last_name": "Schroder",
                "first_name": "J. B."
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We consider the parallel-in-time solution of scalar nonlinear conservation\nlaws in one spatial dimension. The equations are discretized in space with a\nconservative finite-volume method using weighted essentially non-oscillatory\n(WENO) reconstructions, and in time with high-order explicit Runge-Kutta\nmethods. The solution of the global, discretized space-time problem is sought\nvia a nonlinear iteration that uses a novel linearization strategy in cases of\nnon-differentiable equations. Under certain choices of discretization and\nalgorithmic parameters, the nonlinear iteration coincides with Newton's method,\nalthough, more generally, it is a preconditioned residual correction scheme. At\neach nonlinear iteration, the linearized problem takes the form of a certain\ndiscretization of a linear conservation law over the space-time domain in\nquestion. An approximate parallel-in-time solution of the linearized problem is\ncomputed with a single multigrid reduction-in-time (MGRIT) iteration. The MGRIT\niteration employs a novel coarse-grid operator that is a modified conservative\nsemi-Lagrangian discretization and generalizes those we have developed\npreviously for non-conservative scalar linear hyperbolic problems. Numerical\ntests are performed for the inviscid Burgers and Buckley--Leverett equations.\nFor many test problems, the solver converges in just a handful of iterations\nwith convergence rate independent of mesh resolution, including problems with\n(interacting) shocks and rarefactions.\n",
        "title": "Parallel-in-time solution of scalar nonlinear conservation laws",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04938",
        "abstract_url": "http://arxiv.org/abs/2401.04938",
        "authors": [
            {
                "last_name": "Fatima",
                "first_name": "Rumsha"
            },
            {
                "last_name": "Younis",
                "first_name": "Shahzad"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Faraz"
            },
            {
                "last_name": "Imran",
                "first_name": "Hamna"
            },
            {
                "last_name": "Sultan",
                "first_name": "Haseeb"
            },
            {
                "last_name": "Rasool",
                "first_name": "Shahzad"
            },
            {
                "last_name": "Rafiq",
                "first_name": "Mehak"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "LG"
        ],
        "abstract": "  The reliable diagnosis of cardiac conditions through electrocardiogram (ECG)\nanalysis critically depends on accurately detecting P waves and measuring the\nPR interval. However, achieving consistent and generalizable diagnoses across\ndiverse populations presents challenges due to the inherent global variations\nobserved in ECG signals. This paper is focused on applying the Q learning\nreinforcement algorithm to the various ECG datasets available in the\nPhysioNet/Computing in Cardiology Challenge (CinC). Five ECG beats, including\nNormal Sinus Rhythm, Atrial Flutter, Atrial Fibrillation, 1st Degree\nAtrioventricular Block, and Left Atrial Enlargement, are included to study\nvariations of P waves and PR Interval on Lead II and Lead V1. Q-Agent\nclassified 71,672 beat samples in 8,867 patients with an average accuracy of\n90.4% and only 9.6% average hamming loss over misclassification. The average\nclassification time at the 100th episode containing around 40,000 samples is\n0.04 seconds. An average training reward of 344.05 is achieved at an alpha,\ngamma, and SoftMax temperature rate of 0.001, 0.9, and 0.1, respectively.\n",
        "title": "Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform\n  Variations Related to P Wave and PR Interval",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04941",
        "abstract_url": "http://arxiv.org/abs/2401.04941",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Gaojun"
            },
            {
                "last_name": "Ezerman",
                "first_name": "Martianus Frederic"
            },
            {
                "last_name": "G\u00fcneri",
                "first_name": "Cem"
            },
            {
                "last_name": "Ling",
                "first_name": "San"
            },
            {
                "last_name": "\u00d6zbudak",
                "first_name": "Ferruh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The $b$-symbol metric is a generalization of the Hamming metric. Linear\ncodes, in the $b$-symbol metric, have been used in the read channel whose\noutputs consist of $b$ consecutive symbols. The Griesmer bound outperforms the\nSingleton bound for $\\mathbb{F}_q$-linear codes in the Hamming metric, when $q$\nis fixed and the length is large enough. This scenario is also applicable in\nthe $b$-symbol metric. Shi, Zhu, and Helleseth recently made a conjecture on\ncyclic codes in the $b$-symbol metric. In this paper, we present the $b$-symbol\nGriesmer bound for linear codes by concatenating linear codes and simplex\ncodes. Based on cyclic codes and extended cyclic codes, we propose two families\nof distance-optimal linear codes with respect to the $b$-symbol Griesmer bound.\n",
        "title": "Griesmer Bound and Constructions of Linear Codes in $b$-Symbol Metric",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04942",
        "abstract_url": "http://arxiv.org/abs/2401.04942",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Beiwen"
            },
            {
                "last_name": "Gao",
                "first_name": "Huan-ang"
            },
            {
                "last_name": "Cui",
                "first_name": "Leiyao"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yupeng"
            },
            {
                "last_name": "Luo",
                "first_name": "Lan"
            },
            {
                "last_name": "Wang",
                "first_name": "Baofeng"
            },
            {
                "last_name": "Zhi",
                "first_name": "Rong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guyue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the past several years, road anomaly segmentation is actively explored in\nthe academia and drawing growing attention in the industry. The rationale\nbehind is straightforward: if the autonomous car can brake before hitting an\nanomalous object, safety is promoted. However, this rationale naturally calls\nfor a temporally informed setting while existing methods and benchmarks are\ndesigned in an unrealistic frame-wise manner. To bridge this gap, we contribute\nthe first video anomaly segmentation dataset for autonomous driving. Since\nplacing various anomalous objects on busy roads and annotating them in every\nframe are dangerous and expensive, we resort to synthetic data. To improve the\nrelevance of this synthetic dataset to real-world applications, we train a\ngenerative adversarial network conditioned on rendering G-buffers for\nphotorealism enhancement. Our dataset consists of 120,000 high-resolution\nframes at a 60 FPS framerate, as recorded in 7 different towns. As an initial\nbenchmarking, we provide baselines using latest supervised and unsupervised\nroad anomaly segmentation methods. Apart from conventional ones, we focus on\ntwo new metrics: temporal consistency and latencyaware streaming accuracy. We\nbelieve the latter is valuable as it measures whether an anomaly segmentation\nalgorithm can truly prevent a car from crashing in a temporally informed\nsetting.\n",
        "title": "Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic\n  Dataset and New Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04946",
        "abstract_url": "http://arxiv.org/abs/2401.04946",
        "authors": [
            {
                "last_name": "Mustapha",
                "first_name": "Kassem"
            },
            {
                "last_name": "McLean",
                "first_name": "William"
            },
            {
                "last_name": "Dick",
                "first_name": "Josef"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We investigate a second-order accurate time-stepping scheme for solving a\ntime-fractional diffusion equation with a Caputo derivative of order~$\\alpha\n\\in (0,1)$. The basic idea of our scheme is based on local integration followed\nby linear interpolation. It reduces to the standard Crank--Nicolson scheme in\nthe classical diffusion case, that is, as $\\alpha\\to 1$. Using a novel\napproach, we show that the proposed scheme is $\\alpha$-robust and second-order\naccurate in the $L^2(L^2)$-norm, assuming a suitable time-graded mesh. For\ncompleteness, we use the Galerkin finite element method for the spatial\ndiscretization and discuss the error analysis under reasonable regularity\nassumptions on the given data. Some numerical results are presented at the end.\n",
        "title": "An $\\alpha$-robust second-order accurate scheme for a subdiffusion\n  equation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04947",
        "abstract_url": "http://arxiv.org/abs/2401.04947",
        "authors": [
            {
                "last_name": "Hassan-Montero",
                "first_name": "Yusef"
            },
            {
                "last_name": "Herrero-Solana",
                "first_name": "Victor"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Tagging-based systems enable users to categorize web resources by means of\ntags (freely chosen keywords), in order to refinding these resources later.\nTagging is implicitly also a social indexing process, since users share their\ntags and resources, constructing a social tag index, so-called folksonomy. At\nthe same time of tagging-based system, has been popularised an interface model\nfor visual information retrieval known as Tag-Cloud. In this model, the most\nfrequently used tags are displayed in alphabetical order. This paper presents a\nnovel approach to Tag-Cloud's tags selection, and proposes the use of\nclustering algorithms for visual layout, with the aim of improve browsing\nexperience. The results suggest that presented approach reduces the semantic\ndensity of tag set, and improves the visual consistency of Tag-Cloud layout.\n",
        "title": "Improving Tag-Clouds as Visual Information Retrieval Interfaces",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04950",
        "abstract_url": "http://arxiv.org/abs/2401.04950",
        "authors": [
            {
                "last_name": "Hristopulos",
                "first_name": "Dionissios T."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "IT",
            ""
        ],
        "abstract": "  Causal inference seeks to identify cause-and-effect interactions in coupled\nsystems. A recently proposed method by Liang detects causal relations by\nquantifying the direction and magnitude of information flow between time\nseries. The theoretical formulation of information flow for stochastic\ndynamical systems provides a general expression and a data-driven statistic for\nthe rate of entropy transfer between different system units. To advance\nunderstanding of information flow rate in terms of intuitive concepts and\nphysically meaningful parameters, we investigate statistical properties of the\ndata-driven information flow rate between coupled stochastic processes. We\nderive relations between the expectation of the information flow rate statistic\nand properties of the auto- and cross-correlation functions. Thus, we elucidate\nthe dependence of the information flow rate on the analytical properties and\ncharacteristic times of the correlation functions. Our analysis provides\ninsight into the influence of the sampling step, the strength of\ncross-correlations, and the temporal delay of correlations on information flow\nrate. We support the theoretical results with numerical simulations of\ncorrelated Gaussian processes.\n",
        "title": "Information Flow Rate for Cross-Correlated Stochastic Processes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04952",
        "abstract_url": "http://arxiv.org/abs/2401.04952",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Zekun"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Some argue that the essence of humanity, such as creativity and sentiment,\ncan never be mimicked by machines. This paper casts doubt on this belief by\nstudying a vital question: Can AI compose poetry as well as humans? To answer\nthe question, we propose ProFTAP, a novel evaluation framework inspired by\nTuring test to assess AI's poetry writing capability. We apply it on current\nlarge language models (LLMs) and find that recent LLMs do indeed possess the\nability to write classical Chinese poems nearly indistinguishable from those of\nhumans. We also reveal that various open-source LLMs can outperform GPT-4 on\nthis task.\n",
        "title": "Can AI Write Classical Chinese Poetry like Humans? An Empirical Study\n  Inspired by Turing Test",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04954",
        "abstract_url": "http://arxiv.org/abs/2401.04954",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jian-Guo"
            },
            {
                "last_name": "Witelski",
                "first_name": "Thomas"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaoqian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaqi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In this paper, we investigate the tumor instability by employing both\nanalytical and numerical techniques to validate previous results and extend the\nanalytical findings presented in a prior study by Feng et al 2023. Building\nupon the insights derived from the analytical reconstruction of key results in\nthe aforementioned work in one dimension (1D) and two dimensions (2D), we\nextend our analysis to three dimensions (3D). Specifically, we focus on the\ndetermination of boundary instability using perturbation and asymptotic\nanalysis along with spherical harmonics. Additionally, we have validated our\nanalytical results in a two-dimensional framework by implementing the\nAlternating Directional Implicit (ADI) method, as detailed in Witelski and\nBowen (2003). Our primary focus has been on ensuring that the numerical\nsimulation of the propagation speed aligns accurately with the analytical\nfindings. Furthermore, we have matched the simulated boundary stability with\nthe analytical predictions derived from the evolution function, which will be\ndefined in subsequent sections of our paper. These alignment is essential for\naccurately determining the stability or instability of tumor boundaries.\n",
        "title": "A Three-dimensional tumor growth model and its boundary instability",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04956",
        "abstract_url": "http://arxiv.org/abs/2401.04956",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Huafeng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Jin",
                "first_name": "Xin"
            },
            {
                "last_name": "Song",
                "first_name": "Qun"
            },
            {
                "last_name": "El-Yacoubi",
                "first_name": "Mounim A."
            },
            {
                "last_name": "Gao",
                "first_name": "Xinbo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR"
        ],
        "abstract": "  Eye movement (EM) is a new highly secure biometric behavioral modality that\nhas received increasing attention in recent years. Although deep neural\nnetworks, such as convolutional neural network (CNN), have recently achieved\npromising performance, current solutions fail to capture local and global\ntemporal dependencies within eye movement data. To overcome this problem, we\npropose in this paper a mixed transformer termed EmMixformer to extract time\nand frequency domain information for eye movement recognition. To this end, we\npropose a mixed block consisting of three modules, transformer, attention Long\nshort-term memory (attention LSTM), and Fourier transformer. We are the first\nto attempt leveraging transformer to learn long temporal dependencies within\neye movement. Second, we incorporate the attention mechanism into LSTM to\npropose attention LSTM with the aim to learn short temporal dependencies.\nThird, we perform self attention in the frequency domain to learn global\nfeatures. As the three modules provide complementary feature representations in\nterms of local and global dependencies, the proposed EmMixformer is capable of\nimproving recognition accuracy. The experimental results on our eye movement\ndataset and two public eye movement datasets show that the proposed EmMixformer\noutperforms the state of the art by achieving the lowest verification error.\n",
        "title": "EmMixformer: Mix transformer for eye movement recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04958",
        "abstract_url": "http://arxiv.org/abs/2401.04958",
        "authors": [
            {
                "last_name": "Mubasshir",
                "first_name": "Kazi Samin"
            },
            {
                "last_name": "Karim",
                "first_name": "Imtiaz"
            },
            {
                "last_name": "Bertino",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Fake base stations (FBSes) pose a significant security threat by\nimpersonating legitimate base stations. Though efforts have been made to defeat\nthis threat, up to this day, the presence of FBSes and the multi-step attacks\n(MSAs) stemming from them can lead to unauthorized surveillance, interception\nof sensitive information, and disruption of network services for legitimate\nusers. Therefore, detecting these malicious entities is crucial to ensure the\nsecurity and reliability of cellular networks. Traditional detection methods\noften rely on additional hardware, predefined rules, signal scanning, changing\nprotocol specifications, or cryptographic mechanisms that have limitations and\nincur huge infrastructure costs in accurately identifying FBSes. In this paper,\nwe develop FBSDetector-an effective and efficient detection solution that can\nreliably detect FBSes and MSAs from layer-3 network traces using machine\nlearning (ML) at the user equipment (UE) side. To develop FBSDetector, we\ncreated FBSAD and MSAD, the first-ever high-quality and large-scale datasets\nfor training machine learning models capable of detecting FBSes and MSAs. These\ndatasets capture the network traces in different real-world cellular network\nscenarios (including mobility and different attacker capabilities)\nincorporating legitimate base stations and FBSes. The combined network trace\nhas a volume of 6.6 GB containing 751963 packets. Our novel ML models,\nspecially designed to detect FBSes and MSAs, can effectively detect FBSes with\nan accuracy of 92% and a false positive rate of 5.96% and recognize MSAs with\nan accuracy of 86% and a false positive rate of 7.82%. We deploy FBSDetector as\na real-world solution to protect end-users through an Android app and validate\nin a controlled lab environment. Compared to the existing solutions that fail\nto detect FBSes, FBSDetector can detect FBSes in the wild in real time.\n",
        "title": "FBSDetector: Fake Base Station and Multi Step Attack Detection in\n  Cellular Networks using Machine Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04960",
        "abstract_url": "http://arxiv.org/abs/2401.04960",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hanli"
            },
            {
                "last_name": "Srikanthan",
                "first_name": "Anusha"
            },
            {
                "last_name": "Folk",
                "first_name": "Spencer"
            },
            {
                "last_name": "Kumar",
                "first_name": "Vijay"
            },
            {
                "last_name": "Matni",
                "first_name": "Nikolai"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG",
            ""
        ],
        "abstract": "  Motivated by the increasing use of quadrotors for payload delivery, we\nconsider a joint trajectory generation and feedback control design problem for\na quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag\nforces from carried payloads can lead to catastrophic outcomes. Prior work\nmodel aerodynamic effects as residual dynamics or external disturbances in the\ncontrol problem leading to a reactive policy that could be catastrophic.\nMoreover, redesigning controllers and tuning control gains on hardware\nplatforms is a laborious effort. In this paper, we argue that adapting the\ntrajectory generation component keeping the controller fixed can improve\ntrajectory tracking for quadrotor systems experiencing drag forces. To achieve\nthis, we formulate a drag-aware planning problem by applying a suitable\nrelaxation to an optimal quadrotor control problem, introducing a tracking cost\nfunction which measures the ability of a controller to follow a reference\ntrajectory. This tracking cost function acts as a regularizer in trajectory\ngeneration and is learned from data obtained from simulation. Our experiments\nin both simulation and on the Crazyflie hardware platform show that changing\nthe planner reduces tracking error by as much as 83%. Evaluation on hardware\ndemonstrates that our planned path, as opposed to a baseline, avoids controller\nsaturation and catastrophic outcomes during aggressive maneuvers.\n",
        "title": "Why Change Your Controller When You Can Change Your Planner: Drag-Aware\n  Trajectory Generation for Quadrotor Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04961",
        "abstract_url": "http://arxiv.org/abs/2401.04961",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Yuncheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zixun"
            },
            {
                "last_name": "Hu",
                "first_name": "Yiwen"
            },
            {
                "last_name": "Li",
                "first_name": "Guanbin"
            },
            {
                "last_name": "Wan",
                "first_name": "Xiang"
            },
            {
                "last_name": "Wu",
                "first_name": "Song"
            },
            {
                "last_name": "Cui",
                "first_name": "Shuguang"
            },
            {
                "last_name": "Huang",
                "first_name": "Silin"
            },
            {
                "last_name": "Li",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate polyp detection is critical for early colorectal cancer diagnosis.\nAlthough remarkable progress has been achieved in recent years, the complex\ncolon environment and concealed polyps with unclear boundaries still pose\nsevere challenges in this area. Existing methods either involve computationally\nexpensive context aggregation or lack prior modeling of polyps, resulting in\npoor performance in challenging cases. In this paper, we propose the Enhanced\nCenterNet with Contrastive Learning (ECC-PolypDet), a two-stage training \\&\nend-to-end inference framework that leverages images and bounding box\nannotations to train a general model and fine-tune it based on the inference\nscore to obtain a final robust model. Specifically, we conduct Box-assisted\nContrastive Learning (BCL) during training to minimize the intra-class\ndifference and maximize the inter-class difference between foreground polyps\nand backgrounds, enabling our model to capture concealed polyps. Moreover, to\nenhance the recognition of small polyps, we design the Semantic Flow-guided\nFeature Pyramid Network (SFFPN) to aggregate multi-scale features and the\nHeatmap Propagation (HP) module to boost the model's attention on polyp\ntargets. In the fine-tuning stage, we introduce the IoU-guided Sample\nRe-weighting (ISR) mechanism to prioritize hard samples by adaptively adjusting\nthe loss weight for each sample during fine-tuning. Extensive experiments on\nsix large-scale colonoscopy datasets demonstrate the superiority of our model\ncompared with previous state-of-the-art detectors.\n",
        "title": "ECC-PolypDet: Enhanced CenterNet with Contrastive Learning for Automatic\n  Polyp Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04962",
        "abstract_url": "http://arxiv.org/abs/2401.04962",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Kailong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Xia",
                "first_name": "Qianchen"
            },
            {
                "last_name": "Liu",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Keyframe extraction aims to sum up a video's semantics with the minimum\nnumber of its frames. This paper puts forward a Large Model based Sequential\nKeyframe Extraction for video summarization, dubbed LMSKE, which contains three\nstages as below. First, we use the large model \"TransNetV21\" to cut the video\ninto consecutive shots, and employ the large model \"CLIP2\" to generate each\nframe's visual feature within each shot; Second, we develop an adaptive\nclustering algorithm to yield candidate keyframes for each shot, with each\ncandidate keyframe locating nearest to a cluster center; Third, we further\nreduce the above candidate keyframes via redundancy elimination within each\nshot, and finally concatenate them in accordance with the sequence of shots as\nthe final sequential keyframes. To evaluate LMSKE, we curate a benchmark\ndataset and conduct rich experiments, whose results exhibit that LMSKE performs\nmuch better than quite a few SOTA competitors with average F1 of 0.5311,\naverage fidelity of 0.8141, and average compression ratio of 0.9922.\n",
        "title": "Large Model based Sequential Keyframe Extraction for Video Summarization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04964",
        "abstract_url": "http://arxiv.org/abs/2401.04964",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zechen"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haolin"
            },
            {
                "last_name": "Yan",
                "first_name": "YuJie"
            },
            {
                "last_name": "Wu",
                "first_name": "Xihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD",
            ""
        ],
        "abstract": "  Relating speech to EEG holds considerable importance but challenging. In this\nstudy, deep convolutional network was employed to extract spatiotemporal\nfeatures from EEG data. Self-supervised speech representation and contextual\ntext embedding were used as speech features. Contrastive learning was used to\nrelated EEG features to speech features. The experimental results demonstrate\nthe benefits of using self-supervised speech representation and contextual text\nembedding. Through feature fusion and model ensemble, an accuracy of 60.29% was\nachieved, and the performance was ranked as No.2 in Task1 of the Auditory EEG\nChallenge (ICASSP 2024).\n",
        "title": "Self-supervised speech representation and contextual text embedding for\n  match-mismatch classification with EEG recording",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04965",
        "abstract_url": "http://arxiv.org/abs/2401.04965",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Xiran"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Yan",
                "first_name": "Yujie"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haolin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zechen"
            },
            {
                "last_name": "Wu",
                "first_name": "Xihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Jing"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  To investigate the processing of speech in the brain, simple linear models\nare commonly used to establish a relationship between brain signals and speech\nfeatures. However, these linear models are ill-equipped to model a highly\ndynamic and complex non-linear system like the brain. Although non-linear\nmethods with neural networks have been developed recently, reconstructing\nunseen stimuli from unseen subjects' EEG is still a highly challenging task.\nThis work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams\nfrom EEG, in which the deep convolution neural network and extensive\nconcatenation operation were combined. With our ConvConcatNet model, the\nPearson correlation between the reconstructed and the target mel-spectrogram\ncan achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG\nChallenge. The codes and models to implement our work will be available on\nGithub: https://github.com/xuxiran/ConvConcatNet\n",
        "title": "ConvConcatNet: a deep convolutional neural network to reconstruct mel\n  spectrogram from the EEG",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04966",
        "abstract_url": "http://arxiv.org/abs/2401.04966",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Chenguang"
            },
            {
                "last_name": "Sun",
                "first_name": "Jie"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Don",
                "first_name": "WaiSun"
            },
            {
                "last_name": "Ju",
                "first_name": "Lili"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  A high-order multi-time-step (MTS) scheme for the bond-based peridynamic (PD)\nmodel, an extension of classical continuous mechanics widely used for analyzing\ndiscontinuous problems like cracks, is proposed. The MTS scheme discretizes the\nspatial domain with a meshfree method and advances in time with a high-order\nRunge-Kutta method. To effectively handle discontinuities (cracks) that appear\nin a local subdomain in the solution, the scheme employs the Taylor expansion\nand Lagrange interpolation polynomials with a finer time step size, that is,\ncoarse and fine time step sizes for smooth and discontinuous subdomains,\nrespectively, to achieve accurate and efficient simulations. By eliminating\nunnecessary fine-scale resolution imposed on the entire domain, the MTS scheme\noutperforms the standard PD scheme by significantly reducing computational\ncosts, particularly for problems with discontinuous solutions, as demonstrated\nby comprehensive theoretical analysis and numerical experiments.\n",
        "title": "A high-order multi-time-step scheme for bond-based peridynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04968",
        "abstract_url": "http://arxiv.org/abs/2401.04968",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Zhenmin"
            },
            {
                "last_name": "Shen",
                "first_name": "Shaojie"
            },
            {
                "last_name": "Ma",
                "first_name": "Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Cooperative decision-making of Connected Autonomous Vehicles (CAVs) presents\na longstanding challenge due to its inherent nonlinearity, non-convexity, and\ndiscrete characteristics, compounded by the diverse road topologies encountered\nin real-world traffic scenarios. The majority of current methodologies are only\napplicable to a single and specific scenario, predicated on scenario-specific\nassumptions. Consequently, their application in real-world environments is\nrestricted by the innumerable nature of traffic scenarios. In this study, we\npropose a unified optimization approach that exhibits the potential to address\ncooperative decision-making problems related to traffic scenarios with generic\nroad topologies. This development is grounded in the premise that the\ntopologies of various traffic scenarios can be universally represented as\nDirected Acyclic Graphs (DAGs). Particularly, the reference paths and time\nprofiles for all involved CAVs are determined in a fully cooperative manner,\ntaking into account factors such as velocities, accelerations, conflict\nresolutions, and overall traffic efficiency. The cooperative decision-making of\nCAVs is approximated as a mixed-integer linear programming (MILP) problem\nbuilding on the DAGs of road topologies. This favorably facilitates the use of\nstandard numerical solvers and the global optimality can be attained through\nthe optimization. Case studies corresponding to different multi-lane traffic\nscenarios featuring diverse topologies are scheduled as the test itineraries,\nand the efficacy of our proposed methodology is corroborated.\n",
        "title": "A Universal Cooperative Decision-Making Framework for Connected\n  Autonomous Vehicles with Generic Road Topologies",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04971",
        "abstract_url": "http://arxiv.org/abs/2401.04971",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shu"
            },
            {
                "last_name": "Xu",
                "first_name": "Zitao"
            },
            {
                "last_name": "Pan",
                "first_name": "Weike"
            },
            {
                "last_name": "Yang",
                "first_name": "Qiang"
            },
            {
                "last_name": "Ming",
                "first_name": "Zhong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Cross-domain sequential recommendation (CDSR) shifts the modeling of user\npreferences from flat to stereoscopic by integrating and learning interaction\ninformation from multiple domains at different granularities (ranging from\ninter-sequence to intra-sequence and from single-domain to cross-domain).In\nthis survey, we initially define the CDSR problem using a four-dimensional\ntensor and then analyze its multi-type input representations under\nmultidirectional dimensionality reductions. Following that, we provide a\nsystematic overview from both macro and micro views. From a macro view, we\nabstract the multi-level fusion structures of various models across domains and\ndiscuss their bridges for fusion. From a micro view, focusing on the existing\nmodels, we specifically discuss the basic technologies and then explain the\nauxiliary learning technologies. Finally, we exhibit the available public\ndatasets and the representative experimental results as well as provide some\ninsights into future directions for research in CDSR.\n",
        "title": "A Survey on Cross-Domain Sequential Recommendation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04972",
        "abstract_url": "http://arxiv.org/abs/2401.04972",
        "authors": [
            {
                "last_name": "Stewart",
                "first_name": "Ian"
            },
            {
                "last_name": "Mihalcea",
                "first_name": "Rada"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Machine translation often suffers from biased data and algorithms that can\nlead to unacceptable errors in system output. While bias in gender norms has\nbeen investigated, less is known about whether MT systems encode bias about\nsocial relationships, e.g. sentences such as \"the lawyer kissed her wife.\" We\ninvestigate the degree of bias against same-gender relationships in MT systems,\nusing generated template sentences drawn from several noun-gender languages\n(e.g. Spanish). We find that three popular MT services consistently fail to\naccurately translate sentences concerning relationships between nouns of the\nsame gender. The error rate varies considerably based on the context, e.g.\nsame-gender sentences referencing high female-representation occupations are\ntranslated with lower accuracy. We provide this work as a case study in the\nevaluation of intrinsic bias in NLP systems, with respect to social\nrelationships.\n",
        "title": "Whose wife is it anyway? Assessing bias against same-gender\n  relationships in machine translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04973",
        "abstract_url": "http://arxiv.org/abs/2401.04973",
        "authors": [
            {
                "last_name": "Ju",
                "first_name": "Lili"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Lu",
                "first_name": "Junke"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Existing nonlocal diffusion models are predominantly classified into two\ncategories: bond-based models, which involve a single-fold integral and usually\nsimulate isotropic diffusion, and state-based models, which contain a\ndouble-fold integral and can additionally prototype anisotropic diffusion.\nWhile bond-based models exhibit computational efficiency, they are somewhat\nlimited in their modeling capabilities. In this paper, we develop a novel\nbond-based nonlocal diffusion model with matrix-valued coefficients in\nnon-divergence form. Our approach incorporates the coefficients into a\ncovariance matrix and employs the multivariate Gaussian function with\ntruncation to define the kernel function, and subsequently model the nonlocal\ndiffusion process through the bond-based formulation. We successfully establish\nthe well-posedness of the proposed model along with deriving some of its\nproperties on maximum principle and mass conservation. Furthermore, an\nefficient linear collocation scheme is designed for numerical solution of our\nmodel. Comprehensive experiments in two and three dimensions are conducted to\nshowcase application of the proposed nonlocal model to both isotropic and\nanisotropic diffusion problems and to demonstrate numerical accuracy and\neffective asymptotic compatibility of the proposed collocation scheme.\n",
        "title": "A novel bond-based nonlocal diffusion model with matrix-valued\n  coefficients in non-divergence form and its collocation discretization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04975",
        "abstract_url": "http://arxiv.org/abs/2401.04975",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Qian"
            },
            {
                "last_name": "Cui",
                "first_name": "Ruoxuan"
            },
            {
                "last_name": "Li",
                "first_name": "Yuke"
            },
            {
                "last_name": "Zhu",
                "first_name": "Haoqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Action recognition in videos poses a challenge due to its high computational\ncost, especially for Joint Space-Time video transformers (Joint VT). Despite\ntheir effectiveness, the excessive number of tokens in such architectures\nsignificantly limits their efficiency. In this paper, we propose HaltingVT, an\nefficient video transformer adaptively removing redundant video patch tokens,\nwhich is primarily composed of a Joint VT and a Glimpser module. Specifically,\nHaltingVT applies data-adaptive token reduction at each layer, resulting in a\nsignificant reduction in the overall computational cost. Besides, the Glimpser\nmodule quickly removes redundant tokens in shallow transformer layers, which\nmay even be misleading for video recognition tasks based on our observations.\nTo further encourage HaltingVT to focus on the key motion-related information\nin videos, we design an effective Motion Loss during training. HaltingVT\nacquires video analysis capabilities and token halting compression strategies\nsimultaneously in a unified training process, without requiring additional\ntraining procedures or sub-networks. On the Mini-Kinetics dataset, we achieved\n75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely\nlow 9.9 GFLOPs. The code is available at\nhttps://github.com/dun-research/HaltingVT.\n",
        "title": "HaltingVT: Adaptive Token Halting Transformer for Efficient Video\n  Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04976",
        "abstract_url": "http://arxiv.org/abs/2401.04976",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Haobo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Mu",
                "first_name": "Da"
            },
            {
                "last_name": "Dang",
                "first_name": "Yonghao"
            },
            {
                "last_name": "Yin",
                "first_name": "Jianqin"
            },
            {
                "last_name": "Tang",
                "first_name": "Jin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Recently, 2D convolution has been found unqualified in sound event detection\n(SED). It enforces translation equivariance on sound events along frequency\naxis, which is not a shift-invariant dimension. To address this issue, dynamic\nconvolution is used to model the frequency dependency of sound events. In this\npaper, we proposed the first full-dynamic method named \\emph{full-frequency\ndynamic convolution} (FFDConv). FFDConv generates frequency kernels for every\nfrequency band, which is designed directly in the structure for\nfrequency-dependent modeling. It physically furnished 2D convolution with the\ncapability of frequency-dependent modeling. FFDConv outperforms not only the\nbaseline by 6.6\\% in DESED real validation dataset in terms of PSDS1, but\noutperforms the other full-dynamic methods. In addition, by visualizing\nfeatures of sound events, we observed that FFDConv could effectively extract\ncoherent features in specific frequency bands, consistent with the vocal\ncontinuity of sound events. This proves that FFDConv has great\nfrequency-dependent perception ability.\n",
        "title": "Full-frequency dynamic convolution: a physical frequency-dependent\n  convolution for sound event detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04978",
        "abstract_url": "http://arxiv.org/abs/2401.04978",
        "authors": [
            {
                "last_name": "Wetzel",
                "first_name": "Sebastian Johann"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  I introduce a unified framework for interpreting neural network classifiers\ntailored toward automated scientific discovery. In contrast to neural\nnetwork-based regression, for classification, it is in general impossible to\nfind a one-to-one mapping from the neural network to a symbolic equation even\nif the neural network itself bases its classification on a quantity that can be\nwritten as a closed-form equation. In this paper, I embed a trained neural\nnetwork into an equivalence class of classifying functions that base their\ndecisions on the same quantity. I interpret neural networks by finding an\nintersection between this equivalence class and human-readable equations\ndefined by the search space of symbolic regression. The approach is not limited\nto classifiers or full neural networks and can be applied to arbitrary neurons\nin hidden layers or latent spaces or to simplify the process of interpreting\nneural network regressors.\n",
        "title": "Closed-Form Interpretation of Neural Network Classifiers with Symbolic\n  Regression Gradients",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04979",
        "abstract_url": "http://arxiv.org/abs/2401.04979",
        "authors": [
            {
                "last_name": "Oh",
                "first_name": "YongKyung"
            },
            {
                "last_name": "Lim",
                "first_name": "Dongyoung"
            },
            {
                "last_name": "Kim",
                "first_name": "Sungil"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  To handle the complexities of irregular and incomplete time series data, we\npropose an invertible solution of Neural Differential Equations (NDE)-based\nmethod. While NDE-based methods are a powerful method for analyzing\nirregularly-sampled time series, they typically do not guarantee reversible\ntransformations in their standard form. Our method suggests the variation of\nNeural Controlled Differential Equations (Neural CDEs) with Neural Flow, which\nensures invertibility while maintaining a lower computational burden.\nAdditionally, it enables the training of a dual latent space, enhancing the\nmodeling of dynamic temporal dynamics. Our research presents an advanced\nframework that excels in both classification and interpolation tasks. At the\ncore of our approach is an enhanced dual latent states architecture, carefully\ndesigned for high precision across various time series tasks. Empirical\nanalysis demonstrates that our method significantly outperforms existing\nmodels. This work significantly advances irregular time series analysis,\nintroducing innovative techniques and offering a versatile tool for diverse\npractical applications.\n",
        "title": "Invertible Solution of Neural Differential Equations for Analysis of\n  Irregularly-Sampled Time Series",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04980",
        "abstract_url": "http://arxiv.org/abs/2401.04980",
        "authors": [
            {
                "last_name": "Attard",
                "first_name": "Daniel"
            },
            {
                "last_name": "Bajada",
                "first_name": "Josef"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  In recent years, significant advancements have been made in the field of\nautonomous driving with the aim of increasing safety and efficiency. However,\nresearch that focuses on tractor-trailer vehicles is relatively sparse. Due to\nthe physical characteristics and articulated joints, such vehicles require\ntailored models. While turning, the back wheels of the trailer turn at a\ntighter radius and the truck often has to deviate from the centre of the lane\nto accommodate this. Due to the lack of publicly available models, this work\ndevelops truck and trailer models using the high-fidelity simulation software\nCARLA, together with several roundabout scenarios, to establish a baseline\ndataset for benchmarks. Using a twin-q soft actor-critic algorithm, we train a\nquasi-end-to-end autonomous driving model which is able to achieve a 73%\nsuccess rate on different roundabouts.\n",
        "title": "Autonomous Navigation of Tractor-Trailer Vehicles through Roundabout\n  Intersections",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04984",
        "abstract_url": "http://arxiv.org/abs/2401.04984",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Luanyuan"
            },
            {
                "last_name": "Du",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            },
            {
                "last_name": "Tang",
                "first_name": "Jinhui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Learning correspondences aims to find correct correspondences (inliers) from\nthe initial correspondence set with an uneven correspondence distribution and a\nlow inlier rate, which can be regarded as graph data. Recent advances usually\nuse graph neural networks (GNNs) to build a single type of graph or simply\nstack local graphs into the global one to complete the task. But they ignore\nthe complementary relationship between different types of graphs, which can\neffectively capture potential relationships among sparse correspondences. To\naddress this problem, we propose MGNet to effectively combine multiple\ncomplementary graphs. To obtain information integrating implicit and explicit\nlocal graphs, we construct local graphs from implicit and explicit aspects and\ncombine them effectively, which is used to build a global graph. Moreover, we\npropose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse\ncorrespondence information at once in the global graph, which can capture and\namplify discriminative features. Extensive experiments demonstrate that MGNet\noutperforms state-of-the-art methods in different visual tasks. The code is\nprovided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.\n",
        "title": "MGNet: Learning Correspondences via Multiple Graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04986",
        "abstract_url": "http://arxiv.org/abs/2401.04986",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Miyatake",
                "first_name": "Yuto"
            },
            {
                "last_name": "Cui",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Wei",
                "first_name": "Shikui"
            },
            {
                "last_name": "Furihata",
                "first_name": "Daisuke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, there has been growing interest in using physics-informed neural\nnetworks (PINNs) to solve differential equations. However, the preservation of\nstructure, such as energy and stability, in a suitable manner has yet to be\nestablished. This limitation could be a potential reason why the learning\nprocess for PINNs is not always efficient and the numerical results may suggest\nnonphysical behavior. Besides, there is little research on their applications\non downstream tasks. To address these issues, we propose structure-preserving\nPINNs to improve their performance and broaden their applications for\ndownstream tasks. Firstly, by leveraging prior knowledge about the physical\nsystem, a structure-preserving loss function is designed to assist the PINN in\nlearning the underlying structure. Secondly, a framework that utilizes\nstructure-preserving PINN for robust image recognition is proposed. Here,\npreserving the Lyapunov structure of the underlying system ensures the\nstability of the system. Experimental results demonstrate that the proposed\nmethod improves the numerical accuracy of PINNs for partial differential\nequations. Furthermore, the robustness of the model against adversarial\nperturbations in image data is enhanced.\n",
        "title": "Structure-Preserving Physics-Informed Neural Networks With Energy or\n  Lyapunov Structure",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04988",
        "abstract_url": "http://arxiv.org/abs/2401.04988",
        "authors": [
            {
                "last_name": "Jeziorek",
                "first_name": "Kamil"
            },
            {
                "last_name": "Wzorek",
                "first_name": "Piotr"
            },
            {
                "last_name": "Blachut",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Pinna",
                "first_name": "Andrea"
            },
            {
                "last_name": "Kryjak",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Event-based vision is an emerging research field involving processing data\ngenerated by Dynamic Vision Sensors (neuromorphic cameras). One of the latest\nproposals in this area are Graph Convolutional Networks (GCNs), which allow to\nprocess events in its original sparse form while maintaining high detection and\nclassification performance. In this paper, we present the hardware\nimplementation of a~graph generation process from an event camera data stream,\ntaking into account both the advantages and limitations of FPGAs. We propose\nvarious ways to simplify the graph representation and use scaling and\nquantisation of values. We consider both undirected and directed graphs that\nenable the use of PointNet convolution. The results obtained show that by\nappropriately modifying the graph representation, it is possible to create\na~hardware module for graph generation. Moreover, the proposed modifications\nhave no significant impact on object detection performance, only 0.08% mAP less\nfor the base model and the N-Caltech data set.Finally, we describe the proposed\nhardware architecture of the graph generation module.\n",
        "title": "Optimising Graph Representation for Hardware Implementation of Graph\n  Convolutional Networks for Event-based Vision",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04993",
        "abstract_url": "http://arxiv.org/abs/2401.04993",
        "authors": [
            {
                "last_name": "Hamidi",
                "first_name": "Shayan Mohajer"
            },
            {
                "last_name": "Yang",
                "first_name": "En-Hui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Federated learning (FL) is a promising technology via which some edge\ndevices/clients collaboratively train a machine learning model orchestrated by\na server. Learning an unfair model is known as a critical problem in federated\nlearning, where the trained model may unfairly advantage or disadvantage some\nof the devices. To tackle this problem, in this work, we propose AdaFed. The\ngoal of AdaFed is to find an updating direction for the server along which (i)\nall the clients' loss functions are decreasing; and (ii) more importantly, the\nloss functions for the clients with larger values decrease with a higher rate.\nAdaFed adaptively tunes this common direction based on the values of local\ngradients and loss functions. We validate the effectiveness of AdaFed on a\nsuite of federated datasets, and demonstrate that AdaFed outperforms\nstate-of-the-art fair FL methods.\n",
        "title": "AdaFed: Fair Federated Learning via Adaptive Common Descent Direction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04996",
        "abstract_url": "http://arxiv.org/abs/2401.04996",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanyuan"
            },
            {
                "last_name": "Su",
                "first_name": "Lili"
            },
            {
                "last_name": "Joe-Wong",
                "first_name": "Carlee"
            },
            {
                "last_name": "Yeh",
                "first_name": "Edmund"
            },
            {
                "last_name": "Ioannidis",
                "first_name": "Stratis"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  As edge computing capabilities increase, model learning deployments in\ndiverse edge environments have emerged. In experimental design networks,\nintroduced recently, network routing and rate allocation are designed to aid\nthe transfer of data from sensors to heterogeneous learners. We design\nefficient experimental design network algorithms that are (a) distributed and\n(b) use multicast transmissions. This setting poses significant challenges as\nclassic decentralization approaches often operate on (strictly) concave\nobjectives under differentiable constraints. In contrast, the problem we study\nhere has a non-convex, continuous DR-submodular objective, while multicast\ntransmissions naturally result in non-differentiable constraints. From a\ntechnical standpoint, we propose a distributed Frank-Wolfe and a distributed\nprojected gradient ascent algorithm that, coupled with a relaxation of\nnon-differentiable constraints, yield allocations within a $1-1/e$ factor from\nthe optimal. Numerical evaluations show that our proposed algorithms outperform\ncompetitors with respect to model learning quality.\n",
        "title": "Distributed Experimental Design Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04997",
        "abstract_url": "http://arxiv.org/abs/2401.04997",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Lanling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Li",
                "first_name": "Bingqian"
            },
            {
                "last_name": "Wang",
                "first_name": "Jinpeng"
            },
            {
                "last_name": "Cai",
                "first_name": "Mingchen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recently, large language models such as ChatGPT have showcased remarkable\nabilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by\nexperiments to systematically analyze the impact of different factors on two\npublic datasets. Finally, we summarize promising directions to shed lights on\nfuture research.\n",
        "title": "Prompting Large Language Models for Recommender Systems: A Comprehensive\n  Framework and Empirical Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04998",
        "abstract_url": "http://arxiv.org/abs/2401.04998",
        "authors": [
            {
                "last_name": "Jing",
                "first_name": ""
            },
            {
                "last_name": "Lin",
                "first_name": ""
            },
            {
                "last_name": "Silfvenius",
                "first_name": "Christofer"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In the era of sustainable transportation, the significance of electric\nvehicles (EVs) and their battery technology is becoming increasingly paramount.\nThis study addresses the critical aspect of EV battery reliability, an\nessential factor in the vehicles' sustainability, performance, and longevity.\nCurrent efforts to enhance EV battery reliability tend to focus on isolated\nareas, often missing the broader, interconnected challenges within the system.\nThis research investigates these challenges across micro, meso, and macro\nlevels, presenting a novel lifecycle framework that includes \"Zero\"-Life\nreliability and phases such as use, reuse, repurpose, and recycling. By\nadopting a holistic approach and delving into system cognition, the study aims\nto bridge the gap between isolated improvements and comprehensive system\noptimization, aligning with global sustainability goals and contributing to the\nadvancement of sustainable transportation and EV technology.\n",
        "title": "Some Critical Thinking on EV Battery Reliability: from Enhancement to\n  Optimization -- comprehensive perspectives, lifecycle innovation, system\n  cognation, and strategic insights",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05007",
        "abstract_url": "http://arxiv.org/abs/2401.05007",
        "authors": [
            {
                "last_name": "Mukendi",
                "first_name": "Christian Mulomba"
            },
            {
                "last_name": "Choi",
                "first_name": "Hyebong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  he evaluation of the impact of actions undertaken is essential in management.\nThis paper assesses the impact of efforts considered to mitigate risk and\ncreate safe environments on a global scale. We measure this impact by looking\nat the probability of improvement over a specific short period of time. Using\nthe World Risk Index, we conduct a temporal analysis of global disaster risk\ndynamics from 2011 to 2021. This temporal exploration through the lens of the\nWorld Risk Index provides insights into the complex dynamics of disaster risk.\nWe found that, despite sustained efforts, the global landscape remains divided\ninto two main clusters: high susceptibility and moderate susceptibility,\nregardless of geographical location. This clustering was achieved using a\nsemi-supervised approach through the Label Spreading algorithm, with 98%\naccuracy. We also found that the prediction of clusters achieved through\nsupervised learning on the period considered in this study (one, three, and\nfive years) showed that the Logistic regression (almost 99% at each stage)\nperformed better than other classifiers. This suggests that the current\npolicies and mechanisms are not effective in helping countries move from a\nhazardous position to a safer one during the period considered. In fact,\nstatistical projections using a scenario analysis indicate that there is only a\n1% chance of such a shift occurring within a five-year timeframe. This sobering\nreality highlights the need for a paradigm shift. Traditional long-term\ndisaster management strategies are not effective for countries that are highly\nvulnerable. Our findings indicate the need for an innovative approach that is\ntailored to the specific vulnerabilities of these nations. As the threat of\nvulnerability persists, our research calls for the development of new\nstrategies that can effectively address the ongoing challenges of disaster risk\nmanagement\n",
        "title": "Temporal Analysis of World Disaster Risk:A Machine Learning Approach to\n  Cluster Dynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05010",
        "abstract_url": "http://arxiv.org/abs/2401.05010",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Chunpeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Haishuai"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xilu"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhi"
            },
            {
                "last_name": "Bu",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Few-shot Learning aims to learn and distinguish new categories with a very\nlimited number of available images, presenting a significant challenge in the\nrealm of deep learning. Recent researchers have sought to leverage the\nadditional textual or linguistic information of these rare categories with a\npre-trained language model to facilitate learning, thus partially alleviating\nthe problem of insufficient supervision signals. However, the full potential of\nthe textual information and pre-trained language model have been underestimated\nin the few-shot learning till now, resulting in limited performance\nenhancements. To address this, we propose a simple but effective framework for\nfew-shot learning tasks, specifically designed to exploit the textual\ninformation and language model. In more detail, we explicitly exploit the\nzero-shot capability of the pre-trained language model with the learnable\nprompt. And we just add the visual feature with the textual feature for\ninference directly without the intricate designed fusion modules in previous\nworks. Additionally, we apply the self-ensemble and distillation to further\nenhance these components. Our extensive experiments conducted across four\nwidely used few-shot datasets demonstrate that our simple framework achieves\nimpressive results. Particularly noteworthy is its outstanding performance in\nthe 1-shot learning task, surpassing state-of-the-art methods by an average of\n3.0\\% in classification accuracy. \\footnote{We will make the source codes of\nthe proposed framework publicly available upon acceptance. }.\n",
        "title": "Less is More : A Closer Look at Multi-Modal Few-Shot Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05011",
        "abstract_url": "http://arxiv.org/abs/2401.05011",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Na"
            },
            {
                "last_name": "Chen",
                "first_name": "Weiling"
            },
            {
                "last_name": "Ma",
                "first_name": "Keng Teck"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hanwang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semi-supervised 3D object detection is a promising yet under-explored\ndirection to reduce data annotation costs, especially for cluttered indoor\nscenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this\ntask by utilizing a teacher model to generate pseudo-labels for unlabeled\nsamples. However, the availability of unlabeled samples in the 3D domain is\nrelatively limited compared to its 2D counterpart due to the greater effort\nrequired to collect 3D data. Moreover, the loose consistency regularization in\nSESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to\neither low-quality supervision or a limited amount of pseudo labels. To address\nthese issues, we present a novel Dual-Perspective Knowledge Enrichment approach\nnamed DPKE for semi-supervised 3D object detection. Our DPKE enriches the\nknowledge of limited training data, particularly unlabeled data, from two\nperspectives: data-perspective and feature-perspective. Specifically, from the\ndata-perspective, we propose a class-probabilistic data augmentation method\nthat augments the input data with additional instances based on the varying\ndistribution of class probabilities. Our DPKE achieves feature-perspective\nknowledge enrichment by designing a geometry-aware feature matching method that\nregularizes feature-level similarity between object proposals from the student\nand teacher models. Extensive experiments on the two benchmark datasets\ndemonstrate that our DPKE achieves superior performance over existing\nstate-of-the-art approaches under various label ratio conditions. The source\ncode will be made available to the public.\n",
        "title": "Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object\n  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05012",
        "abstract_url": "http://arxiv.org/abs/2401.05012",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Shubao"
            },
            {
                "last_name": "Jin",
                "first_name": "Ming"
            },
            {
                "last_name": "Hou",
                "first_name": "Zhaoxiang"
            },
            {
                "last_name": "Yang",
                "first_name": "Chengyi"
            },
            {
                "last_name": "Li",
                "first_name": "Zengxiang"
            },
            {
                "last_name": "Wen",
                "first_name": "Qingsong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Time series forecasting is crucial and challenging in the real world. The\nrecent surge in interest regarding time series foundation models, which cater\nto a diverse array of downstream tasks, is noteworthy. However, existing\nmethods often overlook the multi-scale nature of time series, an aspect crucial\nfor precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical\nmulti-scale masked time series modeling method designed for long-term\nforecasting. Specifically, it comprises four integral components: (1)\nhierarchical multi-scale transformer (HMT) to capture temporal information at\ndifferent scales; (2) decoupled encoder-decoder (DED) forces the encoder to\nfocus on feature extraction, while the decoder to focus on pretext tasks; (3)\nmulti-scale masked reconstruction (MMR) provides multi-stage supervision\nsignals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to\ncapture dependencies between different scales for forecasting. Collectively,\nthese components enhance multi-scale feature extraction capabilities in masked\ntime series modeling and contribute to improved prediction accuracy. We conduct\nextensive experiments on 7 mainstream datasets to prove that HiMTM has obvious\nadvantages over contemporary self-supervised and end-to-end learning methods.\nThe effectiveness of HiMTM is further showcased by its application in the\nindustry of natural gas demand forecasting.\n",
        "title": "HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for\n  Long-Term Forecasting",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05014",
        "abstract_url": "http://arxiv.org/abs/2401.05014",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Jinjing"
            },
            {
                "last_name": "Chen",
                "first_name": "Yucheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Source-free cross-modal knowledge transfer is a crucial yet challenging task,\nwhich aims to transfer knowledge from one source modality (e.g., RGB) to the\ntarget modality (e.g., depth or infrared) with no access to the task-relevant\n(TR) source data due to memory and privacy concerns. A recent attempt leverages\nthe paired task-irrelevant (TI) data and directly matches the features from\nthem to eliminate the modality gap. However, it ignores a pivotal clue that the\npaired TI data could be utilized to effectively estimate the source data\ndistribution and better facilitate knowledge transfer to the target modality.\nTo this end, we propose a novel yet concise framework to unlock the potential\nof paired TI data for enhancing source-free cross-modal knowledge transfer. Our\nwork is buttressed by two key technical components. Firstly, to better estimate\nthe source data distribution, we introduce a Task-irrelevant data-Guided\nModality Bridging (TGMB) module. It translates the target modality data (e.g.,\ninfrared) into the source-like RGB images based on paired TI data and the\nguidance of the available source model to alleviate two key gaps: 1)\ninter-modality gap between the paired TI data; 2) intra-modality gap between TI\nand TR target data. We then propose a Task-irrelevant data-Guided Knowledge\nTransfer (TGKT) module that transfers knowledge from the source model to the\ntarget model by leveraging the paired TI data. Notably, due to the\nunavailability of labels for the TR target data and its less reliable\nprediction from the source model, our TGKT model incorporates a self-supervised\npseudo-labeling approach to enable the target model to learn from its\npredictions. Extensive experiments show that our method achieves\nstate-of-the-art performance on three datasets (RGB-to-depth and\nRGB-to-infrared).\n",
        "title": "Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential\n  of Task-Irrelevant Data",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05015",
        "abstract_url": "http://arxiv.org/abs/2401.05015",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Farnia",
                "first_name": "Farzan"
            },
            {
                "last_name": "Leung",
                "first_name": "Ho-fung"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Reinforcement learning (RL) problems where the learner attempts to infer an\nunobserved reward from some feedback variables have been studied in several\nrecent papers. The setting of Interaction-Grounded Learning (IGL) is an example\nof such feedback-based reinforcement learning tasks where the learner optimizes\nthe return by inferring latent binary rewards from the interaction with the\nenvironment. In the IGL setting, a relevant assumption used in the RL\nliterature is that the feedback variable $Y$ is conditionally independent of\nthe context-action $(X,A)$ given the latent reward $R$. In this work, we\npropose Variational Information-based IGL (VI-IGL) as an information-theoretic\nmethod to enforce the conditional independence assumption in the IGL-based RL\nproblem. The VI-IGL framework learns a reward decoder using an\ninformation-based objective based on the conditional mutual information (MI)\nbetween the context-action $(X,A)$ and the feedback variable $Y$ observed from\nthe environment. To estimate and optimize the information-based terms for the\ncontinuous random variables in the RL problem, VI-IGL leverages the variational\nrepresentation of mutual information and results in a min-max optimization\nproblem. Furthermore, we extend the VI-IGL framework to general $f$-Information\nmeasures in the information theory literature, leading to the generalized\n$f$-VI-IGL framework to address the RL problem under the IGL condition.\nFinally, we provide the empirical results of applying the VI-IGL method to\nseveral reinforcement learning settings, which indicate an improved performance\nin comparison to the previous IGL-based RL algorithm.\n",
        "title": "An Information Theoretic Approach to Interaction-Grounded Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05018",
        "abstract_url": "http://arxiv.org/abs/2401.05018",
        "authors": [
            {
                "last_name": "Idrees",
                "first_name": "Sarmad"
            },
            {
                "last_name": "Choi",
                "first_name": "Jongeun"
            },
            {
                "last_name": "Sohn",
                "first_name": "Seokman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To achieve seamless collaboration between robots and humans in a shared\nenvironment, accurately predicting future human movements is essential. Human\nmotion prediction has traditionally been approached as a sequence prediction\nproblem, leveraging historical human motion data to estimate future poses.\nBeginning with vanilla recurrent networks, the research community has\ninvestigated a variety of methods for learning human motion dynamics,\nencompassing graph-based and generative approaches. Despite these efforts,\nachieving accurate long-term predictions continues to be a significant\nchallenge. In this regard, we present the Adversarial Motion Transformer\n(AdvMT), a novel model that integrates a transformer-based motion encoder and a\ntemporal continuity discriminator. This combination effectively captures\nspatial and temporal dependencies simultaneously within frames. With\nadversarial training, our method effectively reduces the unwanted artifacts in\npredictions, thereby ensuring the learning of more realistic and fluid human\nmotions. The evaluation results indicate that AdvMT greatly enhances the\naccuracy of long-term predictions while also delivering robust short-term\npredictions\n",
        "title": "AdvMT: Adversarial Motion Transformer for Long-term Human Motion\n  Prediction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05019",
        "abstract_url": "http://arxiv.org/abs/2401.05019",
        "authors": [
            {
                "last_name": "Xin",
                "first_name": "Jinghao"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinwoo"
            },
            {
                "last_name": "Chu",
                "first_name": "Shengjia"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Existing Global Path Planning (GPP) algorithms predominantly presume planning\nin a static environment. This assumption immensely limits their applications to\nUnmanned Surface Vehicles (USVs) that typically navigate in dynamic\nenvironments. To address this limitation, we present OkayPlan, a GPP algorithm\ncapable of generating safe and short paths in dynamic scenarios at a real-time\nexecuting speed (125 Hz on a desktop-class computer). Specifically, we approach\nthe challenge of dynamic obstacle avoidance by formulating the path planning\nproblem as an obstacle kinematics augmented optimization problem, which can be\nefficiently resolved through a PSO-based optimizer at a real-time speed.\nMeanwhile, a Dynamic Prioritized Initialization (DPI) mechanism that adaptively\ninitializes potential solutions for the optimization problem is established to\nfurther ameliorate the solution quality. Additionally, a relaxation strategy\nthat facilitates the autonomous tuning of OkayPlan's hyperparameters in dynamic\nenvironments is devised. Comparative experiments involving canonical and\ncontemporary GPP algorithms, along with ablation studies, have been conducted\nto substantiate the efficacy of our approach. Results indicate that OkayPlan\noutstrips existing methods in terms of path safety, length optimality, and\ncomputational efficiency, establishing it as a potent GPP technique for dynamic\nenvironments. The video and code associated with this paper are accessible at\nhttps://github.com/XinJingHao/OkayPlan.\n",
        "title": "OkayPlan: Obstacle Kinematics Augmented Dynamic Real-time Path Planning\n  via Particle Swarm Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05031",
        "abstract_url": "http://arxiv.org/abs/2401.05031",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenchao"
            },
            {
                "last_name": "Hong",
                "first_name": "Zicong"
            },
            {
                "last_name": "Guo",
                "first_name": "Song"
            },
            {
                "last_name": "Wang",
                "first_name": "Haozhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Zeng",
                "first_name": "Deze"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Transformer model empowered architectures have become a pillar of cloud\nservices that keeps reshaping our society. However, the dynamic query loads and\nheterogeneous user requirements severely challenge current transformer serving\nsystems, which rely on pre-training multiple variants of a foundation model,\ni.e., with different sizes, to accommodate varying service demands.\nUnfortunately, such a mechanism is unsuitable for large transformer models due\nto the additional training costs and excessive I/O delay. In this paper, we\nintroduce OTAS, the first elastic serving system specially tailored for\ntransformer models by exploring lightweight token management. We develop a\nnovel idea called token adaptation that adds prompting tokens to improve\naccuracy and removes redundant tokens to accelerate inference. To cope with\nfluctuating query loads and diverse user requests, we enhance OTAS with\napplication-aware selective batching and online token adaptation. OTAS first\nbatches incoming queries with similar service-level objectives to improve the\ningress throughput. Then, to strike a tradeoff between the overhead of token\nincrement and the potentials for accuracy improvement, OTAS adaptively adjusts\nthe token execution strategy by solving an optimization problem. We implement\nand evaluate a prototype of OTAS with multiple datasets, which show that OTAS\nimproves the system utility by at least 18.2%.\n",
        "title": "OTAS: An Elastic Transformer Serving System via Token Adaptation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05033",
        "abstract_url": "http://arxiv.org/abs/2401.05033",
        "authors": [
            {
                "last_name": "Ulmer",
                "first_name": "Dennis"
            },
            {
                "last_name": "Mansimov",
                "first_name": "Elman"
            },
            {
                "last_name": "Lin",
                "first_name": "Kaixiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Justin"
            },
            {
                "last_name": "Gao",
                "first_name": "Xibin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) are powerful dialogue agents, but specializing\nthem towards fulfilling a specific function can be challenging. Instructing\ntuning, i.e. tuning models on instruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an effective method to do so, yet\nrequires a number of data samples that a) might not be available or b) costly\nto generate. Furthermore, this cost increases when the goal is to make the LLM\nfollow a specific workflow within a dialogue instead of single instructions.\nInspired by the self-play technique in reinforcement learning and the use of\nLLMs to simulate human agents, we propose a more effective method for data\ncollection through LLMs engaging in a conversation in various roles. This\napproach generates a training data via \"self-talk\" of LLMs that can be refined\nand utilized for supervised fine-tuning. We introduce an automated way to\nmeasure the (partial) success of a dialogue. This metric is used to filter the\ngenerated conversational data that is fed back in LLM for training. Based on\nour automated and human evaluations of conversation quality, we demonstrate\nthat such self-talk data improves results. In addition, we examine the various\ncharacteristics that showcase the quality of generated dialogues and how they\ncan be connected to their potential utility as training data.\n",
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05039",
        "abstract_url": "http://arxiv.org/abs/2401.05039",
        "authors": [
            {
                "last_name": "Hsieh",
                "first_name": "Chou-Ying"
            },
            {
                "last_name": "Chang",
                "first_name": "Chia-Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Po-Hsiu"
            },
            {
                "last_name": "Kuo",
                "first_name": "Sy-Yen"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Maximal Biclique Enumeration (MBE) holds critical importance in graph theory\nwith applications extending across fields such as bioinformatics, social\nnetworks, and recommendation systems. However, its computational complexity\npresents barriers for efficiently scaling to large graphs. To address these\nchallenges, we introduce cuMBE, a GPU-optimized parallel algorithm for MBE.\nUtilizing a unique data structure, called compact array, cuMBE eradicates the\nneed for recursion, thereby significantly minimizing dynamic memory\nrequirements and computational overhead. The algorithm utilizes a hybrid\nparallelism approach, in which GPU thread blocks handle coarse-grained tasks\nassociated with part of the search process. Besides, we implement three\nfine-grained optimizations within each thread block to enhance performance.\nFurther, we integrate a work-stealing mechanism to mitigate workload imbalances\namong thread blocks. Our experiments reveal that cuMBE achieves an geometric\nmean speedup of 4.02x and 4.13x compared to the state-of-the-art serial\nalgorithm and parallel CPU-based algorithm on both common and real-world\ndatasets, respectively.\n",
        "title": "Accelerating Maximal Biclique Enumeration on GPUs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05041",
        "abstract_url": "http://arxiv.org/abs/2401.05041",
        "authors": [
            {
                "last_name": "Iommazzo",
                "first_name": "Gabriele"
            },
            {
                "last_name": "D'Ambrosio",
                "first_name": "Claudia"
            },
            {
                "last_name": "Frangioni",
                "first_name": "Antonio"
            },
            {
                "last_name": "Liberti",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We discuss the issue of finding a good mathematical programming solver\nconfiguration for a particular instance of a given problem, and we propose a\ntwo-phase approach to solve it. In the first phase we learn the relationships\nbetween the instance, the configuration and the performance of the configured\nsolver on the given instance. A specific difficulty of learning a good solver\nconfiguration is that parameter settings may not all be independent; this\nrequires enforcing (hard) constraints, something that many widely used\nsupervised learning methods cannot natively achieve. We tackle this issue in\nthe second phase of our approach, where we use the learnt information to\nconstruct and solve an optimization problem having an explicit representation\nof the dependency/consistency constraints on the configuration parameter\nsettings. We discuss computational results for two different instantiations of\nthis approach on a unit commitment problem arising in the short-term planning\nof hydro valleys. We use logistic regression as the supervised learning\nmethodology and consider CPLEX as the solver of interest.\n",
        "title": "Learning to Configure Mathematical Programming Solvers by Mathematical\n  Programming",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05043",
        "abstract_url": "http://arxiv.org/abs/2401.05043",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Kaizheng"
            },
            {
                "last_name": "Shariatmadar",
                "first_name": "Keivan"
            },
            {
                "last_name": "Manchingal",
                "first_name": "Shireen Kudukkil"
            },
            {
                "last_name": "Cuzzolin",
                "first_name": "Fabio"
            },
            {
                "last_name": "Moens",
                "first_name": "David"
            },
            {
                "last_name": "Hallez",
                "first_name": "Hans"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Uncertainty estimation is increasingly attractive for improving the\nreliability of neural networks. In this work, we present novel credal-set\ninterval neural networks (CreINNs) designed for classification tasks. CreINNs\npreserve the traditional interval neural network structure, capturing weight\nuncertainty through deterministic intervals, while forecasting credal sets\nusing the mathematical framework of probability intervals. Experimental\nvalidations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN)\nshowcase that CreINNs outperform epistemic uncertainty estimation when compared\nto variational Bayesian neural networks (BNNs) and deep ensembles (DEs).\nFurthermore, CreINNs exhibit a notable reduction in computational complexity\ncompared to variational BNNs and demonstrate smaller model sizes than DEs.\n",
        "title": "CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation\n  in Classification Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05045",
        "abstract_url": "http://arxiv.org/abs/2401.05045",
        "authors": [
            {
                "last_name": "Barletta",
                "first_name": "Luca"
            },
            {
                "last_name": "Dytso",
                "first_name": "Alex"
            },
            {
                "last_name": "Shamai",
                "first_name": "Shlomo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers a discrete-time Poisson noise channel with an input\namplitude constraint $\\mathsf{A}$ and a dark current parameter $\\lambda$. It is\nknown that the capacity-achieving distribution for this channel is discrete\nwith finitely many points. Recently, for $\\lambda=0$, a lower bound of order\n$\\sqrt{\\mathsf{A}}$ and an upper bound of order $\\mathsf{A} \\log^2(\\mathsf{A})$\nhave been demonstrated on the cardinality of the support of the optimal input\ndistribution.\n  In this work, we improve these results in several ways. First, we provide\nupper and lower bounds that hold for non-zero dark current. Second, we produce\na sharper upper bound with a far simpler technique. In particular, for\n$\\lambda=0$, we sharpen the upper bound from the order of $\\mathsf{A}\n\\log^2(\\mathsf{A})$ to the order of $\\mathsf{A}$. Finally, some other\nadditional information about the location of the support is provided.\n",
        "title": "Improved Bounds on the Number of Support Points of the\n  Capacity-Achieving Input for Amplitude Constrained Poisson Channels",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05049",
        "abstract_url": "http://arxiv.org/abs/2401.05049",
        "authors": [
            {
                "last_name": "Vargis",
                "first_name": "Tom Richard"
            },
            {
                "last_name": "Ghiasvand",
                "first_name": "Siavash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This work prioritizes building a modular pipeline that utilizes existing\nmodels to systematically restore images, rather than creating new restoration\nmodels from scratch. Restoration is carried out at an object-specific level,\nwith each object regenerated using its corresponding class label information.\nThe approach stands out by providing complete user control over the entire\nrestoration process. Users can select models for specialized restoration steps,\ncustomize the sequence of steps to meet their needs, and refine the resulting\nregenerated image with depth awareness. The research provides two distinct\npathways for implementing image regeneration, allowing for a comparison of\ntheir respective strengths and limitations. The most compelling aspect of this\nversatile system is its adaptability. This adaptability enables users to target\nparticular object categories, including medical images, by providing models\nthat are trained on those object classes.\n",
        "title": "Content-Aware Depth-Adaptive Image Restoration",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05054",
        "abstract_url": "http://arxiv.org/abs/2401.05054",
        "authors": [
            {
                "last_name": "Jinnai",
                "first_name": "Yuu"
            },
            {
                "last_name": "Honda",
                "first_name": "Ukyo"
            },
            {
                "last_name": "Morimura",
                "first_name": "Tetsuro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peinan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  One of the most important challenges in text generation systems is to produce\noutputs that are not only correct but also diverse. Recently, Minimum\nBayes-Risk (MBR) decoding has gained prominence for generating sentences of the\nhighest quality among the decoding algorithms. However, existing algorithms\nproposed for generating diverse outputs are predominantly based on beam search\nor random sampling, thus their output quality is capped by these underlying\nmethods. In this paper, we investigate an alternative approach -- we develop\ndiversity-promoting decoding algorithms by enforcing diversity objectives to\nMBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and\n$k$-medoids MBR (KMBR), methods to generate a set of sentences with high\nquality and diversity. We evaluate DMBR and KMBR on a variety of directed text\ngeneration tasks using encoder-decoder models and a large language model with\nprompting. The experimental results show that the proposed method achieves a\nbetter trade-off than the diverse beam search and sampling algorithms.\n",
        "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05055",
        "abstract_url": "http://arxiv.org/abs/2401.05055",
        "authors": [
            {
                "last_name": "Xiang",
                "first_name": "Yawen"
            },
            {
                "last_name": "Zhou",
                "first_name": "Heng"
            },
            {
                "last_name": "Li",
                "first_name": "Chengyang"
            },
            {
                "last_name": "Sun",
                "first_name": "Fangwei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongbo"
            },
            {
                "last_name": "Xie",
                "first_name": "Yongqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Motion deblurring is one of the fundamental problems of computer vision and\nhas received continuous attention. The variability in blur, both within and\nacross images, imposes limitations on non-blind deblurring techniques that rely\non estimating the blur kernel. As a response, blind motion deblurring has\nemerged, aiming to restore clear and detailed images without prior knowledge of\nthe blur type, fueled by the advancements in deep learning methodologies.\nDespite strides in this field, a comprehensive synthesis of recent progress in\ndeep learning-based blind motion deblurring is notably absent. This paper fills\nthat gap by providing an exhaustive overview of the role of deep learning in\nblind motion deblurring, encompassing datasets, evaluation metrics, and methods\ndeveloped over the last six years. Specifically, we first introduce the types\nof motion blur and the fundamental principles of deblurring. Next, we outline\nthe shortcomings of traditional non-blind deblurring algorithms, emphasizing\nthe advantages of employing deep learning techniques for deblurring tasks.\nFollowing this, we categorize and summarize existing blind motion deblurring\nmethods based on different backbone networks, including convolutional neural\nnetworks, generative adversarial networks, recurrent neural networks, and\nTransformer networks. Subsequently, we elaborate not only on the fundamental\nprinciples of these different categories but also provide a comprehensive\nsummary and comparison of their advantages and limitations. Qualitative and\nquantitative experimental results conducted on four widely used datasets\nfurther compare the performance of SOTA methods. Finally, an analysis of\npresent challenges and future pathways. All collected models, benchmark\ndatasets, source code links, and codes for evaluation have been made publicly\navailable at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey\n",
        "title": "Application of Deep Learning in Blind Motion Deblurring: Current Status\n  and Future Prospects",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05057",
        "abstract_url": "http://arxiv.org/abs/2401.05057",
        "authors": [
            {
                "last_name": "Oelerich",
                "first_name": "Thies"
            },
            {
                "last_name": "Beck",
                "first_name": "Florian"
            },
            {
                "last_name": "Hartl-Nesic",
                "first_name": "Christian"
            },
            {
                "last_name": "Kugi",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This work presents a novel online model-predictive trajectory planner for\nrobotic manipulators called BoundMPC. This planner allows the collision-free\nfollowing of Cartesian reference paths in the end-effector's position and\norientation, including via-points, within desired asymmetric bounds of the\northogonal path error. The path parameter synchronizes the position and\norientation reference paths. The decomposition of the path error into the\ntangential direction, describing the path progress, and the orthogonal\ndirection, which represents the deviation from the path, is well known for the\nposition from the path-following control in the literature. This paper extends\nthis idea to the orientation by utilizing the Lie theory of rotations.\nMoreover, the orthogonal error plane is further decomposed into basis\ndirections to define asymmetric Cartesian error bounds easily. Using piecewise\nlinear position and orientation reference paths with via-points is\ncomputationally very efficient and allows replanning the pose trajectories\nduring the robot's motion. This feature makes it possible to use this planner\nfor dynamically changing environments and varying goals. The flexibility and\nperformance of BoundMPC are experimentally demonstrated by two scenarios on a\n7-DoF Kuka LBR iiwa 14 R820 robot. The first scenario shows the transfer of a\nlarger object from a start to a goal pose through a confined space where the\nobject must be tilted. The second scenario deals with grasping an object from a\ntable where the grasping point changes during the robot's motion, and\ncollisions with other obstacles in the scene must be avoided.\n",
        "title": "BoundMPC: Cartesian Trajectory Planning with Error Bounds based on Model\n  Predictive Control in the Joint Space",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05060",
        "abstract_url": "http://arxiv.org/abs/2401.05060",
        "authors": [
            {
                "last_name": "Costa-juss\u00e0",
                "first_name": "Marta R."
            },
            {
                "last_name": "Meglioli",
                "first_name": "Mariano Coria"
            },
            {
                "last_name": "Andrews",
                "first_name": "Pierre"
            },
            {
                "last_name": "Dale",
                "first_name": "David"
            },
            {
                "last_name": "Hansanti",
                "first_name": "Prangthip"
            },
            {
                "last_name": "Kalbassi",
                "first_name": "Elahe"
            },
            {
                "last_name": "Mourachko",
                "first_name": "Alex"
            },
            {
                "last_name": "Ropers",
                "first_name": "Christophe"
            },
            {
                "last_name": "Wood",
                "first_name": "Carleigh"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "",
            ""
        ],
        "abstract": "  Research in toxicity detection in natural language processing for the speech\nmodality (audio-based) is quite limited, particularly for languages other than\nEnglish. To address these limitations and lay the groundwork for truly\nmultilingual audio-based toxicity detection, we introduce MuTox, the first\nhighly multilingual audio-based dataset with toxicity labels. The dataset\ncomprises 20,000 audio utterances for English and Spanish, and 4,000 for the\nother 19 languages. To demonstrate the quality of this dataset, we trained the\nMuTox audio-based toxicity classifier, which enables zero-shot toxicity\ndetection across a wide range of languages. This classifier outperforms\nexisting text-based trainable classifiers by more than 1% AUC, while expanding\nthe language coverage more than tenfold. When compared to a wordlist-based\nclassifier that covers a similar number of languages, MuTox improves precision\nand recall by approximately 2.5 times. This significant improvement underscores\nthe potential of MuTox in advancing the field of audio-based toxicity\ndetection.\n",
        "title": "MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot\n  Detector",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05064",
        "abstract_url": "http://arxiv.org/abs/2401.05064",
        "authors": [
            {
                "last_name": "Torres",
                "first_name": "Bernardo"
            },
            {
                "last_name": "Lattner",
                "first_name": "Stefan"
            },
            {
                "last_name": "Richard",
                "first_name": "Ga\u00ebl"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            ""
        ],
        "abstract": "  Significant strides have been made in creating voice identity representations\nusing speech data. However, the same level of progress has not been achieved\nfor singing voices. To bridge this gap, we suggest a framework for training\nsinger identity encoders to extract representations suitable for various\nsinging-related tasks, such as singing voice similarity and synthesis. We\nexplore different self-supervised learning techniques on a large collection of\nisolated vocal tracks and apply data augmentations during training to ensure\nthat the representations are invariant to pitch and content variations. We\nevaluate the quality of the resulting representations on singer similarity and\nidentification tasks across multiple datasets, with a particular emphasis on\nout-of-domain generalization. Our proposed framework produces high-quality\nembeddings that outperform both speaker verification and wav2vec 2.0\npre-trained baselines on singing voice while operating at 44.1 kHz. We release\nour code and trained models to facilitate further research on singing voice and\nrelated areas.\n",
        "title": "Singer Identity Representation Learning using Self-Supervised Techniques",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05069",
        "abstract_url": "http://arxiv.org/abs/2401.05069",
        "authors": [
            {
                "last_name": "Grzeszczyk",
                "first_name": "Michal K."
            },
            {
                "last_name": "Trzci\u0144ski",
                "first_name": "Tomasz"
            },
            {
                "last_name": "Sitek",
                "first_name": "Arkadiusz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we present a novel, machine-learning approach for constructing\nMulticlass Interpretable Scoring Systems (MISS) - a fully data-driven\nmethodology for generating single, sparse, and user-friendly scoring systems\nfor multiclass classification problems. Scoring systems are commonly utilized\nas decision support models in healthcare, criminal justice, and other domains\nwhere interpretability of predictions and ease of use are crucial. Prior\nmethods for data-driven scoring, such as SLIM (Supersparse Linear Integer\nModel), were limited to binary classification tasks and extensions to\nmulticlass domains were primarily accomplished via one-versus-all-type\ntechniques. The scores produced by our method can be easily transformed into\nclass probabilities via the softmax function. We demonstrate techniques for\ndimensionality reduction and heuristics that enhance the training efficiency\nand decrease the optimality gap, a measure that can certify the optimality of\nthe model. Our approach has been extensively evaluated on datasets from various\ndomains, and the results indicate that it is competitive with other machine\nlearning models in terms of classification performance metrics and provides\nwell-calibrated class probabilities.\n",
        "title": "MISS: Multiclass Interpretable Scoring Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05072",
        "abstract_url": "http://arxiv.org/abs/2401.05072",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yichong"
            },
            {
                "last_name": "Feng",
                "first_name": "Xiaocheng"
            },
            {
                "last_name": "Li",
                "first_name": "Baohang"
            },
            {
                "last_name": "Fu",
                "first_name": "Chengpeng"
            },
            {
                "last_name": "Huo",
                "first_name": "Wenshuai"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            },
            {
                "last_name": "Qin",
                "first_name": "Bing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Although large language models (LLMs) have shown surprising language\nunderstanding and generation capabilities, they have yet to gain a\nrevolutionary advancement in the field of machine translation. One potential\ncause of the limited performance is the misalignment between the\ntranslation-specific understanding and general understanding inside LLMs. To\nalign the translation-specific understanding to the general one, we propose a\nnovel translation process xIoD (Cross-Lingual Interpretation of Difficult\nwords), explicitly incorporating the general understanding on the content\nincurring inconsistent understanding to guide the translation. Specifically,\nxIoD performs the cross-lingual interpretation for the difficult-to-translate\nwords and enhances the translation with the generated interpretations.\nFurthermore, we reframe the external tools of QE to tackle the challenges of\nxIoD in the detection of difficult words and the generation of helpful\ninterpretations. We conduct experiments on the self-constructed benchmark\nChallengeMT, which includes cases in which multiple SOTA translation systems\nconsistently underperform. Experimental results show the effectiveness of our\nxIoD, which improves up to +3.85 COMET.\n",
        "title": "Aligning Translation-Specific Understanding to General Understanding in\n  Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05073",
        "abstract_url": "http://arxiv.org/abs/2401.05073",
        "authors": [
            {
                "last_name": "Leon",
                "first_name": "Florin"
            },
            {
                "last_name": "Gavrilescu",
                "first_name": "Marius"
            },
            {
                "last_name": "Floria",
                "first_name": "Sabina-Adriana"
            },
            {
                "last_name": "Minea",
                "first_name": "Alina-Adriana"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  This paper proposes a classification framework aimed at identifying\ncorrelations between job ad requirements and transversal skill sets, with a\nfocus on predicting the necessary skills for individual job descriptions using\na deep learning model. The approach involves data collection, preprocessing,\nand labeling using ESCO (European Skills, Competences, and Occupations)\ntaxonomy. Hierarchical classification and multi-label strategies are used for\nskill identification, while augmentation techniques address data imbalance,\nenhancing model robustness. A comparison between results obtained with\nEnglish-specific and multi-language sentence embedding models reveals close\naccuracy. The experimental case studies detail neural network configurations,\nhyperparameters, and cross-validation results, highlighting the efficacy of the\nhierarchical approach and the suitability of the multi-language model for the\ndiverse European job market. Thus, a new approach is proposed for the\nhierarchical classification of transversal skills from job ads.\n",
        "title": "Hierarchical Classification of Transversal Skills in Job Ads Based on\n  Sentence Embeddings",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05074",
        "abstract_url": "http://arxiv.org/abs/2401.05074",
        "authors": [
            {
                "last_name": "Wietzke",
                "first_name": "Thore"
            },
            {
                "last_name": "Gall",
                "first_name": "Jan"
            },
            {
                "last_name": "Graichen",
                "first_name": "Knut"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper presents a new approach to predict the occupancy for building\nenergy systems (BES). A Gaussian Process (GP) is used to model the occupancy\nand is represented as a state space model that is equivalent to the full GP if\nKalman filtering and smoothing is used. The combination of GPs and mechanistic\nmodels is called Latent Force Model (LFM). An LFM-based model predictive\ncontrol (MPC) concept for BES is presented that benefits from the extrapolation\ncapability of mechanistic models and the learning ability of GPs to predict the\noccupancy within the building. Simulations with EnergyPlus and a comparison\nwith real-world data from the Bosch Research Campus in Renningen show that a\nreduced energy demand and thermal discomfort can be obtained with the LFM-based\nMPC scheme by accounting for the predicted stochastic occupancy.\n",
        "title": "Occupancy Prediction for Building Energy Systems with Latent Force\n  Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05083",
        "abstract_url": "http://arxiv.org/abs/2401.05083",
        "authors": [
            {
                "last_name": "Onuoha",
                "first_name": "Okechi"
            },
            {
                "last_name": "Kurawa",
                "first_name": "Suleiman"
            },
            {
                "last_name": "Tang",
                "first_name": "Zezhi"
            },
            {
                "last_name": "Dong",
                "first_name": "Yi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA",
            ""
        ],
        "abstract": "  This paper considers the distributed leader-follower stress-matrix-based\naffine formation control problem of discrete-time linear multi-agent systems\nwith static and dynamic leaders. In leader-follower multi-agent formation\ncontrol, the aim is to drive a set of agents comprising leaders and followers\nto form any desired geometric pattern and simultaneously execute any required\nmanoeuvre by controlling only a few agents denoted as leaders. Existing works\nin literature are mostly limited to the cases where the agents' inter-agent\ncommunications are either in the continuous-time settings or the sampled-data\ncases where the leaders are constrained to constant (or zero) velocities or\naccelerations. Here, we relax these constraints and study the discrete-time\ncases where the leaders can have stationary or time-varying velocities. We\npropose control laws in the study of different situations and provide some\nsufficient conditions to guarantee the overall system stability. Simulation\nstudy is used to demonstrate the efficacy of our proposed control laws.\n",
        "title": "Discrete-Time Stress Matrix-Based Formation Control of General Linear\n  Multi-Agent Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05085",
        "abstract_url": "http://arxiv.org/abs/2401.05085",
        "authors": [
            {
                "last_name": "Aute",
                "first_name": "Shubhada"
            },
            {
                "last_name": "Panolan",
                "first_name": "Fahad"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Minimum sum vertex cover of an $n$-vertex graph $G$ is a bijection $\\phi :\nV(G) \\to [n]$ that minimizes the cost $\\sum_{\\{u,v\\} \\in E(G)} \\min \\{\\phi(u),\n\\phi(v) \\}$. Finding a minimum sum vertex cover of a graph (the MSVC problem)\nis NP-hard. MSVC is studied well in the realm of approximation algorithms. The\nbest-known approximation factor in polynomial time for the problem is $16/9$\n[Bansal, Batra, Farhadi, and Tetali, SODA 2021]. Recently, Stankovic\n[APPROX/RANDOM 2022] proved that achieving an approximation ratio better than\n$1.014$ for MSVC is NP-hard, assuming the Unique Games Conjecture. We study the\nMSVC problem from the perspective of parameterized algorithms. The parameters\nwe consider are the size of a minimum vertex cover and the size of a minimum\nclique modulator of the input graph. We obtain the following results.\n  1. MSVC can be solved in $2^{2^{O(k)}} n^{O(1)}$ time, where $k$ is the size\nof a minimum vertex cover.\n  2. MSVC can be solved in $f(k)\\cdot n^{O(1)}$ time for some computable\nfunction $f$, where $k$ is the size of a minimum clique modulator.\n",
        "title": "Parameterized Algorithms for Minimum Sum Vertex Cover",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05093",
        "abstract_url": "http://arxiv.org/abs/2401.05093",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Lei",
                "first_name": "Jie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With recent advancements in aerospace technology, the volume of unlabeled\nremote sensing image (RSI) data has increased dramatically. Effectively\nleveraging this data through self-supervised learning (SSL) is vital in the\nfield of remote sensing. However, current methodologies, particularly\ncontrastive learning (CL), a leading SSL method, encounter specific challenges\nin this domain. Firstly, CL often mistakenly identifies geographically adjacent\nsamples with similar semantic content as negative pairs, leading to confusion\nduring model training. Secondly, as an instance-level discriminative task, it\ntends to neglect the essential fine-grained features and complex details\ninherent in unstructured RSIs. To overcome these obstacles, we introduce\nSwiMDiff, a novel self-supervised pre-training framework designed for RSIs.\nSwiMDiff employs a scene-wide matching approach that effectively recalibrates\nlabels to recognize data from the same scene as false negatives. This\nadjustment makes CL more applicable to the nuances of remote sensing.\nAdditionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through\nthe implementation of pixel-level diffusion constraints, we enhance the\nencoder's ability to capture both the global semantic information and the\nfine-grained features of the images more comprehensively. Our proposed\nframework significantly enriches the information available for downstream tasks\nin remote sensing. Demonstrating exceptional performance in change detection\nand land-cover classification tasks, SwiMDiff proves its substantial utility\nand value in the field of remote sensing.\n",
        "title": "SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion\n  Constraint for Remote Sensing Image",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05097",
        "abstract_url": "http://arxiv.org/abs/2401.05097",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Junhoo"
            },
            {
                "last_name": "Kim",
                "first_name": "Yearim"
            },
            {
                "last_name": "Lee",
                "first_name": "Hyunho"
            },
            {
                "last_name": "Kwak",
                "first_name": "Nojun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Although meta-learning seems promising performance in the realm of rapid\nadaptability, it is constrained by fixed cardinality. When faced with tasks of\nvarying cardinalities that were unseen during training, the model lacks its\nability. In this paper, we address and resolve this challenge by harnessing\n`label equivalence' emerged from stochastic numeric label assignments during\nepisodic task sampling. Questioning what defines ``true\" meta-learning, we\nintroduce the ``any-way\" learning paradigm, an innovative model training\napproach that liberates model from fixed cardinality constraints. Surprisingly,\nthis model not only matches but often outperforms traditional fixed-way models\nin terms of performance, convergence speed, and stability. This disrupts\nestablished notions about domain generalization. Furthermore, we argue that the\ninherent label equivalence naturally lacks semantic information. To bridge this\nsemantic information gap arising from label equivalence, we further propose a\nmechanism for infusing semantic class information into the model. This would\nenhance the model's comprehension and functionality. Experiments conducted on\nrenowned architectures like MAML and ProtoNet affirm the effectiveness of our\nmethod.\n",
        "title": "Any-Way Meta Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05098",
        "abstract_url": "http://arxiv.org/abs/2401.05098",
        "authors": [
            {
                "last_name": "Agouzal",
                "first_name": "Eki"
            },
            {
                "last_name": "Argaud",
                "first_name": "Jean-Philippe"
            },
            {
                "last_name": "Bergmann",
                "first_name": "Michel"
            },
            {
                "last_name": "Fert\u00e9",
                "first_name": "Guilhem"
            },
            {
                "last_name": "Michel-Ponnelle",
                "first_name": "Sylvie"
            },
            {
                "last_name": "Taddei",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We propose a projection-based model order reduction procedure for the ageing\nof large prestressed concrete structures. Our work is motivated by applications\nin the nuclear industry, particularly in the simulation of containment\nbuildings. Such numerical simulations involve a multi-modeling approach: a\nthree-dimensional nonlinear thermo-hydro-visco-elastic rheological model is\nused for concrete; and prestressing cables are described by a one-dimensional\nlinear thermo-elastic behavior. A kinematic linkage is performed in order to\nconnect the concrete nodes and the steel nodes: coincident points in each\nmaterial are assumed to have the same displacement. We develop an adaptive\nalgorithm based on a Proper Orthogonal Decomposition (POD) in time and greedy\nin parameter to build a reduced order model (ROM). The nonlinearity of the\noperator entails that the computational cost of the ROM assembly scales with\nthe size of the high-fidelity model. We develop an hyper-reduction strategy\nbased on empirical quadrature to bypass this computational bottleneck: our\napproach relies on the construction of a reduced mesh to speed up online\nassembly costs of the ROM. We provide numerical results for a standard section\nof a double-walled containment building using a qualified and broadly-used\nindustrial grade finite element solver for structural mechanics\n(code$\\_$aster).\n",
        "title": "Projection-based model order reduction for prestressed concrete with an\n  application to the standard section of a nuclear containment building",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05100",
        "abstract_url": "http://arxiv.org/abs/2401.05100",
        "authors": [
            {
                "last_name": "Moriyasu",
                "first_name": "Ryuta"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Sho"
            },
            {
                "last_name": "Kashima",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Model Predictive Control (MPC) is a versatile approach capable of\naccommodating diverse control requirements, holding significant promise for a\nbroad spectrum of industrial applications. Noteworthy challenges associated\nwith MPC include the substantial computational burden and the inherent\ndifficulty in ensuring system stability. Recently, a rapid computation\ntechnique has been introduced as a potential solution. This method guides the\ninput toward convergence with the optimal control problem solution by employing\nthe primal-dual gradient (PDG) dynamics as a controller. Furthermore, stability\nassurances grounded in dissipativity theory have been established. However,\nthese assurances are applicable solely to continuous-time feedback systems. As\na consequence, when the controller undergoes discretization and is implemented\nas a sampled-data system, stability cannot be guaranteed. In this paper, we\npropose a discrete-time dynamical controller, incorporating specific\nmodifications to the PDG approach, and present stability conditions relevant to\nthe resulting sampled-data system. Additionally, we introduce an extension\ndesigned to enhance control performance. Numerical examples substantiate that\nour proposed method not only enhances control effectiveness but also\neffectively discerns stability degradation resulting from discretization, a\nnuance often overlooked by conventional methods.\n",
        "title": "Sampled-Data Primal-Dual Gradient Dynamics in Model Predictive Control",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05111",
        "abstract_url": "http://arxiv.org/abs/2401.05111",
        "authors": [
            {
                "last_name": "Fujita",
                "first_name": "Kenichi"
            },
            {
                "last_name": "Sato",
                "first_name": "Hiroshi"
            },
            {
                "last_name": "Ashihara",
                "first_name": "Takanori"
            },
            {
                "last_name": "Kanagawa",
                "first_name": "Hiroki"
            },
            {
                "last_name": "Delcroix",
                "first_name": "Marc"
            },
            {
                "last_name": "Moriya",
                "first_name": "Takafumi"
            },
            {
                "last_name": "Ijima",
                "first_name": "Yusuke"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "LG",
            ""
        ],
        "abstract": "  The zero-shot text-to-speech (TTS) method, based on speaker embeddings\nextracted from reference speech using self-supervised learning (SSL) speech\nrepresentations, can reproduce speaker characteristics very accurately.\nHowever, this approach suffers from degradation in speech synthesis quality\nwhen the reference speech contains noise. In this paper, we propose a\nnoise-robust zero-shot TTS method. We incorporated adapters into the SSL model,\nwhich we fine-tuned with the TTS model using noisy reference speech. In\naddition, to further improve performance, we adopted a speech enhancement (SE)\nfront-end. With these improvements, our proposed SSL-based zero-shot TTS\nachieved high-quality speech synthesis with noisy reference speech. Through the\nobjective and subjective evaluations, we confirmed that the proposed method is\nhighly robust to noise in reference speech, and effectively works in\ncombination with SE.\n",
        "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on\n  self-supervised speech-representation model with adapters",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05112",
        "abstract_url": "http://arxiv.org/abs/2401.05112",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shuxin"
            },
            {
                "last_name": "Rigger",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Extensible Markup Language (XML) is a widely used file format for data\nstorage and transmission. Many XML processors support XPath, a query language\nthat enables the extraction of elements from XML documents. These systems can\nbe affected by logic bugs, which are bugs that cause the processor to return\nincorrect results. In order to tackle such bugs, we propose a new approach,\nwhich we realized as a system called XPress. As a test oracle, XPress relies on\ndifferential testing, which compares the results of multiple systems on the\nsame test input, and identifies bugs through discrepancies in their outputs. As\ntest inputs, XPress generates both XML documents and XPath queries. Aiming to\ngenerate meaningful queries that compute non-empty results, XPress selects a\nso-called targeted node to guide the XPath expression generation process. Using\nthe targeted node, XPress generates XPath expressions that reference existing\ncontext related to the targeted node, such as its tag name and attributes,\nwhile also guaranteeing that a predicate evaluates to true before further\nexpanding the query. We tested our approach on six mature XML processors,\nBaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system.\nIn total, we have found 20 unique bugs in these systems, of which 25 have been\nverified by the developers, and 12 of which have been fixed. XPress is\nefficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fast\nas naive random generation. We expect that the effectiveness and simplicity of\nour approach will help to improve the robustness of many XML processors.\n",
        "title": "Finding XPath Bugs in XML Document Processors via Differential Testing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05115",
        "abstract_url": "http://arxiv.org/abs/2401.05115",
        "authors": [
            {
                "last_name": "Tsiakas",
                "first_name": "Kostas"
            },
            {
                "last_name": "Murray-Rust",
                "first_name": "Dave"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  This paper aims to develop a semi-formal design space for Human-AI\ninteractions, by building a set of interaction primitives which specify the\ncommunication between users and AI systems during their interaction. We show\nhow these primitives can be combined into a set of interaction patterns which\ncan provide an abstract specification for exchanging messages between humans\nand AI/ML models to carry out purposeful interactions. The motivation behind\nthis is twofold: firstly, to provide a compact generalisation of existing\npractices, that highlights the similarities and differences between systems in\nterms of their interaction behaviours; and secondly, to support the creation of\nnew systems, in particular by opening the space of possibilities for\ninteractions with models. We present a short literature review on frameworks,\nguidelines and taxonomies related to the design and implementation of HAI\ninteractions, including human-in-the-loop, explainable AI, as well as hybrid\nintelligence and collaborative learning approaches. From the literature review,\nwe define a vocabulary for describing information exchanges in terms of\nproviding and requesting particular model-specific data types. Based on this\nvocabulary, a message passing model for interactions between humans and models\nis presented, which we demonstrate can account for existing systems and\napproaches. Finally, we build this into design patterns as mid-level constructs\nthat capture common interactional structures. We discuss how this approach can\nbe used towards a design space for Human-AI interactions that creates new\npossibilities for designs as well as keeping track of implementation issues and\nconcerns.\n",
        "title": "Unpacking Human-AI interactions: From interaction primitives to a design\n  space",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05121",
        "abstract_url": "http://arxiv.org/abs/2401.05121",
        "authors": [
            {
                "last_name": "Fayza",
                "first_name": "Farbin"
            },
            {
                "last_name": "Rao",
                "first_name": "Satyavolu Papa"
            },
            {
                "last_name": "Bunandar",
                "first_name": "Darius"
            },
            {
                "last_name": "Gupta",
                "first_name": "Udit"
            },
            {
                "last_name": "Joshi",
                "first_name": "Ajay"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG"
        ],
        "abstract": "  Photonic integrated circuits are finding use in a variety of applications\nincluding optical transceivers, LIDAR, bio-sensing, photonic quantum computing,\nand Machine Learning (ML). In particular, with the exponentially increasing\nsizes of ML models, photonics-based accelerators are getting special attention\nas a sustainable solution because they can perform ML inferences with multiple\norders of magnitude higher energy efficiency than CMOS-based accelerators.\nHowever, recent studies have shown that hardware manufacturing and\ninfrastructure contribute significantly to the carbon footprint of computing\ndevices, even surpassing the emissions generated during their use. For example,\nthe manufacturing process accounts for 74% of the total carbon emissions from\nApple in 2019. This prompts us to ask -- if we consider both the embodied\n(manufacturing) and operational carbon cost of photonics, is it indeed a viable\navenue for a sustainable future? So, in this paper, we build a carbon footprint\nmodel for photonic chips and investigate the sustainability of photonics-based\naccelerators by conducting a case study on ADEPT, a photonics-based accelerator\nfor deep neural network inference. Our analysis shows that photonics can reduce\nboth operational and embodied carbon footprints with its high energy efficiency\nand at least 4$\\times$ less fabrication carbon cost per unit area than 28 nm\nCMOS.\n",
        "title": "Photonics for Sustainable Computing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05125",
        "abstract_url": "http://arxiv.org/abs/2401.05125",
        "authors": [
            {
                "last_name": "Garda",
                "first_name": "Samuele"
            },
            {
                "last_name": "Leser",
                "first_name": "Ulf"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Biomedical entity linking (BEL) is the task of grounding entity mentions to a\nknowledge base (KB). A popular approach to the task are name-based methods,\ni.e. those identifying the most appropriate name in the KB for a given mention,\neither via dense retrieval or autoregressive modeling. However, as these\nmethods directly return KB names, they cannot cope with homonyms, i.e.\ndifferent KB entities sharing the exact same name. This significantly affects\ntheir performance, especially for KBs where homonyms account for a large amount\nof entity mentions (e.g. UMLS and NCBI Gene). We therefore present BELHD\n(Biomedical Entity Linking with Homonym Disambiguation), a new name-based\nmethod that copes with this challenge. Specifically, BELHD builds upon the\nBioSyn (Sung et al.,2020) model introducing two crucial extensions. First, it\nperforms a preprocessing of the KB in which it expands homonyms with an\nautomatically chosen disambiguating string, thus enforcing unique linking\ndecisions. Second, we introduce candidate sharing, a novel strategy to select\ncandidates for contrastive learning that enhances the overall training signal.\nExperiments with 10 corpora and five entity types show that BELHD improves upon\nstate-of-the-art approaches, achieving the best results in 6 out 10 corpora\nwith an average improvement of 4.55pp recall@1. Furthermore, the KB\npreprocessing is orthogonal to the core prediction model and thus can also\nimprove other methods, which we exemplify for GenBioEL (Yuan et al, 2022), a\ngenerative name-based BEL approach. Code is available at: link added upon\npublication.\n",
        "title": "BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05126",
        "abstract_url": "http://arxiv.org/abs/2401.05126",
        "authors": [
            {
                "last_name": "Nagamori",
                "first_name": "Teru"
            },
            {
                "last_name": "Shiota",
                "first_name": "Sayaka"
            },
            {
                "last_name": "Kiya",
                "first_name": "Hitoshi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We propose a novel method for privacy-preserving deep neural networks (DNNs)\nwith the Vision Transformer (ViT). The method allows us not only to train\nmodels and test with visually protected images but to also avoid the\nperformance degradation caused from the use of encrypted images, whereas\nconventional methods cannot avoid the influence of image encryption. A domain\nadaptation method is used to efficiently fine-tune ViT with encrypted images.\nIn experiments, the method is demonstrated to outperform conventional methods\nin an image classification task on the CIFAR-10 and ImageNet datasets in terms\nof classification accuracy.\n",
        "title": "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving\n  Vision Transformer",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05131",
        "abstract_url": "http://arxiv.org/abs/2401.05131",
        "authors": [
            {
                "last_name": "Pichon-Pharabod",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SC",
            ""
        ],
        "abstract": "  We provide an algorithm for computing an effective basis of homology of\nelliptic surfaces over the complex projective line on which integration of\nperiods can be carried out. This allows the heuristic recovery of several\nalgebraic invariants of the surface, notably the N\\'eron-Severi lattice, the\ntranscendental lattice, the Mordell-Weil group and the Mordell-Weil lattice.\nThis algorithm comes with a SageMath implementation.\n",
        "title": "A semi-numerical algorithm for the homology lattice and periods of\n  complex elliptic surfaces over the projective line",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05133",
        "abstract_url": "http://arxiv.org/abs/2401.05133",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Siqi"
            },
            {
                "last_name": "Marris",
                "first_name": "Luke"
            },
            {
                "last_name": "Lanctot",
                "first_name": "Marc"
            },
            {
                "last_name": "Piliouras",
                "first_name": "Georgios"
            },
            {
                "last_name": "Leibo",
                "first_name": "Joel Z."
            },
            {
                "last_name": "Heess",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "MA"
        ],
        "abstract": "  We study computationally efficient methods for finding equilibria in n-player\ngeneral-sum games, specifically ones that afford complex visuomotor skills. We\nshow how existing methods would struggle in this setting, either\ncomputationally or in theory. We then introduce NeuPL-JPSRO, a neural\npopulation learning algorithm that benefits from transfer learning of skills\nand converges to a Coarse Correlated Equilibrium (CCE) of the game. We show\nempirical convergence in a suite of OpenSpiel games, validated rigorously by\nexact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our\napproach enables adaptive coordination in a MuJoCo control domain and skill\ntransfer in capture-the-flag. Our work shows that equilibrium convergent\npopulation learning can be implemented at scale and in generality, paving the\nway towards solving real-world games between heterogeneous players with mixed\nmotives.\n",
        "title": "Neural Population Learning beyond Symmetric Zero-sum Games",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05134",
        "abstract_url": "http://arxiv.org/abs/2401.05134",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Abhisek"
            },
            {
                "last_name": "Bera",
                "first_name": "Shreyangshu"
            },
            {
                "last_name": "Saha",
                "first_name": "Sriparna"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Pushpak"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Samrat"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Over the past few years, the use of the Internet for healthcare-related tasks\nhas grown by leaps and bounds, posing a challenge in effectively managing and\nprocessing information to ensure its efficient utilization. During moments of\nemotional turmoil and psychological challenges, we frequently turn to the\ninternet as our initial source of support, choosing this over discussing our\nfeelings with others due to the associated social stigma. In this paper, we\npropose a new task of multi-modal medical concern summary (MMCS) generation,\nwhich provides a short and precise summary of patients' major concerns brought\nup during the consultation. Nonverbal cues, such as patients' gestures and\nfacial expressions, aid in accurately identifying patients' concerns. Doctors\nalso consider patients' personal information, such as age and gender, in order\nto describe the medical condition appropriately. Motivated by the potential\nefficacy of patients' personal context and visual gestures, we propose a\ntransformer-based multi-task, multi-modal intent-recognition, and medical\nconcern summary generation (IR-MMCSG) system. Furthermore, we propose a\nmultitasking framework for intent recognition and medical concern summary\ngeneration for doctor-patient consultations. We construct the first multi-modal\nmedical concern summary generation (MM-MediConSummation) corpus, which includes\npatient-doctor consultations annotated with medical concern summaries, intents,\npatient personal information, doctor's recommendations, and keywords. Our\nexperiments and analysis demonstrate (a) the significant role of patients'\nexpressions/gestures and their personal information in intent identification\nand medical concern summary generation, and (b) the strong correlation between\nintent recognition and patients' medical concern summary generation\n  The dataset and source code are available at https://github.com/NLP-RL/MMCSG.\n",
        "title": "Yes, this is what I was looking for! Towards Multi-modal Medical\n  Consultation Concern Summary Generation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05136",
        "abstract_url": "http://arxiv.org/abs/2401.05136",
        "authors": [
            {
                "last_name": "Tufano",
                "first_name": "Rosalia"
            },
            {
                "last_name": "Dabi\u0107",
                "first_name": "Ozren"
            },
            {
                "last_name": "Mastropaolo",
                "first_name": "Antonio"
            },
            {
                "last_name": "Ciniselli",
                "first_name": "Matteo"
            },
            {
                "last_name": "Bavota",
                "first_name": "Gabriele"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The automation of code review has been tackled by several researchers with\nthe goal of reducing its cost. The adoption of deep learning in software\nengineering pushed the automation to new boundaries, with techniques imitating\ndevelopers in generative tasks, such as commenting on a code change as a\nreviewer would do or addressing a reviewer's comment by modifying code. The\nperformance of these techniques is usually assessed through quantitative\nmetrics, e.g., the percentage of instances in the test set for which correct\npredictions are generated, leaving many open questions on the techniques'\ncapabilities. For example, knowing that an approach is able to correctly\naddress a reviewer's comment in 10% of cases is of little value without knowing\nwhat was asked by the reviewer: What if in all successful cases the code change\nrequired to address the comment was just the removal of an empty line? In this\npaper we aim at characterizing the cases in which three code review automation\ntechniques tend to succeed or fail in the two above-described tasks. The study\nhas a strong qualitative focus, with ~105 man-hours of manual inspection\ninvested in manually analyzing correct and wrong predictions generated by the\nthree techniques, for a total of 2,291 inspected predictions. The output of\nthis analysis are two taxonomies reporting, for each of the two tasks, the\ntypes of code changes on which the experimented techniques tend to succeed or\nto fail, pointing to areas for future work. A result of our manual analysis was\nalso the identification of several issues in the datasets used to train and\ntest the experimented techniques. Finally, we assess the importance of\nresearching in techniques specialized for code review automation by comparing\ntheir performance with ChatGPT, a general purpose large language model, finding\nthat ChatGPT struggles in commenting code as a human reviewer would do.\n",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05137",
        "abstract_url": "http://arxiv.org/abs/2401.05137",
        "authors": [
            {
                "last_name": "Daho",
                "first_name": "Mostafa El Habib"
            },
            {
                "last_name": "Li",
                "first_name": "Yihao"
            },
            {
                "last_name": "Zeghlache",
                "first_name": "Rachid"
            },
            {
                "last_name": "Boit\u00e9",
                "first_name": "Hugo Le"
            },
            {
                "last_name": "Deman",
                "first_name": "Pierre"
            },
            {
                "last_name": "Borderie",
                "first_name": "Laurent"
            },
            {
                "last_name": "Ren",
                "first_name": "Hugang"
            },
            {
                "last_name": "Mannivanan",
                "first_name": "Niranchana"
            },
            {
                "last_name": "Lepicard",
                "first_name": "Capucine"
            },
            {
                "last_name": "Cochener",
                "first_name": "B\u00e9atrice"
            },
            {
                "last_name": "Couturier",
                "first_name": "Aude"
            },
            {
                "last_name": "Tadayoni",
                "first_name": "Ramin"
            },
            {
                "last_name": "Conze",
                "first_name": "Pierre-Henri"
            },
            {
                "last_name": "Lamard",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Quellec",
                "first_name": "Gwenol\u00e9"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading\ncause of blindness worldwide. Traditionally, DR is monitored using Color Fundus\nPhotography (CFP), a widespread 2-D imaging modality. However, DR\nclassifications based on CFP have poor predictive power, resulting in\nsuboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a\nrecent 3-D imaging modality offering enhanced structural and functional\ninformation (blood flow) with a wider field of view. This paper investigates\nautomatic DR severity assessment using 3-D OCTA. A straightforward solution to\nthis task is a 3-D neural network classifier. However, 3-D architectures have\nnumerous parameters and typically require many training samples. A lighter\nsolution consists in using 2-D neural network classifiers processing 2-D\nen-face (or frontal) projections and/or 2-D cross-sectional slices. Such an\napproach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-face\nflow maps are often used to detect avascular zones and neovascularization, and\n2) cross-sectional slices are commonly analyzed to detect macular edemas, for\ninstance. However, arbitrary data reduction or selection might result in\ninformation loss. Two complementary strategies are thus proposed to optimally\nsummarize OCTA volumes with 2-D images: 1) a parametric en-face projection\noptimized through deep learning and 2) a cross-sectional slice selection\nprocess controlled through gradient-based attribution. The full summarization\nand DR classification pipeline is trained from end to end. The automatic 2-D\nsummary can be displayed in a viewer or printed in a report to support the\ndecision. We show that the proposed 2-D summarization and classification\npipeline outperforms direct 3-D classification with the advantage of improved\ninterpretability.\n",
        "title": "DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography\n  Angiography for Automatic Diabetic Retinopathy Diagnosis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05143",
        "abstract_url": "http://arxiv.org/abs/2401.05143",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We propose and analyze a general framework called nonlinear preconditioned\nprimal-dual with projection for solving nonconvex-nonconcave and non-smooth\nsaddle-point problems. The framework consists of two steps. The first is a\nnonlinear preconditioned map followed by a relaxed projection onto the\nseparating hyperspace we construct. One key to the method is the selection of\npreconditioned operators, which tailors to the structure of the saddle-point\nproblem and is allowed to be nonlinear and asymmetric. The other is the\nconstruction of separating hyperspace, which guarantees fast convergence. This\nframework paves the way for constructing nonlinear preconditioned primal-dual\nalgorithms. We show that weak convergence, and so is sublinear convergence\nunder the assumption of the convexity of saddle-point problems and linear\nconvergence under a metric subregularity. We also show that many existing\nprimal-daul methods, such as the generalized primal-dual algorithm method, are\nspecial cases of relaxed preconditioned primal-dual with projection.\n",
        "title": "Nonlinear preconditioned primal-dual method for a class of structured\n  minimax problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05144",
        "abstract_url": "http://arxiv.org/abs/2401.05144",
        "authors": [
            {
                "last_name": "McKechnie",
                "first_name": "Jack"
            },
            {
                "last_name": "McDonald",
                "first_name": "Graham"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large archival collections, such as email or government documents, must be\nmanually reviewed to identify any sensitive information before the collection\ncan be released publicly. Sensitivity classification has received a lot of\nattention in the literature. However, more recently, there has been increasing\ninterest in developing sensitivity-aware search engines that can provide users\nwith relevant search results, while ensuring that no sensitive documents are\nreturned to the user. Sensitivity-aware search would mitigate the need for a\nmanual sensitivity review prior to collections being made available publicly.\nTo develop such systems, there is a need for test collections that contain\nrelevance assessments for a set of information needs as well as ground-truth\nlabels for a variety of sensitivity categories. The well-known Enron email\ncollection contains a classification ground-truth that can be used to represent\nsensitive information, e.g., the Purely Personal and Personal but in\nProfessional Context categories can be used to represent sensitive personal\ninformation. However, the existing Enron collection does not contain a set of\ninformation needs and relevance assessments. In this work, we present a\ncollection of fifty information needs (topics) with crowdsourced query\nformulations (3 per topic) and relevance assessments (11,471 in total) for the\nEnron collection (mean number of relevant documents per topic = 11, variance =\n34.7). The developed information needs, queries and relevance judgements are\navailable on GitHub and will be available along with the existing Enron\ncollection through the popular ir_datasets library. Our proposed collection\nresults in the first freely available test collection for developing\nsensitivity-aware search systems.\n",
        "title": "SARA: A Collection of Sensitivity-Aware Relevance Assessments",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05145",
        "abstract_url": "http://arxiv.org/abs/2401.05145",
        "authors": [
            {
                "last_name": "Beinat",
                "first_name": "Matilda"
            },
            {
                "last_name": "Beinat",
                "first_name": "Julian"
            },
            {
                "last_name": "Shoaib",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Magenti",
                "first_name": "Jorge Gomez"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Projected to impact 1.6 million people in the UK by 2040 and costing\n{\\pounds}25 billion annually, dementia presents a growing challenge to society.\nThis study, a pioneering effort to predict the translational potential of\ndementia research using machine learning, hopes to address the slow translation\nof fundamental discoveries into practical applications despite dementia's\nsignificant societal and economic impact. We used the Dimensions database to\nextract data from 43,091 UK dementia research publications between the years\n1990-2023, specifically metadata (authors, publication year etc.), concepts\nmentioned in the paper, and the paper abstract. To prepare the data for machine\nlearning we applied methods such as one hot encoding and/or word embeddings. We\ntrained a CatBoost Classifier to predict if a publication will be cited in a\nfuture patent or clinical trial. We trained several model variations. The model\ncombining metadata, concept, and abstract embeddings yielded the highest\nperformance: for patent predictions, an Area Under the Receiver Operating\nCharacteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trial\npredictions, an AUROC of 0.81 and 75.11% accuracy. The results demonstrate that\nintegrating machine learning within current research methodologies can uncover\noverlooked publications, expediting the identification of promising research\nand potentially transforming dementia research by predicting real-world impact\nand guiding translational strategies.\n",
        "title": "Machine Learning to Promote Translational Research: Predicting Patent\n  and Clinical Trial Inclusion in Dementia Research",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05146",
        "abstract_url": "http://arxiv.org/abs/2401.05146",
        "authors": [
            {
                "last_name": "Romandini",
                "first_name": "Nicol\u00f2"
            },
            {
                "last_name": "Mora",
                "first_name": "Alessio"
            },
            {
                "last_name": "Mazzocca",
                "first_name": "Carlo"
            },
            {
                "last_name": "Montanari",
                "first_name": "Rebecca"
            },
            {
                "last_name": "Bellavista",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Federated Learning (FL) enables collaborative training of a Machine Learning\n(ML) model across multiple parties, facilitating the preservation of users' and\ninstitutions' privacy by keeping data stored locally. Instead of centralizing\nraw data, FL exchanges locally refined model parameters to build a global model\nincrementally. While FL is more compliant with emerging regulations such as the\nEuropean General Data Protection Regulation (GDPR), ensuring the right to be\nforgotten in this context - allowing FL participants to remove their data\ncontributions from the learned model - remains unclear. In addition, it is\nrecognized that malicious clients may inject backdoors into the global model\nthrough updates, e.g. to generate mispredictions on specially crafted data\nexamples. Consequently, there is the need for mechanisms that can guarantee\nindividuals the possibility to remove their data and erase malicious\ncontributions even after aggregation, without compromising the already acquired\n\"good\" knowledge. This highlights the necessity for novel Federated Unlearning\n(FU) algorithms, which can efficiently remove specific clients' contributions\nwithout full model retraining. This survey provides background concepts,\nempirical evidence, and practical guidelines to design/implement efficient FU\nschemes. Our study includes a detailed analysis of the metrics for evaluating\nunlearning in FL and presents an in-depth literature review categorizing\nstate-of-the-art FU contributions under a novel taxonomy. Finally, we outline\nthe most relevant and still open technical challenges, by identifying the most\npromising research directions in the field.\n",
        "title": "Federated Unlearning: A Survey on Methods, Design Guidelines, and\n  Evaluation Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05148",
        "abstract_url": "http://arxiv.org/abs/2401.05148",
        "authors": [
            {
                "last_name": "Gritz",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Hoppe",
                "first_name": "Anett"
            },
            {
                "last_name": "Ewerth",
                "first_name": "Ralph"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Nowadays, learning increasingly involves the usage of search engines and web\nresources. The related interdisciplinary research field search as learning aims\nto understand how people learn on the web. Previous work has investigated\nseveral feature classes to predict, for instance, the expected knowledge gain\nduring web search. Therein, eye-tracking features have not been extensively\nstudied so far. In this paper, we extend a previously used reading model from a\nline-based one to one that can detect reading sequences across multiple lines.\nWe use publicly available study data from a web-based learning task to examine\nthe relationship between our feature set and the participants' test scores. Our\nfindings demonstrate that learners with higher knowledge gain spent\nsignificantly more time reading, and processing more words in total. We also\nfind evidence that faster reading at the expense of more backward regressions\nmay be an indicator of better web-based learning. We make our code publicly\navailable at https://github.com/TIBHannover/reading_web_search.\n",
        "title": "On the Influence of Reading Sequences on Knowledge Gain during Web\n  Search",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05152",
        "abstract_url": "http://arxiv.org/abs/2401.05152",
        "authors": [
            {
                "last_name": "Fernandez-Cortizas",
                "first_name": "Miguel"
            },
            {
                "last_name": "Bavle",
                "first_name": "Hriday"
            },
            {
                "last_name": "Perez-Saura",
                "first_name": "David"
            },
            {
                "last_name": "Sanchez-Lopez",
                "first_name": "Jose Luis"
            },
            {
                "last_name": "Campoy",
                "first_name": "Pascual"
            },
            {
                "last_name": "Voos",
                "first_name": "Holger"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to\nenable multiple robots to operate in complex environments. Most CSLAM\ntechniques rely on raw sensor measurement or low-level features such as\nkeyframe descriptors, which can lead to wrong loop closures due to the lack of\ndeep understanding of the environment. Moreover, the exchange of these\nmeasurements and low-level features among the robots requires the transmission\nof a significant amount of data, which limits the scalability of the system. To\novercome these limitations, we present Multi S-Graphs, a decentralized CSLAM\nsystem that utilizes high-level semantic-relational information embedded in the\nfour-layered hierarchical and optimizable situational graphs for cooperative\nmap generation and localization while minimizing the information exchanged\nbetween the robots. To support this, we present a novel room-based descriptor\nwhich, along with its connected walls, is used to perform inter-robot loop\nclosures, addressing the challenges of multi-robot kidnapped problem\ninitialization. Multiple experiments in simulated and real environments\nvalidate the improvement in accuracy and robustness of the proposed approach\nwhile reducing the amount of data exchanged between robots compared to other\nstate-of-the-art approaches.\n  Software available within a docker image:\nhttps://github.com/snt-arg/multi_s_graphs_docker\n",
        "title": "Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational\n  Collaborative SLAM",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05153",
        "abstract_url": "http://arxiv.org/abs/2401.05153",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Yinghui"
            },
            {
                "last_name": "Qu",
                "first_name": "Litao"
            },
            {
                "last_name": "Zhang",
                "first_name": "ShiZhou"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiuwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yanning"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Fusion of a panchromatic (PAN) image and corresponding multispectral (MS)\nimage is also known as pansharpening, which aims to combine abundant spatial\ndetails of PAN and spectral information of MS. Due to the absence of\nhigh-resolution MS images, available deep-learning-based methods usually follow\nthe paradigm of training at reduced resolution and testing at both reduced and\nfull resolution. When taking original MS and PAN images as inputs, they always\nobtain sub-optimal results due to the scale variation. In this paper, we\npropose to explore the self-supervised representation of pansharpening by\ndesigning a cross-predictive diffusion model, named CrossDiff. It has two-stage\ntraining. In the first stage, we introduce a cross-predictive pretext task to\npre-train the UNet structure based on conditional DDPM, while in the second\nstage, the encoders of the UNets are frozen to directly extract spatial and\nspectral features from PAN and MS, and only the fusion head is trained to adapt\nfor pansharpening task. Extensive experiments show the effectiveness and\nsuperiority of the proposed model compared with state-of-the-art supervised and\nunsupervised methods. Besides, the cross-sensor experiments also verify the\ngeneralization ability of proposed self-supervised representation learners for\nother satellite's datasets. We will release our code for reproducibility.\n",
        "title": "CrossDiff: Exploring Self-Supervised Representation of Pansharpening via\n  Cross-Predictive Diffusion Model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05154",
        "abstract_url": "http://arxiv.org/abs/2401.05154",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Weichuang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jieru"
            },
            {
                "last_name": "Shen",
                "first_name": "Guan"
            },
            {
                "last_name": "Chen",
                "first_name": "Quan"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Guo",
                "first_name": "Minyi"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "PL"
        ],
        "abstract": "  With the increasing demand for computing capability given limited resource\nand power budgets, it is crucial to deploy applications to customized\naccelerators like FPGAs. However, FPGA programming is non-trivial. Although\nexisting high-level synthesis (HLS) tools improve productivity to a certain\nextent, they are limited in scope and capability to support sufficient\nFPGA-oriented optimizations. This paper focuses on FPGA-based accelerators and\nproposes POM, an optimizing framework built on multi-level intermediate\nrepresentation (MLIR). POM has several features which demonstrate its scope and\ncapability of performance optimization. First, most HLS tools depend\nexclusively on a single-level IR to perform all the optimizations, introducing\nexcessive information into the IR and making debugging an arduous task. In\ncontrast, POM introduces three layers of IR to perform operations at suitable\nabstraction levels, streamlining the implementation and debugging process and\nexhibiting better flexibility, extensibility, and systematicness. Second, POM\nintegrates the polyhedral model into MLIR, enabling advanced dependence\nanalysis and various FPGA-oriented loop transformations. By representing nested\nloops with integer sets and maps, loop transformations can be conducted\nconveniently through manipulations on polyhedral semantics. Finally, to further\nrelieve design effort, POM has a user-friendly programming interface (DSL) that\nallows a concise description of computation and includes a rich collection of\nscheduling primitives. An automatic design space exploration (DSE) engine is\nprovided to search for high-performance optimization schemes efficiently and\ngenerate optimized accelerators automatically. Experimental results show that\nPOM achieves a $6.46\\times$ average speedup on typical benchmark suites and a\n$6.06\\times$ average speedup on real-world applications compared to the\nstate-of-the-art.\n",
        "title": "An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator\n  Generation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05157",
        "abstract_url": "http://arxiv.org/abs/2401.05157",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yitao"
            },
            {
                "last_name": "Li",
                "first_name": "Heng-Chao"
            },
            {
                "last_name": "Liu",
                "first_name": "Nanqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Rui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the conventional change detection (CD) pipeline, two manually registered\nand labeled remote sensing datasets serve as the input of the model for\ntraining and prediction. However, in realistic scenarios, data from different\nperiods or sensors could fail to be aligned as a result of various coordinate\nsystems. Geometric distortion caused by coordinate shifting remains a thorny\nissue for CD algorithms. In this paper, we propose a reusable self-supervised\nframework for bitemporal geometric distortion in CD tasks. The whole framework\nis composed of Pretext Representation Pre-training, Bitemporal Image Alignment,\nand Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the\nkey components of the framework can be reused for assistance in the bitemporal\nimage alignment, while simultaneously enhancing the performance of the CD\ndecoder. Experimental results in 2 large-scale realistic scenarios demonstrate\nthat our proposed method can alleviate the bitemporal geometric distortion in\nCD tasks.\n",
        "title": "Toward distortion-aware change detection in realistic scenarios",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05159",
        "abstract_url": "http://arxiv.org/abs/2401.05159",
        "authors": [
            {
                "last_name": "Farooq",
                "first_name": "Muhammad Ali"
            },
            {
                "last_name": "Yao",
                "first_name": "Wang"
            },
            {
                "last_name": "Schukat",
                "first_name": "Michael"
            },
            {
                "last_name": "Little",
                "first_name": "Mark A"
            },
            {
                "last_name": "Corcoran",
                "first_name": "Peter"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  This study explores the utilization of Dermatoscopic synthetic data generated\nthrough stable diffusion models as a strategy for enhancing the robustness of\nmachine learning model training. Synthetic data generation plays a pivotal role\nin mitigating challenges associated with limited labeled datasets, thereby\nfacilitating more effective model training. In this context, we aim to\nincorporate enhanced data transformation techniques by extending the recent\nsuccess of few-shot learning and a small amount of data representation in\ntext-to-image latent diffusion models. The optimally tuned model is further\nused for rendering high-quality skin lesion synthetic data with diverse and\nrealistic characteristics, providing a valuable supplement and diversity to the\nexisting training data. We investigate the impact of incorporating newly\ngenerated synthetic data into the training pipeline of state-of-art machine\nlearning models, assessing its effectiveness in enhancing model performance and\ngeneralization to unseen real-world data. Our experimental results demonstrate\nthe efficacy of the synthetic data generated through stable diffusion models\nhelps in improving the robustness and adaptability of end-to-end CNN and vision\ntransformer models on two different real-world skin lesion datasets.\n",
        "title": "Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion\n  Models for Enhanced Skin Disease Classification using ViT and CNN",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05163",
        "abstract_url": "http://arxiv.org/abs/2401.05163",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Yang",
                "first_name": "Dingkang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yue"
            },
            {
                "last_name": "Lei",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lihua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Medical visual question answering (VQA) is a challenging multimodal task,\nwhere Vision-Language Pre-training (VLP) models can effectively improve the\ngeneralization performance. However, most methods in the medical field treat\nVQA as an answer classification task which is difficult to transfer to\npractical application scenarios. Additionally, due to the privacy of medical\nimages and the expensive annotation process, large-scale medical image-text\npairs datasets for pretraining are severely lacking. In this paper, we propose\na large-scale MultI-task Self-Supervised learning based framework (MISS) for\nmedical VQA tasks. Unlike existing methods, we treat medical VQA as a\ngenerative task. We unify the text encoder and multimodal encoder and align\nimage-text features through multi-task learning. Furthermore, we propose a\nTransfer-and-Caption method that extends the feature space of single-modal\nimage datasets using large language models (LLMs), enabling those traditional\nmedical vision field task data to be applied to VLP. Experiments show that our\nmethod achieves excellent results with fewer multimodal datasets and\ndemonstrates the advantages of generative VQA models. The code and model\nweights will be released upon the paper's acceptance.\n",
        "title": "MISS: A Generative Pretraining and Finetuning Approach for Med-VQA",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05164",
        "abstract_url": "http://arxiv.org/abs/2401.05164",
        "authors": [
            {
                "last_name": "Tarable",
                "first_name": "Alberto"
            },
            {
                "last_name": "Dossi",
                "first_name": "Laura"
            },
            {
                "last_name": "Virone",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Nordio",
                "first_name": "Alessandro"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Motivated by the challenges of future 6G communications where terahertz (THz)\nfrequencies, intelligent reflective surfaces (IRSs) and ultra-wideband (UWB)\nsignals coexist, we analyse and propose a set of efficient techniques for\nconfiguring the IRS when the signal bandwidth is a significant fraction of the\ncentral frequency (up to 50%). To the best of our knowledge this is the first\ntime that IRS configuration techniques are analyzed for such huge bandwidths.\nIn our work we take into account for the channel model, the power spectral\ndensity of the signal reflected by the IRS and the network geometry. We\nevaluate the proposed solutions in terms of achievable rate and compare it\nagainst an upper bound we derived. Our results hint rules for designing\nIRS-aided communication systems and allow to draw conclusions on the trade-off\nbetween performance and complexity required for configuring the IRS.\n",
        "title": "IRS Configuration Techniques for Ultra Wideband Signals and THz\n  Communications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05165",
        "abstract_url": "http://arxiv.org/abs/2401.05165",
        "authors": [
            {
                "last_name": "Seidl",
                "first_name": "Helmut"
            },
            {
                "last_name": "Erhard",
                "first_name": "Julian"
            },
            {
                "last_name": "Tilscher",
                "first_name": "Sarah"
            },
            {
                "last_name": "Schwarz",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL",
            ""
        ],
        "abstract": "  The weakly relational domain of Octagons offers a decent compromise between\nprecision and efficiency for numerical properties. Here, we are concerned with\nthe construction of non-numerical relational domains. We provide a general\nconstruction of weakly relational domains, which we exemplify with an extension\nof constant propagation by disjunctions. Since for the resulting domain of\n2-disjunctive formulas, satisfiability is NP-complete, we provide a general\nconstruction for a further, more abstract weakly relational domain where the\nabstract operations of restriction and least upper bound can be efficiently\nimplemented. In the second step, we consider a relational domain that tracks\nconjunctions of inequalities between variables, and between variables and\nconstants for arbitrary partial orders of values. Examples are sub(multi)sets,\nas well as prefix, substring or scattered substring orderings on strings. When\nthe partial order is a lattice, we provide precise polynomial algorithms for\nsatisfiability, restriction, and the best abstraction of disjunction.\nComplementary to the constructions for lattices, we find that, in general,\nsatisfiability of conjunctions is NP-complete. We therefore again provide\npolynomial abstract versions of restriction, conjunction, and join. By using\nour generic constructions, these domains are extended to weakly relational\ndomains that additionally track disjunctions. For all our domains, we indicate\nhow abstract transformers for assignments and guards can be constructed.\n",
        "title": "Non-Numerical Weakly Relational Domains",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05166",
        "abstract_url": "http://arxiv.org/abs/2401.05166",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Siyang"
            },
            {
                "last_name": "Spitale",
                "first_name": "Micol"
            },
            {
                "last_name": "Luo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Palmero",
                "first_name": "Cristina"
            },
            {
                "last_name": "Barquero",
                "first_name": "German"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hengde"
            },
            {
                "last_name": "Escalera",
                "first_name": "Sergio"
            },
            {
                "last_name": "Valstar",
                "first_name": "Michel"
            },
            {
                "last_name": "Baur",
                "first_name": "Tobias"
            },
            {
                "last_name": "Ringeval",
                "first_name": "Fabien"
            },
            {
                "last_name": "Andre",
                "first_name": "Elisabeth"
            },
            {
                "last_name": "Gunes",
                "first_name": "Hatice"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In dyadic interactions, humans communicate their intentions and state of mind\nusing verbal and non-verbal cues, where multiple different facial reactions\nmight be appropriate in response to a specific speaker behaviour. Then, how to\ndevelop a machine learning (ML) model that can automatically generate multiple\nappropriate, diverse, realistic and synchronised human facial reactions from an\npreviously unseen speaker behaviour is a challenging task. Following the\nsuccessful organisation of the first REACT challenge (REACT 2023), this edition\nof the challenge (REACT 2024) employs a subset used by the previous challenge,\nwhich contains segmented 30-secs dyadic interaction clips originally recorded\nas part of the NOXI and RECOLA datasets, encouraging participants to develop\nand benchmark Machine Learning (ML) models that can generate multiple\nappropriate facial reactions (including facial image sequences and their\nattributes) given an input conversational partner's stimulus under various\ndyadic video conference scenarios. This paper presents: (i) the guidelines of\nthe REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)\nthe performance of the baseline systems on the two proposed sub-challenges:\nOffline Multiple Appropriate Facial Reaction Generation and Online Multiple\nAppropriate Facial Reaction Generation, respectively. The challenge baseline\ncode is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2024.\n",
        "title": "REACT 2024: the Second Multiple Appropriate Facial Reaction Generation\n  Challenge",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05168",
        "abstract_url": "http://arxiv.org/abs/2401.05168",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Nanqing"
            },
            {
                "last_name": "Xu",
                "first_name": "Xun"
            },
            {
                "last_name": "Su",
                "first_name": "Yongyi"
            },
            {
                "last_name": "Liu",
                "first_name": "Chengxin"
            },
            {
                "last_name": "Gong",
                "first_name": "Peiliang"
            },
            {
                "last_name": "Li",
                "first_name": "Heng-Chao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Domain adaptation is crucial in aerial imagery, as the visual representation\nof these images can significantly vary based on factors such as geographic\nlocation, time, and weather conditions. Additionally, high-resolution aerial\nimages often require substantial storage space and may not be readily\naccessible to the public. To address these challenges, we propose a novel\nSource-Free Object Detection (SFOD) method. Specifically, our approach is built\nupon a self-training framework; however, self-training can lead to inaccurate\nlearning in the absence of labeled training data. To address this issue, we\nfurther integrate Contrastive Language-Image Pre-training (CLIP) to guide the\ngeneration of pseudo-labels, termed CLIP-guided Aggregation. By leveraging\nCLIP's zero-shot classification capability, we use it to aggregate scores with\nthe original predicted bounding boxes, enabling us to obtain refined scores for\nthe pseudo-labels. To validate the effectiveness of our method, we constructed\ntwo new datasets from different domains based on the DIOR dataset, named DIOR-C\nand DIOR-Cloudy. Experiments demonstrate that our method outperforms other\ncomparative algorithms.\n",
        "title": "CLIP-guided Source-free Object Detection in Aerial Images",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05170",
        "abstract_url": "http://arxiv.org/abs/2401.05170",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Junshuo"
            },
            {
                "last_name": "Huang",
                "first_name": "Yunlong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jianan"
            },
            {
                "last_name": "Xiong",
                "first_name": "Rujing"
            },
            {
                "last_name": "Qiu",
                "first_name": "Robert Caiming"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Device-free human activity recognition plays a pivotal role in wireless\nsensing. However, current systems often fail to accommodate signal transmission\nthrough walls or necessitate dedicated noise removal algorithms. To overcome\nthese limitations, we introduce TRTAR: a device-free passive human activity\nrecognition system integrated with a transmissive reconfigurable intelligent\nsurface (RIS). TRTAR eliminates the necessity for dedicated devices or noise\nremoval algorithms, while specifically addressing signal propagation through\nwalls. Unlike existing approaches, TRTAR solely employs a transmissive RIS at\nthe transmitter or receiver without modifying the inherent hardware structure.\nExperimental results demonstrate that TRTAR attains an average accuracy of\n98.13% when signals traverse concrete walls.\n",
        "title": "TRTAR: Transmissive RIS-assisted Through-the-wall Human Activity\n  Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05176",
        "abstract_url": "http://arxiv.org/abs/2401.05176",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Zhaokun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziyin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Inspired by the increasing interest in leveraging large language models for\ntranslation, this paper evaluates the capabilities of large language models\n(LLMs) represented by ChatGPT in comparison to the mainstream neural machine\ntranslation (NMT) engines in translating Chinese diplomatic texts into English.\nSpecifically, we examine the translation quality of ChatGPT and NMT engines as\nmeasured by four automated metrics and human evaluation based on an\nerror-typology and six analytic rubrics. Our findings show that automated\nmetrics yield similar results for ChatGPT under different prompts and NMT\nsystems, while human annotators tend to assign noticeably higher scores to\nChatGPT when it is provided an example or contextual information about the\ntranslation task. Pairwise correlation between automated metrics and dimensions\nof human evaluation produces weak and non-significant results, suggesting the\ndivergence between the two methods of translation quality assessment. These\nfindings provide valuable insights into the potential of ChatGPT as a capable\nmachine translator, and the influence of prompt engineering on its performance.\n",
        "title": "Can ChatGPT Rival Neural Machine Translation? A Comparative Study",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05182",
        "abstract_url": "http://arxiv.org/abs/2401.05182",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Pingping"
            },
            {
                "last_name": "Wang",
                "first_name": "Jintao"
            },
            {
                "last_name": "Shao",
                "first_name": "Yulin"
            },
            {
                "last_name": "Ma",
                "first_name": "Shaodan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  This paper presents a new integrated sensing and communication (ISAC)\nframework, leveraging the recent advancements of reconfigurable distributed\nantenna and reflecting surface (RDARS). RDARS is a programmable surface\nstructure comprising numerous elements, each of which can be flexibly\nconfigured to operate either in a reflection mode, resembling a passive\nreconfigurable intelligent surface (RIS), or in a connected mode, functioning\nas a remote transmit or receive antenna. Our RDARS-aided ISAC framework\neffectively mitigates the adverse impact of multiplicative fading when compared\nto the passive RIS-aided ISAC, and reduces cost and energy consumption when\ncompared to the active RIS-aided ISAC. Within our RDARS-aided ISAC framework,\nwe consider a radar output signal-to-noise ratio (SNR) maximization problem\nunder communication constraints to jointly optimize the active transmit\nbeamforming matrix of the base station (BS), the reflection and mode selection\nmatrices of RDARS, and the receive filter. To tackle the inherent non-convexity\nand the binary integer optimization introduced by the mode selection in this\noptimization challenge, we propose an efficient iterative algorithm with proved\nconvergence based on majorization minimization (MM) and penalty-based\nmethods.Numerical and simulation results demonstrate the superior performance\nof our new framework, and clearly verify substantial distribution, reflection\nas well as selection gains obtained by properly configuring the RDARS.\n",
        "title": "Integrated Sensing and Communication with Reconfigurable Distributed\n  Antenna and Reflecting Surface: Joint Beamforming and Mode Selection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05190",
        "abstract_url": "http://arxiv.org/abs/2401.05190",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Zijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhaopeng"
            },
            {
                "last_name": "Feng",
                "first_name": "Yang"
            },
            {
                "last_name": "Wang",
                "first_name": "Gaoang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Joey Tianyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Zuozhu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have shown impressive performance in various\nreasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its\nderivative methods, particularly in tasks involving multi-choice questions\n(MCQs). However, current works all process data uniformly without considering\nthe problem-solving difficulty, which means an excessive focus on simple\nquestions while insufficient to intricate ones. To address this challenge, we\ninspired by humans using heuristic strategies to categorize tasks and handle\nthem individually, propose to apply the Divide and Conquer to LLMs reasoning.\nFirst, we divide questions into different subsets based on the statistical\nconfidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer\ndemanding nuanced process ones with elaborately designed methods, including\nPrior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR),\nas well as their integration variants. Our experiments demonstrate that this\nproposed strategy significantly boosts the models' reasoning abilities across\nnine datasets involving arithmetic, commonsense, and logic tasks. For instance,\ncompared to baseline, we make a striking improvement on low confidence subsets\nof 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense. In\naddition, through extensive analysis on length of rationale and number of\noptions, we verify that longer reasoning paths in PKR could prevent models from\nreferring infer-harmful shortcuts, and also find that removing irrelevant\nchoices in FCR would substantially avoid models' confusion. The code is at\n\\url{https://github.com/AiMijie/Divide-and-Conquer}\n",
        "title": "Divide and Conquer for Large Language Models Reasoning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05191",
        "abstract_url": "http://arxiv.org/abs/2401.05191",
        "authors": [
            {
                "last_name": "Lai",
                "first_name": "Riwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Rui"
            },
            {
                "last_name": "Han",
                "first_name": "Qilong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chi"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Negative sampling is essential for implicit collaborative filtering to\nprovide proper negative training signals so as to achieve desirable\nperformance. We experimentally unveil a common limitation of all existing\nnegative sampling methods that they can only select negative samples of a fixed\nhardness level, leading to the false positive problem (FPP) and false negative\nproblem (FNP). We then propose a new paradigm called adaptive hardness negative\nsampling (AHNS) and discuss its three key criteria. By adaptively selecting\nnegative samples with appropriate hardnesses during the training process, AHNS\ncan well mitigate the impacts of FPP and FNP. Next, we present a concrete\ninstantiation of AHNS called AHNS_{p<0}, and theoretically demonstrate that\nAHNS_{p<0} can fit the three criteria of AHNS well and achieve a larger lower\nbound of normalized discounted cumulative gain. Besides, we note that existing\nnegative sampling methods can be regarded as more relaxed cases of AHNS.\nFinally, we conduct comprehensive experiments, and the results show that\nAHNS_{p<0} can consistently and substantially outperform several\nstate-of-the-art competitors on multiple datasets.\n",
        "title": "Adaptive Hardness Negative Sampling for Collaborative Filtering",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05193",
        "abstract_url": "http://arxiv.org/abs/2401.05193",
        "authors": [
            {
                "last_name": "Pacchiano",
                "first_name": "Aldo"
            },
            {
                "last_name": "Lee",
                "first_name": "Jonathan N."
            },
            {
                "last_name": "Brunskill",
                "first_name": "Emma"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We study the problem of experiment planning with function approximation in\ncontextual bandit problems. In settings where there is a significant overhead\nto deploying adaptive algorithms -- for example, when the execution of the data\ncollection policies is required to be distributed, or a human in the loop is\nneeded to implement these policies -- producing in advance a set of policies\nfor data collection is paramount. We study the setting where a large dataset of\ncontexts but not rewards is available and may be used by the learner to design\nan effective data collection strategy. Although when rewards are linear this\nproblem has been well studied, results are still missing for more complex\nreward models. In this work we propose two experiment planning strategies\ncompatible with function approximation. The first is an eluder planning and\nsampling procedure that can recover optimality guarantees depending on the\neluder dimension of the reward function class. For the second, we show that a\nuniform sampler achieves competitive optimality rates in the setting where the\nnumber of actions is small. We finalize our results introducing a statistical\ngap fleshing out the fundamental differences between planning and adaptive\nlearning and provide results for planning with model selection.\n",
        "title": "Experiment Planning with Function Approximation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05194",
        "abstract_url": "http://arxiv.org/abs/2401.05194",
        "authors": [
            {
                "last_name": "Caponio",
                "first_name": "Carmine"
            },
            {
                "last_name": "Stano",
                "first_name": "Pietro"
            },
            {
                "last_name": "Carli",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Olivieri",
                "first_name": "Ignazio"
            },
            {
                "last_name": "Ragone",
                "first_name": "Daniele"
            },
            {
                "last_name": "Sorniotti",
                "first_name": "Aldo"
            },
            {
                "last_name": "Montanaro",
                "first_name": "Umberto"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            ""
        ],
        "abstract": "  Mobile robotic systems are becoming increasingly popular. These systems are\nused in various indoor applications, raging from warehousing and manufacturing\nto test benches for assessment of advanced control strategies, such as\nartificial intelligence (AI)-based control solutions, just to name a few.\nScaled robotic cars are commonly equipped with a hierarchical control\nacthiecture that includes tasks dedicated to vehicle state estimation and\ncontrol. This paper covers both aspects by proposing (i) a federeted extended\nKalman filter (FEKF), and (ii) a novel deep reinforcement learning (DRL) path\ntracking controller trained via an expert demonstrator to expedite the learning\nphase and increase robustess to the simulation-to-reality gap. The paper also\npresents the formulation of a vehicle model along with an effective yet simple\nprocedure for identifying tis paramters. The experimentally validated model is\nused for (i) supporting the design of the FEKF and (ii) serving as a digital\ntwin for training the proposed DRL-based path tracking algorithm. Experimental\nresults confirm the ability of the FEKF to improve the estimate of the mobile\nrobot's position. Furthermore, the effectiveness of the DRL path tracking\nstrateguy is experimentally tested along manoeuvres not considered during\ntraining, showing also the ability of the AI-based solution to outpeform\nmodel-based control strategies and the demonstrator. The comparison with\nbenchmraking controllers is quantitavely evalueted through a set of key\nperformance indicators.\n",
        "title": "Modelling, Positioning, and Deep Reinforcement Learning Path Tracking\n  Control of Scaled Robotic Vehicles: Design and Experimental Validation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05196",
        "abstract_url": "http://arxiv.org/abs/2401.05196",
        "authors": [
            {
                "last_name": "Raus",
                "first_name": "Maren"
            },
            {
                "last_name": "Elshiaty",
                "first_name": "Yara"
            },
            {
                "last_name": "Petra",
                "first_name": "Stefania"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  We investigate the problem of minimizing Kullback-Leibler divergence between\na linear model $Ax$ and a positive vector $b$ in different convex domains\n(positive orthant, $n$-dimensional box, probability simplex). Our focus is on\nthe SMART method that employs efficient multiplicative updates. We explore the\nexponentiated gradient method, which can be viewed as a Bregman proximal\ngradient method and as a Riemannian gradient descent on the parameter manifold\nof a corresponding distribution of the exponential family. This dual\ninterpretation enables us to establish connections and achieve accelerated\nSMART iterates while smoothly incorporating constraints. The performance of the\nproposed acceleration schemes is demonstrated by large-scale numerical\nexamples.\n",
        "title": "Accelerated Bregmann divergence optimization with SMART: an information\n  geometry point of view",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05199",
        "abstract_url": "http://arxiv.org/abs/2401.05199",
        "authors": [
            {
                "last_name": "Taneja",
                "first_name": "Karan"
            },
            {
                "last_name": "Segal",
                "first_name": "Richard"
            },
            {
                "last_name": "Goodwin",
                "first_name": "Richard"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Automatic food recipe generation methods provide a creative tool for chefs to\nexplore and to create new, and interesting culinary delights. Given the recent\nsuccess of large language models (LLMs), they have the potential to create new\nrecipes that can meet individual preferences, dietary constraints, and adapt to\nwhat is in your refrigerator. Existing research on using LLMs to generate\nrecipes has shown that LLMs can be finetuned to generate realistic-sounding\nrecipes. However, on close examination, these generated recipes often fail to\nmeet basic requirements like including chicken as an ingredient in chicken\ndishes. In this paper, we propose RecipeMC, a text generation method using\nGPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to\ndefine reward functions to put soft constraints on text generation and thus\nimprove the credibility of the generated recipes. Our results show that human\nevaluators prefer recipes generated with RecipeMC more often than recipes\ngenerated with other baseline methods when compared with real recipes.\n",
        "title": "Monte Carlo Tree Search for Recipe Generation using GPT-2",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05200",
        "abstract_url": "http://arxiv.org/abs/2401.05200",
        "authors": [
            {
                "last_name": "Freire",
                "first_name": "Samuel Kernan"
            },
            {
                "last_name": "Wang",
                "first_name": "Chaofan"
            },
            {
                "last_name": "Foosherian",
                "first_name": "Mina"
            },
            {
                "last_name": "Wellsandt",
                "first_name": "Stefan"
            },
            {
                "last_name": "Ruiz-Arenas",
                "first_name": "Santiago"
            },
            {
                "last_name": "Niforatos",
                "first_name": "Evangelos"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "IR"
        ],
        "abstract": "  Managing knowledge efficiently is crucial for organizational success. In\nmanufacturing, operating factories has become increasing knowledge-intensive\nputting strain on the factory's capacity to train and support new operators. In\nthis paper, we introduce a Large Language Model (LLM)-based system designed to\nuse the extensive knowledge contained in factory documentation. The system aims\nto efficiently answer queries from operators and facilitate the sharing of new\nknowledge. To assess its effectiveness, we conducted an evaluation in a factory\nsetting. The results of this evaluation demonstrated the system's benefits;\nnamely, in enabling quicker information retrieval and more efficient resolution\nof issues. However, the study also highlighted a preference for learning from a\nhuman expert when such an option is available. Furthermore, we benchmarked\nseveral closed and open-sourced LLMs for this system. GPT-4 consistently\noutperformed its counterparts, with open-source models like StableBeluga2\ntrailing closely, presenting an attractive option given its data privacy and\ncustomization benefits. Overall, this work offers preliminary insights for\nfactories considering using LLM-tools for knowledge management.\n",
        "title": "Knowledge Sharing in Manufacturing using Large Language Models: User\n  Evaluation and Model Benchmarking",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05202",
        "abstract_url": "http://arxiv.org/abs/2401.05202",
        "authors": [
            {
                "last_name": "Russello",
                "first_name": "Helena"
            },
            {
                "last_name": "van der Tol",
                "first_name": "Rik"
            },
            {
                "last_name": "Holzhauer",
                "first_name": "Menno"
            },
            {
                "last_name": "van Henten",
                "first_name": "Eldert J."
            },
            {
                "last_name": "Kootstra",
                "first_name": "Gert"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This study presents an automated lameness detection system that uses\ndeep-learning image processing techniques to extract multiple locomotion traits\nassociated with lameness. Using the T-LEAP pose estimation model, the motion of\nnine keypoints was extracted from videos of walking cows. The videos were\nrecorded outdoors, with varying illumination conditions, and T-LEAP extracted\n99.6% of correct keypoints. The trajectories of the keypoints were then used to\ncompute six locomotion traits: back posture measurement, head bobbing, tracking\ndistance, stride length, stance duration, and swing duration. The three most\nimportant traits were back posture measurement, head bobbing, and tracking\ndistance. For the ground truth, we showed that a thoughtful merging of the\nscores of the observers could improve intra-observer reliability and agreement.\nWe showed that including multiple locomotion traits improves the classification\naccuracy from 76.6% with only one trait to 79.9% with the three most important\ntraits and to 80.1% with all six locomotion traits.\n",
        "title": "Video-based Automatic Lameness Detection of Dairy Cows using Pose\n  Estimation and Multiple Locomotion Traits",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05204",
        "abstract_url": "http://arxiv.org/abs/2401.05204",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yong"
            },
            {
                "last_name": "Luo",
                "first_name": "Senlin"
            },
            {
                "last_name": "Shang",
                "first_name": "Yu-Ming"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The verbalizer, which serves to map label words to class labels, is an\nessential component of prompt-tuning. In this paper, we present a novel\napproach to constructing verbalizers. While existing methods for verbalizer\nconstruction mainly rely on augmenting and refining sets of synonyms or related\nwords based on class names, this paradigm suffers from a narrow perspective and\nlack of abstraction, resulting in limited coverage and high bias in the\nlabel-word space. To address this issue, we propose a label-word construction\nprocess that incorporates scenario-specific concepts. Specifically, we extract\nrich concepts from task-specific scenarios as label-word candidates and then\ndevelop a novel cascade calibration module to refine the candidates into a set\nof label words for each class. We evaluate the effectiveness of our proposed\napproach through extensive experiments on {five} widely used datasets for\nzero-shot text classification. The results demonstrate that our method\noutperforms existing methods and achieves state-of-the-art results.\n",
        "title": "A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts\n  into a Verbalizer",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05206",
        "abstract_url": "http://arxiv.org/abs/2401.05206",
        "authors": [
            {
                "last_name": "Nordhagen",
                "first_name": "Even Marius"
            },
            {
                "last_name": "Sveinsson",
                "first_name": "Henrik Andersen"
            },
            {
                "last_name": "Malthe-S\u00f8renssen",
                "first_name": "Anders"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This Letter introduces an approach for precisely designing surface friction\nproperties using a conditional generative machine learning model, specifically\na diffusion denoising probabilistic model (DDPM). We created a dataset of\nsynthetic surfaces with frictional properties determined by molecular dynamics\nsimulations, which trained the DDPM to predict surface structures from desired\nfrictional outcomes. Unlike traditional trial-and-error and numerical\noptimization methods, our approach directly yields surface designs meeting\nspecified frictional criteria with high accuracy and efficiency. This\nadvancement in material surface engineering demonstrates the potential of\nmachine learning in reducing the iterative nature of surface design processes.\nOur findings not only provide a new pathway for precise surface property\ntailoring but also suggest broader applications in material science where\nsurface characteristics are critical.\n",
        "title": "Tailoring Frictional Properties of Surfaces Using Diffusion Models",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05211",
        "abstract_url": "http://arxiv.org/abs/2401.05211",
        "authors": [
            {
                "last_name": "Stiasny",
                "first_name": "Jochen"
            },
            {
                "last_name": "Chatzivasileiadis",
                "first_name": "Spyros"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  The ability to accurately approximate trajectories of dynamical systems\nenables their analysis, prediction, and control. Neural network (NN)-based\napproximations have attracted significant interest due to fast evaluation with\ngood accuracy over long integration time steps. In contrast to established\nnumerical approximation schemes such as Runge-Kutta methods, the estimation of\nthe error of the NN-based approximations proves to be difficult. In this work,\nwe propose to use the NN's predictions in a high-order implicit Runge-Kutta\n(IRK) method. The residuals in the implicit system of equations can be related\nto the NN's prediction error, hence, we can provide an error estimate at\nseveral points along a trajectory. We find that this error estimate highly\ncorrelates with the NN's prediction error and that increasing the order of the\nIRK method improves this estimate. We demonstrate this estimation methodology\nfor Physics-Informed Neural Network (PINNs) on the logistic equation as an\nillustrative example and then apply it to a four-state electric generator model\nthat is regularly used in power system modelling.\n",
        "title": "Error estimation for physics-informed neural networks with implicit\n  Runge-Kutta methods",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05215",
        "abstract_url": "http://arxiv.org/abs/2401.05215",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Wei"
            },
            {
                "last_name": "Gong",
                "first_name": "Dihong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Financial sentiment analysis refers to classifying financial text contents\ninto sentiment categories (e.g. positive, negative, and neutral). In this\npaper, we focus on the classification of financial news title, which is a\nchallenging task due to a lack of large amount of training samples. To overcome\nthis difficulty, we propose to adapt the pretrained large language models\n(LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge\namount of text corpora,have an advantage in text understanding and can be\neffectively adapted to domain-specific task while requiring very few amount of\ntraining samples. In particular, we adapt the open-source Llama2-7B model\n(2023) with the supervised fine-tuning (SFT) technique [4]. Experimental\nevaluation shows that even with the 7B model (which is relatively small for\nLLMs), our approach significantly outperforms the previous state-of-the-art\nalgorithms.\n",
        "title": "Pre-trained Large Language Models for Financial Sentiment Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05217",
        "abstract_url": "http://arxiv.org/abs/2401.05217",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Chenxi"
            },
            {
                "last_name": "Liu",
                "first_name": "Yujia"
            },
            {
                "last_name": "Li",
                "first_name": "Dingquan"
            },
            {
                "last_name": "jiang",
                "first_name": "Tingting"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality\nscores consistent with human perception without relying on pristine reference\nimages, serving as a crucial component in various visual tasks. Ensuring the\nrobustness of NR-IQA methods is vital for reliable comparisons of different\nimage processing techniques and consistent user experiences in recommendations.\nThe attack methods for NR-IQA provide a powerful instrument to test the\nrobustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on\nthe gradient of the NR-IQA model, leading to limitations when the gradient\ninformation is unavailable. In this paper, we present a pioneering query-based\nblack box attack against NR-IQA methods. We propose the concept of \\emph{score\nboundary} and leverage an adaptive iterative approach with multiple score\nboundaries. Meanwhile, the initial attack directions are also designed to\nleverage the characteristics of the Human Visual System (HVS). Experiments show\nour attack method outperforms all compared state-of-the-art methods and is far\nahead of previous black-box methods. The effective DBCNN model suffers a\nSpearman rank-order correlation coefficient (SROCC) decline of $0.6972$\nattacked by our method, revealing the vulnerability of NR-IQA to black-box\nattacks. The proposed attack method also provides a potent tool for further\nexploration into NR-IQA robustness.\n",
        "title": "Exploring Vulnerabilities of No-Reference Image Quality Assessment\n  Models: A Query-Based Black-Box Method",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05218",
        "abstract_url": "http://arxiv.org/abs/2401.05218",
        "authors": [
            {
                "last_name": "Mey",
                "first_name": "Alexander"
            },
            {
                "last_name": "Castro",
                "first_name": "Rui Manuel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We consider the task of identifying the causal parents of a target variable\namong a set of candidate variables from observational data. Our main assumption\nis that the candidate variables are observed in different environments which\nmay, for example, correspond to different settings of a machine or different\ntime intervals in a dynamical process. Under certain assumptions different\nenvironments can be regarded as interventions on the observed system. We assume\na linear relationship between target and covariates, which can be different in\neach environment with the only restriction that the causal structure is\ninvariant across environments. This is an extension of the ICP\n($\\textbf{I}$nvariant $\\textbf{C}$ausal $\\textbf{P}$rediction) principle by\nPeters et al. [2016], who assumed a fixed linear relationship across all\nenvironments. Within our proposed setting we provide sufficient conditions for\nidentifiability of the causal parents and introduce a practical method called\nLoLICaP ($\\textbf{Lo}$cally $\\textbf{L}$inear $\\textbf{I}$nvariant\n$\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based on a hypothesis test\nfor parent identification using a ratio of minimum and maximum statistics. We\nthen show in a simplified setting that the statistical power of LoLICaP\nconverges exponentially fast in the sample size, and finally we analyze the\nbehavior of LoLICaP experimentally in more general settings.\n",
        "title": "Invariant Causal Prediction with Locally Linear Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05219",
        "abstract_url": "http://arxiv.org/abs/2401.05219",
        "authors": [
            {
                "last_name": "Karayanni",
                "first_name": "Nader"
            },
            {
                "last_name": "Shahla",
                "first_name": "Robert J."
            },
            {
                "last_name": "Hsiao",
                "first_name": "Chieh-Lien"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            ""
        ],
        "abstract": "  The digital era has seen a marked increase in financial fraud. edge ML\nemerged as a promising solution for smartphone payment services fraud\ndetection, enabling the deployment of ML models directly on edge devices. This\napproach enables a more personalized real-time fraud detection. However, a\nsignificant gap in current research is the lack of a robust system for\nmonitoring data distribution shifts in these distributed edge ML applications.\nOur work bridges this gap by introducing a novel open-source framework designed\nfor continuous monitoring of data distribution shifts on a network of edge\ndevices. Our system includes an innovative calculation of the\nKolmogorov-Smirnov (KS) test over a distributed network of edge devices,\nenabling efficient and accurate monitoring of users behavior shifts. We\ncomprehensively evaluate the proposed framework employing both real-world and\nsynthetic financial transaction datasets and demonstrate the framework's\neffectiveness.\n",
        "title": "Distributed Monitoring for Data Distribution Shifts in Edge-ML Fraud\n  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05221",
        "abstract_url": "http://arxiv.org/abs/2401.05221",
        "authors": [
            {
                "last_name": "Lips",
                "first_name": "Johannes"
            },
            {
                "last_name": "DeYoung",
                "first_name": "Stefan"
            },
            {
                "last_name": "Sch\u00f6nsteiner",
                "first_name": "Max"
            },
            {
                "last_name": "Lens",
                "first_name": "Hendrik"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The creation of low-order dynamic models for complex industrial systems is\ncomplicated by disturbances and limited sensor accuracy. This work presents a\nsystem identification procedure that uses machine learning methods and process\nknowledge to robustly identify a low-order closed-loop model of a municipal\nsolid waste (MSW) grate incineration plant. These types of plants are known for\ntheir strong disturbances coming from fuel composition fluctuations. Using\nBayesian optimization, the algorithm ranks and selects inputs from the\navailable sensor data and chooses the model structure. This results in accurate\nmodels with low complexity while avoiding overfitting. The method is applied\nand validated using data of an industrial MSW incineration plant. The obtained\nmodels give excellent predictions and confidence intervals for the steam\ncapacity and intermediate quantities such as supply air flow and flue gas\ntemperature. The identified continuous-time models are fully given, and their\nstep-response dynamics are discussed. The models can be used to develop\nmodel-based unit control schemes for grate incineration plants. The presented\nmethod shows great potential for the identification of over-actuated systems or\ndisturbed systems with many sensors.\n",
        "title": "Closed-loop Identification of a MSW Grate Incinerator using Bayesian\n  Optimization for Selecting Model Inputs and Structure",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05224",
        "abstract_url": "http://arxiv.org/abs/2401.05224",
        "authors": [
            {
                "last_name": "Maniparambil",
                "first_name": "Mayug"
            },
            {
                "last_name": "Akshulakov",
                "first_name": "Raiymbek"
            },
            {
                "last_name": "Djilali",
                "first_name": "Yasser Abdelaziz Dahou"
            },
            {
                "last_name": "Narayan",
                "first_name": "Sanath"
            },
            {
                "last_name": "Seddik",
                "first_name": "Mohamed El Amine"
            },
            {
                "last_name": "Mangalam",
                "first_name": "Karttikeya"
            },
            {
                "last_name": "O'Connor",
                "first_name": "Noel E."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL",
            "LG"
        ],
        "abstract": "  Aligned text-image encoders such as CLIP have become the de facto model for\nvision-language tasks. Furthermore, modality-specific encoders achieve\nimpressive performances in their respective domains. This raises a central\nquestion: does an alignment exist between uni-modal vision and language\nencoders since they fundamentally represent the same physical world? Analyzing\nthe latent spaces structure of vision and language models on image-caption\nbenchmarks using the Centered Kernel Alignment (CKA), we find that the\nrepresentation spaces of unaligned and aligned encoders are semantically\nsimilar. In the absence of statistical similarity in aligned encoders like\nCLIP, we show that a possible matching of unaligned encoders exists without any\ntraining. We frame this as a seeded graph-matching problem exploiting the\nsemantic similarity between graphs and propose two methods - a Fast Quadratic\nAssignment Problem optimization, and a novel localized CKA metric-based\nmatching/retrieval. We demonstrate the effectiveness of this on several\ndownstream tasks including cross-lingual, cross-domain caption matching and\nimage classification.\n",
        "title": "Do Vision and Language Encoders Represent the World Similarly?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05225",
        "abstract_url": "http://arxiv.org/abs/2401.05225",
        "authors": [
            {
                "last_name": "Mart\u00edn-P\u00e9rez",
                "first_name": "Jorge"
            },
            {
                "last_name": "Lentisco",
                "first_name": "Carlos M."
            },
            {
                "last_name": "Bellido",
                "first_name": "Luis"
            },
            {
                "last_name": "Soto",
                "first_name": "Ignacio"
            },
            {
                "last_name": "Fern\u00e1ndez",
                "first_name": "David"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Tele-operated Driving (ToD) is a challenging use case for mobile network\noperators. Video captured by the built-in vehicle cameras must be streamed\nmeeting a latency requirement of 5 ms with a 99.999% reliability. Although 5G\noffers high bandwidth, ultra-low latencies and high reliability; ToD service\nrequirements are violated due to bad channel conditions. Ignoring the channel\nstate may lead to over-estimate the number of ToD vehicles that can meet the\nservice requirements, hence comprising the vehicle security. To fill this gap,\nin this letter we propose TOVAC, an algorithm that guarantees ToD service\nrequirements by taking adequate admission control and routing decisions. This\nis achieved by using a channel-based capacity graph that determines the maximum\nnumber of vehicles that can be tele-operated in any road section. We evaluate\nTOVAC considering cellular deployments from Turin and show that, unlike a state\nof the art solution, TOVAC guarantees the ToD service requirements.\n",
        "title": "TOVAC: Tele-operated Vehicle Admission Control and Routing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05226",
        "abstract_url": "http://arxiv.org/abs/2401.05226",
        "authors": [
            {
                "last_name": "Barletta",
                "first_name": "Giulio"
            },
            {
                "last_name": "Trezza",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Chiavazzo",
                "first_name": "Eliodoro"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  We assume that a sufficiently large database is available, where a physical\nproperty of interest and a number of associated ruling primitive variables or\nobservables are stored. We introduce and test two machine learning approaches\nto discover possible groups or combinations of primitive variables: The first\napproach is based on regression models whereas the second on classification\nmodels. The variable group (here referred to as the new effective good\nvariable) can be considered as successfully found, when the physical property\nof interest is characterized by the following effective invariant behaviour: In\nthe first method, invariance of the group implies invariance of the property up\nto a given accuracy; in the other method, upon partition of the physical\nproperty values into two or more classes, invariance of the group implies\ninvariance of the class. For the sake of illustration, the two methods are\nsuccessfully applied to two popular empirical correlations describing the\nconvective heat transfer phenomenon and to the Newton's law of universal\ngravitation.\n",
        "title": "Learning effective good variables from physical data",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05232",
        "abstract_url": "http://arxiv.org/abs/2401.05232",
        "authors": [
            {
                "last_name": "Jakab",
                "first_name": "Daniel"
            },
            {
                "last_name": "Grua",
                "first_name": "Eoin Martino"
            },
            {
                "last_name": "Deegan",
                "first_name": "Brian Micheal"
            },
            {
                "last_name": "Scanlan",
                "first_name": "Anthony"
            },
            {
                "last_name": "Van De Ven",
                "first_name": "Pepijn"
            },
            {
                "last_name": "Eising",
                "first_name": "Ciar\u00e1n"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The Modulation Transfer Function (MTF) is an important image quality metric\ntypically used in the automotive domain. However, despite the fact that optical\nquality has an impact on the performance of computer vision in vehicle\nautomation, for many public datasets, this metric is unknown. Additionally,\nwide field-of-view (FOV) cameras have become increasingly popular, particularly\nfor low-speed vehicle automation applications. To investigate image quality in\ndatasets, this paper proposes an adaptation of the Natural Scenes Spatial\nFrequency Response (NS-SFR) algorithm to suit cameras with a wide\nfield-of-view.\n",
        "title": "Measuring Natural Scenes SFR of Automotive Fisheye Cameras",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05233",
        "abstract_url": "http://arxiv.org/abs/2401.05233",
        "authors": [
            {
                "last_name": "Duan",
                "first_name": "Yaqi"
            },
            {
                "last_name": "Wainwright",
                "first_name": "Martin J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT",
            "",
            "",
            ""
        ],
        "abstract": "  We introduce a novel framework for analyzing reinforcement learning (RL) in\ncontinuous state-action spaces, and use it to prove fast rates of convergence\nin both off-line and on-line settings. Our analysis highlights two key\nstability properties, relating to how changes in value functions and/or\npolicies affect the Bellman operator and occupation measures. We argue that\nthese properties are satisfied in many continuous state-action Markov decision\nprocesses, and demonstrate how they arise naturally when using linear function\napproximation methods. Our analysis offers fresh perspectives on the roles of\npessimism and optimism in off-line and on-line RL, and highlights the\nconnection between off-line RL and transfer learning.\n",
        "title": "Taming \"data-hungry\" reinforcement learning? Stability in continuous\n  state-action spaces",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05235",
        "abstract_url": "http://arxiv.org/abs/2401.05235",
        "authors": [
            {
                "last_name": "Camur",
                "first_name": "Mustafa Can"
            },
            {
                "last_name": "Vogiatzis",
                "first_name": "Chrysafis"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            ""
        ],
        "abstract": "  Centrality metrics have become a popular concept in network science and\noptimization. Over the years, centrality has been used to assign importance and\nidentify influential elements in various settings, including transportation,\ninfrastructure, biological, and social networks, among others. That said, most\nof the literature has focused on nodal versions of centrality. Recently, group\ncounterparts of centrality have started attracting scientific and practitioner\ninterest. The identification of sets of nodes that are influential within a\nnetwork is becoming increasingly more important. This is even more pronounced\nwhen these sets of nodes are required to induce a certain motif or structure.\nIn this study, we review group centrality metrics from an operations research\nand optimization perspective for the first time. This is particularly\ninteresting due to the rapid evolution and development of this area in the\noperations research community over the last decade. We first present a\nhistorical overview of how we have reached this point in the study of group\ncentrality. We then discuss the different structures and motifs that appear\nprominently in the literature, alongside the techniques and methodologies that\nare popular. We finally present possible avenues and directions for future\nwork, mainly in three areas: (i) probabilistic metrics to account for\nrandomness along with stochastic optimization techniques; (ii) structures and\nrelaxations that have not been yet studied; and (iii) new emerging applications\nthat can take advantage of group centrality. Our survey offers a concise review\nof group centrality and its intersection with network analysis and\noptimization.\n",
        "title": "A Survey on Optimization Studies of Group Centrality Metrics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05236",
        "abstract_url": "http://arxiv.org/abs/2401.05236",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Ma",
                "first_name": "Wei-Chiu"
            },
            {
                "last_name": "Guan",
                "first_name": "Kaiyu"
            },
            {
                "last_name": "Torralba",
                "first_name": "Antonio"
            },
            {
                "last_name": "Wang",
                "first_name": "Shenlong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Our world is full of identical objects (\\emphe.g., cans of coke, cars of same\nmodel). These duplicates, when seen together, provide additional and strong\ncues for us to effectively reason about 3D. Inspired by this observation, we\nintroduce Structure from Duplicates (SfD), a novel inverse graphics framework\nthat reconstructs geometry, material, and illumination from a single image\ncontaining multiple identical objects. SfD begins by identifying multiple\ninstances of an object within an image, and then jointly estimates the 6DoF\npose for all instances.An inverse graphics pipeline is subsequently employed to\njointly reason about the shape, material of the object, and the environment\nlight, while adhering to the shared geometry and material constraint across\ninstances. Our primary contributions involve utilizing object duplicates as a\nrobust prior for single-image inverse graphics and proposing an in-plane\nrotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object\npose estimation. By leveraging multi-view cues from a single image, SfD\ngenerates more realistic and detailed 3D reconstructions, significantly\noutperforming existing single image reconstruction models and multi-view\nreconstruction approaches with a similar or greater number of observations.\n",
        "title": "Structure from Duplicates: Neural Inverse Graphics from a Pile of\n  Objects",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05240",
        "abstract_url": "http://arxiv.org/abs/2401.05240",
        "authors": [
            {
                "last_name": "Luzio",
                "first_name": "Emanuele"
            },
            {
                "last_name": "Ponti",
                "first_name": "Moacir Antonelli"
            },
            {
                "last_name": "Arevalo",
                "first_name": "Christian Ramirez"
            },
            {
                "last_name": "Argerich",
                "first_name": "Luis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Machine learning models typically focus on specific targets like creating\nclassifiers, often based on known population feature distributions in a\nbusiness context. However, models calculating individual features adapt over\ntime to improve precision, introducing the concept of decoupling: shifting from\npoint evaluation to data distribution. We use calibration strategies as\nstrategy for decoupling machine learning (ML) classifiers from score-based\nactions within business logic frameworks. To evaluate these strategies, we\nperform a comparative analysis using a real-world business scenario and\nmultiple ML models. Our findings highlight the trade-offs and performance\nimplications of the approach, offering valuable insights for practitioners\nseeking to optimize their decoupling efforts. In particular, the Isotonic and\nBeta calibration methods stand out for scenarios in which there is shift\nbetween training and testing data.\n",
        "title": "Decoupling Decision-Making in Fraud Prevention through Classifier\n  Calibration for Business Logic Action",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05244",
        "abstract_url": "http://arxiv.org/abs/2401.05244",
        "authors": [
            {
                "last_name": "Thaler",
                "first_name": "Denny"
            },
            {
                "last_name": "Dhulipala",
                "first_name": "Somayajulu L. N."
            },
            {
                "last_name": "Bamer",
                "first_name": "Franz"
            },
            {
                "last_name": "Markert",
                "first_name": "Bernd"
            },
            {
                "last_name": "Shields",
                "first_name": "Michael D."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  We present a new Subset Simulation approach using Hamiltonian neural\nnetwork-based Monte Carlo sampling for reliability analysis. The proposed\nstrategy combines the superior sampling of the Hamiltonian Monte Carlo method\nwith computationally efficient gradient evaluations using Hamiltonian neural\nnetworks. This combination is especially advantageous because the neural\nnetwork architecture conserves the Hamiltonian, which defines the acceptance\ncriteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves\nhigh acceptance rates at low computational cost. Our approach estimates small\nfailure probabilities using Subset Simulations. However, in low-probability\nsample regions, the gradient evaluation is particularly challenging. The\nremarkable accuracy of the proposed strategy is demonstrated on different\nreliability problems, and its efficiency is compared to the traditional\nHamiltonian Monte Carlo method. We note that this approach can reach its\nlimitations for gradient estimations in low-probability regions of complex and\nhigh-dimensional distributions. Thus, we propose techniques to improve gradient\nprediction in these particular situations and enable accurate estimations of\nthe probability of failure. The highlight of this study is the reliability\nanalysis of a system whose parameter distributions must be inferred with\nBayesian inference problems. In such a case, the Hamiltonian Monte Carlo method\nrequires a full model evaluation for each gradient evaluation and, therefore,\ncomes at a very high cost. However, using Hamiltonian neural networks in this\nframework replaces the expensive model evaluation, resulting in tremendous\nimprovements in computational efficiency.\n",
        "title": "Reliability Analysis of Complex Systems using Subset Simulations with\n  Hamiltonian Neural Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05247",
        "abstract_url": "http://arxiv.org/abs/2401.05247",
        "authors": [
            {
                "last_name": "Fern\u00e1ndez-C\u00f3rdoba",
                "first_name": "Cristina"
            },
            {
                "last_name": "Torres",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Vela",
                "first_name": "Carlos"
            },
            {
                "last_name": "Villanueva",
                "first_name": "Merc\u00e8"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The Zps-additive codes of length n are subgroups of Zps^n , and can be seen\nas a generalization of linear codes over Z2, Z4, or more general over Z2s . In\nthis paper, we show two methods for computing a parity-check matrix of a\nZps-additive code from a generator matrix of the code in standard form. We also\ncompare the performance of our results implemented in Magma with the current\navailable function in Magma for codes over finite rings in general. A time\ncomplexity analysis is also shown.\n",
        "title": "Computing efficiently a parity-check matrix for Zps-additive codes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05249",
        "abstract_url": "http://arxiv.org/abs/2401.05249",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Feng",
                "first_name": "Yansong"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The argument sufficiency assessment task aims to determine if the premises of\na given argument support its conclusion. To tackle this task, existing works\noften train a classifier on data annotated by humans. However, annotating data\nis laborious, and annotations are often inconsistent due to subjective\ncriteria. Motivated by the probability of sufficiency (PS) definition in the\ncausal literature, we propose CASA, a zero-shot causality-driven argument\nsufficiency assessment framework. PS measures how likely introducing the\npremise event would lead to the conclusion, when both the premise and\nconclusion events are absent. To estimate this probability, we propose to use\nlarge language models (LLMs) to generate contexts that are inconsistent with\nthe premise and conclusion, and revise them by injecting the premise event.\nExperiments on two logical fallacy detection datasets demonstrate that CASA\naccurately identifies insufficient arguments. We further deploy CASA in a\nwriting assistance application, and find that suggestions generated by CASA\nenhance the sufficiency of student-written arguments. Code and data are\navailable at https://github.com/xxxiaol/CASA.\n",
        "title": "CASA: Causality-driven Argument Sufficiency Assessment",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05251",
        "abstract_url": "http://arxiv.org/abs/2401.05251",
        "authors": [
            {
                "last_name": "Rudolf",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fl\u00f6gel",
                "first_name": "Daniel"
            },
            {
                "last_name": "Sch\u00fcrmann",
                "first_name": "Tobias"
            },
            {
                "last_name": "S\u00fc\u00df",
                "first_name": "Simon"
            },
            {
                "last_name": "Schwab",
                "first_name": "Stefan"
            },
            {
                "last_name": "Hohmann",
                "first_name": "S\u00f6ren"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  Robust and performant controllers are essential for industrial applications.\nHowever, deriving controller parameters for complex and nonlinear systems is\nchallenging and time-consuming. To facilitate automatic controller\nparametrization, this work presents a novel approach using deep reinforcement\nlearning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the\ncontrol of parameter-variant systems, a class of systems with complex behavior\nwhich depends on the operating conditions. For this system class,\ngain-scheduling control structures are widely used in applications across\nindustries due to well-known design principles. Facilitating the expensive\ncontroller parametrization task regarding these control structures, we deploy\nan DRL agent. Based on control system observations, the agent autonomously\ndecides how to adapt the controller parameters. We make the adaptation process\nmore efficient by introducing BSGs to map the controller parameters which may\ndepend on numerous operating conditions. To preprocess time-series data and\nextract a fixed-length feature vector, we use a long short-term memory (LSTM)\nneural networks. Furthermore, this work contributes actor regularizations that\nare relevant to real-world environments which differ from training.\nAccordingly, we apply dropout layer normalization to the actor and critic\nnetworks of the truncated quantile critic (TQC) algorithm. To show our\napproach's working principle and effectiveness, we train and evaluate the DRL\nagent on the parametrization task of an industrial control structure with\nparameter lookup tables.\n",
        "title": "ReACT: Reinforcement Learning for Controller Parametrization using\n  B-Spline Geometries",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05252",
        "abstract_url": "http://arxiv.org/abs/2401.05252",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junsong"
            },
            {
                "last_name": "Wu",
                "first_name": "Yue"
            },
            {
                "last_name": "Luo",
                "first_name": "Simian"
            },
            {
                "last_name": "Xie",
                "first_name": "Enze"
            },
            {
                "last_name": "Paul",
                "first_name": "Sayak"
            },
            {
                "last_name": "Luo",
                "first_name": "Ping"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hang"
            },
            {
                "last_name": "Li",
                "first_name": "Zhenguo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This technical report introduces PIXART-{\\delta}, a text-to-image synthesis\nframework that integrates the Latent Consistency Model (LCM) and ControlNet\ninto the advanced PIXART-{\\alpha} model. PIXART-{\\alpha} is recognized for its\nability to generate high-quality images of 1024px resolution through a\nremarkably efficient training process. The integration of LCM in\nPIXART-{\\delta} significantly accelerates the inference speed, enabling the\nproduction of high-quality images in just 2-4 steps. Notably, PIXART-{\\delta}\nachieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,\nmarking a 7x improvement over the PIXART-{\\alpha}. Additionally,\nPIXART-{\\delta} is designed to be efficiently trainable on 32GB V100 GPUs\nwithin a single day. With its 8-bit inference capability (von Platen et al.,\n2023), PIXART-{\\delta} can synthesize 1024px images within 8GB GPU memory\nconstraints, greatly enhancing its usability and accessibility. Furthermore,\nincorporating a ControlNet-like module enables fine-grained control over\ntext-to-image diffusion models. We introduce a novel ControlNet-Transformer\narchitecture, specifically tailored for Transformers, achieving explicit\ncontrollability alongside high-quality image generation. As a state-of-the-art,\nopen-source image generation model, PIXART-{\\delta} offers a promising\nalternative to the Stable Diffusion family of models, contributing\nsignificantly to text-to-image synthesis.\n",
        "title": "PIXART-{\\delta}: Fast and Controllable Image Generation with Latent\n  Consistency Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05268",
        "abstract_url": "http://arxiv.org/abs/2401.05268",
        "authors": [
            {
                "last_name": "Qiao",
                "first_name": "Shuofei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ningyu"
            },
            {
                "last_name": "Fang",
                "first_name": "Runnan"
            },
            {
                "last_name": "Luo",
                "first_name": "Yujie"
            },
            {
                "last_name": "Zhou",
                "first_name": "Wangchunshu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yuchen Eleanor"
            },
            {
                "last_name": "Lv",
                "first_name": "Chengfei"
            },
            {
                "last_name": "Chen",
                "first_name": "Huajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "HC",
            "LG",
            "MA"
        ],
        "abstract": "  Language agents have achieved considerable performance on various complex\ntasks. Despite the incessant exploration in this field, existing language agent\nsystems still struggle with costly, non-reproducible data reliance and face the\nchallenge of compelling a single model for multiple functions. To this end, we\nintroduce AutoAct, an automatic agent learning framework that does not rely on\nlarge-scale annotated data and synthetic trajectories from closed-source models\n(e.g., GPT-4). Given limited data with a tool library, AutoAct first\nautomatically synthesizes planning trajectories without any assistance from\nhumans or strong closed-source models. Then, AutoAct leverages a\ndivision-of-labor strategy to automatically differentiate based on the target\ntask information and synthesized trajectories, producing a sub-agent group to\ncomplete the task. We conduct comprehensive experiments with different LLMs,\nwhich demonstrates that AutoAct yields better or parallel performance compared\nto various strong baselines. We even notice that AutoAct, when using the\nLlama-2-13b model, can achieve performance comparable to that of the\nGPT-3.5-Turbo agent. Code will be available at\nhttps://github.com/zjunlp/AutoAct.\n",
        "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05272",
        "abstract_url": "http://arxiv.org/abs/2401.05272",
        "authors": [
            {
                "last_name": "Pueyo",
                "first_name": "Pablo"
            },
            {
                "last_name": "Dendarieta",
                "first_name": "Juan"
            },
            {
                "last_name": "Montijano",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Murillo",
                "first_name": "Ana C."
            },
            {
                "last_name": "Schwager",
                "first_name": "Mac"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We present CineMPC, a complete cinematographic system that autonomously\ncontrols a drone to film multiple targets recording user-specified aesthetic\nobjectives. Existing solutions in autonomous cinematography control only the\ncamera extrinsics, namely its position, and orientation. In contrast, CineMPC\nis the first solution that includes the camera intrinsic parameters in the\ncontrol loop, which are essential tools for controlling cinematographic effects\nlike focus, depth-of-field, and zoom. The system estimates the relative poses\nbetween the targets and the camera from an RGB-D image and optimizes a\ntrajectory for the extrinsic and intrinsic camera parameters to film the\nartistic and technical requirements specified by the user. The drone and the\ncamera are controlled in a nonlinear Model Predicted Control (MPC) loop by\nre-optimizing the trajectory at each time step in response to current\nconditions in the scene. The perception system of CineMPC can track the\ntargets' position and orientation despite the camera effects. Experiments in a\nphotorealistic simulation and with a real platform demonstrate the capabilities\nof the system to achieve a full array of cinematographic effects that are not\npossible without the control of the intrinsics of the camera. Code for CineMPC\nis implemented following a modular architecture in ROS and released to the\ncommunity.\n",
        "title": "CineMPC: A Fully Autonomous Drone Cinematography System Incorporating\n  Zoom, Focus, Pose, and Scene Composition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05273",
        "abstract_url": "http://arxiv.org/abs/2401.05273",
        "authors": [
            {
                "last_name": "Pereira",
                "first_name": "Jayr"
            },
            {
                "last_name": "Assumpcao",
                "first_name": "Andre"
            },
            {
                "last_name": "Trecenti",
                "first_name": "Julio"
            },
            {
                "last_name": "Airosa",
                "first_name": "Luiz"
            },
            {
                "last_name": "Lente",
                "first_name": "Caio"
            },
            {
                "last_name": "Cl\u00e9to",
                "first_name": "Jhonatan"
            },
            {
                "last_name": "Dobins",
                "first_name": "Guilherme"
            },
            {
                "last_name": "Nogueira",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Mitchell",
                "first_name": "Luis"
            },
            {
                "last_name": "Lotufo",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^encia\nArtificial), a groundbreaking system designed to integrate Large Language\nModels (LLMs) into the operational framework of Brazilian Federal Court of\nAccounts (TCU). The system automates various stages of case analysis, including\nbasic information extraction, admissibility examination, Periculum in mora and\nFumus boni iuris analyses, and recommendations generation. Through a series of\nexperiments, we demonstrate INACIA's potential in extracting relevant\ninformation from case documents, evaluating its legal plausibility, and\ngenerating judicial recommendations. Utilizing a validation dataset alongside\nLLMs, our evaluation methodology presents an innovative approach to assessing\nsystem performance, correlating highly with human judgment. The results\nhighlight INACIA's proficiency in handling complex legal tasks, indicating its\nsuitability for augmenting efficiency and judicial fairness within legal\nsystems. The paper also discusses potential enhancements and future\napplications, positioning INACIA as a model for worldwide AI integration in\nlegal domains.\n",
        "title": "INACIA: Integrating Large Language Models in Brazilian Audit Courts:\n  Opportunities and Challenges",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05286",
        "abstract_url": "http://arxiv.org/abs/2401.05286",
        "authors": [
            {
                "last_name": "Cavicchioni",
                "first_name": "Giulia"
            },
            {
                "last_name": "Guerrini",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Meneghetti",
                "first_name": "Alessio"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Locally recoverable codes deal with the task of reconstructing a lost symbol\nby relying on a portion of the remaining coordinates smaller than an\ninformation set. We consider the case of codes over finite chain rings,\ngeneralizing known results and bounds for codes over fields. In particular, we\npropose a new family of locally recoverable codes by extending a construction\nproposed in 2014 by Tamo and Barg, and we discuss its optimality.\n",
        "title": "A class of locally recoverable codes over finite chain rings",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05290",
        "abstract_url": "http://arxiv.org/abs/2401.05290",
        "authors": [
            {
                "last_name": "Hauser",
                "first_name": "Kris"
            },
            {
                "last_name": "Watson",
                "first_name": "Eleanor"
            },
            {
                "last_name": "Bae",
                "first_name": "Joonbum"
            },
            {
                "last_name": "Bankston",
                "first_name": "Josh"
            },
            {
                "last_name": "Behnke",
                "first_name": "Sven"
            },
            {
                "last_name": "Borgia",
                "first_name": "Bill"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            },
            {
                "last_name": "Dafarra",
                "first_name": "Stefano"
            },
            {
                "last_name": "van Erp",
                "first_name": "Jan B. F."
            },
            {
                "last_name": "Ferris",
                "first_name": "Thomas"
            },
            {
                "last_name": "Fishel",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Hoffman",
                "first_name": "Guy"
            },
            {
                "last_name": "Ivaldi",
                "first_name": "Serena"
            },
            {
                "last_name": "Kanehiro",
                "first_name": "Fumio"
            },
            {
                "last_name": "Kheddar",
                "first_name": "Abderrahmane"
            },
            {
                "last_name": "Lannuzel",
                "first_name": "Gaelle"
            },
            {
                "last_name": "Morie",
                "first_name": "Jacqueline Ford"
            },
            {
                "last_name": "Naughton",
                "first_name": "Patrick"
            },
            {
                "last_name": "NGuyen",
                "first_name": "Steve"
            },
            {
                "last_name": "Oh",
                "first_name": "Paul"
            },
            {
                "last_name": "Padir",
                "first_name": "Taskin"
            },
            {
                "last_name": "Pippine",
                "first_name": "Jim"
            },
            {
                "last_name": "Park",
                "first_name": "Jaeheung"
            },
            {
                "last_name": "Pucci",
                "first_name": "Daniele"
            },
            {
                "last_name": "Vaz",
                "first_name": "Jean"
            },
            {
                "last_name": "Whitney",
                "first_name": "Peter"
            },
            {
                "last_name": "Wu",
                "first_name": "Peggy"
            },
            {
                "last_name": "Locke",
                "first_name": "David"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  The ANA Avatar XPRIZE was a four-year competition to develop a robotic\n\"avatar\" system to allow a human operator to sense, communicate, and act in a\nremote environment as though physically present. The competition featured a\nunique requirement that judges would operate the avatars after less than one\nhour of training on the human-machine interfaces, and avatar systems were\njudged on both objective and subjective scoring metrics. This paper presents a\nunified summary and analysis of the competition from technical, judging, and\norganizational perspectives. We study the use of telerobotics technologies and\ninnovations pursued by the competing teams in their avatar systems, and\ncorrelate the use of these technologies with judges' task performance and\nsubjective survey ratings. It also summarizes perspectives from team leads,\njudges, and organizers about the competition's execution and impact to inform\nthe future development of telerobotics and telepresence.\n",
        "title": "Analysis and Perspectives on the ANA Avatar XPRIZE Competition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05293",
        "abstract_url": "http://arxiv.org/abs/2401.05293",
        "authors": [
            {
                "last_name": "Alldieck",
                "first_name": "Thiemo"
            },
            {
                "last_name": "Kolotouros",
                "first_name": "Nikos"
            },
            {
                "last_name": "Sminchisescu",
                "first_name": "Cristian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Score Distillation Sampling (SDS) is a recent but already widely popular\nmethod that relies on an image diffusion model to control optimization problems\nusing text prompts. In this paper, we conduct an in-depth analysis of the SDS\nloss function, identify an inherent problem with its formulation, and propose a\nsurprisingly easy but effective fix. Specifically, we decompose the loss into\ndifferent factors and isolate the component responsible for noisy gradients. In\nthe original formulation, high text guidance is used to account for the noise,\nleading to unwanted side effects. Instead, we train a shallow network mimicking\nthe timestep-dependent denoising deficiency of the image diffusion model in\norder to effectively factor it out. We demonstrate the versatility and the\neffectiveness of our novel loss formulation through several qualitative and\nquantitative experiments, including optimization-based image synthesis and\nediting, zero-shot image translation network training, and text-to-3D\nsynthesis.\n",
        "title": "Score Distillation Sampling with Learned Manifold Corrective",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05294",
        "abstract_url": "http://arxiv.org/abs/2401.05294",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Mathai",
                "first_name": "Tejas Sudharshan"
            },
            {
                "last_name": "Liu",
                "first_name": "Jianfei"
            },
            {
                "last_name": "Parnell",
                "first_name": "Christopher"
            },
            {
                "last_name": "Summers",
                "first_name": "Ronald M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Purpose: Body composition measurements from routine abdominal CT can yield\npersonalized risk assessments for asymptomatic and diseased patients. In\nparticular, attenuation and volume measures of muscle and fat are associated\nwith important clinical outcomes, such as cardiovascular events, fractures, and\ndeath. This study evaluates the reliability of an Internal tool for the\nsegmentation of muscle and fat (subcutaneous and visceral) as compared to the\nwell-established public TotalSegmentator tool.\n  Methods: We assessed the tools across 900 CT series from the publicly\navailable SAROS dataset, focusing on muscle, subcutaneous fat, and visceral\nfat. The Dice score was employed to assess accuracy in subcutaneous fat and\nmuscle segmentation. Due to the lack of ground truth segmentations for visceral\nfat, Cohen's Kappa was utilized to assess segmentation agreement between the\ntools.\n  Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) for\nsubcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentation\nrespectively. A Wilcoxon signed-rank test revealed that our results were\nstatistically different with p<0.01. For visceral fat, the Cohen's kappa score\nof 0.856 indicated near-perfect agreement between the two tools. Our internal\ntool also showed very strong correlations for muscle volume (R^2=0.99), muscle\nattenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderate\ncorrelation for subcutaneous fat attenuation (R^2=0.45).\n  Conclusion: Our findings indicated that our Internal tool outperformed\nTotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen's\nKappa score for visceral fat suggests a reliable level of agreement between the\ntwo tools. These results demonstrate the potential of our tool in advancing the\naccuracy of body composition analysis.\n",
        "title": "Enhanced Muscle and Fat Segmentation for CT-Based Body Composition\n  Analysis: A Comparative Study",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05295",
        "abstract_url": "http://arxiv.org/abs/2401.05295",
        "authors": [
            {
                "last_name": "Regad\u00edo",
                "first_name": "Alberto"
            },
            {
                "last_name": "Esteban",
                "first_name": "Luis"
            },
            {
                "last_name": "S\u00e1nchez-Prieto",
                "first_name": "Sebasti\u00e1n"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  To address the possible lack or total absence of pulses from particle\ndetectors during the development of its associate electronics, we propose a\nmodel that can generate them without losing the features of the real ones. This\nmodel is based on artificial neural networks, namely Generative Adversarial\nNetworks (GAN). We describe the proposed network architecture, its training\nmethodology and the approach to train the GAN with real pulses from a\nscintillator receiving radiation from sources of ${}^{137}$Cs and ${}^{22}$Na.\nThe Generator was installed in a Xilinx's System-On-Chip (SoC). We show how the\nnetwork is capable of generating pulses with the same shape as the real ones\nthat even match the data distributions in the original pulse-height histogram\ndata.\n",
        "title": "Synthesis of pulses from particle detectors with a Generative\n  Adversarial Network (GAN)",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05300",
        "abstract_url": "http://arxiv.org/abs/2401.05300",
        "authors": [
            {
                "last_name": "Thrush",
                "first_name": "Tristan"
            },
            {
                "last_name": "Moore",
                "first_name": "Jared"
            },
            {
                "last_name": "Monares",
                "first_name": "Miguel"
            },
            {
                "last_name": "Potts",
                "first_name": "Christopher"
            },
            {
                "last_name": "Kiela",
                "first_name": "Douwe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Statements involving metalinguistic self-reference (\"This paper has six\nsections.\") are prevalent in many domains. Can large language models (LLMs)\nhandle such language? In this paper, we present \"I am a Strange Dataset\", a new\ndataset for addressing this question. There are two subtasks: generation and\nverification. In generation, models continue statements like \"The penultimate\nword in this sentence is\" (where a correct continuation is \"is\"). In\nverification, models judge the truth of statements like \"The penultimate word\nin this sentence is sentence.\" (false). We also provide minimally different\nmetalinguistic non-self-reference examples to complement the main dataset by\nprobing for whether models can handle metalinguistic language at all. The\ndataset is hand-crafted by experts and validated by non-expert annotators. We\ntest a variety of open-source LLMs (7B to 70B parameters) as well as\nclosed-source LLMs through APIs. All models perform close to chance across both\nsubtasks and even on the non-self-referential metalinguistic control data,\nthough we find some steady improvement with model scale. GPT 4 is the only\nmodel to consistently do significantly better than chance, and it is still only\nin the 60% range, while our untrained human annotators score well in the 89-93%\nrange. The dataset and evaluation toolkit are available at\nhttps://github.com/TristanThrush/i-am-a-strange-dataset.\n",
        "title": "I am a Strange Dataset: Metalinguistic Tests for Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05302",
        "abstract_url": "http://arxiv.org/abs/2401.05302",
        "authors": [
            {
                "last_name": "Verma",
                "first_name": "Mudit"
            },
            {
                "last_name": "Bhambri",
                "first_name": "Siddhant"
            },
            {
                "last_name": "Kambhampati",
                "first_name": "Subbarao"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            "HC"
        ],
        "abstract": "  Large Language Models have shown exceptional generative abilities in various\nnatural language and generation tasks. However, possible anthropomorphization\nand leniency towards failure cases have propelled discussions on emergent\nabilities of Large Language Models especially on Theory of Mind (ToM) abilities\nin Large Language Models. While several false-belief tests exists to verify the\nability to infer and maintain mental models of another entity, we study a\nspecial application of ToM abilities that has higher stakes and possibly\nirreversible consequences : Human Robot Interaction. In this work, we explore\nthe task of Perceived Behavior Recognition, where a robot employs a Large\nLanguage Model (LLM) to assess the robot's generated behavior in a manner\nsimilar to human observer. We focus on four behavior types, namely -\nexplicable, legible, predictable, and obfuscatory behavior which have been\nextensively used to synthesize interpretable robot behaviors. The LLMs goal is,\ntherefore to be a human proxy to the agent, and to answer how a certain agent\nbehavior would be perceived by the human in the loop, for example \"Given a\nrobot's behavior X, would the human observer find it explicable?\". We conduct a\nhuman subject study to verify that the users are able to correctly answer such\na question in the curated situations (robot setting and plan) across five\ndomains. A first analysis of the belief test yields extremely positive results\ninflating ones expectations of LLMs possessing ToM abilities. We then propose\nand perform a suite of perturbation tests which breaks this illusion, i.e.\nInconsistent Belief, Uninformative Context and Conviction Test. We conclude\nthat, the high score of LLMs on vanilla prompts showcases its potential use in\nHRI settings, however to possess ToM demands invariance to trivial or\nirrelevant perturbations in the context which LLMs lack.\n",
        "title": "Theory of Mind abilities of Large Language Models in Human-Robot\n  Interaction : An Illusion?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05304",
        "abstract_url": "http://arxiv.org/abs/2401.05304",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Jessica"
            },
            {
                "last_name": "Flanigan",
                "first_name": "Bailey"
            },
            {
                "last_name": "Haghtalab",
                "first_name": "Nika"
            },
            {
                "last_name": "Jagadeesan",
                "first_name": "Meena"
            },
            {
                "last_name": "Podimata",
                "first_name": "Chara"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  A common explanation for negative user impacts of content recommender systems\nis misalignment between the platform's objective and user welfare. In this\nwork, we show that misalignment in the platform's objective is not the only\npotential cause of unintended impacts on users: even when the platform's\nobjective is fully aligned with user welfare, the platform's learning algorithm\ncan induce negative downstream impacts on users. The source of these user\nimpacts is that different pieces of content may generate observable user\nreactions (feedback information) at different rates; these feedback rates may\ncorrelate with content properties, such as controversiality or demographic\nsimilarity of the creator, that affect the user experience. Since differences\nin feedback rates can impact how often the learning algorithm engages with\ndifferent content, the learning algorithm may inadvertently promote content\nwith certain such properties. Using the multi-armed bandit framework with\nprobabilistic feedback, we examine the relationship between feedback rates and\na learning algorithm's engagement with individual arms for different no-regret\nalgorithms. We prove that no-regret algorithms can exhibit a wide range of\ndependencies: if the feedback rate of an arm increases, some no-regret\nalgorithms engage with the arm more, some no-regret algorithms engage with the\narm less, and other no-regret algorithms engage with the arm approximately the\nsame number of times. From a platform design perspective, our results highlight\nthe importance of looking beyond regret when measuring an algorithm's\nperformance, and assessing the nature of a learning algorithm's engagement with\ndifferent types of content as well as their resulting downstream impacts.\n",
        "title": "Can Probabilistic Feedback Drive User Impacts in Online Platforms?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05308",
        "abstract_url": "http://arxiv.org/abs/2401.05308",
        "authors": [
            {
                "last_name": "Farajzadeh",
                "first_name": "Amin"
            },
            {
                "last_name": "Yadav",
                "first_name": "Animesh"
            },
            {
                "last_name": "Yanikomeroglu",
                "first_name": "Halim"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CV",
            "LG"
        ],
        "abstract": "  The deployment of federated learning (FL) within vertical heterogeneous\nnetworks, such as those enabled by high-altitude platform station (HAPS),\noffers the opportunity to engage a wide array of clients, each endowed with\ndistinct communication and computational capabilities. This diversity not only\nenhances the training accuracy of FL models but also hastens their convergence.\nYet, applying FL in these expansive networks presents notable challenges,\nparticularly the significant non-IIDness in client data distributions. Such\ndata heterogeneity often results in slower convergence rates and reduced\neffectiveness in model training performance. Our study introduces a client\nselection strategy tailored to address this issue, leveraging user network\ntraffic behaviour. This strategy involves the prediction and classification of\nclients based on their network usage patterns while prioritizing user privacy.\nBy strategically selecting clients whose data exhibit similar patterns for\nparticipation in FL training, our approach fosters a more uniform and\nrepresentative data distribution across the network. Our simulations\ndemonstrate that this targeted client selection methodology significantly\nreduces the training loss of FL models in HAPS networks, thereby effectively\ntackling a crucial challenge in implementing large-scale FL systems.\n",
        "title": "Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05314",
        "abstract_url": "http://arxiv.org/abs/2401.05314",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Kevin"
            },
            {
                "last_name": "Liu",
                "first_name": "Chonghua"
            },
            {
                "last_name": "Chan",
                "first_name": "David M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "CV",
            "SD"
        ],
        "abstract": "  The Internet's wealth of content, with up to 60% published in English,\nstarkly contrasts the global population, where only 18.8% are English speakers,\nand just 5.1% consider it their native language, leading to disparities in\nonline information access. Unfortunately, automated processes for dubbing of\nvideo - replacing the audio track of a video with a translated alternative -\nremains a complex and challenging task due to pipelines, necessitating precise\ntiming, facial movement synchronization, and prosody matching. While end-to-end\ndubbing offers a solution, data scarcity continues to impede the progress of\nboth end-to-end and pipeline-based methods. In this work, we introduce\nAnim-400K, a comprehensive dataset of over 425K aligned animated video segments\nin Japanese and English supporting various video-related tasks, including\nautomated dubbing, simultaneous translation, guided video summarization, and\ngenre/theme/style classification. Our dataset is made publicly available for\nresearch purposes at https://github.com/davidmchan/Anim400K.\n",
        "title": "ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of\n  Video",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05318",
        "abstract_url": "http://arxiv.org/abs/2401.05318",
        "authors": [
            {
                "last_name": "Piazza",
                "first_name": "Cristina"
            },
            {
                "last_name": "Della Santina",
                "first_name": "Cosimo"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            },
            {
                "last_name": "Grioli",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Bicchi",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robot feet are crucial for maintaining dynamic stability and propelling the\nbody during walking, especially on uneven terrains. Traditionally, robot feet\nwere mostly designed as flat and stiff pieces of metal, which meets its\nlimitations when the robot is required to step on irregular grounds, e.g.\nstones. While one could think that adding compliance under such feet would\nsolve the problem, this is not the case. To address this problem, we introduced\nthe SoftFoot, an adaptive foot design that can enhance walking performance over\nirregular grounds. The proposed design is completely passive and varies its\nshape and stiffness based on the exerted forces, through a system of pulley,\ntendons, and springs opportunely placed in the structure. This paper outlines\nthe motivation behind the SoftFoot and describes the theoretical model which\nled to its final design. The proposed system has been experimentally tested and\ncompared with two analogous conventional feet, a rigid one and a compliant one,\nwith similar footprints and soles. The experimental validation focuses on the\nanalysis of the standing performance, measured in terms of the equivalent\nsupport surface extension and the compensatory ankle angle, and the rejection\nof impulsive forces, which is important in events such as stepping on\nunforeseen obstacles. Results show that the SoftFoot has the largest equivalent\nsupport surface when standing on obstacles, and absorbs impulsive loads in a\nway almost as good as a compliant foot.\n",
        "title": "Analytical Model and Experimental Testing of the SoftFoot: an Adaptive\n  Robot Foot for Walking over Obstacles and Irregular Terrains",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05319",
        "abstract_url": "http://arxiv.org/abs/2401.05319",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xueyu"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Sun",
                "first_name": "Jiankai"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  Large language models (LLMs) have made significant progress in code\ngeneration tasks, but their performance in tackling programming problems with\ncomplex data structures and algorithms remains suboptimal. To address this\nissue, we propose an in-context learning approach that guides LLMs to debug by\nusing a \"print debugging\" method, which involves inserting print statements to\ntrace and analysing logs for fixing the bug. We collect a Leetcode problem\ndataset and evaluate our method using the Leetcode online judging system.\nExperiments with GPT-4 demonstrate the effectiveness of our approach,\noutperforming rubber duck debugging in easy and medium-level Leetcode problems\nby 1.5% and 17.9%.\n",
        "title": "Leveraging Print Debugging to Improve Code Generation in Large Language\n  Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05321",
        "abstract_url": "http://arxiv.org/abs/2401.05321",
        "authors": [
            {
                "last_name": "Beame",
                "first_name": "Paul"
            },
            {
                "last_name": "Kornerup",
                "first_name": "Niels"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "",
            "",
            ""
        ],
        "abstract": "  We consider the time and space required for quantum computers to solve a wide\nvariety of problems involving matrices, many of which have only been analyzed\nclassically in prior work. Our main results show that for a range of linear\nalgebra problems -- including matrix-vector product, matrix inversion, matrix\nmultiplication and powering -- existing classical time-space tradeoffs, several\nof which are tight for every space bound, also apply to quantum algorithms. For\nexample, for almost all matrices $A$, including the discrete Fourier transform\n(DFT) matrix, we prove that quantum circuits with at most $T$ input queries and\n$S$ qubits of memory require $T=\\Omega(n^2/S)$ to compute matrix-vector product\n$Ax$ for $x \\in \\{0,1\\}^n$. We similarly prove that matrix multiplication for\n$n\\times n$ binary matrices requires $T=\\Omega(n^3 / \\sqrt{S})$. Because many\nof our lower bounds match deterministic algorithms with the same time and space\ncomplexity, we show that quantum computers cannot provide any asymptotic\nadvantage for these problems with any space bound. We obtain matching lower\nbounds for the stronger notion of quantum cumulative memory complexity -- the\nsum of the space per layer of a circuit.\n  We also consider Boolean (i.e. AND-OR) matrix multiplication and\nmatrix-vector products, improving the previous quantum time-space tradeoff\nlower bounds for $n\\times n$ Boolean matrix multiplication to\n$T=\\Omega(n^{2.5}/S^{1/3})$ from $T=\\Omega(n^{2.5}/S^{1/2})$.\n  Our improved lower bound for Boolean matrix multiplication is based on a new\ncoloring argument that extracts more from the strong direct product theorem\nused in prior work. Our tight lower bounds for linear algebra problems require\nadding a new bucketing method to the recording-query technique of Zhandry that\nlets us apply classical arguments to upper bound the success probability of\nquantum circuits.\n",
        "title": "Quantum Time-Space Tradeoffs for Matrix Problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05322",
        "abstract_url": "http://arxiv.org/abs/2401.05322",
        "authors": [
            {
                "last_name": "Schmidt",
                "first_name": "Carolin"
            },
            {
                "last_name": "Tygesen",
                "first_name": "Mathias"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Filipe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Urban mobility is on the cusp of transformation with the emergence of shared,\nconnected, and cooperative automated vehicles. Yet, for them to be accepted by\ncustomers, trust in their punctuality is vital. Many pilot initiatives operate\nwithout a fixed schedule, thus enhancing the importance of reliable arrival\ntime (AT) predictions. This study presents an AT prediction system for\nautonomous shuttles, utilizing separate models for dwell and running time\npredictions, validated on real-world data from five cities. Alongside\nestablished methods such as XGBoost, we explore the benefits of integrating\nspatial data using graph neural networks (GNN). To accurately handle the case\nof a shuttle bypassing a stop, we propose a hierarchical model combining a\nrandom forest classifier and a GNN. The results for the final AT prediction are\npromising, showing low errors even when predicting several stops ahead. Yet, no\nsingle model emerges as universally superior, and we provide insights into the\ncharacteristics of pilot sites that influence the model selection process.\nFinally, we identify dwell time prediction as the key determinant in overall AT\nprediction accuracy when autonomous shuttles are deployed in low-traffic areas\nor under regulatory speed limits. This research provides insights into the\ncurrent state of autonomous public transport prediction models and paves the\nway for more data-informed decision-making as the field advances.\n",
        "title": "Arrival Time Prediction for Autonomous Shuttle Services in the Real\n  World: Evidence from Five Cities",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05329",
        "abstract_url": "http://arxiv.org/abs/2401.05329",
        "authors": [
            {
                "last_name": "Sen",
                "first_name": "Argha"
            },
            {
                "last_name": "Pal",
                "first_name": "Bhupendra"
            },
            {
                "last_name": "Achari",
                "first_name": "Seemant"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Sandip"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In the landscape of next-generation cellular networks, a projected surge of\nover 12 billion subscriptions foreshadows a considerable upswing in the\nnetwork's overall energy consumption. The proliferation of User Equipment (UE)\ndrives this energy demand, urging 5G deployments to seek more energy-efficient\nmethodologies. In this work, we propose SmartMME, as a pivotal solution aimed\nat optimizing Base Station (BS) energy usage. By harnessing and analyzing\ncritical network states-such as UE connections, data traffic at individual UEs,\nand other pertinent metrics-our methodology intelligently orchestrates the BS's\npower states, making informed decisions on when to activate or deactivate the\nBS. This meticulous approach significantly curtails the network's overall\nenergy consumption. In a bid to validate its efficiency, we seamlessly\nintegrated our module into Network Simulator-3 (ns-3), conducting extensive\ntesting to demonstrate its prowess in effectively managing and reducing net\nenergy consumption. As advocates of collaborative progress, we've opted to\nopen-source this module, inviting the engagement and feedback of the wider\nresearch community on GitHub.\n",
        "title": "\\textit{SmartMME}: Implementation of Base Station Switching Off Strategy\n  in ns-3",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05334",
        "abstract_url": "http://arxiv.org/abs/2401.05334",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhaoxi"
            },
            {
                "last_name": "Moon",
                "first_name": "Gyeongsik"
            },
            {
                "last_name": "Guo",
                "first_name": "Kaiwen"
            },
            {
                "last_name": "Cao",
                "first_name": "Chen"
            },
            {
                "last_name": "Pidhorskyi",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Simon",
                "first_name": "Tomas"
            },
            {
                "last_name": "Joshi",
                "first_name": "Rohan"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Yichen"
            },
            {
                "last_name": "Pires",
                "first_name": "Bernardo"
            },
            {
                "last_name": "Wen",
                "first_name": "He"
            },
            {
                "last_name": "Evans",
                "first_name": "Lucas"
            },
            {
                "last_name": "Peng",
                "first_name": "Bo"
            },
            {
                "last_name": "Buffalini",
                "first_name": "Julia"
            },
            {
                "last_name": "Trimble",
                "first_name": "Autumn"
            },
            {
                "last_name": "McPhail",
                "first_name": "Kevyn"
            },
            {
                "last_name": "Schoeller",
                "first_name": "Melissa"
            },
            {
                "last_name": "Yu",
                "first_name": "Shoou-I"
            },
            {
                "last_name": "Romero",
                "first_name": "Javier"
            },
            {
                "last_name": "Zollh\u00f6fer",
                "first_name": "Michael"
            },
            {
                "last_name": "Sheikh",
                "first_name": "Yaser"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Saito",
                "first_name": "Shunsuke"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Existing photorealistic relightable hand models require extensive\nidentity-specific observations in different views, poses, and illuminations,\nand face challenges in generalizing to natural illuminations and novel\nidentities. To bridge this gap, we present URHand, the first universal\nrelightable hand model that generalizes across viewpoints, poses,\nilluminations, and identities. Our model allows few-shot personalization using\nimages captured with a mobile phone, and is ready to be photorealistically\nrendered under novel illuminations. To simplify the personalization process\nwhile retaining photorealism, we build a powerful universal relightable prior\nbased on neural relighting from multi-view images of hands captured in a light\nstage with hundreds of identities. The key challenge is scaling the\ncross-identity training while maintaining personalized fidelity and sharp\ndetails without compromising generalization under natural illuminations. To\nthis end, we propose a spatially varying linear lighting model as the neural\nrenderer that takes physics-inspired shading as input feature. By removing\nnon-linear activations and bias, our specifically designed lighting model\nexplicitly keeps the linearity of light transport. This enables single-stage\ntraining from light-stage data while generalizing to real-time rendering under\narbitrary continuous illuminations across diverse identities. In addition, we\nintroduce the joint learning of a physically based model and our neural\nrelighting model, which further improves fidelity and generalization. Extensive\nexperiments show that our approach achieves superior performance over existing\nmethods in terms of both quality and generalizability. We also demonstrate\nquick personalization of URHand from a short phone scan of an unseen identity.\n",
        "title": "URHand: Universal Relightable Hands",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05335",
        "abstract_url": "http://arxiv.org/abs/2401.05335",
        "authors": [
            {
                "last_name": "Shahbazi",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Claessens",
                "first_name": "Liesbeth"
            },
            {
                "last_name": "Niemeyer",
                "first_name": "Michael"
            },
            {
                "last_name": "Collins",
                "first_name": "Edo"
            },
            {
                "last_name": "Tonioni",
                "first_name": "Alessio"
            },
            {
                "last_name": "Van Gool",
                "first_name": "Luc"
            },
            {
                "last_name": "Tombari",
                "first_name": "Federico"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "LG"
        ],
        "abstract": "  We introduce InseRF, a novel method for generative object insertion in the\nNeRF reconstructions of 3D scenes. Based on a user-provided textual description\nand a 2D bounding box in a reference viewpoint, InseRF generates new objects in\n3D scenes. Recently, methods for 3D scene editing have been profoundly\ntransformed, owing to the use of strong priors of text-to-image diffusion\nmodels in 3D generative modeling. Existing methods are mostly effective in\nediting 3D scenes via style and appearance changes or removing existing\nobjects. Generating new objects, however, remains a challenge for such methods,\nwhich we address in this study. Specifically, we propose grounding the 3D\nobject insertion to a 2D object insertion in a reference view of the scene. The\n2D edit is then lifted to 3D using a single-view object reconstruction method.\nThe reconstructed object is then inserted into the scene, guided by the priors\nof monocular depth estimation methods. We evaluate our method on various 3D\nscenes and provide an in-depth analysis of the proposed components. Our\nexperiments with generative insertion of objects in several 3D scenes indicate\nthe effectiveness of our method compared to the existing methods. InseRF is\ncapable of controllable and 3D-consistent object insertion without requiring\nexplicit 3D information as input. Please visit our project page at\nhttps://mohamad-shahbazi.github.io/inserf.\n",
        "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05336",
        "abstract_url": "http://arxiv.org/abs/2401.05336",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Ronglai"
            },
            {
                "last_name": "Wei",
                "first_name": "Fangyun"
            },
            {
                "last_name": "Mak",
                "first_name": "Brian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The objective of sign language recognition is to bridge the communication gap\nbetween the deaf and the hearing. Numerous previous works train their models\nusing the well-established connectionist temporal classification (CTC) loss.\nDuring the inference stage, the CTC-based models typically take the entire sign\nvideo as input to make predictions. This type of inference scheme is referred\nto as offline recognition. In contrast, while mature speech recognition systems\ncan efficiently recognize spoken words on the fly, sign language recognition\nstill falls short due to the lack of practical online solutions. In this work,\nwe take the first step towards filling this gap. Our approach comprises three\nphases: 1) developing a sign language dictionary encompassing all glosses\npresent in a target sign language dataset; 2) training an isolated sign\nlanguage recognition model on augmented signs using both conventional\nclassification loss and our novel saliency loss; 3) employing a sliding window\napproach on the input sign sequence and feeding each sign clip to the\nwell-optimized model for online recognition. Furthermore, our online\nrecognition model can be extended to boost the performance of any offline\nmodel, and to support online translation by appending a gloss-to-text network\nonto the recognition model. By integrating our online framework with the\npreviously best-performing offline model, TwoStream-SLR, we achieve new\nstate-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T,\nand CSL-Daily. Code and models will be available at\nhttps://github.com/FangyunWei/SLRT\n",
        "title": "Towards Online Sign Language Recognition and Translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05337",
        "abstract_url": "http://arxiv.org/abs/2401.05337",
        "authors": [
            {
                "last_name": "Renucci",
                "first_name": "Pierre"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This study presents an unsupervised machine learning approach for optimizing\nProfit and Loss (PnL) in quantitative finance. Our algorithm, akin to an\nunsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL\ngenerated from signals constructed linearly from exogenous variables. The\nmethodology employs a linear relationship between exogenous variables and the\ntrading signal, with the objective of maximizing the Sharpe Ratio through\nparameter optimization. Empirical application on an ETF representing U.S.\nTreasury bonds demonstrates the model's effectiveness, supported by\nregularization techniques to mitigate overfitting. The study concludes with\npotential avenues for further development, including generalized time steps and\nenhanced corrective terms.\n",
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to\n  Optimize PnL with Linear Signals",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05338",
        "abstract_url": "http://arxiv.org/abs/2401.05338",
        "authors": [
            {
                "last_name": "Shao",
                "first_name": "Daqian"
            },
            {
                "last_name": "Fesser",
                "first_name": "Lukas"
            },
            {
                "last_name": "Kwiatkowska",
                "first_name": "Marta"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Robustness certification, which aims to formally certify the predictions of\nneural networks against adversarial inputs, has become an integral part of\nimportant tool for safety-critical applications. Despite considerable progress,\nexisting certification methods are limited to elementary architectures, such as\nconvolutional networks, recurrent networks and recently Transformers, on\nbenchmark datasets such as MNIST. In this paper, we focus on the robustness\ncertification of scene text recognition (STR), which is a complex and\nextensively deployed image-based sequence prediction problem. We tackle three\ntypes of STR model architectures, including the standard STR pipelines and the\nVision Transformer. We propose STR-Cert, the first certification method for STR\nmodels, by significantly extending the DeepPoly polyhedral verification\nframework via deriving novel polyhedral bounds and algorithms for key STR model\ncomponents. Finally, we certify and compare STR models on six datasets,\ndemonstrating the efficiency and scalability of robustness certification,\nparticularly for the Vision Transformer.\n",
        "title": "STR-Cert: Robustness Certification for Deep Text Recognition on Deep\n  Learning Pipelines and Vision Transformers",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05339",
        "abstract_url": "http://arxiv.org/abs/2401.05339",
        "authors": [
            {
                "last_name": "Chong",
                "first_name": "Toby"
            },
            {
                "last_name": "Chadwick",
                "first_name": "Alina"
            },
            {
                "last_name": "Shen",
                "first_name": "I-chao"
            },
            {
                "last_name": "Xie",
                "first_name": "Haoran"
            },
            {
                "last_name": "Igarashi",
                "first_name": "Takeo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  In this paper, we present a cosmetic-specific skin image dataset. It consists\nof skin images from $45$ patches ($5$ skin patches each from $9$ participants)\nof size $8mm^*8mm$ under three cosmetic products (i.e., foundation, blusher,\nand highlighter). We designed a novel capturing device inspired by Light Stage.\nUsing the device, we captured over $600$ images of each skin patch under\ndiverse lighting conditions in $30$ seconds. We repeated the process for the\nsame skin patch under three cosmetic products. Finally, we demonstrate the\nviability of the dataset with an image-to-image translation-based pipeline for\ncosmetic rendering and compared our data-driven approach to an existing\ncosmetic rendering method.\n",
        "title": "MicroGlam: Microscopic Skin Image Dataset with Cosmetics",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05341",
        "abstract_url": "http://arxiv.org/abs/2401.05341",
        "authors": [
            {
                "last_name": "Vogt",
                "first_name": "Yannick"
            },
            {
                "last_name": "Naouar",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Kalweit",
                "first_name": "Maria"
            },
            {
                "last_name": "Miething",
                "first_name": "Christoph Cornelius"
            },
            {
                "last_name": "Duyster",
                "first_name": "Justus"
            },
            {
                "last_name": "Mertelsmann",
                "first_name": "Roland"
            },
            {
                "last_name": "Kalweit",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Boedecker",
                "first_name": "Joschka"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The field of antibody-based therapeutics has grown significantly in recent\nyears, with targeted antibodies emerging as a potentially effective approach to\npersonalized therapies. Such therapies could be particularly beneficial for\ncomplex, highly individual diseases such as cancer. However, progress in this\nfield is often constrained by the extensive search space of amino acid\nsequences that form the foundation of antibody design. In this study, we\nintroduce a novel reinforcement learning method specifically tailored to\naddress the unique challenges of this domain. We demonstrate that our method\ncan learn the design of high-affinity antibodies against multiple targets in\nsilico, utilizing either online interaction or offline datasets. To the best of\nour knowledge, our approach is the first of its kind and outperforms existing\nmethods on all tested antigens in the Absolut! database.\n",
        "title": "Stable Online and Offline Reinforcement Learning for Antibody CDRH3\n  Design",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05342",
        "abstract_url": "http://arxiv.org/abs/2401.05342",
        "authors": [
            {
                "last_name": "Burg",
                "first_name": "Max F."
            },
            {
                "last_name": "Zenkel",
                "first_name": "Thomas"
            },
            {
                "last_name": "Vystr\u010dilov\u00e1",
                "first_name": "Michaela"
            },
            {
                "last_name": "Oesterle",
                "first_name": "Jonathan"
            },
            {
                "last_name": "H\u00f6fling",
                "first_name": "Larissa"
            },
            {
                "last_name": "Willeke",
                "first_name": "Konstantin F."
            },
            {
                "last_name": "Lause",
                "first_name": "Jan"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Sarah"
            },
            {
                "last_name": "Fahey",
                "first_name": "Paul G."
            },
            {
                "last_name": "Ding",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Restivo",
                "first_name": "Kelli"
            },
            {
                "last_name": "Sridhar",
                "first_name": "Shashwat"
            },
            {
                "last_name": "Gollisch",
                "first_name": "Tim"
            },
            {
                "last_name": "Berens",
                "first_name": "Philipp"
            },
            {
                "last_name": "Tolias",
                "first_name": "Andreas S."
            },
            {
                "last_name": "Euler",
                "first_name": "Thomas"
            },
            {
                "last_name": "Bethge",
                "first_name": "Matthias"
            },
            {
                "last_name": "Ecker",
                "first_name": "Alexander S."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Identifying cell types and understanding their functional properties is\ncrucial for unraveling the mechanisms underlying perception and cognition. In\nthe retina, functional types can be identified by carefully selected stimuli,\nbut this requires expert domain knowledge and biases the procedure towards\npreviously known cell types. In the visual cortex, it is still unknown what\nfunctional types exist and how to identify them. Thus, for unbiased\nidentification of the functional cell types in retina and visual cortex, new\napproaches are needed. Here we propose an optimization-based clustering\napproach using deep predictive models to obtain functional clusters of neurons\nusing Most Discriminative Stimuli (MDS). Our approach alternates between\nstimulus optimization with cluster reassignment akin to an\nexpectation-maximization algorithm. The algorithm recovers functional clusters\nin mouse retina, marmoset retina and macaque visual area V4. This demonstrates\nthat our approach can successfully find discriminative stimuli across species,\nstages of the visual system and recording techniques. The resulting most\ndiscriminative stimuli can be used to assign functional cell types fast and on\nthe fly, without the need to train complex predictive models or show a large\nnatural scene dataset, paving the way for experiments that were previously\nlimited by experimental time. Crucially, MDS are interpretable: they visualize\nthe distinctive stimulus patterns that most unambiguously identify a specific\ntype of neuron. We will make our code available online upon publication.\n",
        "title": "Most discriminative stimuli for functional cell type identification",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05345",
        "abstract_url": "http://arxiv.org/abs/2401.05345",
        "authors": [
            {
                "last_name": "Durvasula",
                "first_name": "Sankeerth"
            },
            {
                "last_name": "Zhao",
                "first_name": "Adrian"
            },
            {
                "last_name": "Chen",
                "first_name": "Fan"
            },
            {
                "last_name": "Liang",
                "first_name": "Ruofan"
            },
            {
                "last_name": "Sanjaya",
                "first_name": "Pawan Kumar"
            },
            {
                "last_name": "Vijaykumar",
                "first_name": "Nandita"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "PF"
        ],
        "abstract": "  Differentiable rendering is a technique used in an important emerging class\nof visual computing applications that involves representing a 3D scene as a\nmodel that is trained from 2D images using gradient descent. Recent works (e.g.\n3D Gaussian Splatting) use a rasterization pipeline to enable rendering high\nquality photo-realistic imagery at high speeds from these learned 3D models.\nThese methods have been demonstrated to be very promising, providing\nstate-of-art quality for many important tasks. However, training a model to\nrepresent a scene is still a time-consuming task even when using powerful GPUs.\nIn this work, we observe that the gradient computation phase during training is\na significant bottleneck on GPUs due to the large number of atomic operations\nthat need to be processed. These atomic operations overwhelm atomic units in\nthe L2 partitions causing stalls. To address this challenge, we leverage the\nobservations that during the gradient computation: (1) for most warps, all\nthreads atomically update the same memory locations; and (2) warps generate\nvarying amounts of atomic traffic (since some threads may be inactive). We\npropose DISTWAR, a software-approach to accelerate atomic operations based on\ntwo key ideas: First, we enable warp-level reduction of threads at the SM\nsub-cores using registers to leverage the locality in intra-warp atomic\nupdates. Second, we distribute the atomic computation between the warp-level\nreduction at the SM and the L2 atomic units to increase the throughput of\natomic computation. Warps with many threads performing atomic updates to the\nsame memory locations are scheduled at the SM, and the rest using L2 atomic\nunits. We implement DISTWAR using existing warp-level primitives. We evaluate\nDISTWAR on widely used raster-based differentiable rendering workloads. We\ndemonstrate significant speedups of 2.44x on average (up to 5.7x).\n",
        "title": "DISTWAR: Fast Differentiable Rendering on Raster-based Rendering\n  Pipelines",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05346",
        "abstract_url": "http://arxiv.org/abs/2401.05346",
        "authors": [
            {
                "last_name": "Attapu",
                "first_name": "Amitha"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Robots can be very useful to automate tasks and reduce the human effort\nrequired. But for the robot to know, how to perform tasks, we need to give it a\nclear set of steps to follow. It is nearly impossible to provide a robot with\ninstructions for every possible task. Therefore we have a Universal Functional\nobject-oriented network (FOON) which was created and expanded and has a lot of\nexisting recipe information [1]. But certain tasks are complicated for robots\nto perform and similarly, some tasks are complicated for humans to perform.\nTherefore weights have been added to functional units to represent the chance\nof successful execution of the motion by the robot [2]. Given a set of kitchen\nitems and a goal node, using Universal FOON, a robot must be able to determine\nif the required items are present in the kitchen, and if yes, get the steps to\nconvert the required kitchen items to the goal node. Now through this paper, we\nuse two algorithms (IDS and GBFS) to retrieve a task tree (if possible) for a\ngoal node and a given set of kitchen items. The following would be the\ndifferent parts of the paper: Section II FOON creation, where we will discuss\nthe different terminologies related to FOON and visualization of FOON. In\nSection III Methodology we discuss the IDS and GBFS search algorithms and the\ntwo different heuristics implemented and used in GBFS. In Section IV\nExperiment/Discussion, we compare the performance of different algorithms. In\nthe final section V, we specify the references of the papers that have been\ncited.\n",
        "title": "Task tree retrieval from FOON using search algorithms",
        "date": "2023-12-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05350",
        "abstract_url": "http://arxiv.org/abs/2401.05350",
        "authors": [
            {
                "last_name": "Aydin",
                "first_name": "Mehmet Emin"
            },
            {
                "last_name": "Durgut",
                "first_name": "Rafet"
            },
            {
                "last_name": "Rakib",
                "first_name": "Abdur"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "LG"
        ],
        "abstract": "  Optimisation problems, particularly combinatorial optimisation problems, are\ndifficult to solve due to their complexity and hardness. Such problems have\nbeen successfully solved by evolutionary and swarm intelligence algorithms,\nespecially in binary format. However, the approximation may suffer due to the\nthe issues in balance between exploration and exploitation activities (EvE),\nwhich remain as the major challenge in this context. Although the complementary\nusage of multiple operators is becoming more popular for managing EvE with\nadaptive operator selection schemes, a bespoke adaptive selection system is\nstill an important topic in research. Reinforcement Learning (RL) has recently\nbeen proposed as a way to customise and shape up a highly effective adaptive\nselection system. However, it is still challenging to handle the problem in\nterms of scalability. This paper proposes and assesses a RL-based novel\napproach to help develop a generalised framework for gaining, processing, and\nutilising the experiences for both the immediate and future use. The\nexperimental results support the proposed approach with a certain level of\nsuccess.\n",
        "title": "Adaptive operator selection utilising generalised experience",
        "date": "2023-12-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05351",
        "abstract_url": "http://arxiv.org/abs/2401.05351",
        "authors": [
            {
                "last_name": "Runge",
                "first_name": "Frederic"
            },
            {
                "last_name": "Franke",
                "first_name": "J\u00f6rg K. H."
            },
            {
                "last_name": "Fertmann",
                "first_name": "Daniel"
            },
            {
                "last_name": "Hutter",
                "first_name": "Frank"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Accurate RNA secondary structure prediction is vital for understanding\ncellular regulation and disease mechanisms. Deep learning (DL) methods have\nsurpassed traditional algorithms by predicting complex features like\npseudoknots and multi-interacting base pairs. However, traditional distance\nmeasures can hardly deal with such tertiary interactions and the currently used\nevaluation measures (F1 score, MCC) have limitations. We propose the\nWeisfeiler-Lehman graph kernel (WL) as an alternative metric. Embracing\ngraph-based metrics like WL enables fair and accurate evaluation of RNA\nstructure prediction algorithms. Further, WL provides informative guidance, as\ndemonstrated in an RNA design experiment.\n",
        "title": "Rethinking Performance Measures of RNA Secondary Structure Problems",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05352",
        "abstract_url": "http://arxiv.org/abs/2401.05352",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyun"
            },
            {
                "last_name": "Meinel",
                "first_name": "Christoph"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Generalized Class Discovery (GCD) plays a pivotal role in discerning both\nknown and unknown categories from unlabeled datasets by harnessing the insights\nderived from a labeled set comprising recognized classes. A significant\nlimitation in prevailing GCD methods is their presumption of an equitably\ndistributed category occurrence in unlabeled data. Contrary to this assumption,\nvisual classes in natural environments typically exhibit a long-tailed\ndistribution, with known or prevalent categories surfacing more frequently than\ntheir rarer counterparts. Our research endeavors to bridge this disconnect by\nfocusing on the long-tailed Generalized Category Discovery (Long-tailed GCD)\nparadigm, which echoes the innate imbalances of real-world unlabeled datasets.\nIn response to the unique challenges posed by Long-tailed GCD, we present a\nrobust methodology anchored in two strategic regularizations: (i) a reweighting\nmechanism that bolsters the prominence of less-represented, tail-end\ncategories, and (ii) a class prior constraint that aligns with the anticipated\nclass distribution. Comprehensive experiments reveal that our proposed method\nsurpasses previous state-of-the-art GCD methods by achieving an improvement of\napproximately 6 - 9% on ImageNet100 and competitive performance on CIFAR100.\n",
        "title": "Generalized Categories Discovery for Long-tailed Recognition",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05353",
        "abstract_url": "http://arxiv.org/abs/2401.05353",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyun"
            },
            {
                "last_name": "Dai",
                "first_name": "Ben"
            },
            {
                "last_name": "Simsek",
                "first_name": "Furkan"
            },
            {
                "last_name": "Meinel",
                "first_name": "Christoph"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Generalized class discovery (GCD) aims to infer known and unknown categories\nin an unlabeled dataset leveraging prior knowledge of a labeled set comprising\nknown classes. Existing research implicitly/explicitly assumes that the\nfrequency of occurrence for each category, whether known or unknown, is\napproximately the same in the unlabeled data. However, in nature, we are more\nlikely to encounter known/common classes than unknown/uncommon ones, according\nto the long-tailed property of visual classes. Therefore, we present a\nchallenging and practical problem, Imbalanced Generalized Category Discovery\n(ImbaGCD), where the distribution of unlabeled data is imbalanced, with known\nclasses being more frequent than unknown ones. To address these issues, we\npropose ImbaGCD, A novel optimal transport-based expectation maximization\nframework that accomplishes generalized category discovery by aligning the\nmarginal class prior distribution. ImbaGCD also incorporates a systematic\nmechanism for estimating the imbalanced class prior distribution under the GCD\nsetup. Our comprehensive experiments reveal that ImbaGCD surpasses previous\nstate-of-the-art GCD methods by achieving an improvement of approximately 2 -\n4% on CIFAR-100 and 15 - 19% on ImageNet-100, indicating its superior\neffectiveness in solving the Imbalanced GCD problem.\n",
        "title": "ImbaGCD: Imbalanced Generalized Category Discovery",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05354",
        "abstract_url": "http://arxiv.org/abs/2401.05354",
        "authors": [
            {
                "last_name": "Namura",
                "first_name": "Norihisa"
            },
            {
                "last_name": "Nakao",
                "first_name": "Hiroya"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  We propose a method for optimizing mutual coupling functions to achieve fast\nand global synchronization between a pair of weakly coupled limit-cycle\noscillators. Our method is based on phase reduction that provides a concise\nlow-dimensional representation of the synchronization dynamics of mutually\ncoupled oscillators, including the case where the coupling depends on past time\nseries of the oscillators. We first describe a method for a pair of identical\noscillators and then generalize it to the case of slightly nonidentical\noscillators. The coupling function is designed in two optimization steps for\nthe functional form and amplitude, where the amplitude is numerically optimized\nto minimize the average convergence time under a constraint on the total power.\nWe perform numerical simulations of the synchronization dynamics with the\noptimized coupling functions using the FitzHugh-Nagumo and R\\\"{o}ssler\noscillators as examples. We show that the coupling function optimized by the\nproposed method can achieve global synchronization more efficiently than the\nprevious methods.\n",
        "title": "Optimal coupling functions for fast and global synchronization of weakly\n  coupled limit-cycle oscillators",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05355",
        "abstract_url": "http://arxiv.org/abs/2401.05355",
        "authors": [
            {
                "last_name": "Mih",
                "first_name": "Atah Nuh"
            },
            {
                "last_name": "Cao",
                "first_name": "Hung"
            },
            {
                "last_name": "Kawnine",
                "first_name": "Asfia"
            },
            {
                "last_name": "Wachowicz",
                "first_name": "Monica"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Resource constraints have restricted several EdgeAI applications to machine\nlearning inference approaches, where models are trained on the cloud and\ndeployed to the edge device. This poses challenges such as bandwidth, latency,\nand privacy associated with storing data off-site for model building. Training\non the edge device can overcome these challenges by eliminating the need to\ntransfer data to another device for storage and model development. On-device\ntraining also provides robustness to data variations as models can be retrained\non newly acquired data to improve performance. We, therefore, propose a\nlightweight EdgeAI architecture modified from Xception, for on-device training\nin a resource-constraint edge environment. We evaluate our model on a PCB\ndefect detection task and compare its performance against existing lightweight\nmodels - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our\nexperiment show that our model has a remarkable performance with a test\naccuracy of 73.45% without pre-training. This is comparable to the test\naccuracy of non-pre-trained MobileViT-XXS (75.40%) and much better than other\nnon-pre-trained models (MobileNetV2 - 50.05%, EfficientNetV2B0 - 54.30%). The\ntest accuracy of our model without pre-training is comparable to pre-trained\nMobileNetV2 model - 75.45% and better than pre-trained EfficientNetV2B0 model -\n58.10%. In terms of memory efficiency, our model performs better than\nEfficientNetV2B0 and MobileViT-XXS. We find that the resource efficiency of\nmachine learning models does not solely depend on the number of parameters but\nalso depends on architectural considerations. Our method can be applied to\nother resource-constraint applications while maintaining significant\nperformance.\n",
        "title": "Developing a Resource-Constraint EdgeAI model for Surface Defect\n  Detection",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05357",
        "abstract_url": "http://arxiv.org/abs/2401.05357",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Zheyu"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaobo Sharon"
            },
            {
                "last_name": "Shi",
                "first_name": "Yiyu"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  Architectures that incorporate Computing-in-Memory (CiM) using emerging\nnon-volatile memory (NVM) devices have become strong contenders for deep neural\nnetwork (DNN) acceleration due to their impressive energy efficiency. Yet, a\nsignificant challenge arises when using these emerging devices: they can show\nsubstantial variations during the weight-mapping process. This can severely\nimpact DNN accuracy if not mitigated. A widely accepted remedy for imperfect\nweight mapping is the iterative write-verify approach, which involves verifying\nconductance values and adjusting devices if needed. In all existing\npublications, this procedure is applied to every individual device, resulting\nin a significant programming time overhead. In our research, we illustrate that\nonly a small fraction of weights need this write-verify treatment for the\ncorresponding devices and the DNN accuracy can be preserved, yielding a notable\nprogramming acceleration. Building on this, we introduce USWIM, a novel method\nbased on the second derivative. It leverages a single iteration of forward and\nbackpropagation to pinpoint the weights demanding write-verify. Through\nextensive tests on diverse DNN designs and datasets, USWIM manifests up to a\n10x programming acceleration against the traditional exhaustive write-verify\nmethod, all while maintaining a similar accuracy level. Furthermore, compared\nto our earlier SWIM technique, USWIM excels, showing a 7x speedup when dealing\nwith devices exhibiting non-uniform variations.\n",
        "title": "U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural\n  Accelerators",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05358",
        "abstract_url": "http://arxiv.org/abs/2401.05358",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhong",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Zheng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The development of U.S. Army and NATO data link systems is introduced first,\nand then the development trend of future intelligent data link is summarized\ninto integration, generalization, multifunctionality and high security. A\nunit-level combat system architecture based on the global combat cloud, which\nis capable of realizing the flexible scheduling of global combat resources and\nmaximizing the overall combat effectiveness, is proposed. Intelligent data link\nis an important part of this solution, providing strong information support for\nfuture urban unit-level warfare.\n",
        "title": "Future Intelligent Data link and Unit-Level Combat System Based on\n  Global Combat Cloud",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05362",
        "abstract_url": "http://arxiv.org/abs/2401.05362",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Ziqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Liyuan"
            },
            {
                "last_name": "Ding",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xingxing"
            },
            {
                "last_name": "Zhong",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Ai",
                "first_name": "Jianyong"
            },
            {
                "last_name": "Li",
                "first_name": "Jianmin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In real-world applications, an object detector often encounters object\ninstances from new classes and needs to accommodate them effectively. Previous\nwork formulated this critical problem as incremental object detection (IOD),\nwhich assumes the object instances of new classes to be fully annotated in\nincremental data. However, as supervisory signals are usually rare and\nexpensive, the supervised IOD may not be practical for implementation. In this\nwork, we consider a more realistic setting named semi-supervised IOD (SSIOD),\nwhere the object detector needs to learn new classes incrementally from a few\nlabelled data and massive unlabelled data without catastrophic forgetting of\nold classes. A commonly-used strategy for supervised IOD is to encourage the\ncurrent model (as a student) to mimic the behavior of the old model (as a\nteacher), but it generally fails in SSIOD because a dominant number of object\ninstances from old and new classes are coexisting and unlabelled, with the\nteacher only recognizing a fraction of them. Observing that learning only the\nclasses of interest tends to preclude detection of other classes, we propose to\nbridge the coexistence of unlabelled classes by constructing two teacher models\nrespectively for old and new classes, and using the concatenation of their\npredictions to instruct the student. This approach is referred to as\nDualTeacher, which can serve as a strong baseline for SSIOD with limited\nresource overhead and no extra hyperparameters. We build various benchmarks for\nSSIOD and perform extensive experiments to demonstrate the superiority of our\napproach (e.g., the performance lead is up to 18.28 AP on MS-COCO). Our code is\navailable at \\url{https://github.com/chuxiuhong/DualTeacher}.\n",
        "title": "DualTeacher: Bridging Coexistence of Unlabelled Classes for\n  Semi-supervised Incremental Object Detection",
        "date": "2023-12-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05363",
        "abstract_url": "http://arxiv.org/abs/2401.05363",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiquan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Sha"
            },
            {
                "last_name": "Jiang",
                "first_name": "Haiteng"
            },
            {
                "last_name": "Li",
                "first_name": "Shijian"
            },
            {
                "last_name": "Li",
                "first_name": "Tao"
            },
            {
                "last_name": "Pan",
                "first_name": "Gang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Automatic sleep staging is essential for sleep assessment and disorder\ndiagnosis. Most existing methods depend on one specific dataset and are limited\nto be generalized to other unseen datasets, for which the training data and\ntesting data are from the same dataset. In this paper, we introduce domain\ngeneralization into automatic sleep staging and propose the task of\ngeneralizable sleep staging which aims to improve the model generalization\nability to unseen datasets. Inspired by existing domain generalization methods,\nwe adopt the feature alignment idea and propose a framework called SleepDG to\nsolve it. Considering both of local salient features and sequential features\nare important for sleep staging, we propose a Multi-level Feature Alignment\ncombining epoch-level and sequence-level feature alignment to learn\ndomain-invariant feature representations. Specifically, we design an\nEpoch-level Feature Alignment to align the feature distribution of each single\nsleep epoch among different domains, and a Sequence-level Feature Alignment to\nminimize the discrepancy of sequential features among different domains.\nSleepDG is validated on five public datasets, achieving the state-of-the-art\nperformance.\n",
        "title": "Generalizable Sleep Staging via Multi-level Domain Alignment",
        "date": "2023-12-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05365",
        "abstract_url": "http://arxiv.org/abs/2401.05365",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Rapetti",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Grieco",
                "first_name": "Riccardo"
            },
            {
                "last_name": "Draicchio",
                "first_name": "Francesco"
            },
            {
                "last_name": "Pucci",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  This paper proposes a framework that combines online human state estimation,\naction recognition and motion prediction to enable early assessment and\nprevention of worker biomechanical risk during lifting tasks. The framework\nleverages the NIOSH index to perform online risk assessment, thus fitting\nreal-time applications. In particular, the human state is retrieved via inverse\nkinematics/dynamics algorithms from wearable sensor data. Human action\nrecognition and motion prediction are achieved by implementing an LSTM-based\nGuided Mixture of Experts architecture, which is trained offline and inferred\nonline. With the recognized actions, a single lifting activity is divided into\na series of continuous movements and the Revised NIOSH Lifting Equation can be\napplied for risk assessment. Moreover, the predicted motions enable\nanticipation of future risks. A haptic actuator, embedded in the wearable\nsystem, can alert the subject of potential risk, acting as an active prevention\ndevice. The performance of the proposed framework is validated by executing\nreal lifting tasks, while the subject is equipped with the iFeel wearable\nsystem.\n",
        "title": "Online Action Recognition for Human Risk Prediction with Anticipated\n  Haptic Alert via Wearables",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05367",
        "abstract_url": "http://arxiv.org/abs/2401.05367",
        "authors": [
            {
                "last_name": "Aqajari",
                "first_name": "Seyed Amir Hossein"
            },
            {
                "last_name": "Labbaf",
                "first_name": "Sina"
            },
            {
                "last_name": "Tran",
                "first_name": "Phuc Hoang"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Brenda"
            },
            {
                "last_name": "Mehrabadi",
                "first_name": "Milad Asgari"
            },
            {
                "last_name": "Levorato",
                "first_name": "Marco"
            },
            {
                "last_name": "Dutt",
                "first_name": "Nikil"
            },
            {
                "last_name": "Rahmani",
                "first_name": "Amir M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Daily monitoring of stress is a critical component of maintaining optimal\nphysical and mental health. Physiological signals and contextual information\nhave recently emerged as promising indicators for detecting instances of\nheightened stress. Nonetheless, developing a real-time monitoring system that\nutilizes both physiological and contextual data to anticipate stress levels in\neveryday settings while also gathering stress labels from participants\nrepresents a significant challenge. We present a monitoring system that\nobjectively tracks daily stress levels by utilizing both physiological and\ncontextual data in a daily-life environment. Additionally, we have integrated a\nsmart labeling approach to optimize the ecological momentary assessment (EMA)\ncollection, which is required for building machine learning models for stress\ndetection. We propose a three-tier Internet-of-Things-based system architecture\nto address the challenges. We utilized a cross-validation technique to\naccurately estimate the performance of our stress models. We achieved the\nF1-score of 70\\% with a Random Forest classifier using both PPG and contextual\ndata, which is considered an acceptable score in models built for everyday\nsettings. Whereas using PPG data alone, the highest F1-score achieved is\napproximately 56\\%, emphasizing the significance of incorporating both PPG and\ncontextual data in stress detection tasks.\n",
        "title": "Context-Aware Stress Monitoring using Wearable and Mobile Technologies\n  in Everyday Settings",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05369",
        "abstract_url": "http://arxiv.org/abs/2401.05369",
        "authors": [
            {
                "last_name": "Gandhi",
                "first_name": "Govind"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG",
            ""
        ],
        "abstract": "  Growing interest in modelling complex systems from brains to societies to\ncities using networks has led to increased efforts to describe generative\nprocesses that explain those networks. Recent successes in machine learning\nhave prompted the usage of evolutionary computation, especially genetic\nprogramming to evolve computer programs that effectively forage a\nmultidimensional search space to iteratively find better solutions that explain\nnetwork structure. Symbolic regression contributes to these approaches by\nreplicating network morphologies using both structure and processes, all while\nnot relying on the scientists intuition or expertise. It distinguishes itself\nby introducing a novel formulation of a network generator and a parameter-free\nfitness function to evaluate the generated network and is found to consistently\nretrieve synthetically generated growth processes as well as simple,\ninterpretable rules for a range of empirical networks. We extend this approach\nby modifying generator semantics to create and retrieve rules for time-varying\nnetworks. Lexicon to study networks created dynamically in multiple stages is\nintroduced. The framework was improved using methods from the genetic\nprogramming toolkit (recombination) and computational improvements (using\nheuristic distance measures) and used to test the consistency and robustness of\nthe upgrades to the semantics using synthetically generated networks. Using\nrecombination was found to improve retrieval rate and fitness of the solutions.\nThe framework was then used on three empirical datasets - subway networks of\nmajor cities, regions of street networks and semantic co-occurrence networks of\nliterature in Artificial Intelligence to illustrate the possibility of\nobtaining interpretable, decentralised growth processes from complex networks.\n",
        "title": "Symbolic Regression of Dynamic Network Models",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05370",
        "abstract_url": "http://arxiv.org/abs/2401.05370",
        "authors": [
            {
                "last_name": "Ghorbani",
                "first_name": "Mahdi"
            },
            {
                "last_name": "Gendelev",
                "first_name": "Leo"
            },
            {
                "last_name": "Beroza",
                "first_name": "Paul"
            },
            {
                "last_name": "Keiser",
                "first_name": "Michael J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  In this work, we introduce AutoFragDiff, a fragment-based autoregressive\ndiffusion model for generating 3D molecular structures conditioned on target\nprotein structures. We employ geometric vector perceptrons to predict atom\ntypes and spatial coordinates of new molecular fragments conditioned on\nmolecular scaffolds and protein pockets. Our approach improves the local\ngeometry of the resulting 3D molecules while maintaining high predicted binding\naffinity to protein targets. The model can also perform scaffold extension from\nuser-provided starting molecular scaffold.\n",
        "title": "Autoregressive fragment-based diffusion for pocket-aware ligand design",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05373",
        "abstract_url": "http://arxiv.org/abs/2401.05373",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Nan"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengzhu"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhenghan"
            },
            {
                "last_name": "De Masi",
                "first_name": "Giulia"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            },
            {
                "last_name": "Xiong",
                "first_name": "Huan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "LG"
        ],
        "abstract": "  The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks\n(GNNs) is gradually attracting attention due to the low power consumption and\nhigh efficiency in processing the non-Euclidean data represented by graphs.\nHowever, as a common problem, dynamic graph representation learning faces\nchallenges such as high complexity and large memory overheads. Current work\noften uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary\nfeatures instead of continuous ones for efficient training, which would\noverlooks graph structure information and leads to the loss of details during\npropagation. Additionally, optimizing dynamic spiking models typically requires\npropagation of information across time steps, which increases memory\nrequirements. To address these challenges, we present a framework named\n\\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph\n\\underline{N}eural Networks (\\method{}). To mitigate the information loss\nproblem, \\method{} propagates early-layer information directly to the last\nlayer for information compensation. To accommodate the memory requirements, we\napply the implicit differentiation on the equilibrium state, which does not\nrely on the exact reverse of the forward computation. While traditional\nimplicit differentiation methods are usually used for static situations,\n\\method{} extends it to the dynamic graph setting. Extensive experiments on\nthree large-scale real-world dynamic graph datasets validate the effectiveness\nof \\method{} on dynamic node classification tasks with lower computational\ncosts.\n",
        "title": "Dynamic Spiking Graph Neural Networks",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05375",
        "abstract_url": "http://arxiv.org/abs/2401.05375",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Taining"
            },
            {
                "last_name": "Goldstein",
                "first_name": "Adam"
            },
            {
                "last_name": "Levin",
                "first_name": "Michael"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "DS",
            "MA"
        ],
        "abstract": "  The emerging field of Diverse Intelligence seeks to identify, formalize, and\nunderstand commonalities in behavioral competencies across a wide range of\nimplementations. Especially interesting are simple systems that provide\nunexpected examples of memory, decision-making, or problem-solving in\nsubstrates that at first glance do not appear to be complex enough to implement\nsuch capabilities. We seek to develop tools to help understand the minimal\nrequirements for such capabilities, and to learn to recognize and predict basal\nforms of intelligence in unconventional substrates. Here, we apply novel\nanalyses to the behavior of classical sorting algorithms, short pieces of code\nwhich have been studied for many decades. To study these sorting algorithms as\na model of biological morphogenesis and its competencies, we break two\nformerly-ubiquitous assumptions: top-down control (instead, showing how each\nelement within a array of numbers can exert minimal agency and implement\nsorting policies from the bottom up), and fully reliable hardware (instead,\nallowing some of the elements to be \"damaged\" and fail to execute the\nalgorithm). We quantitatively characterize sorting activity as the traversal of\na problem space, showing that arrays of autonomous elements sort themselves\nmore reliably and robustly than traditional implementations in the presence of\nerrors. Moreover, we find the ability to temporarily reduce progress in order\nto navigate around a defect, and unexpected clustering behavior among the\nelements in chimeric arrays whose elements follow one of two different\nalgorithms. The discovery of emergent problem-solving capacities in simple,\nfamiliar algorithms contributes a new perspective to the field of Diverse\nIntelligence, showing how basal forms of intelligence can emerge in simple\nsystems without being explicitly encoded in their underlying mechanics.\n",
        "title": "Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting\n  arrays reveal unexpected competencies in a minimal model of basal\n  intelligence",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05376",
        "abstract_url": "http://arxiv.org/abs/2401.05376",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chunzhuo"
            },
            {
                "last_name": "Kumar",
                "first_name": "T. Sunil"
            },
            {
                "last_name": "De Raedt",
                "first_name": "Walter"
            },
            {
                "last_name": "Camps",
                "first_name": "Guido"
            },
            {
                "last_name": "Hallez",
                "first_name": "Hans"
            },
            {
                "last_name": "Vanrumste",
                "first_name": "Bart"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC"
        ],
        "abstract": "  Eating speed is an important indicator that has been widely scrutinized in\nnutritional studies. The relationship between eating speed and several\nintake-related problems such as obesity, diabetes, and oral health has received\nincreased attention from researchers. However, existing studies mainly use\nself-reported questionnaires to obtain participants' eating speed, where they\nchoose options from slow, medium, and fast. Such a non-quantitative method is\nhighly subjective and coarse in individual level. In this study, we propose a\nnovel approach to measure eating speed in free-living environments\nautomatically and objectively using wrist-worn inertial measurement unit (IMU)\nsensors. Specifically, a temporal convolutional network combined with a\nmulti-head attention module (TCN-MHA) is developed to detect bites (including\neating and drinking gestures) from free-living IMU data. The predicted bite\nsequences are then clustered to eating episodes. Eating speed is calculated by\nusing the time taken to finish the eating episode to divide the number of\nbites. To validate the proposed approach on eating speed measurement, a 7-fold\ncross validation is applied to the self-collected fine-annotated full-day-I\n(FD-I) dataset, and a hold-out experiment is conducted on the full-day-II\n(FD-II) dataset. The two datasets are collected from 61 participants in\nfree-living environments with a total duration of 513 h, which are publicly\navailable. Experimental results shows that the proposed approach achieves a\nmean absolute percentage error (MAPE) of 0.110 and 0.146 in the FD-I and FD-II\ndatasets, respectively, showcasing the feasibility of automated eating speed\nmeasurement. To the best of our knowledge, this is the first study\ninvestigating automated eating speed measurement.\n",
        "title": "Eating Speed Measurement Using Wrist-Worn IMU Sensors in Free-Living\n  Environments",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05377",
        "abstract_url": "http://arxiv.org/abs/2401.05377",
        "authors": [
            {
                "last_name": "Capraro",
                "first_name": "Valerio"
            },
            {
                "last_name": "Lentsch",
                "first_name": "Austin"
            },
            {
                "last_name": "Acemoglu",
                "first_name": "Daron"
            },
            {
                "last_name": "Akgun",
                "first_name": "Selin"
            },
            {
                "last_name": "Akhmedova",
                "first_name": "Aisel"
            },
            {
                "last_name": "Bilancini",
                "first_name": "Ennio"
            },
            {
                "last_name": "Bonnefon",
                "first_name": "Jean-Fran\u00e7ois"
            },
            {
                "last_name": "Bra\u00f1as-Garza",
                "first_name": "Pablo"
            },
            {
                "last_name": "Butera",
                "first_name": "Luigi"
            },
            {
                "last_name": "Douglas",
                "first_name": "Karen M."
            },
            {
                "last_name": "Everett",
                "first_name": "Jim A. C."
            },
            {
                "last_name": "Gigerenzer",
                "first_name": "Gerd"
            },
            {
                "last_name": "Greenhow",
                "first_name": "Christine"
            },
            {
                "last_name": "Hashimoto",
                "first_name": "Daniel A."
            },
            {
                "last_name": "Holt-Lunstad",
                "first_name": "Julianne"
            },
            {
                "last_name": "Jetten",
                "first_name": "Jolanda"
            },
            {
                "last_name": "Johnson",
                "first_name": "Simon"
            },
            {
                "last_name": "Longoni",
                "first_name": "Chiara"
            },
            {
                "last_name": "Lunn",
                "first_name": "Pete"
            },
            {
                "last_name": "Natale",
                "first_name": "Simone"
            },
            {
                "last_name": "Rahwan",
                "first_name": "Iyad"
            },
            {
                "last_name": "Selwyn",
                "first_name": "Neil"
            },
            {
                "last_name": "Singh",
                "first_name": "Vivek"
            },
            {
                "last_name": "Suri",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Sutcliffe",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Tomlinson",
                "first_name": "Joe"
            },
            {
                "last_name": "van der Linden",
                "first_name": "Sander"
            },
            {
                "last_name": "Van Lange",
                "first_name": "Paul A. M."
            },
            {
                "last_name": "Wall",
                "first_name": "Friederike"
            },
            {
                "last_name": "Van Bavel",
                "first_name": "Jay J."
            },
            {
                "last_name": "Viale",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Generative artificial intelligence, including chatbots like ChatGPT, has the\npotential to both exacerbate and ameliorate existing socioeconomic\ninequalities. In this article, we provide a state-of-the-art interdisciplinary\noverview of the probable impacts of generative AI on four critical domains:\nwork, education, health, and information. Our goal is to warn about how\ngenerative AI could worsen existing inequalities while illuminating directions\nfor using AI to resolve pervasive social problems. Generative AI in the\nworkplace can boost productivity and create new jobs, but the benefits will\nlikely be distributed unevenly. In education, it offers personalized learning\nbut may widen the digital divide. In healthcare, it improves diagnostics and\naccessibility but could deepen pre-existing inequalities. For information, it\ndemocratizes content creation and access but also dramatically expands the\nproduction and proliferation of misinformation. Each section covers a specific\ntopic, evaluates existing research, identifies critical gaps, and recommends\nresearch directions. We conclude with a section highlighting the role of\npolicymaking to maximize generative AI's potential to reduce inequalities while\nmitigating its harmful effects. We discuss strengths and weaknesses of existing\npolicy frameworks in the European Union, the United States, and the United\nKingdom, observing that each fails to fully confront the socioeconomic\nchallenges we have identified. We contend that these policies should promote\nshared prosperity through the advancement of generative AI. We suggest several\nconcrete policies to encourage further research and debate. This article\nemphasizes the need for interdisciplinary collaborations to understand and\naddress the complex challenges of generative AI.\n",
        "title": "The impact of generative artificial intelligence on socioeconomic\n  inequalities and policy making",
        "date": "2023-12-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05378",
        "abstract_url": "http://arxiv.org/abs/2401.05378",
        "authors": [
            {
                "last_name": "Alam",
                "first_name": "Ridwan"
            },
            {
                "last_name": "Aguirre",
                "first_name": "Aaron"
            },
            {
                "last_name": "Stultz",
                "first_name": "Collin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  For a number of antiarrhythmics, drug loading requires a 3 day\nhospitalization with monitoring for QT prolongation. Automated QT monitoring\nwith wearable ECG monitors would facilitate out-of-hospital care. We develop a\ndeep learning model that infers QT intervals from ECG lead-I - the lead most\noften acquired from ambulatory ECG monitors - and to use this model to detect\nclinically meaningful QT-prolongation episodes during Dofetilide drug loading.\nUsing 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the\nMassachusetts General Hospital, we develop a deep learning model, QTNet, that\ninfers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients\nare used to train the model and an internal-test set containing 633 thousand\nECGs from 135 thousand patients was used for testing. QTNet is further\nevaluated on an external-validation set containing 3.1 million ECGs from 667\nthousand patients at another institution. QTNet was used to detect\nDofetilide-induced QT prolongation in a publicly available database\n(ECGRDVQ-dataset) containing ECGs from subjects enrolled in a clinical trial\nevaluating the effects of antiarrhythmic drugs. QTNet achieves mean absolute\nerrors of 12.63ms (internal-test) and 12.30ms (external-validation) for\nestimating absolute QT intervals. The associated Pearson correlation\ncoefficients are 0.91 (internal-test) and 0.92 (external-validation). For the\nECGRDVQ-dataset, QTNet detects Dofetilide-induced QTc prolongation with 87%\nsensitivity and 77% specificity. The negative predictive value of the model is\ngreater than 95% when the pre-test probability of drug-induced QTc prolongation\nis below 25%. Drug-induced QT prolongation risk can be tracked from ECG lead-I\nusing deep learning.\n",
        "title": "Detecting QT prolongation From a Single-lead ECG With Deep Learning",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05379",
        "abstract_url": "http://arxiv.org/abs/2401.05379",
        "authors": [
            {
                "last_name": "Hashemi",
                "first_name": "Amirreza"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This study presents a comprehensive evaluation of tools available on the\nHuggingFace platform for two pivotal applications in artificial intelligence:\nimage segmentation and voice conversion. The primary objective was to identify\nthe top three tools within each category and subsequently install and configure\nthese tools on Linux systems. We leveraged the power of pre-trained\nsegmentation models such as SAM and DETR Model with ResNet-50 backbone for\nimage segmentation, and the so-vits-svc-fork model for voice conversion. This\npaper delves into the methodologies and challenges encountered during the\nimplementation process, and showcases the successful combination of video\nsegmentation and voice conversion in a unified project named AutoVisual Fusion\nSuite.\n",
        "title": "AutoVisual Fusion Suite: A Comprehensive Evaluation of Image\n  Segmentation and Voice Conversion Tools on HuggingFace Platform",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05380",
        "abstract_url": "http://arxiv.org/abs/2401.05380",
        "authors": [
            {
                "last_name": "Dyoub",
                "first_name": "Abeer"
            },
            {
                "last_name": "Letteri",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  In this study, we investigated the application of bio-inspired optimization\nalgorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale\nOptimization Algorithm, for feature selection in chronic disease prediction.\nThe primary goal was to enhance the predictive accuracy of models streamline\ndata dimensionality, and make predictions more interpretable and actionable.\n  The research encompassed a comparative analysis of the three bio-inspired\nfeature selection approaches across diverse chronic diseases, including\ndiabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such\nas accuracy, precision, recall, and f1 score are used to assess the\neffectiveness of the algorithms in reducing the number of features needed for\naccurate classification.\n  The results in general demonstrate that the bio-inspired optimization\nalgorithms are effective in reducing the number of features required for\naccurate classification. However, there have been variations in the performance\nof the algorithms on different datasets.\n  The study highlights the importance of data pre-processing and cleaning in\nensuring the reliability and effectiveness of the analysis.\n  This study contributes to the advancement of predictive analytics in the\nrealm of chronic diseases. The potential impact of this work extends to early\nintervention, precision medicine, and improved patient outcomes, providing new\navenues for the delivery of healthcare services tailored to individual needs.\nThe findings underscore the potential benefits of using bio-inspired\noptimization algorithms for feature selection in chronic disease prediction,\noffering valuable insights for improving healthcare outcomes.\n",
        "title": "Dataset Optimization for Chronic Disease Prediction with Bio-Inspired\n  Feature Selection",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05381",
        "abstract_url": "http://arxiv.org/abs/2401.05381",
        "authors": [
            {
                "last_name": "Petralia",
                "first_name": "Adrien"
            },
            {
                "last_name": "Charpentier",
                "first_name": "Philippe"
            },
            {
                "last_name": "Palpanas",
                "first_name": "Themis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Over the past decade, millions of smart meters have been installed by\nelectricity suppliers worldwide, allowing them to collect a large amount of\nelectricity consumption data, albeit sampled at a low frequency (one point\nevery 30min). One of the important challenges these suppliers face is how to\nutilize these data to detect the presence/absence of different appliances in\nthe customers' households. This valuable information can help them provide\npersonalized offers and recommendations to help customers towards the energy\ntransition. Appliance detection can be cast as a time series classification\nproblem. However, the large amount of data combined with the long and variable\nlength of the consumption series pose challenges when training a classifier. In\nthis paper, we propose ADF, a framework that uses subsequences of a client\nconsumption series to detect the presence/absence of appliances. We also\nintroduce TransApp, a Transformer-based time series classifier that is first\npretrained in a self-supervised way to enhance its performance on appliance\ndetection tasks. We test our approach on two real datasets, including a\npublicly available one. The experimental results with two large real datasets\nshow that the proposed approach outperforms current solutions, including\nstate-of-the-art time series classifiers applied to appliance detection. This\npaper appeared in VLDB 2024.\n",
        "title": "ADF & TransApp: A Transformer-Based Framework for Appliance Detection\n  Using Smart Meter Consumption Series",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05382",
        "abstract_url": "http://arxiv.org/abs/2401.05382",
        "authors": [
            {
                "last_name": "Ghasemi",
                "first_name": "Zahra"
            },
            {
                "last_name": "Neumann",
                "first_name": "Frank"
            },
            {
                "last_name": "Zanin",
                "first_name": "Max"
            },
            {
                "last_name": "Karageorgos",
                "first_name": "John"
            },
            {
                "last_name": "Chen",
                "first_name": "Lei"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding\ncircuit of mineral processing plants. Accurate prediction of SAG mill\nthroughput as a crucial performance metric is of utmost importance. While\nempirical models have been developed in previous studies for SAG mill\nthroughput prediction, the potential of applying machine learning (ML)\ntechniques for this purpose remains underexplored. Unlike empirical modelling,\nwhich relies on expensive and time-consuming experimental data, ML techniques\ncan utilize data collected during regular operations. Genetic programming (GP)\nis one of ML techniques that offers the advantage of providing a transparent\nequation for precise mill throughput prediction. This study explores the\napplication of GP to predict SAG mill throughput and introduces five new GP\nvariants to enhance prediction performance. These variants extract multiple\nequations, each accurately predicting mill throughput for specific clusters of\ntraining data. These equations are then employed to predict mill throughput for\ntest data using various approaches. To assess the effect of distance measures\non the new GP variants, four different distance measures are employed.\nComparative analysis reveals that the new GP variants achieve an average\nimprovement of 12.49% in prediction accuracy. Further investigation of distance\nmeasures indicates that the Euclidean distance measure yields the most accurate\nresults for the majority of data splits. Additionally, the most precise new GP\nvariant considers all equations and incorporates both the number of data points\nin each data cluster and the distance to clusters when calculating the final\nprediction. The developed GP variants in this study present a precise,\ntransparent, and cost-effective approach for modelling SAG mill throughput in\nmineral processing plants.\n",
        "title": "An improved genetic programming for predicting semi autogenous grinding\n  mill throughput",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05384",
        "abstract_url": "http://arxiv.org/abs/2401.05384",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Nuo"
            },
            {
                "last_name": "Li",
                "first_name": "Hongguang"
            },
            {
                "last_name": "Wang",
                "first_name": "Baoyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper investigates the performance of Large Language Models (LLMs) and\nTool-augmented LLMs in tackling complex mathematical reasoning tasks. We\nintroduce IMP-TIP: Improving Math Reasoning with Tool-augmented Interleaf\nPrompting, a framework that combines the strengths of both LLMs and\nTool-augmented LLMs. IMP-TIP follows the ``From Good to Great\" concept,\ncollecting multiple potential solutions from both LLMs and their Tool-Augmented\ncounterparts for the same math problem, and then selecting or re-generating the\nmost accurate answer after cross-checking these solutions via tool-augmented\ninterleaf prompting. The framework incorporates two key aspects: self-prompt\nand tool-augmented interleaf prompting (TIP). The former allows LLMs to\nautonomously refine and improve an initial prompt related to tool usage, while\nthe latter enables LLMs to derive the final answer by dynamically analyzing the\nproblem, cross-checking potential solutions, and revising previous reasoning\nhints in an interleaved manner. Experimental analysis shows that IMP-TIP\nachieves enhanced mathematical capabilities and outperforms traditional LLMs\nand tool-augmented LLMs in accuracy and reasoning diversity on math reasoning\ntasks. For instance, IMP-TIP can improve Tool-augmented ChatGPT on GSM8K-Hard\nfrom 56.0% to 65.2%.\n",
        "title": "From Good to Great: Improving Math Reasoning with Tool-Augmented\n  Interleaf Prompting",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05385",
        "abstract_url": "http://arxiv.org/abs/2401.05385",
        "authors": [
            {
                "last_name": "Oswald",
                "first_name": "Christian"
            },
            {
                "last_name": "Toth",
                "first_name": "Mate"
            },
            {
                "last_name": "Meissner",
                "first_name": "Paul"
            },
            {
                "last_name": "Pernkopf",
                "first_name": "Franz"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  In automotive applications, frequency modulated continuous wave (FMCW) radar\nis an established technology to determine the distance, velocity and angle of\nobjects in the vicinity of the vehicle. The quality of predictions might be\nseriously impaired if mutual interference between radar sensors occurs.\nPrevious work processes data from the entire receiver array in parallel to\nincrease interference mitigation quality using neural networks (NNs). However,\nthese architectures do not generalize well across different angles of arrival\n(AoAs) of interferences and objects. In this paper we introduce fully\nconvolutional neural network (CNN) with rank-three convolutions which is able\nto transfer learned patterns between different AoAs. Our proposed architecture\noutperforms previous work while having higher robustness and a lower number of\ntrainable parameters. We evaluate our network on a diverse data set and\ndemonstrate its angle equivariance.\n",
        "title": "Angle-Equivariant Convolutional Neural Networks for Interference\n  Mitigation in Automotive Radar",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05386",
        "abstract_url": "http://arxiv.org/abs/2401.05386",
        "authors": [
            {
                "last_name": "Colot",
                "first_name": "Martin"
            },
            {
                "last_name": "Simar",
                "first_name": "C\u00e9dric"
            },
            {
                "last_name": "Petieau",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Ana Maria Cebolla"
            },
            {
                "last_name": "Cheron",
                "first_name": "Guy"
            },
            {
                "last_name": "Bontempi",
                "first_name": "Gianluca"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            "LG"
        ],
        "abstract": "  Electromyograms (EMG)-based hand gesture recognition systems are a promising\ntechnology for human/machine interfaces. However, one of their main limitations\nis the long calibration time that is typically required to handle new users.\nThe paper discusses and analyses the challenge of cross-subject generalization\nthanks to an original dataset containing the EMG signals of 14 human subjects\nduring hand gestures. The experimental results show that, though an accurate\ngeneralization based on pooling multiple subjects is hardly achievable, it is\npossible to improve the cross-subject estimation by identifying a robust\nlow-dimensional subspace for multiple subjects and aligning it to a target\nsubject. A visualization of the subspace enables us to provide insights for the\nimprovement of cross-subject generalization with EMG signals.\n",
        "title": "EMG subspace alignment and visualization for cross-subject hand gesture\n  classification",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05388",
        "abstract_url": "http://arxiv.org/abs/2401.05388",
        "authors": [
            {
                "last_name": "Cardoso",
                "first_name": "Gabriel V."
            },
            {
                "last_name": "Bedin",
                "first_name": "Lisa"
            },
            {
                "last_name": "Duchateau",
                "first_name": "Josselin"
            },
            {
                "last_name": "Dubois",
                "first_name": "R\u00e9mi"
            },
            {
                "last_name": "Moulines",
                "first_name": "Eric"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  In this work, we propose a denoising diffusion generative model (DDGM)\ntrained with healthy electrocardiogram (ECG) data that focuses on ECG\nmorphology and inter-lead dependence. Our results show that this innovative\ngenerative model can successfully generate realistic ECG signals. Furthermore,\nwe explore the application of recent breakthroughs in solving linear inverse\nBayesian problems using DDGM. This approach enables the development of several\nimportant clinical tools. These include the calculation of corrected QT\nintervals (QTc), effective noise suppression of ECG signals, recovery of\nmissing ECG leads, and identification of anomalous readings, enabling\nsignificant advances in cardiac health monitoring and diagnosis.\n",
        "title": "Bayesian ECG reconstruction using denoising diffusion generative models",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05390",
        "abstract_url": "http://arxiv.org/abs/2401.05390",
        "authors": [
            {
                "last_name": "Troncoso-Pastoriza",
                "first_name": "Francisco"
            },
            {
                "last_name": "Egu\u00eda-Oller",
                "first_name": "Pablo"
            },
            {
                "last_name": "D\u00edaz-Redondo",
                "first_name": "Rebeca P."
            },
            {
                "last_name": "Granada-\u00c1lvarez",
                "first_name": "Enrique"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper we introduce a method that supports the detection,\nidentification and localization of lamps in a building, with the main goal of\nautomatically feeding its energy model by means of Building Information\nModeling (BIM) methods. The proposed method, thus, provides useful information\nto apply energy-saving strategies to reduce energy consumption in the building\nsector through the correct management of the lighting infrastructure. Based on\nthe unique geometry and brightness of lamps and the use of only greyscale\nimages, our methodology is able to obtain accurate results despite its low\ncomputational needs, resulting in near-real-time processing. The main novelty\nis that the focus of the candidate search is not over the entire image but\ninstead only on a limited region that summarizes the specific characteristics\nof the lamp. The information obtained from our approach was used on the Green\nBuilding XML Schema to illustrate the automatic generation of BIM data from the\nresults of the algorithm.\n",
        "title": "Generation of BIM data based on the automatic detection, identification\n  and localization of lamps in buildings",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05391",
        "abstract_url": "http://arxiv.org/abs/2401.05391",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hui"
            },
            {
                "last_name": "Gan",
                "first_name": "Yi"
            },
            {
                "last_name": "Yuan",
                "first_name": "Feng"
            },
            {
                "last_name": "Ma",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Yutao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoli"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinghui"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            ""
        ],
        "abstract": "  Transformer based Large Language Models (LLMs) have been widely used in many\nfields, and the efficiency of LLM inference becomes hot topic in real\napplications. However, LLMs are usually complicatedly designed in model\nstructure with massive operations and perform inference in the auto-regressive\nmode, making it a challenging task to design a system with high efficiency.\n  In this paper, we propose an efficient LLM inference solution with low\nlatency and high throughput. Firstly, we simplify the LLM decoder layer by\nfusing data movement and element-wise operations to reduce the memory access\nfrequency and lower system latency. We also propose a segment KV cache policy\nto keep key/value of the request and response tokens in separate physical\nmemory for effective device memory management, helping enlarge the runtime\nbatch size and improve system throughput. A customized\nScaled-Dot-Product-Attention kernel is designed to match our fusion policy\nbased on the segment KV cache solution. We implement our LLM inference solution\non Intel GPU and publish it publicly. Compared with the standard HuggingFace\nimplementation, the proposed solution achieves up to 7x lower token latency and\n27x higher throughput for some popular LLMs on Intel GPU.\n",
        "title": "Efficient LLM inference solution on Intel GPU",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05392",
        "abstract_url": "http://arxiv.org/abs/2401.05392",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Vikas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Noise is inevitably common in digital images, leading to visual image\ndeterioration. Therefore, a suitable filtering method is required to lessen the\nnoise while preserving the image features (edges, corners, etc.). This paper\npresents the efficient type-2 fuzzy weighted mean filter with an adaptive\nthreshold to remove the SAP noise. The present filter has two primary steps:\nThe first stage categorizes images as lightly, medium, and heavily corrupted\nbased on an adaptive threshold by comparing the M-ALD of processed pixels with\nthe upper and lower MF of the type-2 fuzzy identifier. The second stage\neliminates corrupted pixels by computing the appropriate weight using GMF with\nthe mean and variance of the uncorrupted pixels in the filter window.\nSimulation results vividly show that the obtained denoised images preserve\nimage features, i.e., edges, corners, and other sharp structures, compared with\ndifferent filtering methods.\n",
        "title": "AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted\n  with Salt-and-Pepper",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05394",
        "abstract_url": "http://arxiv.org/abs/2401.05394",
        "authors": [
            {
                "last_name": "de Vazelhes",
                "first_name": "William"
            },
            {
                "last_name": "Mukhoty",
                "first_name": "Bhaskar"
            },
            {
                "last_name": "Yuan",
                "first_name": "Xiao-Tong"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  Sparse recovery is ubiquitous in machine learning and signal processing. Due\nto the NP-hard nature of sparse recovery, existing methods are known to suffer\neither from restrictive (or even unknown) applicability conditions, or high\ncomputational cost. Recently, iterative regularization methods have emerged as\na promising fast approach because they can achieve sparse recovery in one pass\nthrough early stopping, rather than the tedious grid-search used in the\ntraditional methods. However, most of those iterative methods are based on the\n$\\ell_1$ norm which requires restrictive applicability conditions and could\nfail in many cases. Therefore, achieving sparse recovery with iterative\nregularization methods under a wider range of conditions has yet to be further\nexplored. To address this issue, we propose a novel iterative regularization\nalgorithm, IRKSN, based on the $k$-support norm regularizer rather than the\n$\\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and\ncompare them with traditional conditions for recovery with $\\ell_1$ norm\nregularizers. Additionally, we give an early stopping bound on the model error\nof IRKSN with explicit constants, achieving the standard linear rate for sparse\nrecovery. Finally, we illustrate the applicability of our algorithm on several\nexperiments, including a support recovery experiment with a correlated design\nmatrix.\n",
        "title": "Iterative Regularization with k-Support Norm: an Important Complement to\n  Sparse Recovery",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05395",
        "abstract_url": "http://arxiv.org/abs/2401.05395",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Ruixin"
            },
            {
                "last_name": "Chen",
                "first_name": "Bowei"
            },
            {
                "last_name": "Wilson",
                "first_name": "James M."
            },
            {
                "last_name": "Yan",
                "first_name": "Zhi"
            },
            {
                "last_name": "Huang",
                "first_name": "Yufei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CY",
            "LG"
        ],
        "abstract": "  The automotive industry plays a critical role in the global economy, and\nparticularly important is the expanding Chinese automobile market due to its\nimmense scale and influence. However, existing automotive sector datasets are\nlimited in their coverage, failing to adequately consider the growing demand\nfor more and diverse variables. This paper aims to bridge this data gap by\nintroducing a comprehensive dataset spanning the years from 2016 to 2022,\nencompassing sales data, online reviews, and a wealth of information related to\nthe Chinese automotive industry. This dataset serves as a valuable resource,\nsignificantly expanding the available data. Its impact extends to various\ndimensions, including improving forecasting accuracy, expanding the scope of\nbusiness applications, informing policy development and regulation, and\nadvancing academic research within the automotive sector. To illustrate the\ndataset's potential applications in both business and academic contexts, we\npresent two application examples. Our developed dataset enhances our\nunderstanding of the Chinese automotive market and offers a valuable tool for\nresearchers, policymakers, and industry stakeholders worldwide.\n",
        "title": "SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive\n  market",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05396",
        "abstract_url": "http://arxiv.org/abs/2401.05396",
        "authors": [
            {
                "last_name": "\u00c1lvarez-Tu\u00f1\u00f3n",
                "first_name": "Olaya"
            },
            {
                "last_name": "Brodskiy",
                "first_name": "Yury"
            },
            {
                "last_name": "Kayacan",
                "first_name": "Erdal"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper overviews different pose representations and metric functions in\nvisual odometry (VO) networks. The performance of VO networks heavily relies on\nhow their architecture encodes the information. The choice of pose\nrepresentation and loss function significantly impacts network convergence and\ngeneralization. We investigate these factors in the VO network DeepVO by\nimplementing loss functions based on Euler, quaternion, and chordal distance\nand analyzing their influence on performance. The results of this study provide\ninsights into how loss functions affect the designing of efficient and accurate\nVO networks for camera motion estimation. The experiments illustrate that a\ndistance that complies with the mathematical requirements of a metric, such as\nthe chordal distance, provides better generalization and faster convergence.\nThe code for the experiments can be found at\nhttps://github.com/remaro-network/Loss_VO_right\n",
        "title": "Loss it right: Euclidean and Riemannian Metrics in Learning-based Visual\n  Odometry",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05397",
        "abstract_url": "http://arxiv.org/abs/2401.05397",
        "authors": [
            {
                "last_name": "Marto",
                "first_name": "Sim\u00e3o da Gra\u00e7a"
            },
            {
                "last_name": "Vasile",
                "first_name": "Massimiliano"
            },
            {
                "last_name": "Campbell",
                "first_name": "Andrew"
            },
            {
                "last_name": "Murray",
                "first_name": "Paul"
            },
            {
                "last_name": "Marshall",
                "first_name": "Stephen"
            },
            {
                "last_name": "Savitski",
                "first_name": "Vasili"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Spectral lightcurves consisting of time series single-pixel spectral\nmeasurements of spacecraft are used to infer the spacecraft's attitude and\nrotation. Two methods are used. One based on numerical optimisation of a\nregularised least squares cost function, and another based on machine learning\nwith a neural network model. The aim is to work with minimal information, thus\nno prior is available on the attitude nor on the inertia tensor. The\ntheoretical and practical aspects of this task are investigated, and the\nmethodology is tested on synthetic data. Results are shown based on synthetic\ndata.\n",
        "title": "Hyperspectral Lightcurve Inversion for Attitude Determination",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05398",
        "abstract_url": "http://arxiv.org/abs/2401.05398",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenwen"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  GeoAI, or geospatial artificial intelligence, is an exciting new area that\nleverages artificial intelligence (AI), geospatial big data, and massive\ncomputing power to solve problems with high automation and intelligence. This\npaper reviews the progress of AI in social science research, highlighting\nimportant advancements in using GeoAI to fill critical data and knowledge gaps.\nIt also discusses the importance of breaking down data silos, accelerating\nconvergence among GeoAI research methods, as well as moving GeoAI beyond\ngeospatial benefits.\n",
        "title": "GeoAI in Social Science",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05399",
        "abstract_url": "http://arxiv.org/abs/2401.05399",
        "authors": [
            {
                "last_name": "Oli",
                "first_name": "Priti"
            },
            {
                "last_name": "Banjade",
                "first_name": "Rabin"
            },
            {
                "last_name": "Chapagain",
                "first_name": "Jeevan"
            },
            {
                "last_name": "Rus",
                "first_name": "Vasile"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "",
            "CL"
        ],
        "abstract": "  Assessing student's answers and in particular natural language answers is a\ncrucial challenge in the field of education. Advances in machine learning,\nincluding transformer-based models such as Large Language Models(LLMs), have\nled to significant progress in various natural language tasks. Nevertheless,\namidst the growing trend of evaluating LLMs across diverse tasks, evaluating\nLLMs in the realm of automated answer assesment has not received much\nattention. To address this gap, we explore the potential of using LLMs for\nautomated assessment of student's short and open-ended answer. Particularly, we\nuse LLMs to compare students' explanations with expert explanations in the\ncontext of line-by-line explanations of computer programs.\n  For comparison purposes, we assess both Large Language Models (LLMs) and\nencoder-based Semantic Textual Similarity (STS) models in the context of\nassessing the correctness of students' explanation of computer code. Our\nfindings indicate that LLMs, when prompted in few-shot and chain-of-thought\nsetting perform comparable to fine-tuned encoder-based models in evaluating\nstudents' short answers in programming domain.\n",
        "title": "Automated Assessment of Students' Code Comprehension using LLMs",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05400",
        "abstract_url": "http://arxiv.org/abs/2401.05400",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Gyeong-Geon"
            },
            {
                "last_name": "Mun",
                "first_name": "Seonyeong"
            },
            {
                "last_name": "Shin",
                "first_name": "Myeong-Kyeong"
            },
            {
                "last_name": "Zhai",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  This research aims to demonstrate that AI can function not only as a tool for\nlearning, but also as an intelligent agent with which humans can engage in\ncollaborative learning (CL) to change epistemic practices in science\nclassrooms. We adopted a design and development research approach, following\nthe Analysis, Design, Development, Implementation and Evaluation (ADDIE) model,\nto prototype a tangible instructional system called Collaborative Learning with\nAI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners\njoin an AI speaker to form a small group, where humans and AI are considered as\npeers participating in the Jigsaw learning process. The development was carried\nout using the NUGU AI speaker platform. The CLAIS system was successfully\nimplemented in a Science Education course session with 15 pre-service\nelementary science teachers. The participants evaluated the CLAIS system\nthrough mixed methods surveys as teachers, learners, peers, and users.\nQuantitative data showed that the participants' Intelligent-Technological,\nPedagogical, And Content Knowledge was significantly increased after the CLAIS\nsession, the perception of the CLAIS learning experience was positive, the peer\nassessment on AI speakers and human peers was different, and the user\nexperience was ambivalent. Qualitative data showed that the participants\nanticipated future changes in the epistemic process in science classrooms,\nwhile acknowledging technical issues such as speech recognition performance and\nresponse latency. This study highlights the potential of Human-AI Collaboration\nfor knowledge co-construction in authentic classroom settings and exemplify how\nAI could shape the future landscape of epistemic practices in the classroom.\n",
        "title": "Collaborative Learning with Artificial Intelligence Speakers (CLAIS):\n  Pre-Service Elementary Science Teachers' Responses to the Prototype",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05401",
        "abstract_url": "http://arxiv.org/abs/2401.05401",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xisheng"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Song",
                "first_name": "Pinhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingjun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The inherent characteristics and light fluctuations of water bodies give rise\nto the huge difference between different layers and regions in underwater\nenvironments. When the test set is collected in a different marine area from\nthe training set, the issue of domain shift emerges, significantly compromising\nthe model's ability to generalize. The Domain Adversarial Learning (DAL)\ntraining strategy has been previously utilized to tackle such challenges.\nHowever, DAL heavily depends on manually one-hot domain labels, which implies\nno difference among the samples in the same domain. Such an assumption results\nin the instability of DAL. This paper introduces the concept of Domain\nSimilarity-Perceived Label Assignment (DSP). The domain label for each image is\nregarded as its similarity to the specified domains. Through domain-specific\ndata augmentation techniques, we achieved state-of-the-art results on the\nunderwater cross-domain object detection benchmark S-UODAC2020. Furthermore, we\nvalidated the effectiveness of our method in the Cityscapes dataset.\n",
        "title": "Domain Similarity-Perceived Label Assignment for Domain Generalized\n  Underwater Object Detection",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05402",
        "abstract_url": "http://arxiv.org/abs/2401.05402",
        "authors": [
            {
                "last_name": "Klipfel",
                "first_name": "Astrid"
            },
            {
                "last_name": "Fregier",
                "first_name": "Ya\u00ebl"
            },
            {
                "last_name": "Sayede",
                "first_name": "Adlane"
            },
            {
                "last_name": "Bouraoui",
                "first_name": "Zied"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Discovering crystal structures with specific chemical properties has become\nan increasingly important focus in material science. However, current models\nare limited in their ability to generate new crystal lattices, as they only\nconsider atomic positions or chemical composition. To address this issue, we\npropose a probabilistic diffusion model that utilizes a geometrically\nequivariant GNN to consider atomic positions and crystal lattices jointly. To\nevaluate the effectiveness of our model, we introduce a new generation metric\ninspired by Frechet Inception Distance, but based on GNN energy prediction\nrather than InceptionV3 used in computer vision. In addition to commonly used\nmetrics like validity, which assesses the plausibility of a structure, this new\nmetric offers a more comprehensive evaluation of our model's capabilities. Our\nexperiments on existing benchmarks show the significance of our diffusion\nmodel. We also show that our method can effectively learn meaningful\nrepresentations.\n",
        "title": "Vector Field Oriented Diffusion Model for Crystal Material Generation",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05403",
        "abstract_url": "http://arxiv.org/abs/2401.05403",
        "authors": [
            {
                "last_name": "Honghu",
                "first_name": "Yi"
            },
            {
                "last_name": "Ting",
                "first_name": "Liu"
            },
            {
                "last_name": "Gongjin",
                "first_name": "Lan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            ""
        ],
        "abstract": "  Artificial Intelligence (AI) technologies have been applied in various\ndomains, including early childhood education (ECE). Integration of AI\neducational technology is a recent significant trend in ECE. Currently, there\nare more and more studies of AI in ECE. To date, there is a lack of survey\narticles that discuss the studies of AI in ECE. In this paper, we provide an\nup-to-date and in-depth overview of the key AI technologies in ECE that\nprovides a historical perspective, summarizes the representative works,\noutlines open questions, discusses the trends and challenges through a detailed\nbibliometric analysis, and provides insightful recommendations for future\nresearch. We mainly discuss the studies that apply AI-based robots and AI\ntechnologies to ECE, including improving the social interaction of children\nwith an autism spectrum disorder. This paper significantly contributes to\nprovide an up-to-date and in-depth survey that is suitable as introductory\nmaterial for beginners to AI in ECE, as well as supplementary material for\nadvanced users.\n",
        "title": "The Key Artificial Intelligence Technologies in Early Childhood\n  Education: A Review",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05405",
        "abstract_url": "http://arxiv.org/abs/2401.05405",
        "authors": [
            {
                "last_name": "Del Pup",
                "first_name": "Federico"
            },
            {
                "last_name": "Zanola",
                "first_name": "Andrea"
            },
            {
                "last_name": "Tshimanga",
                "first_name": "Louis Fabrice"
            },
            {
                "last_name": "Mazzon",
                "first_name": "Paolo Emilio"
            },
            {
                "last_name": "Atzori",
                "first_name": "Manfredo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  SelfEEG is an open-source Python library developed to assist researchers in\nconducting Self-Supervised Learning (SSL) experiments on electroencephalography\n(EEG) data. Its primary objective is to offer a user-friendly but highly\ncustomizable environment, enabling users to efficiently design and execute\nself-supervised learning tasks on EEG data.\n  SelfEEG covers all the stages of a typical SSL pipeline, ranging from data\nimport to model design and training. It includes modules specifically designed\nto: split data at various granularity levels (e.g., session-, subject-, or\ndataset-based splits); effectively manage data stored with different\nconfigurations (e.g., file extensions, data types) during mini-batch\nconstruction; provide a wide range of standard deep learning models, data\naugmentations and SSL baseline methods applied to EEG data.\n  Most of the functionalities offered by selfEEG can be executed both on GPUs\nand CPUs, expanding its usability beyond the self-supervised learning area.\nAdditionally, these functionalities can be employed for the analysis of other\nbiomedical signals often coupled with EEGs, such as electromyography or\nelectrocardiography data.\n  These features make selfEEG a versatile deep learning tool for biomedical\napplications and a useful resource in SSL, one of the currently most active\nfields of Artificial Intelligence.\n",
        "title": "SelfEEG: A Python library for Self-Supervised Learning in\n  Electroencephalography",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05406",
        "abstract_url": "http://arxiv.org/abs/2401.05406",
        "authors": [
            {
                "last_name": "Rosen",
                "first_name": "Daniel"
            },
            {
                "last_name": "Rochez",
                "first_name": "Illa"
            },
            {
                "last_name": "McIrvin",
                "first_name": "Caleb"
            },
            {
                "last_name": "Lee",
                "first_name": "Joshua"
            },
            {
                "last_name": "D'Alessandro",
                "first_name": "Kevin"
            },
            {
                "last_name": "Wiecek",
                "first_name": "Max"
            },
            {
                "last_name": "Hoang",
                "first_name": "Nhan"
            },
            {
                "last_name": "Saffarini",
                "first_name": "Ramzy"
            },
            {
                "last_name": "Philips",
                "first_name": "Sam"
            },
            {
                "last_name": "Jones",
                "first_name": "Vanessa"
            },
            {
                "last_name": "Ivey",
                "first_name": "Will"
            },
            {
                "last_name": "Harris-Smart",
                "first_name": "Zavier"
            },
            {
                "last_name": "Harris-Smart",
                "first_name": "Zavion"
            },
            {
                "last_name": "Chin",
                "first_name": "Zayden"
            },
            {
                "last_name": "Johnson",
                "first_name": "Amos"
            },
            {
                "last_name": "Jones",
                "first_name": "Alyse M."
            },
            {
                "last_name": "Headley",
                "first_name": "William C."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            "NI"
        ],
        "abstract": "  Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely\napplicable technology in the next generation of wireless communication systems,\nparticularly 6G and next-gen military communications. Given this, our research\nis focused on developing a tool to promote the development of RFRL techniques\nthat leverage spectrum sensing. In particular, the tool was designed to address\ntwo cognitive radio applications, specifically dynamic spectrum access and\njamming. In order to train and test reinforcement learning (RL) algorithms for\nthese applications, a simulation environment is necessary to simulate the\nconditions that an agent will encounter within the Radio Frequency (RF)\nspectrum. In this paper, such an environment has been developed, herein\nreferred to as the RFRL Gym. Through the RFRL Gym, users can design their own\nscenarios to model what an RL agent may encounter within the RF spectrum as\nwell as experiment with different spectrum sensing techniques. Additionally,\nthe RFRL Gym is a subclass of OpenAI gym, enabling the use of third-party ML/RL\nLibraries. We plan to open-source this codebase to enable other researchers to\nutilize the RFRL Gym to test their own scenarios and RL algorithms, ultimately\nleading to the advancement of RL research in the wireless communications\ndomain. This paper describes in further detail the components of the Gym,\nresults from example scenarios, and plans for future additions.\n  Index Terms-machine learning, reinforcement learning, wireless\ncommunications, dynamic spectrum access, OpenAI gym\n",
        "title": "RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio\n  Applications",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05407",
        "abstract_url": "http://arxiv.org/abs/2401.05407",
        "authors": [
            {
                "last_name": "Koffi",
                "first_name": "Tresor Y."
            },
            {
                "last_name": "Mourchid",
                "first_name": "Youssef"
            },
            {
                "last_name": "Hindawi",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Dupuis",
                "first_name": "Yohan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Falls among individuals, especially the elderly population, can lead to\nserious injuries and complications. Detecting impact moments within a fall\nevent is crucial for providing timely assistance and minimizing the negative\nconsequences. In this work, we aim to address this challenge by applying\nthorough preprocessing techniques to the multisensor dataset, the goal is to\neliminate noise and improve data quality. Furthermore, we employ a feature\nselection process to identify the most relevant features derived from the\nmultisensor UP-FALL dataset, which in turn will enhance the performance and\nefficiency of machine learning models. We then evaluate the efficiency of\nvarious machine learning models in detecting the impact moment using the\nresulting data information from multiple sensors. Through extensive\nexperimentation, we assess the accuracy of our approach using various\nevaluation metrics. Our results achieve high accuracy rates in impact\ndetection, showcasing the power of leveraging multisensor data for fall\ndetection tasks. This highlights the potential of our approach to enhance fall\ndetection systems and improve the overall safety and well-being of individuals\nat risk of falls.\n",
        "title": "Machine Learning and Feature Ranking for Impact Fall Detection Event\n  Using Multisensor Data",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05408",
        "abstract_url": "http://arxiv.org/abs/2401.05408",
        "authors": [
            {
                "last_name": "Grzeszczyk",
                "first_name": "Michal K."
            },
            {
                "last_name": "Lisowska",
                "first_name": "Anna"
            },
            {
                "last_name": "Sitek",
                "first_name": "Arkadiusz"
            },
            {
                "last_name": "Lisowska",
                "first_name": "Aneta"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Automatic detection and tracking of emotional states has the potential for\nhelping individuals with various mental health conditions. While previous\nstudies have captured physiological signals using wearable devices in\nlaboratory settings, providing valuable insights into the relationship between\nphysiological responses and mental states, the transfer of these findings to\nreal-life scenarios is still in its nascent stages. Our research aims to bridge\nthe gap between laboratory-based studies and real-life settings by leveraging\nconsumer-grade wearables and self-report measures. We conducted a preliminary\nstudy involving 15 healthy participants to assess the efficacy of wearables in\ncapturing user valence in real-world settings. In this paper, we present the\ninitial analysis of the collected data, focusing primarily on the results of\nvalence classification. Our findings demonstrate promising results in\ndistinguishing between high and low positive valence, achieving an F1 score of\n0.65. This research opens up avenues for future research in the field of mobile\nmental health interventions.\n",
        "title": "Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True\n  Feelings?",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05409",
        "abstract_url": "http://arxiv.org/abs/2401.05409",
        "authors": [
            {
                "last_name": "Maiwald",
                "first_name": "Aaron"
            },
            {
                "last_name": "Ackermann",
                "first_name": "Leon"
            },
            {
                "last_name": "Kalcher",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Wu",
                "first_name": "Daniel J."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Alternative data representations are powerful tools that augment the\nperformance of downstream models. However, there is an abundance of such\nrepresentations within the machine learning toolbox, and the field lacks a\ncomparative understanding of the suitability of each representation method.\n  In this paper, we propose artifact detection and classification within EEG\ndata as a testbed for profiling image-based data representations of time series\ndata. We then evaluate eleven popular deep learning architectures on each of\nsix commonly-used representation methods.\n  We find that, while the choice of representation entails a choice within the\ntradeoff between bias and variance, certain representations are practically\nmore effective in highlighting features which increase the signal-to-noise\nratio of the data. We present our results on EEG data, and open-source our\ntesting framework to enable future comparative analyses in this vein.\n",
        "title": "Image-based Data Representations of Time Series: A Comparative Analysis\n  in EEG Artifact Detection",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05410",
        "abstract_url": "http://arxiv.org/abs/2401.05410",
        "authors": [
            {
                "last_name": "Laham",
                "first_name": "Saria Al"
            },
            {
                "last_name": "Baghi",
                "first_name": "Bobak H."
            },
            {
                "last_name": "Lajoie",
                "first_name": "Pierre-Yves"
            },
            {
                "last_name": "Feriani",
                "first_name": "Amal"
            },
            {
                "last_name": "Herath",
                "first_name": "Sachini"
            },
            {
                "last_name": "Liu",
                "first_name": "Steve"
            },
            {
                "last_name": "Dudek",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CY",
            "LG",
            "NI"
        ],
        "abstract": "  We present a human state estimation framework that allows us to estimate the\nlocation, and even the activities, of people in an indoor environment without\nthe requirement that they carry a specific devices with them. To achieve this\n\"device free\" localization we use a small number of low-cost Ultra-Wide Band\n(UWB) sensors distributed across the environment of interest. To achieve high\nquality estimation from the UWB signals merely reflected of people in the\nenvironment, we exploit a deep network that can learn to make inferences. The\nhardware setup consists of commercial off-the-shelf (COTS) single antenna UWB\nmodules for sensing, paired with Raspberry PI units for computational\nprocessing and data transfer. We make use of the channel impulse response (CIR)\nmeasurements from the UWB sensors to estimate the human state - comprised of\nlocation and activity - in a given area. Additionally, we can also estimate the\nnumber of humans that occupy this region of interest. In our approach, first,\nwe pre-process the CIR data which involves meticulous aggregation of\nmeasurements and extraction of key statistics. Afterwards, we leverage a\nconvolutional deep neural network to map the CIRs into precise location\nestimates with sub-30 cm accuracy. Similarly, we achieve accurate human\nactivity recognition and occupancy counting results. We show that we can\nquickly fine-tune our model for new out-of-distribution users, a process that\nrequires only a few minutes of data and a few epochs of training. Our results\nshow that UWB is a promising solution for adaptable smart-home localization and\nactivity recognition problems.\n",
        "title": "Device-Free Human State Estimation using UWB Multi-Static Radios",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05411",
        "abstract_url": "http://arxiv.org/abs/2401.05411",
        "authors": [
            {
                "last_name": "Ben-Moshe",
                "first_name": "Noam"
            },
            {
                "last_name": "Tsutsui",
                "first_name": "Kenta"
            },
            {
                "last_name": "Biton",
                "first_name": "Shany"
            },
            {
                "last_name": "S\u00f6rnmo",
                "first_name": "Leif"
            },
            {
                "last_name": "Behar",
                "first_name": "Joachim A."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Introduction: Deep learning models for detecting episodes of atrial\nfibrillation (AF) using rhythm information in long-term, ambulatory ECG\nrecordings have shown high performance. However, the rhythm-based approach does\nnot take advantage of the morphological information conveyed by the different\nECG waveforms, particularly the f-waves. As a result, the performance of such\nmodels may be inherently limited. Methods: To address this limitation, we have\ndeveloped a deep learning model, named RawECGNet, to detect episodes of AF and\natrial flutter (AFl) using the raw, single-lead ECG. We compare the\ngeneralization performance of RawECGNet on two external data sets that account\nfor distribution shifts in geography, ethnicity, and lead position. RawECGNet\nis further benchmarked against a state-of-the-art deep learning model, named\nArNet2, which utilizes rhythm information as input. Results: Using RawECGNet,\nthe results for the different leads in the external test sets in terms of the\nF1 score were 0.91--0.94 in RBDB and 0.93 in SHDB, compared to 0.89--0.91 in\nRBDB and 0.91 in SHDB for ArNet2. The results highlight RawECGNet as a\nhigh-performance, generalizable algorithm for detection of AF and AFl episodes,\nexploiting information on both rhythm and morphology.\n",
        "title": "RawECGNet: Deep Learning Generalization for Atrial Fibrillation\n  Detection from the Raw ECG",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05412",
        "abstract_url": "http://arxiv.org/abs/2401.05412",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xueyuan"
            },
            {
                "last_name": "Yao",
                "first_name": "Chao"
            },
            {
                "last_name": "Ban",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            ""
        ],
        "abstract": "  Leveraging wearable devices for motion reconstruction has emerged as an\neconomical and viable technique. Certain methodologies employ sparse Inertial\nMeasurement Units (IMUs) on the human body and harness data-driven strategies\nto model human poses. However, the reconstruction of motion based solely on\nsparse IMUs data is inherently fraught with ambiguity, a consequence of\nnumerous identical IMU readings corresponding to different poses. In this\npaper, we explore the spatial importance of multiple sensors, supervised by\ntext that describes specific actions. Specifically, uncertainty is introduced\nto derive weighted features for each IMU. We also design a Hierarchical\nTemporal Transformer (HTT) and apply contrastive learning to achieve precise\ntemporal and feature alignment of sensor data with textual semantics.\nExperimental results demonstrate our proposed approach achieves significant\nimprovements in multiple metrics compared to existing methods. Notably, with\ntextual supervision, our method not only differentiates between ambiguous\nactions such as sitting and standing but also produces more precise and natural\nmotion.\n",
        "title": "Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted\n  with Textual Semantics",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05414",
        "abstract_url": "http://arxiv.org/abs/2401.05414",
        "authors": [
            {
                "last_name": "Dong",
                "first_name": "Xinshuai"
            },
            {
                "last_name": "Dai",
                "first_name": "Haoyue"
            },
            {
                "last_name": "Fan",
                "first_name": "Yewen"
            },
            {
                "last_name": "Jin",
                "first_name": "Songyao"
            },
            {
                "last_name": "Rajendran",
                "first_name": "Sathyamoorthy"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kun"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Financial data is generally time series in essence and thus suffers from\nthree fundamental issues: the mismatch in time resolution, the time-varying\nproperty of the distribution - nonstationarity, and causal factors that are\nimportant but unknown/unobserved. In this paper, we follow a causal perspective\nto systematically look into these three demons in finance. Specifically, we\nreexamine these issues in the context of causality, which gives rise to a novel\nand inspiring understanding of how the issues can be addressed. Following this\nperspective, we provide systematic solutions to these problems, which hopefully\nwould serve as a foundation for future research in the area.\n",
        "title": "On the Three Demons in Causality in Finance: Time Resolution,\n  Nonstationarity, and Latent Factors",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05415",
        "abstract_url": "http://arxiv.org/abs/2401.05415",
        "authors": [
            {
                "last_name": "Bano",
                "first_name": "Muneera"
            },
            {
                "last_name": "Chaudhri",
                "first_name": "Zahid"
            },
            {
                "last_name": "Zowghi",
                "first_name": "Didar"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  As Artificial Intelligence (AI) transforms the domain of diplomacy in the\n21st century, this research addresses the pressing need to evaluate the\ndualistic nature of these advancements, unpacking both the challenges they pose\nand the opportunities they offer. It has been almost a year since the launch of\nChatGPT by OpenAI that revolutionised various work domains with its\ncapabilities. The scope of application of these capabilities to diplomacy is\nyet to be fully explored or understood. Our research objective is to\nsystematically examine the current discourse on Digital and AI Diplomacy, thus\ninforming the development of a comprehensive framework for the role of\nGenerative AI in modern diplomatic practices. Through the systematic analysis\nof 230 scholarly articles, we identified a spectrum of opportunities and\nchallenges, culminating in a strategic framework that captures the multifaceted\nconcepts for integration of Generative AI, setting a course for future research\nand innovation in diplomacy.\n",
        "title": "The Role of Generative AI in Global Diplomatic Practices: A Strategic\n  Framework",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05416",
        "abstract_url": "http://arxiv.org/abs/2401.05416",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yifeng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  As attitude and motion sensing components, inertial sensors are widely used\nin various portable devices. But the severe errors of inertial sensors restrain\ntheir function, especially the trajectory recovery and semantic recognition. As\na mainstream signal processing method, wavelet is hailed as the mathematical\nmicroscope of signal due to the plentiful and diverse wavelet basis functions.\nHowever, complicated noise types and application scenarios of inertial sensors\nmake selecting wavelet basis perplexing. To this end, we propose a wavelet\ndynamic selection network (WDSNet), which intelligently selects the appropriate\nwavelet basis for variable inertial signals. In addition, existing deep\nlearning architectures excel at extracting features from input data but neglect\nto learn the characteristics of target categories, which is essential to\nenhance the category awareness capability, thereby improving the selection of\nwavelet basis. Therefore, we propose a category representation mechanism (CRM),\nwhich enables the network to extract and represent category features without\nincreasing trainable parameters. Furthermore, CRM transforms the common fully\nconnected network into category representations, which provide closer\nsupervision to the feature extractor than the far and trivial one-hot\nclassification labels. We call this process of imposing interpretability on a\nnetwork and using it to supervise the feature extractor the feature supervision\nmechanism, and its effectiveness is demonstrated experimentally and\ntheoretically in this paper. The enhanced inertial signal can perform\nimpracticable tasks with regard to the original signal, such as trajectory\nreconstruction. Both quantitative and visual results show that WDSNet\noutperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised\nmethod, achieves the state-of-the-art performance of all the compared\nfully-supervised methods.\n",
        "title": "Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05418",
        "abstract_url": "http://arxiv.org/abs/2401.05418",
        "authors": [
            {
                "last_name": "Haidri",
                "first_name": "Salman"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "IR",
            "LG"
        ],
        "abstract": "  The advent of compact, handheld devices has given us a pool of tracked\nmovement data that could be used to infer trends and patterns that can be made\nto use. With this flooding of various trajectory data of animals, humans,\nvehicles, etc., the idea of ANALYTiC originated, using active learning to infer\nsemantic annotations from the trajectories by learning from sets of labeled\ndata. This study explores the application of dimensionality reduction and\ndecision boundaries in combination with the already present active learning,\nhighlighting patterns and clusters in data. We test these features with three\ndifferent trajectory datasets with objective of exploiting the the already\nlabeled data and enhance their interpretability. Our experimental analysis\nexemplifies the potential of these combined methodologies in improving the\nefficiency and accuracy of trajectory labeling. This study serves as a\nstepping-stone towards the broader integration of machine learning and visual\nmethods in context of movement data analysis.\n",
        "title": "ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction\n  in Machine Learning",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05420",
        "abstract_url": "http://arxiv.org/abs/2401.05420",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Debamita"
            },
            {
                "last_name": "Hanawal",
                "first_name": "Manjesh Kumar"
            },
            {
                "last_name": "Zlatanova",
                "first_name": "Nikola"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Holographic Metasurface Transceivers (HMTs) are emerging as cost-effective\nsubstitutes to large antenna arrays for beamforming in Millimeter and TeraHertz\nwave communication. However, to achieve desired channel gains through\nbeamforming in HMT, phase-shifts of a large number of elements need to be\nappropriately set, which is challenging. Also, these optimal phase-shifts\ndepend on the location of the receivers, which could be unknown. In this work,\nwe develop a learning algorithm using a {\\it fixed-budget multi-armed bandit\nframework} to beamform and maximize received signal strength at the receiver\nfor far-field regions. Our algorithm, named \\Algo exploits the parametric form\nof channel gains of the beams, which can be expressed in terms of two {\\it\nphase-shifting parameters}. Even after parameterization, the problem is still\nchallenging as phase-shifting parameters take continuous values. To overcome\nthis, {\\it\\HB} works with the discrete values of phase-shifting parameters and\nexploits their unimodal relations with channel gains to learn the optimal\nvalues faster. We upper bound the probability of {\\it\\HB} incorrectly\nidentifying the (discrete) optimal phase-shift parameters in terms of the\nnumber of pilots used in learning. We show that this probability decays\nexponentially with the number of pilot signals. We demonstrate that {\\it\\HB}\noutperforms state-of-the-art algorithms through extensive simulations.\n",
        "title": "HoloBeam: Learning Optimal Beamforming in Far-Field Holographic\n  Metasurface Transceivers",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05421",
        "abstract_url": "http://arxiv.org/abs/2401.05421",
        "authors": [
            {
                "last_name": "Al-Lawati",
                "first_name": "Ali"
            },
            {
                "last_name": "Eshra",
                "first_name": "Elsayed"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  Trajectory generation is an important concern in pedestrian, vehicle, and\nwildlife movement studies. Generated trajectories help enrich the training\ncorpus in relation to deep learning applications, and may be used to facilitate\nsimulation tasks. This is especially significant in the wildlife domain, where\nthe cost of obtaining additional real data can be prohibitively expensive,\ntime-consuming, and bear ethical considerations. In this paper, we introduce\nWildGEN: a conceptual framework that addresses this challenge by employing a\nVariational Auto-encoders (VAEs) based method for the acquisition of movement\ncharacteristics exhibited by wild geese over a long horizon using a sparse set\nof truth samples. A subsequent post-processing step of the generated\ntrajectories is performed based on smoothing filters to reduce excessive\nwandering. Our evaluation is conducted through visual inspection and the\ncomputation of the Hausdorff distance between the generated and real\ntrajectories. In addition, we utilize the Pearson Correlation Coefficient as a\nway to measure how realistic the trajectories are based on the similarity of\nclusters evaluated on the generated and real trajectories.\n",
        "title": "WildGEN: Long-horizon Trajectory Generation for Wildlife",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05422",
        "abstract_url": "http://arxiv.org/abs/2401.05422",
        "authors": [
            {
                "last_name": "M",
                "first_name": "Karthik R"
            },
            {
                "last_name": "Hegde",
                "first_name": "Dhiraj Nagaraja"
            },
            {
                "last_name": "Sarajlic",
                "first_name": "Muris"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Beam management (BM) protocols are critical for establishing and maintaining\nconnectivity between network radio nodes and User Equipments (UEs). In\nDistributed Multiple Input Multiple Output systems (D-MIMO), a number of access\npoints (APs), coordinated by a central processing unit (CPU), serves a number\nof UEs. At mmWave frequencies, the problem of finding the best AP and beam to\nserve the UEs is challenging due to a large number of beams that need to be\nsounded with Downlink (DL) reference signals. The objective of this paper is to\ninvestigate whether the best AP/beam can be reliably inferred from sounding\nonly a small subset of beams and leveraging AI/ML for inference of best\nbeam/AP. We use Random Forest (RF), MissForest (MF) and conditional Generative\nAdversarial Networks (c-GAN) for demonstrating the performance benefits of\ninference.\n",
        "title": "Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave\n  Distributed Multiple Input Multiple Output (D-MIMO) systems",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05424",
        "abstract_url": "http://arxiv.org/abs/2401.05424",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Djemili",
                "first_name": "Karim"
            },
            {
                "last_name": "Elezi",
                "first_name": "Denis"
            },
            {
                "last_name": "Shalman",
                "first_name": "Aaneel"
            },
            {
                "last_name": "P\u00e9rez-Ortiz",
                "first_name": "Mar\u00eda"
            },
            {
                "last_name": "Yilmaz",
                "first_name": "Emine"
            },
            {
                "last_name": "Shawe-Taylor",
                "first_name": "John"
            },
            {
                "last_name": "Bulathwela",
                "first_name": "Sahan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "IR",
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  With the advancement and utility of Artificial Intelligence (AI),\npersonalising education to a global population could be a cornerstone of new\neducational systems in the future. This work presents the PEEKC dataset and the\nTrueLearn Python library, which contains a dataset and a series of online\nlearner state models that are essential to facilitate research on learner\nengagement modelling.TrueLearn family of models was designed following the\n\"open learner\" concept, using humanly-intuitive user representations. This\nfamily of scalable, online models also help end-users visualise the learner\nmodels, which may in the future facilitate user interaction with their\nmodels/recommenders. The extensive documentation and coding examples make the\nlibrary highly accessible to both machine learning developers and educational\ndata mining and learning analytics practitioners. The experiments show the\nutility of both the dataset and the library with predictive performance\nsignificantly exceeding comparative baseline models. The dataset contains a\nlarge amount of AI-related educational videos, which are of interest for\nbuilding and validating AI-specific educational recommenders.\n",
        "title": "A Toolbox for Modelling Engagement with Educational Videos",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05425",
        "abstract_url": "http://arxiv.org/abs/2401.05425",
        "authors": [
            {
                "last_name": "Aziz",
                "first_name": "Abdul"
            },
            {
                "last_name": "Pham",
                "first_name": "Nhat"
            },
            {
                "last_name": "Vora",
                "first_name": "Neel"
            },
            {
                "last_name": "Reynolds",
                "first_name": "Cody"
            },
            {
                "last_name": "Lehnen",
                "first_name": "Jaime"
            },
            {
                "last_name": "Venkatesh",
                "first_name": "Pooja"
            },
            {
                "last_name": "Yao",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Harvey",
                "first_name": "Jay"
            },
            {
                "last_name": "Vu",
                "first_name": "Tam"
            },
            {
                "last_name": "Ding",
                "first_name": "Kan"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Phuc"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Epilepsy is one of the most common neurological diseases globally, affecting\naround 50 million people worldwide. Fortunately, up to 70 percent of people\nwith epilepsy could live seizure-free if properly diagnosed and treated, and a\nreliable technique to monitor the onset of seizures could improve the quality\nof life of patients who are constantly facing the fear of random seizure\nattacks. The scalp-based EEG test, despite being the gold standard for\ndiagnosing epilepsy, is costly, necessitates hospitalization, demands skilled\nprofessionals for operation, and is discomforting for users. In this paper, we\npropose EarSD, a novel lightweight, unobtrusive, and socially acceptable\near-worn system to detect epileptic seizure onsets by measuring the\nphysiological signals from behind the user's ears. EarSD includes an integrated\ncustom-built sensing, computing, and communication PCB to collect and amplify\nthe signals of interest, remove the noises caused by motion artifacts and\nenvironmental impacts, and stream the data wirelessly to the computer or mobile\nphone nearby, where data are uploaded to the host computer for further\nprocessing. We conducted both in-lab and in-hospital experiments with epileptic\nseizure patients who were hospitalized for seizure studies. The preliminary\nresults confirm that EarSD can detect seizures with up to 95.3 percent accuracy\nby just using classical machine learning algorithms.\n",
        "title": "An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic\n  Seizure Detection",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05426",
        "abstract_url": "http://arxiv.org/abs/2401.05426",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zimin"
            },
            {
                "last_name": "Gei\u00dfler",
                "first_name": "Daniel"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Suh",
                "first_name": "Sungho"
            },
            {
                "last_name": "Lukowicz",
                "first_name": "Paul"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Recent advancements in Artificial Neural Networks have significantly improved\nhuman activity recognition using multiple time-series sensors. While employing\nnumerous sensors with high-frequency sampling rates usually improves the\nresults, it often leads to data inefficiency and unnecessary expansion of the\nANN, posing a challenge for their practical deployment on edge devices.\nAddressing these issues, our work introduces a pragmatic framework for\ndata-efficient utilization in HAR tasks, considering the optimization of both\nsensor modalities and sampling rate simultaneously. Central to our approach are\nthe designed trainable parameters, termed 'Weight Scores,' which assess the\nsignificance of each sensor modality and sampling rate during the training\nphase. These scores guide the sensor modalities and sampling rate selection.\nThe pruning method allows users to make a trade-off between computational\nbudgets and performance by selecting the sensor modalities and sampling rates\naccording to the weight score ranking. We tested our framework's effectiveness\nin optimizing sensor modality and sampling rate selection using three public\nHAR benchmark datasets. The results show that the sensor and sampling rate\ncombination selected via CoSS achieves similar classification performance to\nconfigurations using the highest sampling rate with all sensors but at a\nreduced hardware cost.\n",
        "title": "CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in\n  Human Activity Recognition",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05427",
        "abstract_url": "http://arxiv.org/abs/2401.05427",
        "authors": [
            {
                "last_name": "van Putten",
                "first_name": "Maurice H. P. M."
            },
            {
                "last_name": "Wilson",
                "first_name": "Leighton"
            },
            {
                "last_name": "Lavely",
                "first_name": "Adam W."
            },
            {
                "last_name": "Hair",
                "first_name": "Mark"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "DC",
            "DS"
        ],
        "abstract": "  Searches for signals at low signal-to-noise ratios frequently involve the\nFast Fourier Transform (FFT). For high-throughput searches, we here consider\nFFT on the homogeneous mesh of Processing Elements (PEs) of a wafer-scale\nengine (WSE). To minimize memory overhead in the inherently non-local FFT\nalgorithm, we introduce a new synchronous slide operation ({\\em Slide})\nexploiting the fast interconnect between adjacent PEs. Feasibility of\ncompute-limited performance is demonstrated in linear scaling of Slide\nexecution times with varying array size in preliminary benchmarks on the CS-2\nWSE. The proposed implementation appears opportune to accelerate and open the\nfull discovery potential of FFT-based signal processing in multi-messenger\nastronomy.\n",
        "title": "Slide FFT on a homogeneous mesh in wafer-scale computing",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05430",
        "abstract_url": "http://arxiv.org/abs/2401.05430",
        "authors": [
            {
                "last_name": "You",
                "first_name": "Zinuo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Pengju"
            },
            {
                "last_name": "Zheng",
                "first_name": "Jin"
            },
            {
                "last_name": "Cartlidge",
                "first_name": "John"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "NE"
        ],
        "abstract": "  Stock trend classification remains a fundamental yet challenging task, owing\nto the intricate time-evolving dynamics between and within stocks. To tackle\nthese two challenges, we propose a graph-based representation learning approach\naimed at predicting the future movements of multiple stocks. Initially, we\nmodel the complex time-varying relationships between stocks by generating\ndynamic multi-relational stock graphs. This is achieved through a novel edge\ngeneration algorithm that leverages information entropy and signal energy to\nquantify the intensity and directionality of inter-stock relations on each\ntrading day. Then, we further refine these initial graphs through a stochastic\nmulti-relational diffusion process, adaptively learning task-optimal edges.\nSubsequently, we implement a decoupled representation learning scheme with\nparallel retention to obtain the final graph representation. This strategy\nbetter captures the unique temporal features within individual stocks while\nalso capturing the overall structure of the stock graph. Comprehensive\nexperiments conducted on real-world datasets from two US markets (NASDAQ and\nNYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the\neffectiveness of our method. Our approach consistently outperforms\nstate-of-the-art baselines in forecasting next trading day stock trends across\nthree test periods spanning seven years. Datasets and code have been released\n(https://github.com/pixelhero98/MGDPR).\n",
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention\n  for Stock Trends Classification",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05431",
        "abstract_url": "http://arxiv.org/abs/2401.05431",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Luyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Cong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhai",
                "first_name": "Shengfang"
            },
            {
                "last_name": "Fang",
                "first_name": "Yuejian"
            },
            {
                "last_name": "Shen",
                "first_name": "Qingni"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhonghai"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Representation learning frameworks in unlabeled time series have been\nproposed for medical signal processing. Despite the numerous excellent\nprogresses have been made in previous works, we observe the representation\nextracted for the time series still does not generalize well. In this paper, we\npresent a Time series (medical signal) Representation Learning framework via\nSpectrogram (TRLS) to get more informative representations. We transform the\ninput time-domain medical signals into spectrograms and design a time-frequency\nencoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale\nrepresentations from the augmented spectrograms. Our TRLS takes spectrogram as\ninput with two types of different data augmentations and maximizes the\nsimilarity between positive ones, which effectively circumvents the problem of\ndesigning negative samples. Our evaluation of four real-world medical signal\ndatasets focusing on medical signal classification shows that TRLS is superior\nto the existing frameworks.\n",
        "title": "TRLS: A Time Series Representation Learning Framework via Spectrogram\n  for Medical Signal Processing",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05432",
        "abstract_url": "http://arxiv.org/abs/2401.05432",
        "authors": [
            {
                "last_name": "Hossain",
                "first_name": "Khondoker Murad"
            },
            {
                "last_name": "Oates",
                "first_name": "Tim"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CR"
        ],
        "abstract": "  As deep neural networks and the datasets used to train them get larger, the\ndefault approach to integrating them into research and commercial projects is\nto download a pre-trained model and fine tune it. But these models can have\nuncertain provenance, opening up the possibility that they embed hidden\nmalicious behavior such as trojans or backdoors, where small changes to an\ninput (triggers) can cause the model to produce incorrect outputs (e.g., to\nmisclassify). This paper introduces a novel approach to backdoor detection that\nuses two tensor decomposition methods applied to network activations. This has\na number of advantages relative to existing detection methods, including the\nability to analyze multiple models at the same time, working across a wide\nvariety of network architectures, making no assumptions about the nature of\ntriggers used to alter network behavior, and being computationally efficient.\nWe provide a detailed description of the detection pipeline along with results\non models trained on the MNIST digit dataset, CIFAR-10 dataset, and two\ndifficult datasets from NIST's TrojAI competition. These results show that our\nmethod detects backdoored networks more accurately and efficiently than current\nstate-of-the-art methods.\n",
        "title": "TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep\n  Neural Networks",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05433",
        "abstract_url": "http://arxiv.org/abs/2401.05433",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Che",
                "first_name": "Chang"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Liu",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  The objective of this study is to improve automated feedback tools designed\nfor English Language Learners (ELLs) through the utilization of data science\ntechniques encompassing machine learning, natural language processing, and\neducational data analytics. Automated essay scoring (AES) research has made\nstrides in evaluating written essays, but it often overlooks the specific needs\nof English Language Learners (ELLs) in language development. This study\nexplores the application of BERT-related techniques to enhance the assessment\nof ELLs' writing proficiency within AES.\n  To address the specific needs of ELLs, we propose the use of DeBERTa, a\nstate-of-the-art neural language model, for improving automated feedback tools.\nDeBERTa, pretrained on large text corpora using self-supervised learning,\nlearns universal language representations adaptable to various natural language\nunderstanding tasks. The model incorporates several innovative techniques,\nincluding adversarial training through Adversarial Weights Perturbation (AWP)\nand Metric-specific AttentionPooling (6 kinds of AP) for each label in the\ncompetition.\n  The primary focus of this research is to investigate the impact of\nhyperparameters, particularly the adversarial learning rate, on the performance\nof the model. By fine-tuning the hyperparameter tuning process, including the\ninfluence of 6AP and AWP, the resulting models can provide more accurate\nevaluations of language proficiency and support tailored learning tasks for\nELLs. This work has the potential to significantly benefit ELLs by improving\ntheir English language proficiency and facilitating their educational journey.\n",
        "title": "Enhancing Essay Scoring with Adversarial Weights Perturbation and\n  Metric-specific AttentionPooling",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05434",
        "abstract_url": "http://arxiv.org/abs/2401.05434",
        "authors": [
            {
                "last_name": "Akan",
                "first_name": "Taymaz"
            },
            {
                "last_name": "Alp",
                "first_name": "Sait"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Mohammad Alfrad Nobel"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat.\nThere are various types of arrhythmias that can originate from different areas\nof the heart, resulting in either a rapid, slow, or irregular heartbeat. An\nelectrocardiogram (ECG) is a vital diagnostic tool used to detect heart\nirregularities and abnormalities, allowing experts to analyze the heart's\nelectrical signals to identify intricate patterns and deviations from the norm.\nOver the past few decades, numerous studies have been conducted to develop\nautomated methods for classifying heartbeats based on ECG data. In recent\nyears, deep learning has demonstrated exceptional capabilities in tackling\nvarious medical challenges, particularly with transformers as a model\narchitecture for sequence processing. By leveraging the transformers, we\ndeveloped the ECGformer model for the classification of various arrhythmias\npresent in electrocardiogram data. We assessed the suggested approach using the\nMIT-BIH and PTB datasets. ECG heartbeat arrhythmia classification results show\nthat the proposed method is highly effective.\n",
        "title": "ECGformer: Leveraging transformer for ECG heartbeat arrhythmia\n  classification",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05436",
        "abstract_url": "http://arxiv.org/abs/2401.05436",
        "authors": [
            {
                "last_name": "Jameel",
                "first_name": "Abu Shafin Mohammad Mahdee"
            },
            {
                "last_name": "Malhotra",
                "first_name": "Akshay"
            },
            {
                "last_name": "Gamal",
                "first_name": "Aly El"
            },
            {
                "last_name": "Hamidi-Rad",
                "first_name": "Shahab"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            "NI"
        ],
        "abstract": "  In this paper, we propose a deep-learning-based channel estimation scheme in\nan orthogonal frequency division multiplexing (OFDM) system. Our proposed\nmethod, named Single Slot Recurrence Along Frequency Network (SisRafNet), is\nbased on a novel study of recurrent models for exploiting sequential behavior\nof channels across frequencies. Utilizing the fact that wireless channels have\na high degree of correlation across frequencies, we employ recurrent neural\nnetwork techniques within a single OFDM slot, thus overcoming the latency and\nmemory constraints typically associated with recurrence based methods. The\nproposed SisRafNet delivers superior estimation performance compared to\nexisting deep-learning-based channel estimation techniques and the performance\nhas been validated on a wide range of 3rd Generation Partnership Project (3GPP)\ncompliant channel scenarios at multiple signal-to-noise ratios.\n",
        "title": "Deep OFDM Channel Estimation: Capturing Frequency Recurrence",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05437",
        "abstract_url": "http://arxiv.org/abs/2401.05437",
        "authors": [
            {
                "last_name": "Jungo",
                "first_name": "Janosch"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yutong"
            },
            {
                "last_name": "Gashi",
                "first_name": "Shkurta"
            },
            {
                "last_name": "Holz",
                "first_name": "Christian"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Wearable devices continuously collect sensor data and use it to infer an\nindividual's behavior, such as sleep, physical activity, and emotions. Despite\nthe significant interest and advancements in this field, modeling multimodal\nsensor data in real-world environments is still challenging due to low data\nquality and limited data annotations. In this work, we investigate\nrepresentation learning for imputing missing wearable data and compare it with\nstate-of-the-art statistical approaches. We investigate the performance of the\ntransformer model on 10 physiological and behavioral signals with different\nmasking ratios. Our results show that transformers outperform baselines for\nmissing data imputation of signals that change more frequently, but not for\nmonotonic signals. We further investigate the impact of imputation strategies\nand masking rations on downstream classification tasks. Our study provides\ninsights for the design and development of masking-based self-supervised\nlearning tasks and advocates the adoption of hybrid-based imputation strategies\nto address the challenge of missing data in wearable devices.\n",
        "title": "Representation Learning for Wearable-Based Applications in the Case of\n  Missing Data",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05438",
        "abstract_url": "http://arxiv.org/abs/2401.05438",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Jiayi"
            },
            {
                "last_name": "Hnatyshyn",
                "first_name": "Rostyslav"
            },
            {
                "last_name": "Santos",
                "first_name": "Ebrar A. D."
            },
            {
                "last_name": "Maciejewski",
                "first_name": "Ross"
            },
            {
                "last_name": "Isenberg",
                "first_name": "Tobias"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  We examine visual representations of data that make use of combinations of\nboth 2D and 3D data mappings. Combining 2D and 3D representations is a common\ntechnique that allows viewers to understand multiple facets of the data with\nwhich they are interacting. While 3D representations focus on the spatial\ncharacter of the data or the dedicated 3D data mapping, 2D representations\noften show abstract data properties and take advantage of the unique benefits\nof mapping to a plane. Many systems have used unique combinations of both types\nof data mappings effectively. Yet there are no systematic reviews of the\nmethods in linking 2D and 3D representations. We systematically survey the\nrelationships between 2D and 3D visual representations in major visualization\npublications -- IEEE VIS, IEEE TVCG, and EuroVis -- from 2012 to 2022. We\nclosely examined 105 papers where 2D and 3D representations are connected\nvisually, interactively, or through animation. These approaches are designed\nbased on their visual environment, the relationships between their visual\nrepresentations, and their possible layouts. Through our analysis, we introduce\na design space as well as provide design guidelines for effectively linking 2D\nand 3D visual representations.\n",
        "title": "A Survey of Designs for Combined 2D+3D Visual Representations",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05439",
        "abstract_url": "http://arxiv.org/abs/2401.05439",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Biao"
            },
            {
                "last_name": "Heitor",
                "first_name": "Ana"
            },
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaohui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  The emergence of neural networks constrained by physical governing equations\nhas sparked a new trend in deep learning research, which is known as\nPhysics-Informed Neural Networks (PINNs). However, solving high-dimensional\nproblems with PINNs is still a substantial challenge, the space complexity\nbrings difficulty to solving large multidirectional problems. In this paper, a\nnovel PINN framework to quickly predict several three-dimensional Terzaghi\nconsolidation cases under different conditions is proposed. Meanwhile, the loss\nfunctions for different cases are introduced, and their differences in\nthree-dimensional consolidation problems are highlighted. The tuning strategies\nfor the PINNs framework for three-dimensional consolidation problems are\nintroduced. Then, the performance of PINNs is tested and compared with\ntraditional numerical methods adopted in forward problems, and the coefficients\nof consolidation and the impact of noisy data in inverse problems are\nidentified. Finally, the results are summarized and presented from\nthree-dimensional simulations of PINNs, which show an accuracy rate of over 99%\ncompared with ground truth for both forward and inverse problems. These results\nare desirable with good accuracy and can be used for soil settlement\nprediction, which demonstrates that the proposed PINNs framework can learn the\nthree-dimensional consolidation PDE well.\n  Keywords: Three-dimensional Terzaghi consolidation; Physics-informed neural\nnetworks (PINNs); Forward problems; Inverse problems; soil settlement\n",
        "title": "Physics-informed Deep Learning to Solve Three-dimensional Terzaghi\n  Consolidation Equation: Forward and Inverse Problems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05440",
        "abstract_url": "http://arxiv.org/abs/2401.05440",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Qian"
            },
            {
                "last_name": "Hao",
                "first_name": "Yanling"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanwei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            "LG"
        ],
        "abstract": "  WiFi human sensing is highly regarded for its low-cost and privacy advantages\nin recognizing human activities. However, its effectiveness is largely confined\nto controlled, single-user, line-of-sight settings, limited by data collection\ncomplexities and the scarcity of labeled datasets. Traditional cross-modal\nmethods, aimed at mitigating these limitations by enabling self-supervised\nlearning without labeled data, struggle to extract meaningful features from\namplitude-phase combinations. In response, we introduce AutoSen, an innovative\nautomatic WiFi sensing solution that departs from conventional approaches.\nAutoSen establishes a direct link between amplitude and phase through automated\ncross-modal autoencoder learning. This autoencoder efficiently extracts\nvaluable features from unlabeled CSI data, encompassing amplitude and phase\ninformation while eliminating their respective unique noises. These features\nare then leveraged for specific tasks using few-shot learning techniques.\nAutoSen's performance is rigorously evaluated on a publicly accessible\nbenchmark dataset, demonstrating its exceptional capabilities in automatic WiFi\nsensing through the extraction of comprehensive cross-modal features.\n",
        "title": "Autosen: improving automatic wifi human sensing through cross-modal\n  autoencoder",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05441",
        "abstract_url": "http://arxiv.org/abs/2401.05441",
        "authors": [
            {
                "last_name": "Mehrban",
                "first_name": "Ali"
            },
            {
                "last_name": "Ahadian",
                "first_name": "Pegah"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "CR",
            "LG"
        ],
        "abstract": "  This paper describes an architecture for predicting the price of\ncryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy\nInference System (ANFIS). Historical data of cryptocurrencies and indexes that\nare considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D),\nand Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach\nthe data are hybrid and backpropagation algorithms, as well as grid partition,\nsubtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which\nare used in data clustering. The architectural performance designed in this\npaper has been compared with different inputs and neural network models in\nterms of statistical evaluation criteria. Finally, the proposed method can\npredict the price of digital currencies in a short time.\n",
        "title": "An adaptive network-based approach for advanced forecasting of\n  cryptocurrency values",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05442",
        "abstract_url": "http://arxiv.org/abs/2401.05442",
        "authors": [
            {
                "last_name": "Kuba",
                "first_name": "Jakub Grudzien"
            },
            {
                "last_name": "Uehara",
                "first_name": "Masatoshi"
            },
            {
                "last_name": "Abbeel",
                "first_name": "Pieter"
            },
            {
                "last_name": "Levine",
                "first_name": "Sergey"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While machine learning models are typically trained to solve prediction\nproblems, we might often want to use them for optimization problems. For\nexample, given a dataset of proteins and their corresponding fluorescence\nlevels, we might want to optimize for a new protein with the highest possible\nfluorescence. This kind of data-driven optimization (DDO) presents a range of\nchallenges beyond those in standard prediction problems, since we need models\nthat successfully predict the performance of new designs that are better than\nthe best designs seen in the training set. It is not clear theoretically when\nexisting approaches can even perform better than the naive approach that simply\nselects the best design in the dataset. In this paper, we study how structure\ncan enable sample-efficient data-driven optimization. To formalize the notion\nof structure, we introduce functional graphical models (FGMs) and show\ntheoretically how they can provide for principled data-driven optimization by\ndecomposing the original high-dimensional optimization problem into smaller\nsub-problems. This allows us to derive much more practical regret bounds for\nDDO, and the result implies that DDO with FGMs can achieve nearly optimal\ndesigns in situations where naive approaches fail due to insufficient coverage\nof the offline data. We further present a data-driven optimization algorithm\nthat inferes the FGM structure itself, either over the original input variables\nor a latent variable representation of the inputs.\n",
        "title": "Functional Graphical Models: Structure Enables Offline Data-Driven\n  Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05443",
        "abstract_url": "http://arxiv.org/abs/2401.05443",
        "authors": [
            {
                "last_name": "Fakih",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Dharmaji",
                "first_name": "Rahul"
            },
            {
                "last_name": "Moghaddas",
                "first_name": "Yasamin"
            },
            {
                "last_name": "Araya",
                "first_name": "Gustavo Quiros"
            },
            {
                "last_name": "Ogundare",
                "first_name": "Oluwatosin"
            },
            {
                "last_name": "Faruque",
                "first_name": "Mohammad Abdullah Al"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "",
            "CL",
            "PL",
            "",
            "",
            ""
        ],
        "abstract": "  Although Large Language Models (LLMs) have established pre-dominance in\nautomated code generation, they are not devoid of shortcomings. The pertinent\nissues primarily relate to the absence of execution guarantees for generated\ncode, a lack of explainability, and suboptimal support for essential but niche\nprogramming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to\nproduce valid programs for Industrial Control Systems (ICS) operated by\nProgrammable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided\niterative pipeline leveraging user feedback and external verification tools\nincluding grammar checkers, compilers and SMV verifiers to guide the LLM's\ngeneration. We further enhance the generation potential of LLM by employing\nPrompt Engineering and model fine-tuning through the creation and usage of\nLoRAs. We validate this system using a FischerTechnik Manufacturing TestBed\n(MFTB), illustrating how LLMs can evolve from generating structurally flawed\ncode to producing verifiably correct programs for industrial applications. We\nrun a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code\nLlama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The\nproposed pipeline improved the generation success rate from 47% to 72%, and the\nSurvey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open\nresearch, we share the complete experimental setup, the LLM Fine-Tuning\nWeights, and the video demonstrations of the different programs on our\ndedicated webpage.\n",
        "title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of\n  PLCs in Industrial Control Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05444",
        "abstract_url": "http://arxiv.org/abs/2401.05444",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Ding"
            },
            {
                "last_name": "Peng",
                "first_name": "Peixi"
            },
            {
                "last_name": "Huang",
                "first_name": "Tiejun"
            },
            {
                "last_name": "Tian",
                "first_name": "Yonghong"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "",
            "LG"
        ],
        "abstract": "  With the help of special neuromorphic hardware, spiking neural networks\n(SNNs) are expected to realize artificial intelligence (AI) with less energy\nconsumption. It provides a promising energy-efficient way for realistic control\ntasks by combining SNNs with deep reinforcement learning (DRL). In this paper,\nwe focus on the task where the agent needs to learn multi-dimensional\ndeterministic policies to control, which is very common in real scenarios.\nRecently, the surrogate gradient method has been utilized for training\nmulti-layer SNNs, which allows SNNs to achieve comparable performance with the\ncorresponding deep networks in this task. Most existing spike-based RL methods\ntake the firing rate as the output of SNNs, and convert it to represent\ncontinuous action space (i.e., the deterministic policy) through a\nfully-connected (FC) layer. However, the decimal characteristic of the firing\nrate brings the floating-point matrix operations to the FC layer, making the\nwhole SNN unable to deploy on the neuromorphic hardware directly. To develop a\nfully spiking actor network without any floating-point matrix operations, we\ndraw inspiration from the non-spiking interneurons found in insects and employ\nthe membrane voltage of the non-spiking neurons to represent the action. Before\nthe non-spiking neurons, multiple population neurons are introduced to decode\ndifferent dimensions of actions. Since each population is used to decode a\ndimension of action, we argue that the neurons in each population should be\nconnected in time domain and space domain. Hence, the intra-layer connections\nare used in output populations to enhance the representation capacity. Finally,\nwe propose a fully spiking actor network with intra-layer connections\n(ILC-SAN).\n",
        "title": "Fully Spiking Actor Network with Intra-layer Connections for\n  Reinforcement Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05446",
        "abstract_url": "http://arxiv.org/abs/2401.05446",
        "authors": [
            {
                "last_name": "Weng",
                "first_name": "Weining"
            },
            {
                "last_name": "Gu",
                "first_name": "Yang"
            },
            {
                "last_name": "Guo",
                "first_name": "Shuai"
            },
            {
                "last_name": "Ma",
                "first_name": "Yuan"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhaohua"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Yiqiang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Electroencephalogram (EEG) is a non-invasive technique to record\nbioelectrical signals. Integrating supervised deep learning techniques with EEG\nsignals has recently facilitated automatic analysis across diverse EEG-based\ntasks. However, the label issues of EEG signals have constrained the\ndevelopment of EEG-based deep models. Obtaining EEG annotations is difficult\nthat requires domain experts to guide collection and labeling, and the\nvariability of EEG signals among different subjects causes significant label\nshifts. To solve the above challenges, self-supervised learning (SSL) has been\nproposed to extract representations from unlabeled samples through\nwell-designed pretext tasks. This paper concentrates on integrating SSL\nframeworks with temporal EEG signals to achieve efficient representation and\nproposes a systematic review of the SSL for EEG signals. In this paper, 1) we\nintroduce the concept and theory of self-supervised learning and typical SSL\nframeworks. 2) We provide a comprehensive review of SSL for EEG analysis,\nincluding taxonomy, methodology, and technique details of the existing\nEEG-based SSL frameworks, and discuss the difference between these methods. 3)\nWe investigate the adaptation of the SSL approach to various downstream tasks,\nincluding the task description and related benchmark datasets. 4) Finally, we\ndiscuss the potential directions for future SSL-EEG research.\n",
        "title": "Self-supervised Learning for Electroencephalogram: A Systematic Survey",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05447",
        "abstract_url": "http://arxiv.org/abs/2401.05447",
        "authors": [
            {
                "last_name": "Lefort",
                "first_name": "Baptiste"
            },
            {
                "last_name": "Benhamou",
                "first_name": "Eric"
            },
            {
                "last_name": "Ohana",
                "first_name": "Jean-Jacques"
            },
            {
                "last_name": "Saltiel",
                "first_name": "David"
            },
            {
                "last_name": "Guez",
                "first_name": "Beatrice"
            },
            {
                "last_name": "Challet",
                "first_name": "Damien"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to\n2023, reposted on large financial media, to determine how global news headlines\nmay affect stock market movements using ChatGPT and a two-stage prompt\napproach. We document a statistically significant positive correlation between\nthe sentiment score and future equity market returns over short to medium term,\nwhich reverts to a negative correlation over longer horizons. Validation of\nthis correlation pattern across multiple equity markets indicates its\nrobustness across equity regions and resilience to non-linearity, evidenced by\ncomparison of Pearson and Spearman correlations. Finally, we provide an\nestimate of the optimal horizon that strikes a balance between reactivity to\nnew information and correlation.\n",
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market\n  Wraps?",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05450",
        "abstract_url": "http://arxiv.org/abs/2401.05450",
        "authors": [
            {
                "last_name": "Mandran",
                "first_name": "Nadine"
            },
            {
                "last_name": "Prior",
                "first_name": "Estelle"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Eric"
            },
            {
                "last_name": "Vermeulen",
                "first_name": "Mathieu"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  One of the main difficulties remains the collaboration between the various\nexperts involved in designing the Learning Games (LG). Our literature review\nfocuses on the pitfalls and principles that have been identified by various\nauthors in learning games design. Based on this review, a prototype was\ndesigned to support the LG design process and to study more precisely the\ncollaboration between actors (teachers, researchers, game designers, data\nanalyst and computer scientist). Indeed, according to the state of the art, the\nskills and knowledge involved in design are difficult to integrate. It has been\ntested in a real-world scenario for designing learning games to teach\nalgorithmic. Through participant observation in thirty-three workshops\ninvolving nine experts, we were able to identify recurring pitfalls as we\napplied the recommendations in the literature. The analysis of these workshops\nled to propose eight principles aimed at facilitating the collaboration between\nthe learning games design process and re-evaluating research on its.\n",
        "title": "Reorienting Learning Game Design in Design-Based Research: a Case Study",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05451",
        "abstract_url": "http://arxiv.org/abs/2401.05451",
        "authors": [
            {
                "last_name": "Joshy",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Assessing and comparing player skill in online multiplayer gaming\nenvironments is essential for fair matchmaking and player engagement.\nTraditional ranking models like Elo and Glicko-2, designed for two-player\ngames, are insufficient for the complexity of multi-player, asymmetric\nteam-based matches. To address this gap, the OpenSkill library offers a suite\nof sophisticated, fast, and adaptable models tailored for such dynamics.\nDrawing from Bayesian inference methods, OpenSkill provides a more accurate\nrepresentation of individual player contributions and speeds up the computation\nof ranks. This paper introduces the OpenSkill library, featuring a Python\nimplementation of the Plackett-Luce model among others, highlighting its\nperformance advantages and predictive accuracy against proprietary systems like\nTrueSkill. OpenSkill is a valuable tool for game developers and researchers,\nensuring a responsive and fair gaming experience by efficiently adjusting\nplayer rankings based on game outcomes. The library's support for time decay\nand diligent documentation further aid in its practical application, making it\na robust solution for the nuanced world of multiplayer ranking systems. This\npaper also acknowledges areas for future enhancement, such as partial play and\ncontribution weighting, emphasizing the library's ongoing development to meet\nthe evolving needs of online gaming communities.\n",
        "title": "OpenSkill: A faster asymmetric multi-team, multiplayer rating system",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05452",
        "abstract_url": "http://arxiv.org/abs/2401.05452",
        "authors": [
            {
                "last_name": "Tahir",
                "first_name": "Muhammad Ahmad"
            },
            {
                "last_name": "Mehmood",
                "first_name": "Ahsan"
            },
            {
                "last_name": "Rahman",
                "first_name": "Muhammad Mahboob Ur"
            },
            {
                "last_name": "Nawaz",
                "first_name": "Muhammad Wasim"
            },
            {
                "last_name": "Riaz",
                "first_name": "Kashif"
            },
            {
                "last_name": "Abbasi",
                "first_name": "Qammer H."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "LG"
        ],
        "abstract": "  We propose two novel purpose-built deep learning (DL) models for synthesis of\nthe arterial blood pressure (ABP) waveform in a cuff-less manner, using a\nsingle-site photoplethysmography (PPG) signal. We utilize the public UCI\ndataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our\nDL models. Firstly, we implement a transformer model that incorporates\npositional encoding, multi-head attention, layer normalization, and dropout\ntechniques, and synthesizes the ABP waveform with a mean absolute error (MAE)\nof 14. Secondly, we implement a frequency-domain (FD) learning approach where\nwe first obtain the discrete cosine transform (DCT) coefficients of the PPG and\nABP signals corresponding to two cardiac cycles, and then learn a\nlinear/non-linear (L/NL) regression between them. We learn that the FD L/NL\nregression model outperforms the transformer model by achieving an MAE of 11.87\nand 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP),\nrespectively. Our FD L/NL regression model also fulfills the AAMI criterion of\nutilizing data from more than 85 subjects, and achieves grade B by the BHS\ncriterion.\n",
        "title": "Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site\n  PPG using Transformer & Frequency-domain Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05453",
        "abstract_url": "http://arxiv.org/abs/2401.05453",
        "authors": [
            {
                "last_name": "Anderberg",
                "first_name": "Alastair"
            },
            {
                "last_name": "Bailey",
                "first_name": "James"
            },
            {
                "last_name": "Campello",
                "first_name": "Ricardo J. G. B."
            },
            {
                "last_name": "Houle",
                "first_name": "Michael E."
            },
            {
                "last_name": "Marques",
                "first_name": "Henrique O."
            },
            {
                "last_name": "Radovanovi\u0107",
                "first_name": "Milo\u0161"
            },
            {
                "last_name": "Zimek",
                "first_name": "Arthur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            ""
        ],
        "abstract": "  We present a nonparametric method for outlier detection that takes full\naccount of local variations in intrinsic dimensionality within the dataset.\nUsing the theory of Local Intrinsic Dimensionality (LID), our\n'dimensionality-aware' outlier detection method, DAO, is derived as an\nestimator of an asymptotic local expected density ratio involving the query\npoint and a close neighbor drawn at random. The dimensionality-aware behavior\nof DAO is due to its use of local estimation of LID values in a\ntheoretically-justified way. Through comprehensive experimentation on more than\n800 synthetic and real datasets, we show that DAO significantly outperforms\nthree popular and important benchmark outlier detection methods: Local Outlier\nFactor (LOF), Simplified LOF, and kNN.\n",
        "title": "Dimensionality-Aware Outlier Detection: Theoretical and Experimental\n  Analysis",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05458",
        "abstract_url": "http://arxiv.org/abs/2401.05458",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dongyu"
            },
            {
                "last_name": "Hu",
                "first_name": "Ruofan"
            },
            {
                "last_name": "Rundensteiner",
                "first_name": "Elke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Deep neural networks (DNNs) have advanced many machine learning tasks, but\ntheir performance is often harmed by noisy labels in real-world data.\nAddressing this, we introduce CoLafier, a novel approach that uses Local\nIntrinsic Dimensionality (LID) for learning with noisy labels. CoLafier\nconsists of two subnets: LID-dis and LID-gen. LID-dis is a specialized\nclassifier. Trained with our uniquely crafted scheme, LID-dis consumes both a\nsample's features and its label to predict the label - which allows it to\nproduce an enhanced internal representation. We observe that LID scores\ncomputed from this representation effectively distinguish between correct and\nincorrect labels across various noise scenarios. In contrast to LID-dis,\nLID-gen, functioning as a regular classifier, operates solely on the sample's\nfeatures. During training, CoLafier utilizes two augmented views per instance\nto feed both subnets. CoLafier considers the LID scores from the two views as\nproduced by LID-dis to assign weights in an adapted loss function for both\nsubnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.\nLID-dis then processes these pseudo-labels along with two views to derive LID\nscores. Finally, these LID scores along with the differences in predictions\nfrom the two subnets guide the label update decisions. This dual-view and\ndual-subnet approach enhances the overall reliability of the framework. Upon\ncompletion of the training, we deploy the LID-gen subnet of CoLafier as the\nfinal classification model. CoLafier demonstrates improved prediction accuracy,\nsurpassing existing methods, particularly under severe label noise. For more\ndetails, see the code at https://github.com/zdy93/CoLafier.\n",
        "title": "CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic\n  Dimensionality Guidance",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05459",
        "abstract_url": "http://arxiv.org/abs/2401.05459",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanchun"
            },
            {
                "last_name": "Wen",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Weijun"
            },
            {
                "last_name": "Li",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yizhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Guohong"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenxing"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Yi"
            },
            {
                "last_name": "Kong",
                "first_name": "Rui"
            },
            {
                "last_name": "Wang",
                "first_name": "Yile"
            },
            {
                "last_name": "Geng",
                "first_name": "Hanfei"
            },
            {
                "last_name": "Luan",
                "first_name": "Jian"
            },
            {
                "last_name": "Jin",
                "first_name": "Xuefeng"
            },
            {
                "last_name": "Ye",
                "first_name": "Zilong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Guanjing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Mengwei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhijun"
            },
            {
                "last_name": "Li",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ya-Qin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunxin"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "SE"
        ],
        "abstract": "  Since the advent of personal computing devices, intelligent personal\nassistants (IPAs) have been one of the key technologies that researchers and\nengineers have focused on, aiming to help users efficiently obtain information\nand execute tasks, and provide users with more intelligent, convenient, and\nrich interaction experiences. With the development of smartphones and IoT,\ncomputing and sensing devices have become ubiquitous, greatly expanding the\nboundaries of IPAs. However, due to the lack of capabilities such as user\nintent understanding, task planning, tool using, and personal data management\netc., existing IPAs still have limited practicality and scalability. Recently,\nthe emergence of foundation models, represented by large language models\n(LLMs), brings new opportunities for the development of IPAs. With the powerful\nsemantic understanding and reasoning capabilities, LLM can enable intelligent\nagents to solve complex problems autonomously. In this paper, we focus on\nPersonal LLM Agents, which are LLM-based agents that are deeply integrated with\npersonal data and personal devices and used for personal assistance. We\nenvision that Personal LLM Agents will become a major software paradigm for\nend-users in the upcoming era. To realize this vision, we take the first step\nto discuss several important questions about Personal LLM Agents, including\ntheir architecture, capability, efficiency and security. We start by\nsummarizing the key components and design choices in the architecture of\nPersonal LLM Agents, followed by an in-depth analysis of the opinions collected\nfrom domain experts. Next, we discuss several key challenges to achieve\nintelligent, efficient and secure Personal LLM Agents, followed by a\ncomprehensive survey of representative solutions to address these challenges.\n",
        "title": "Personal LLM Agents: Insights and Survey about the Capability,\n  Efficiency and Security",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05461",
        "abstract_url": "http://arxiv.org/abs/2401.05461",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Zhanliang"
            },
            {
                "last_name": "Xiong",
                "first_name": "Nuoye"
            },
            {
                "last_name": "Li",
                "first_name": "Hongsheng"
            },
            {
                "last_name": "Shen",
                "first_name": "Peiyi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "LG"
        ],
        "abstract": "  Despite neural networks (NN) have been widely applied in various fields and\ngenerally outperforms humans, they still lack interpretability to a certain\nextent, and humans are unable to intuitively understand the decision logic of\nNN. This also hinders the knowledge interaction between humans and NN,\npreventing humans from getting involved to give direct guidance when NN's\ndecisions go wrong. While recent research in explainable AI has achieved\ninterpretability of NN from various perspectives, it has not yet provided\neffective methods for knowledge exchange between humans and NN. To address this\nproblem, we constructed a two-way interaction interface that uses structured\nrepresentations of visual concepts and their relationships as the \"language\"\nfor knowledge exchange between humans and NN. Specifically, NN provide\nintuitive reasoning explanations to humans based on the class-specific\nstructural concepts graph (C-SCG). On the other hand, humans can modify the\nbiases present in the C-SCG through their prior knowledge and reasoning\nability, and thus provide direct knowledge guidance to NN through this\ninterface. Through experimental validation, based on this interaction\ninterface, NN can provide humans with easily understandable explanations of the\nreasoning process. Furthermore, human involvement and prior knowledge can\ndirectly and effectively contribute to enhancing the performance of NN.\n",
        "title": "The two-way knowledge interaction interface between humans and neural\n  networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05462",
        "abstract_url": "http://arxiv.org/abs/2401.05462",
        "authors": [
            {
                "last_name": "Ge",
                "first_name": "Mouzhi"
            },
            {
                "last_name": "Rossi",
                "first_name": "Bruno"
            },
            {
                "last_name": "Chren",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Blanco",
                "first_name": "Jos\u00e9 Miguel"
            }
        ],
        "primary_category": "OH",
        "categories": [
            "OH"
        ],
        "abstract": "  Since the energy domain is in a transformative shift towards sustainability,\nthe integration of new technologies and smart systems into traditional power\ngrids has emerged. As an effective approach, Petri Nets (PN) have been applied\nto model and analyze the complex dynamics in Smart Grid (SG) environments.\nHowever, we are currently missing an overview of types of PNs applied to\ndifferent areas and problems related to SGs. Therefore, this paper proposes\nfour fundamental research questions related to the application areas of PNs in\nSGs, PNs types, aspects modelled by PNs in the identified areas, and the\nvalidation methods in the evaluation. The answers to the research questions are\nderived from a comprehensive and interdisciplinary literature analysis. The\nresults capture a valuable overview of PNs applications in the global energy\nlandscape and can offer indications for future research directions.\n",
        "title": "Petri Nets for Smart Grids: The Story So Far",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05465",
        "abstract_url": "http://arxiv.org/abs/2401.05465",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lin"
            },
            {
                "last_name": "Xu",
                "first_name": "Linghan"
            },
            {
                "last_name": "Motamed",
                "first_name": "Saman"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Shayok"
            },
            {
                "last_name": "De la Torre",
                "first_name": "Fernando"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unsupervised domain adaptation (UDA) for image classification has made\nremarkable progress in transferring classification knowledge from a labeled\nsource domain to an unlabeled target domain, thanks to effective domain\nalignment techniques. Recently, in order to further improve performance on a\ntarget domain, many Single-Target Active Domain Adaptation (ST-ADA) methods\nhave been proposed to identify and annotate the salient and exemplar target\nsamples. However, it requires one model to be trained and deployed for each\ntarget domain and the domain label associated with each test sample. This\nlargely restricts its application in the ubiquitous scenarios with multiple\ntarget domains. Therefore, we propose a Multi-Target Active Domain Adaptation\n(MT-ADA) framework for image classification, named D3GU, to simultaneously\nalign different domains and actively select samples from them for annotation.\nThis is the first research effort in this field to our best knowledge. D3GU\napplies Decomposed Domain Discrimination (D3) during training to achieve both\nsource-target and target-target domain alignments. Then during active sampling,\na Gradient Utility (GU) score is designed to weight every unlabeled target\nimage by its contribution towards classification and domain alignment tasks,\nand is further combined with KMeans clustering to form GU-KMeans for diverse\nimage sampling. Extensive experiments on three benchmark datasets, Office31,\nOfficeHome, and DomainNet, have been conducted to validate consistently\nsuperior performance of D3GU for MT-ADA.\n",
        "title": "D3GU: Multi-Target Active Domain Adaptation via Enhancing Domain\n  Alignment",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05467",
        "abstract_url": "http://arxiv.org/abs/2401.05467",
        "authors": [
            {
                "last_name": "Taneja",
                "first_name": "Karan"
            },
            {
                "last_name": "Goel",
                "first_name": "Ashok"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The recent advances in large language models (LLMs) have led to the creation\nof many modular AI agents. These agents employ LLMs as zero-shot learners to\nperform sub-tasks in order to solve complex tasks set forth by human users. We\npropose an approach to enhance the robustness and performance of modular AI\nagents that utilize LLMs as zero-shot learners. Our iterative machine teaching\nmethod offers an efficient way to teach AI agents over time with limited human\nfeedback, addressing the limit posed by the quality of zero-shot learning. We\nadvocate leveraging the data traces from initial deployments and outputs or\nannotations from the zero-shot learners to train smaller and task-specific\nsubstitute models which can reduce both the monetary costs and environmental\nimpact. Our machine teaching process avails human expertise to correct examples\nwith a high likelihood of misannotations. Results on three tasks, common to\nconversational AI agents, show that close-to-oracle performance can be achieved\nwith supervision on 20-70% of the dataset depending upon the complexity of the\ntask and performance of zero-shot learners.\n",
        "title": "Machine Teaching for Building Modular AI Agents based on Zero-shot\n  Learners",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05468",
        "abstract_url": "http://arxiv.org/abs/2401.05468",
        "authors": [
            {
                "last_name": "Zanardini",
                "first_name": "Damiano"
            },
            {
                "last_name": "Serrano",
                "first_name": "Emilio"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            "LG",
            "",
            ""
        ],
        "abstract": "  This paper introduces a new problem in the field of graph mining and social\nnetwork analysis called new node prediction. More technically, the task can be\ncategorized as zero-shot out-of-graph all-links prediction. This challenging\nproblem aims to predict all links from a new, isolated, and unobserved node\nthat was previously disconnected from the graph. Unlike classic approaches to\nlink prediction (including few-shot out-of-graph link prediction), this problem\npresents two key differences: (1) the new node has no existing links from which\nto extract patterns for new predictions; and (2) the goal is to predict not\njust one, but all the links of this new node, or at least a significant part of\nthem. Experiments demonstrate that an architecture based on Deep Graph Neural\nNetworks can learn to solve this challenging problem in a bibliographic\ncitation network.\n",
        "title": "Introducing New Node Prediction in Graph Mining: Predicting All Links\n  from Isolated Nodes with Graph Neural Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05469",
        "abstract_url": "http://arxiv.org/abs/2401.05469",
        "authors": [
            {
                "last_name": "Kazemi",
                "first_name": "Kianoosh"
            },
            {
                "last_name": "Azimi",
                "first_name": "Iman"
            },
            {
                "last_name": "Liljeberg",
                "first_name": "Pasi"
            },
            {
                "last_name": "Rahmani",
                "first_name": "Amir M."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Respiratory rate (RR) serves as an indicator of various medical conditions,\nsuch as cardiovascular diseases and sleep disorders. These RR estimation\nmethods were mostly designed for finger-based PPG collected from subjects in\nstationary situations (e.g., in hospitals). In contrast to finger-based PPG\nsignals, wrist-based PPG are more susceptible to noise, particularly in their\nlow frequency range, which includes respiratory information. Therefore, the\nexisting methods struggle to accurately extract RR when PPG data are collected\nfrom wrist area under free-living conditions. The increasing popularity of\nsmartwatches, equipped with various sensors including PPG, has prompted the\nneed for a robust RR estimation method. In this paper, we propose a\nconvolutional neural network-based approach to extract RR from PPG,\naccelerometer, and gyroscope signals captured via smartwatches. Our method,\nincluding a dilated residual inception module and 1D convolutions, extract the\ntemporal information from the signals, enabling RR estimation. Our method is\ntrained and tested using data collected from 36 subjects under free-living\nconditions for one day using Samsung Gear Sport watches. For evaluation, we\ncompare the proposed method with four state-of-the-art RR estimation methods.\nThe RR estimates are compared with RR references obtained from a chest-band\ndevice. The results show that our method outperforms the existing methods with\nthe Mean-Absolute-Error and Root-Mean-Square-Error of 1.85 and 2.34, while the\nbest results obtained by the other methods are 2.41 and 3.29, respectively.\nMoreover, compared to the other methods, the absolute error distribution of our\nmethod was narrow (with the lowest median), indicating a higher level of\nagreement between the estimated and reference RR values.\n",
        "title": "Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05470",
        "abstract_url": "http://arxiv.org/abs/2401.05470",
        "authors": [
            {
                "last_name": "Estopinan",
                "first_name": "Joaquim"
            },
            {
                "last_name": "Bonnet",
                "first_name": "Pierre"
            },
            {
                "last_name": "Servajean",
                "first_name": "Maximilien"
            },
            {
                "last_name": "Munoz",
                "first_name": "Fran\u00e7ois"
            },
            {
                "last_name": "Joly",
                "first_name": "Alexis"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  The post-2020 global biodiversity framework needs ambitious, research-based\ntargets. Estimating the accelerated extinction risk due to climate change is\ncritical. The International Union for Conservation of Nature (IUCN) measures\nthe extinction risk of species. Automatic methods have been developed to\nprovide information on the IUCN status of under-assessed taxa. However, these\ncompensatory methods are based on current species characteristics, mainly\ngeographical, which precludes their use in future projections. Here, we\nevaluate a novel method for classifying the IUCN status of species benefiting\nfrom the generalisation power of species distribution models based on deep\nlearning. Our method matches state-of-the-art classification performance while\nrelying on flexible SDM-based features that capture species' environmental\npreferences. Cross-validation yields average accuracies of 0.61 for status\nclassification and 0.78 for binary classification. Climate change will reshape\nfuture species distributions. Under the species-environment equilibrium\nhypothesis, SDM projections approximate plausible future outcomes. Two extremes\nof species dispersal capacity are considered: unlimited or null. The projected\nspecies distributions are translated into features feeding our IUCN\nclassification method. Finally, trends in threatened species are analysed over\ntime and i) by continent and as a function of average ii) latitude or iii)\naltitude. The proportion of threatened species is increasing globally, with\ncritical rates in Africa, Asia and South America. Furthermore, the proportion\nof threatened species is predicted to peak around the two Tropics, at the\nEquator, in the lowlands and at altitudes of 800-1,500 m.\n",
        "title": "Modelling Species Distributions with Deep Learning to Predict Plant\n  Extinction Risk and Assess Climate Change Impacts",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05476",
        "abstract_url": "http://arxiv.org/abs/2401.05476",
        "authors": [
            {
                "last_name": "Kapsalis",
                "first_name": "Timo"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "GR"
        ],
        "abstract": "  This paper introduces CADgpt, an innovative plugin integrating Natural\nLanguage Processing (NLP) with Rhino3D for enhancing 3D modelling in\ncomputer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgpt\nsimplifies the CAD interface, enabling users, particularly beginners, to\nperform complex 3D modelling tasks through intuitive natural language commands.\nThis approach significantly reduces the learning curve associated with\ntraditional CAD software, fostering a more inclusive and engaging educational\nenvironment. The paper discusses CADgpt's technical architecture, including its\nintegration within Rhino3D and the adaptation of GPT-4 capabilities for CAD\ntasks. It presents case studies demonstrating CADgpt's efficacy in various\ndesign scenarios, highlighting its potential to democratise design education by\nmaking sophisticated design tools accessible to a broader range of students.\nThe discussion further explores CADgpt's implications for pedagogy and\ncurriculum development, emphasising its role in enhancing creative exploration\nand conceptual thinking in design education.\n  Keywords: Natural Language Processing, Computer-Aided Design, 3D Modelling,\nDesign Automation, Design Education, Architectural Education\n",
        "title": "CADgpt: Harnessing Natural Language Processing for 3D Modelling to\n  Enhance Computer-Aided Design Workflows",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05477",
        "abstract_url": "http://arxiv.org/abs/2401.05477",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yiran"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haibin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yexu"
            },
            {
                "last_name": "Riedel",
                "first_name": "Till"
            },
            {
                "last_name": "Beigl",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In recent years, deep learning has emerged as a potent tool across a\nmultitude of domains, leading to a surge in research pertaining to its\napplication in the wearable human activity recognition (WHAR) domain. Despite\nthe rapid development, concerns have been raised about the lack of\nstandardization and consistency in the procedures used for experimental model\ntraining, which may affect the reproducibility and reliability of research\nresults. In this paper, we provide an exhaustive review of contemporary deep\nlearning research in the field of WHAR and collate information pertaining to\nthe training procedure employed in various studies. Our findings suggest that a\nmajor trend is the lack of detail provided by model training protocols.\nBesides, to gain a clearer understanding of the impact of missing descriptions,\nwe utilize a control variables approach to assess the impact of key tunable\ncomponents (e.g., optimization techniques and early stopping criteria) on the\ninter-subject generalization capabilities of HAR models. With insights from the\nanalyses, we define a novel integrated training procedure tailored to the WHAR\nmodel. Empirical results derived using five well-known \\ac{whar} benchmark\ndatasets and three classical HAR model architectures demonstrate the\neffectiveness of our proposed methodology: in particular, there is a\nsignificant improvement in macro F1 leave one subject out cross-validation\nperformance.\n",
        "title": "Standardizing Your Training Process for Human Activity Recognition\n  Models: A Comprehensive Review in the Tunable Factors",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05478",
        "abstract_url": "http://arxiv.org/abs/2401.05478",
        "authors": [
            {
                "last_name": "Stephens",
                "first_name": "Anna"
            },
            {
                "last_name": "Santos",
                "first_name": "Francisco"
            },
            {
                "last_name": "Tan",
                "first_name": "Pang-Ning"
            },
            {
                "last_name": "Esfahanian",
                "first_name": "Abdol-Hossein"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            "LG"
        ],
        "abstract": "  Graph neural networks (GNN) are a powerful tool for combining imaging and\nnon-imaging medical information for node classification tasks. Cross-network\nnode classification extends GNN techniques to account for domain drift,\nallowing for node classification on an unlabeled target network. In this paper\nwe present OTGCN, a powerful, novel approach to cross-network node\nclassification. This approach leans on concepts from graph convolutional\nnetworks to harness insights from graph data structures while simultaneously\napplying strategies rooted in optimal transport to correct for the domain drift\nthat can occur between samples from different data collection sites. This\nblended approach provides a practical solution for scenarios with many distinct\nforms of data collected across different locations and equipment. We\ndemonstrate the effectiveness of this approach at classifying Autism Spectrum\nDisorder subjects using a blend of imaging and non-imaging data.\n",
        "title": "Population Graph Cross-Network Node Classification for Autism Detection\n  Across Sample Groups",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05479",
        "abstract_url": "http://arxiv.org/abs/2401.05479",
        "authors": [
            {
                "last_name": "Miniak-G\u00f3recka",
                "first_name": "Alicja"
            },
            {
                "last_name": "Podlaski",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Gwizda\u0142\u0142a",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The problem of data clustering is one of the most important in data analysis.\nIt can be problematic when dealing with experimental data characterized by\nmeasurement uncertainties and errors. Our paper proposes a recursive scheme for\nclustering data obtained in geographical (climatological) experiments. The\ndiscussion of results obtained by k-means and SOM methods with the developed\nrecursive procedure is presented. We show that the clustering using the new\napproach gives more acceptable results when compared to experts assessments.\n",
        "title": "The recursive scheme of clustering",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05480",
        "abstract_url": "http://arxiv.org/abs/2401.05480",
        "authors": [
            {
                "last_name": "Zavanelli",
                "first_name": "Nathan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "HC",
            ""
        ],
        "abstract": "  This paper summarizes and presents PulsatioMech: an open-source MATLAB\ntoolbox for seismocardiography (SCG) signal processing. The toolbox may be\nfound here: https://github.com/nzavanelli/SCG_master_toolbox PulsatioMech is\ncurrently under development as a common tool to promote new studies and\ndiscoveries in the use of cardiac mechanical signal for wearable health\nmonitoring. This toolbox is designed to assist users in analyzing SCG signals\nwithout the need to devote significant effort into signal processing and coding\ntasks. Simultaneously, it provides a uniform basis to assess the\nreproducibility of works based on this toolbox, including those cited here\n[1-6]. The referenced works contain a great deal more detail regarding the\nspecific algorithms implemented here, whereas this paper will present a short\noverview of the PulsatioMech Toolbox.\n",
        "title": "PulsatioMech: An Open-Source MATLAB Toolbox for Seismocardiography\n  Signal Processing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05481",
        "abstract_url": "http://arxiv.org/abs/2401.05481",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Siddharth"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  The segmentation of medical images is important for the improvement and\ncreation of healthcare systems, particularly for early disease detection and\ntreatment planning. In recent years, the use of convolutional neural networks\n(CNNs) and other state-of-the-art methods has greatly advanced medical image\nsegmentation. However, CNNs have been found to struggle with learning\nlong-range dependencies and capturing global context due to the limitations of\nconvolution operations. In this paper, we explore the use of transformers and\nCNNs for medical image segmentation and propose a hybrid architecture that\ncombines the ability of transformers to capture global dependencies with the\nability of CNNs to capture low-level spatial details. We compare various\narchitectures and configurations and conduct multiple experiments to evaluate\ntheir effectiveness.\n",
        "title": "Transformer-CNN Fused Architecture for Enhanced Skin Lesion Segmentation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05501",
        "abstract_url": "http://arxiv.org/abs/2401.05501",
        "authors": [
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Carley",
                "first_name": "Kathleen M."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  As digitalization increases, countries employ digital diplomacy, harnessing\ndigital resources to project their desired image. Digital diplomacy also\nencompasses the interactivity of digital platforms, providing a trove of public\nopinion that diplomatic agents can collect. Social media bots actively\nparticipate in political events through influencing political communication and\npurporting coordinated narratives to influence human behavior. This article\nprovides a methodology towards identifying three types of bots: General Bots,\nNews Bots and Bridging Bots, then further identify these classes of bots on\nTwitter during a diplomatic incident involving the United States and China.\nUsing a series of computational methods, this article examines the impact of\nbots on the topics disseminated, the influence and the use of information\nmaneuvers of bots within the social communication network. Among others, our\nresults observe that all three types of bots are present across the two\ncountries; bots geotagged to the US are generally concerned with the balloon\nlocation while those geotagged to China discussed topics related to escalating\ntensions; and perform different extent of positive narrative and network\ninformation maneuvers.\n",
        "title": "Deflating the Chinese Balloon: Types of Twitter Bots in US-China balloon\n  incident",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05502",
        "abstract_url": "http://arxiv.org/abs/2401.05502",
        "authors": [
            {
                "last_name": "Thejaswi",
                "first_name": "Suhas"
            },
            {
                "last_name": "Gadekar",
                "first_name": "Ameet"
            },
            {
                "last_name": "Ordozgoiti",
                "first_name": "Bruno"
            },
            {
                "last_name": "Gionis",
                "first_name": "Aristides"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "",
            "CC",
            "LG"
        ],
        "abstract": "  In this work, we study diversity-aware clustering problems where the data\npoints are associated with multiple attributes resulting in intersecting\ngroups. A clustering solution need to ensure that a minimum number of cluster\ncenters are chosen from each group while simultaneously minimizing the\nclustering objective, which can be either $k$-median, $k$-means or\n$k$-supplier. We present parameterized approximation algorithms with\napproximation ratios $1+ \\frac{2}{e}$, $1+\\frac{8}{e}$ and $3$ for\ndiversity-aware $k$-median, diversity-aware $k$-means and diversity-aware\n$k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH\nand FPT $\\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint\nfaicility groups, we present parameterized approximation algorithm with\napproximation ratios $1+\\frac{2}{e}$ and $1+\\frac{8}{e}$, respectively. For\nfair $k$-supplier with disjoint facility groups, we present a polynomial-time\napproximation algorithm with factor $3$, improving the previous best known\napproximation ratio of factor $5$.\n",
        "title": "Diversity-aware clustering: Computational Complexity and Approximation\n  Algorithms",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05507",
        "abstract_url": "http://arxiv.org/abs/2401.05507",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xueyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Wei",
                "first_name": "Shuang"
            },
            {
                "last_name": "Chai",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Wang",
                "first_name": "Guoyin"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuwu"
            },
            {
                "last_name": "Su",
                "first_name": "Jing"
            },
            {
                "last_name": "Xu",
                "first_name": "Jingjing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jianbo"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Yang",
                "first_name": "Yang"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In this paper, we introduce \"InfiAgent-DABench\", the first benchmark\nspecifically designed to evaluate LLM-based agents in data analysis tasks. This\nbenchmark contains DAEval, a dataset consisting of 311 data analysis questions\nderived from 55 CSV files, and an agent framework to evaluate LLMs as data\nanalysis agents. We adopt a format-prompting technique, ensuring questions to\nbe closed-form that can be automatically evaluated. Our extensive benchmarking\nof 23 state-of-the-art LLMs uncovers the current challenges encountered in data\nanalysis tasks. In addition, we have developed DAAgent, a specialized agent\ntrained on instruction-tuning datasets. Evaluation datasets and toolkits for\nInfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.\n",
        "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05509",
        "abstract_url": "http://arxiv.org/abs/2401.05509",
        "authors": [
            {
                "last_name": "Injadat",
                "first_name": "MohammadNoor"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            ""
        ],
        "abstract": "  The continued growth in the deployment of Internet-of-Things (IoT) devices\nhas been fueled by the increased connectivity demand, particularly in\nindustrial environments. However, this has led to an increase in the number of\nnetwork related attacks due to the increased number of potential attack\nsurfaces. Industrial IoT (IIoT) devices are prone to various network related\nattacks that can have severe consequences on the manufacturing process as well\nas on the safety of the workers in the manufacturing plant. One promising\nsolution that has emerged in recent years for attack detection is Machine\nlearning (ML). More specifically, ensemble learning models have shown great\npromise in improving the performance of the underlying ML models. Accordingly,\nthis paper proposes a framework based on the combined use of Bayesian\nOptimization-Gaussian Process (BO-GP) with an ensemble tree-based learning\nmodel to improve the performance of intrusion and attack detection in IIoT\nenvironments. The proposed framework's performance is evaluated using the\nWindows 10 dataset collected by the Cyber Range and IoT labs at University of\nNew South Wales. Experimental results illustrate the improvement in detection\naccuracy, precision, and F-score when compared to standard tree and ensemble\ntree models.\n",
        "title": "Optimized Ensemble Model Towards Secured Industrial IoT Devices",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05511",
        "abstract_url": "http://arxiv.org/abs/2401.05511",
        "authors": [
            {
                "last_name": "Rogha",
                "first_name": "Milad"
            },
            {
                "last_name": "Sah",
                "first_name": "Subham"
            },
            {
                "last_name": "Karduni",
                "first_name": "Alireza"
            },
            {
                "last_name": "Markant",
                "first_name": "Douglas"
            },
            {
                "last_name": "Dou",
                "first_name": "Wenwen"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  News articles containing data visualizations play an important role in\ninforming the public on issues ranging from public health to politics. Recent\nresearch on the persuasive appeal of data visualizations suggests that prior\nattitudes can be notoriously difficult to change. Inspired by an NYT article,\nwe designed two experiments to evaluate the impact of elicitation and\ncontrasting narratives on attitude change, recall, and engagement. We\nhypothesized that eliciting prior beliefs leads to more elaborative thinking\nthat ultimately results in higher attitude change, better recall, and\nengagement. Our findings revealed that visual elicitation leads to higher\nengagement in terms of feelings of surprise. While there is an overall attitude\nchange across all experiment conditions, we did not observe a significant\neffect of belief elicitation on attitude change. With regard to recall error,\nwhile participants in the draw trend elicitation exhibited significantly lower\nrecall error than participants in the categorize trend condition, we found no\nsignificant difference in recall error when comparing elicitation conditions to\nno elicitation. In a follow-up study, we added contrasting narratives with the\npurpose of making the main visualization (communicating data on the focal\nissue) appear strikingly different. Compared to the results of study 1, we\nfound that contrasting narratives improved engagement in terms of surprise and\ninterest but interestingly resulted in higher recall error and no significant\nchange in attitude. We discuss the effects of elicitation and contrasting\nnarratives in the context of topic involvement and the strengths of temporal\ntrends encoded in the data visualization.\n",
        "title": "The Impact of Elicitation and Contrasting Narratives on Engagement,\n  Recall and Attitude Change with News Articles Containing Data Visualization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05516",
        "abstract_url": "http://arxiv.org/abs/2401.05516",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "GeonU"
            },
            {
                "last_name": "Youwang",
                "first_name": "Kim"
            },
            {
                "last_name": "Oh",
                "first_name": "Tae-Hyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "GR"
        ],
        "abstract": "  We present FPRF, a feed-forward photorealistic style transfer method for\nlarge-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with\narbitrary, multiple style reference images without additional optimization\nwhile preserving multi-view appearance consistency. Prior arts required tedious\nper-style/-scene optimization and were limited to small-scale 3D scenes. FPRF\nefficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D\nneural radiance field, which inherits AdaIN's feed-forward stylization\nmachinery, supporting arbitrary style reference images. Furthermore, FPRF\nsupports multi-reference stylization with the semantic correspondence matching\nand local AdaIN, which adds diverse user control for 3D scene styles. FPRF also\npreserves multi-view consistency by applying semantic matching and style\ntransfer processes directly onto queried features in 3D space. In experiments,\nwe demonstrate that FPRF achieves favorable photorealistic quality 3D scene\nstylization for large-scale scenes with diverse reference images. Project page:\nhttps://kim-geonu.github.io/FPRF/\n",
        "title": "FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D\n  Neural Radiance Fields",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05518",
        "abstract_url": "http://arxiv.org/abs/2401.05518",
        "authors": [
            {
                "last_name": "Panferov",
                "first_name": "Andrei"
            },
            {
                "last_name": "Demidovich",
                "first_name": "Yury"
            },
            {
                "last_name": "Rammal",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Richt\u00e1rik",
                "first_name": "Peter"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "DC",
            ""
        ],
        "abstract": "  Quantization (Alistarh et al., 2017) is an important (stochastic) compression\ntechnique that reduces the volume of transmitted bits during each communication\nround in distributed model training. Suresh et al. (2022) introduce correlated\nquantizers and show their advantages over independent counterparts by analyzing\ndistributed SGD communication complexity. We analyze the forefront distributed\nnon-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the\nproposed correlated quantizers and show that it outperforms the original MARINA\nand distributed SGD of Suresh et al. (2022) with regard to the communication\ncomplexity. We significantly refine the original analysis of MARINA without any\nadditional assumptions using the weighted Hessian variance (Tyurin et al.,\n2022), and then we expand the theoretical framework of MARINA to accommodate a\nsubstantially broader range of potentially correlated and biased compressors,\nthus dilating the applicability of the method beyond the conventional\nindependent unbiased compressor setup. Extensive experimental results\ncorroborate our theoretical findings.\n",
        "title": "Correlated Quantization for Faster Nonconvex Distributed Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05520",
        "abstract_url": "http://arxiv.org/abs/2401.05520",
        "authors": [
            {
                "last_name": "Amadeus",
                "first_name": "Marcellus"
            },
            {
                "last_name": "Casta\u00f1eda",
                "first_name": "William Alberto Cruz"
            },
            {
                "last_name": "Zanella",
                "first_name": "Andr\u00e9 Felipe"
            },
            {
                "last_name": "Mahlow",
                "first_name": "Felipe Rodrigues Perche"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "CL"
        ],
        "abstract": "  Generative AI has become pervasive in society, witnessing significant\nadvancements in various domains. Particularly in the realm of Text-to-Image\n(TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities\nin generating visual content based on textual prompts. This paper addresses the\npotential of LDMs in representing local cultural concepts, historical figures,\nand endangered species. In this study, we use the cultural heritage of Rio\nGrande do Sul (RS), Brazil, as an illustrative case. Our objective is to\ncontribute to the broader understanding of how generative models can help to\ncapture and preserve the cultural and historical identity of regions. The paper\noutlines the methodology, including subject selection, dataset creation, and\nthe fine-tuning process. The results showcase the images generated, alongside\nthe challenges and feasibility of each concept. In conclusion, this work shows\nthe power of these models to represent and preserve unique aspects of diverse\nregions and communities.\n",
        "title": "From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\\'ucho\n  Heritage",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05521",
        "abstract_url": "http://arxiv.org/abs/2401.05521",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Danjie"
            },
            {
                "last_name": "Yang",
                "first_name": "Simon X."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "",
            ""
        ],
        "abstract": "  The paper presents an innovative approach (CBNNTAP) that addresses the\ncomplexities and challenges introduced by ocean currents when optimizing target\nassignment and motion planning for a multi-unmanned underwater vehicle (UUV)\nsystem. The core of the proposed algorithm involves the integration of several\nkey components. Firstly, it incorporates a bio-inspired neural network-based\n(BINN) approach which predicts the most efficient paths for individual UUVs\nwhile simultaneously ensuring collision avoidance among the vehicles. Secondly,\nan efficient target assignment component is integrated by considering the path\ndistances determined by the BINN algorithm. In addition, a critical innovation\nwithin the CBNNTAP algorithm is its capacity to address the disruptive effects\nof ocean currents, where an adjustment component is seamlessly integrated to\ncounteract the deviations caused by these currents, which enhances the accuracy\nof both motion planning and target assignment for the UUVs. The effectiveness\nof the CBNNTAP algorithm is demonstrated through comprehensive simulation\nresults and the outcomes underscore the superiority of the developed algorithm\nin nullifying the effects of static and dynamic ocean currents in 2D and 3D\nscenarios.\n",
        "title": "Current Effect-eliminated Optimal Target Assignment and Motion Planning\n  for a Multi-UUV System",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05523",
        "abstract_url": "http://arxiv.org/abs/2401.05523",
        "authors": [
            {
                "last_name": "Levit",
                "first_name": "Vadim E."
            },
            {
                "last_name": "Mandrescu",
                "first_name": "Eugen"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM",
            "",
            ""
        ],
        "abstract": "  The graph G=(V,E) is called Konig-Egervary if the sum of its independence\nnumber and its matching number equals its order. Let RV(G) denote the number of\nvertices v such that G-v is Konig-Egervary, and let RE(G) denote the number of\nedges e such that G-e is Konig-Egervary. Clearly, RV(G) = |V| and RE(G) = |E|\nfor bipartite graphs. Unlike the bipartiteness, the property of being a\nKonig-Egervary graph is not hereditary. In this paper, we present an equality\nexpressing RV(G) in terms of some graph parameters, and a tight inequality\nbounding RE(G) in terms of the same parameters, when G is Konig-Egervary.\n",
        "title": "On the Number of Vertices/Edges whose Deletion Preserves the\n  Konig-Egervary Property",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05525",
        "abstract_url": "http://arxiv.org/abs/2401.05525",
        "authors": [
            {
                "last_name": "Dinh",
                "first_name": "Lam"
            },
            {
                "last_name": "Quang",
                "first_name": "Pham Tran Anh"
            },
            {
                "last_name": "Leguay",
                "first_name": "J\u00e9r\u00e9mie"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG"
        ],
        "abstract": "  Deep Reinforcement Learning (DRL) algorithms have recently made significant\nstrides in improving network performance. Nonetheless, their practical use is\nstill limited in the absence of safe exploration and safe decision-making. In\nthe context of commercial solutions, reliable and safe-to-operate systems are\nof paramount importance. Taking this problem into account, we propose a safe\nlearning-based load balancing algorithm for Software Defined-Wide Area Network\n(SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with\na Control Barrier Function (CBF). It safely projects unsafe actions into\nfeasible ones during both training and testing, and it guides learning towards\nsafe policies. We successfully implemented the solution on GPU to accelerate\ntraining by approximately 110x times and achieve model updates for on-policy\nmethods within a few seconds, making the solution practical. We show that our\napproach delivers near-optimal Quality-of-Service (QoS performance in terms of\nend-to-end delay while respecting safety requirements related to link capacity\nconstraints. We also demonstrated that on-policy learning based on Proximal\nPolicy Optimization (PPO) performs better than off-policy learning with Deep\nDeterministic Policy Gradient (DDPG) when both are combined with a CBF for safe\nload balancing.\n",
        "title": "Towards Safe Load Balancing based on Control Barrier Functions and Deep\n  Reinforcement Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05529",
        "abstract_url": "http://arxiv.org/abs/2401.05529",
        "authors": [
            {
                "last_name": "Di",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Bingchang"
            },
            {
                "last_name": "Gao",
                "first_name": "Yiyi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  This paper presents a novel fuzzing framework, called MicroFuzz, specifically\ndesigned for Microservices. Mocking-Assisted Seed Execution, Distributed\nTracing, Seed Refresh and Pipeline Parallelism approaches are adopted to\naddress the environmental complexities and dynamics of Microservices and\nimprove the efficiency of fuzzing. MicroFuzz has been successfully implemented\nand deployed in Ant Group, a prominent FinTech company. Its performance has\nbeen evaluated in three distinct industrial scenarios: normalized fuzzing,\niteration testing, and taint verification.Throughout five months of operation,\nMicroFuzz has diligently analyzed a substantial codebase, consisting of 261\nApps with over 74.6 million lines of code (LOC). The framework's effectiveness\nis evident in its detection of 5,718 potential quality or security risks, with\n1,764 of them confirmed and fixed as actual security threats by software\nspecialists. Moreover, MicroFuzz significantly increased program coverage by\n12.24% and detected program behavior by 38.42% in the iteration testing.\n",
        "title": "MicroFuzz: An Efficient Fuzzing Framework for Microservices",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05530",
        "abstract_url": "http://arxiv.org/abs/2401.05530",
        "authors": [
            {
                "last_name": "Salgado",
                "first_name": "Erik Isai Valle"
            },
            {
                "last_name": "Li",
                "first_name": "Chen"
            },
            {
                "last_name": "Han",
                "first_name": "Yaqi"
            },
            {
                "last_name": "Shi",
                "first_name": "Linchao"
            },
            {
                "last_name": "Li",
                "first_name": "Xinghui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Ensemble methods exploit the availability of a given number of classifiers or\ndetectors trained in single or multiple source domains and tasks to address\nmachine learning problems such as domain adaptation or multi-source transfer\nlearning. Existing research measures the domain distance between the sources\nand the target dataset, trains multiple networks on the same data with\ndifferent samples per class, or combines predictions from models trained under\nvaried hyperparameters and settings. Their solutions enhanced the performance\non small or tail categories but hurt the rest. To this end, we propose a\nmodified consensus focus for semi-supervised and long-tailed object detection.\nWe introduce a voting system based on source confidence that spots the\ncontribution of each model in a consensus, lets the user choose the relevance\nof each class in the target label space so that it relaxes minority bounding\nboxes suppression, and combines multiple models' results without discarding the\npoisonous networks. Our tests on synthetic driving datasets retrieved higher\nconfidence and more accurate bounding boxes than the NMS, soft-NMS, and WBF.\n",
        "title": "Consensus Focus for Object Detection and minority classes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05531",
        "abstract_url": "http://arxiv.org/abs/2401.05531",
        "authors": [
            {
                "last_name": "Fischer",
                "first_name": "John"
            },
            {
                "last_name": "Orescanin",
                "first_name": "Marko"
            },
            {
                "last_name": "Eckstrand",
                "first_name": "Eric"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD",
            ""
        ],
        "abstract": "  Transfer learning (TL) is an increasingly popular approach to training deep\nlearning (DL) models that leverages the knowledge gained by training a\nfoundation model on diverse, large-scale datasets for use on downstream tasks\nwhere less domain- or task-specific data is available. The literature is rich\nwith TL techniques and applications; however, the bulk of the research makes\nuse of deterministic DL models which are often uncalibrated and lack the\nability to communicate a measure of epistemic (model) uncertainty in\nprediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models\nare often well-calibrated, provide access to epistemic uncertainty for a\nprediction, and are capable of achieving competitive predictive performance. In\nthis study, we propose variational inference pre-trained audio neural networks\n(VI-PANNs). VI-PANNs are a variational inference variant of the popular\nResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio\nevent detection dataset. We evaluate the quality of the resulting uncertainty\nwhen transferring knowledge from VI-PANNs to other downstream acoustic\nclassification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets. We\ndemonstrate, for the first time, that it is possible to transfer calibrated\nuncertainty information along with knowledge from upstream tasks to enhance a\nmodel's capability to perform downstream tasks.\n",
        "title": "VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational\n  Inference for Improved Generalization in Audio Pattern Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05533",
        "abstract_url": "http://arxiv.org/abs/2401.05533",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Ningfeng"
            },
            {
                "last_name": "Ren",
                "first_name": "Jing"
            },
            {
                "last_name": "Sorkine-Hornung",
                "first_name": "Olga"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  We formalize Italian smocking, an intricate embroidery technique that gathers\nflat fabric into pleats along meandering lines of stitches, resulting in pleats\nthat fold and gather where the stitching veers. In contrast to English\nsmocking, characterized by colorful stitches decorating uniformly shaped\npleats, and Canadian smocking, which uses localized knots to form voluminous\npleats, Italian smocking permits the fabric to move freely along the stitched\nthreads following curved paths, resulting in complex and unpredictable pleats\nwith highly diverse, irregular structures, achieved simply by pulling on the\nthreads. We introduce a novel method for digital previewing of Italian smocking\nresults, given the thread stitching path as input. Our method uses a\ncoarse-grained mass-spring system to simulate the interaction between the\nthreads and the fabric. This configuration guides the fine-level fabric\ndeformation through an adaptation of the state-of-the-art simulator, C-IPC. Our\nmethod models the general problem of fabric-thread interaction and can be\nreadily adapted to preview Canadian smocking as well. We compare our results to\nbaseline approaches and physical fabrications to demonstrate the accuracy of\nour method.\n",
        "title": "Computational Smocking through Fabric-Thread Interaction",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05535",
        "abstract_url": "http://arxiv.org/abs/2401.05535",
        "authors": [
            {
                "last_name": "Dorador",
                "first_name": "Albert"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  Decades after their inception, random forests continue to provide\nstate-of-the-art accuracy in a variety of learning problems, outperforming in\nthis respect alternative machine learning algorithms such as decision trees or\neven neural networks. However, being an ensemble method, the one aspect where\nrandom forests tend to severely underperform decision trees is\ninterpretability. In the present work, we propose a post-hoc approach that aims\nto have the best of both worlds: the accuracy of random forests and the\ninterpretability of decision trees. To this end, we present two forest-pruning\nmethods to find an optimal sub-forest within a given random forest, and then,\nwhen applicable, combine the selected trees into one. Our first method relies\non constrained exhaustive search, while our second method is based on an\nadaptation of the LASSO methodology. Extensive experiments over synthetic and\nreal world datasets show that, in the majority of scenarios, at least one of\nthe two methods proposed is more accurate than the original random forest,\nwhile just using a small fraction of the trees, aiding result interpretability.\nCompared to current state-of-the-art forestpruning methods, namely sequential\nforward selection and (a variation of) sequential backward selection, our\nmethods tend to outperform both of them, whether in terms of accuracy, number\nof trees employed, or both.\n",
        "title": "Improving the Accuracy and Interpretability of Random Forests via Forest\n  Pruning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05538",
        "abstract_url": "http://arxiv.org/abs/2401.05538",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Le Ngu"
            },
            {
                "last_name": "Casado",
                "first_name": "Constantino \u00c1lvarez"
            },
            {
                "last_name": "Ca\u00f1ellas",
                "first_name": "Manuel Lage"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Anirban"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Nhi"
            },
            {
                "last_name": "Jayagopi",
                "first_name": "Dinesh Babu"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Miguel Bordallo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Radio frequency (RF) signals have facilitated the development of non-contact\nhuman monitoring tasks, such as vital signs measurement, activity recognition,\nand user identification. In some specific scenarios, an RF signal analysis\nframework may prioritize the performance of one task over that of others. In\nresponse to this requirement, we employ a multi-objective optimization approach\ninspired by biological principles to select discriminative features that\nenhance the accuracy of breathing patterns recognition while simultaneously\nimpeding the identification of individual users. This approach is validated\nusing a novel vital signs dataset consisting of 50 subjects engaged in four\ndistinct breathing patterns. Our findings indicate a remarkable result: a\nsubstantial divergence in accuracy between breathing recognition and user\nidentification. As a complementary viewpoint, we present a contrariwise result\nto maximize user identification accuracy and minimize the system's capacity for\nbreathing activity recognition.\n",
        "title": "Multi-objective Feature Selection in Remote Health Monitoring\n  Applications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05544",
        "abstract_url": "http://arxiv.org/abs/2401.05544",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yong"
            },
            {
                "last_name": "Luo",
                "first_name": "Senlin"
            },
            {
                "last_name": "Shang",
                "first_name": "Yu-Ming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yifei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengjun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Researchers have explored the potential of utilizing pre-trained language\nmodels, such as CodeBERT, to improve source code-related tasks. Previous\nstudies have mainly relied on CodeBERT's text embedding capability and the\n`[CLS]' sentence embedding information as semantic representations for\nfine-tuning downstream source code-related tasks. However, these methods\nrequire additional neural network layers to extract effective features,\nresulting in higher computational costs. Furthermore, existing approaches have\nnot leveraged the rich knowledge contained in both source code and related\ntext, which can lead to lower accuracy. This paper presents a novel approach,\nCodePrompt, which utilizes rich knowledge recalled from a pre-trained model by\nprompt learning and an attention mechanism to improve source code-related\nclassification tasks. Our approach initially motivates the language model with\nprompt information to retrieve abundant knowledge associated with the input as\nrepresentative features, thus avoiding the need for additional neural network\nlayers and reducing computational costs. Subsequently, we employ an attention\nmechanism to aggregate multiple layers of related knowledge for each task as\nfinal features to boost their accuracy. We conducted extensive experiments on\nfour downstream source code-related tasks to evaluate our approach and our\nresults demonstrate that CodePrompt achieves new state-of-the-art performance\non the accuracy metric while also exhibiting computation cost-saving\ncapabilities.\n",
        "title": "CodePrompt: Improving Source Code-Related Classification with Knowledge\n  Features through Prompt Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05548",
        "abstract_url": "http://arxiv.org/abs/2401.05548",
        "authors": [
            {
                "last_name": "Machetti",
                "first_name": "Simone"
            },
            {
                "last_name": "Schiavone",
                "first_name": "Pasquale Davide"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Thomas Christoph"
            },
            {
                "last_name": "Pe\u00f3n-Quir\u00f3s",
                "first_name": "Miguel"
            },
            {
                "last_name": "Atienza",
                "first_name": "David"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The field of edge computing has witnessed remarkable growth owing to the\nincreasing demand for real-time processing of data in applications. However,\nchallenges persist due to limitations in performance and power consumption. To\novercome these challenges, heterogeneous architectures have emerged that\ncombine host processors with specialized accelerators tailored to specific\napplications, leading to improved performance and reduced power consumption.\nHowever, most of the existing platforms lack the necessary configurability and\nextendability options for integrating custom accelerators. To overcome these\nlimitations, we introduce in this paper the eXtendible Heterogeneous\nEnergy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed\nto natively support the integration of ultra-low-power edge accelerators. It\nprovides customization options to match specific application requirements by\nexploring various core types, bus topologies, addressing modes, memory sizes,\nand peripherals. Moreover, the platform prioritizes energy efficiency by\nimplementing low-power strategies, such as clock-gating and power-gating. We\ndemonstrate the real-world applicability of X-HEEP by providing an integration\nexample tailored for healthcare applications that includes a coarse-grained\nreconfigurable array (CGRA) and in-memory computing (IMC) accelerators. The\nresulting design, called HEEPocrates, has been implemented both in field\nprogrammable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with\nTSMC 65 nm low-power CMOS technology. We run a set of healthcare applications\nand measure their energy consumption to demonstrate the alignment of our chip\nwith other state-of-the-art microcontrollers commonly adopted in this domain.\nMoreover, we showcase the energy benefit of 4.9 x gained by exploiting the\nintegrated CGRA accelerator, compared to running on the host CPU.\n",
        "title": "X-HEEP: An Open-Source, Configurable and Extendible RISC-V\n  Microcontroller for the Exploration of Ultra-Low-Power Edge Accelerators",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05551",
        "abstract_url": "http://arxiv.org/abs/2401.05551",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Changye"
            },
            {
                "last_name": "Xu",
                "first_name": "Weizhe"
            },
            {
                "last_name": "Cohen",
                "first_name": "Trevor"
            },
            {
                "last_name": "Pakhomov",
                "first_name": "Serguei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  \\textbf{Objectives}: We aimed to investigate how errors from automatic speech\nrecognition (ASR) systems affect dementia classification accuracy, specifically\nin the ``Cookie Theft'' picture description task. We aimed to assess whether\nimperfect ASR-generated transcripts could provide valuable information for\ndistinguishing between language samples from cognitively healthy individuals\nand those with Alzheimer's disease (AD).\n  \\textbf{Methods}: We conducted experiments using various ASR models, refining\ntheir transcripts with post-editing techniques. Both these imperfect ASR\ntranscripts and manually transcribed ones were used as inputs for the\ndownstream dementia classification. We conducted comprehensive error analysis\nto compare model performance and assess ASR-generated transcript effectiveness\nin dementia classification.\n  \\textbf{Results}: Imperfect ASR-generated transcripts surprisingly\noutperformed manual transcription for distinguishing between individuals with\nAD and those without in the ``Cookie Theft'' task. These ASR-based models\nsurpassed the previous state-of-the-art approach, indicating that ASR errors\nmay contain valuable cues related to dementia. The synergy between ASR and\nclassification models improved overall accuracy in dementia classification.\n  \\textbf{Conclusion}: Imperfect ASR transcripts effectively capture linguistic\nanomalies linked to dementia, improving accuracy in classification tasks. This\nsynergy between ASR and classification models underscores ASR's potential as a\nvaluable tool in assessing cognitive impairment and related clinical\napplications.\n",
        "title": "Useful Blunders: Can Automated Speech Recognition Errors Improve\n  Downstream Dementia Classification?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05553",
        "abstract_url": "http://arxiv.org/abs/2401.05553",
        "authors": [
            {
                "last_name": "Pareschi",
                "first_name": "Lorenzo"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Probably one of the most striking examples of the close connections between\nglobal optimization processes and statistical physics is the simulated\nannealing method, inspired by the famous Monte Carlo algorithm devised by\nMetropolis et al. in the middle of the last century. In this paper we show how\nthe tools of linear kinetic theory allow to describe this gradient-free\nalgorithm from the perspective of statistical physics and how convergence to\nthe global minimum can be related to classical entropy inequalities. This\nanalysis highlight the strong link between linear Boltzmann equations and\nstochastic optimization methods governed by Markov processes. Thanks to this\nformalism we can establish the connections between the simulated annealing\nprocess and the corresponding mean-field Langevin dynamics characterized by a\nstochastic gradient descent approach. Generalizations to other selection\nstrategies in simulated annealing that avoid the acceptance-rejection dynamic\nare also provided.\n",
        "title": "Optimization by linear kinetic equations and mean-field Langevin\n  dynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05554",
        "abstract_url": "http://arxiv.org/abs/2401.05554",
        "authors": [
            {
                "last_name": "Lo",
                "first_name": "John"
            },
            {
                "last_name": "Parslew",
                "first_name": "Ben"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            ""
        ],
        "abstract": "  Previous design methodologies for spring-driven jumping robots focused on\njump height optimization for specific tasks. In doing so, numerous designs have\nbeen proposed including using nonlinear spring-linkages to increase the elastic\nenergy storage and jump height. However, these systems can never achieve their\ntheoretical maximum jump height due to taking off before the spring energy is\nfully released, resulting in an incomplete transfer of stored elastic energy to\ngravitational potential energy. This paper presents low-order models aimed at\ncharacterising the energy conversion during the acceleration phase of jumping.\nIt also proposes practical solutions for increasing the energy efficiency of\njumping robots. A dynamic analysis is conducted on a multibody system comprised\nof rotational links, which is experimentally validated using a physical\ndemonstrator. The analysis reveals that inefficient energy conversion is\nattributed to inertial effects caused by rotational and unsprung masses. Since\nthese masses cannot be entirely eliminated from a physical linkage, a practical\napproach to improving energy efficiency involves structural redesign to reduce\nstructural mass and moments of inertia while maintaining compliance with\nstructural strength and stiffness requirements.\n",
        "title": "Characterising the take-off dynamics and energy efficiency in\n  spring-driven jumping robots",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05558",
        "abstract_url": "http://arxiv.org/abs/2401.05558",
        "authors": [
            {
                "last_name": "Asinowski",
                "first_name": "Andrei"
            },
            {
                "last_name": "Banderier",
                "first_name": "Cyril"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "CG",
            "FL",
            ""
        ],
        "abstract": "  We enumerate several classes of pattern-avoiding rectangulations. We\nestablish bijective links with pattern-avoiding permutations, prove that their\ngenerating functions are algebraic, and confirm several conjectures by Merino\nand M\\\"utze. We also analyze a new class of rectangulations, called whirls,\nusing a generating tree.\n",
        "title": "From geometry to generating functions: rectangulations and permutations",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05561",
        "abstract_url": "http://arxiv.org/abs/2401.05561",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Lichao"
            },
            {
                "last_name": "Huang",
                "first_name": "Yue"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Wu",
                "first_name": "Siyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qihui"
            },
            {
                "last_name": "Gao",
                "first_name": "Chujie"
            },
            {
                "last_name": "Huang",
                "first_name": "Yixin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Wenhan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yixuan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiner"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengliang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yixin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yijue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhikun"
            },
            {
                "last_name": "Kailkhura",
                "first_name": "Bhavya"
            },
            {
                "last_name": "Xiong",
                "first_name": "Caiming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            },
            {
                "last_name": "Xiao",
                "first_name": "Chaowei"
            },
            {
                "last_name": "Li",
                "first_name": "Chunyuan"
            },
            {
                "last_name": "Xing",
                "first_name": "Eric"
            },
            {
                "last_name": "Huang",
                "first_name": "Furong"
            },
            {
                "last_name": "Liu",
                "first_name": "Hao"
            },
            {
                "last_name": "Ji",
                "first_name": "Heng"
            },
            {
                "last_name": "Wang",
                "first_name": "Hongyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Huan"
            },
            {
                "last_name": "Yao",
                "first_name": "Huaxiu"
            },
            {
                "last_name": "Kellis",
                "first_name": "Manolis"
            },
            {
                "last_name": "Zitnik",
                "first_name": "Marinka"
            },
            {
                "last_name": "Jiang",
                "first_name": "Meng"
            },
            {
                "last_name": "Bansal",
                "first_name": "Mohit"
            },
            {
                "last_name": "Zou",
                "first_name": "James"
            },
            {
                "last_name": "Pei",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jian"
            },
            {
                "last_name": "Gao",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Han",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jieyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiliang"
            },
            {
                "last_name": "Wang",
                "first_name": "Jindong"
            },
            {
                "last_name": "Mitchell",
                "first_name": "John"
            },
            {
                "last_name": "Shu",
                "first_name": "Kai"
            },
            {
                "last_name": "Xu",
                "first_name": "Kaidi"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            },
            {
                "last_name": "He",
                "first_name": "Lifang"
            },
            {
                "last_name": "Huang",
                "first_name": "Lifu"
            },
            {
                "last_name": "Backes",
                "first_name": "Michael"
            },
            {
                "last_name": "Gong",
                "first_name": "Neil Zhenqiang"
            },
            {
                "last_name": "Yu",
                "first_name": "Philip S."
            },
            {
                "last_name": "Chen",
                "first_name": "Pin-Yu"
            },
            {
                "last_name": "Gu",
                "first_name": "Quanquan"
            },
            {
                "last_name": "Xu",
                "first_name": "Ran"
            },
            {
                "last_name": "Ying",
                "first_name": "Rex"
            },
            {
                "last_name": "Ji",
                "first_name": "Shuiwang"
            },
            {
                "last_name": "Jana",
                "first_name": "Suman"
            },
            {
                "last_name": "Chen",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianming"
            },
            {
                "last_name": "Zhou",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Willian"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiangliang"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Xie",
                "first_name": "Xing"
            },
            {
                "last_name": "Chen",
                "first_name": "Xun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Yan"
            },
            {
                "last_name": "Ye",
                "first_name": "Yanfang"
            },
            {
                "last_name": "Cao",
                "first_name": "Yinzhi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.\n",
        "title": "TrustLLM: Trustworthiness in Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05562",
        "abstract_url": "http://arxiv.org/abs/2401.05562",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhangchen"
            },
            {
                "last_name": "Jiang",
                "first_name": "Fengqing"
            },
            {
                "last_name": "Niu",
                "first_name": "Luyao"
            },
            {
                "last_name": "Jia",
                "first_name": "Jinyuan"
            },
            {
                "last_name": "Poovendran",
                "first_name": "Radha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "DC"
        ],
        "abstract": "  Federated learning (FL) enables multiple participants to train a global\nmachine learning model without sharing their private training data.\nPeer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating\nthe server that aggregates local models from participants and then updates the\nglobal model. However, P2P FL is vulnerable to (i) honest-but-curious\nparticipants whose objective is to infer private training data of other\nparticipants, and (ii) Byzantine participants who can transmit arbitrarily\nmanipulated local models to corrupt the learning process. P2P FL schemes that\nsimultaneously guarantee Byzantine resilience and preserve privacy have been\nless studied. In this paper, we develop Brave, a protocol that ensures\nByzantine Resilience And privacy-preserving property for P2P FL in the presence\nof both types of adversaries. We show that Brave preserves privacy by\nestablishing that any honest-but-curious adversary cannot infer other\nparticipants' private data by observing their models. We further prove that\nBrave is Byzantine-resilient, which guarantees that all benign participants\nconverge to an identical model that deviates from a global model trained\nwithout Byzantine adversaries by a bounded distance. We evaluate Brave against\nthree state-of-the-art adversaries on a P2P FL for image classification tasks\non benchmark datasets CIFAR10 and MNIST. Our results show that the global model\nlearned with Brave in the presence of adversaries achieves comparable\nclassification accuracy to a global model trained in the absence of any\nadversary.\n",
        "title": "Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated\n  Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05563",
        "abstract_url": "http://arxiv.org/abs/2401.05563",
        "authors": [
            {
                "last_name": "Dwarakanath",
                "first_name": "Kshama"
            },
            {
                "last_name": "Vyetrenko",
                "first_name": "Svitlana"
            },
            {
                "last_name": "Oyebode",
                "first_name": "Toks"
            },
            {
                "last_name": "Balch",
                "first_name": "Tucker"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Is transparency always beneficial in complex systems such as traffic networks\nand stock markets? How is transparency defined in multi-agent systems, and what\nis its optimal degree at which social welfare is highest? We take an\nagent-based view to define transparency (or its lacking) as delay in agent\nobservability of environment states, and utilize simulations to analyze the\nimpact of delay on social welfare. To model the adaptation of agent strategies\nwith varying delays, we model agents as learners maximizing the same objectives\nunder different delays in a simulated environment. Focusing on two agent types\n- constrained and unconstrained, we use multi-agent reinforcement learning to\nevaluate the impact of delay on agent outcomes and social welfare. Empirical\ndemonstration of our framework in simulated financial markets shows opposing\ntrends in outcomes of the constrained and unconstrained agents with delay, with\nan optimal partial transparency regime at which social welfare is maximal.\n",
        "title": "Transparency as Delayed Observability in Multi-Agent Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05566",
        "abstract_url": "http://arxiv.org/abs/2401.05566",
        "authors": [
            {
                "last_name": "Hubinger",
                "first_name": "Evan"
            },
            {
                "last_name": "Denison",
                "first_name": "Carson"
            },
            {
                "last_name": "Mu",
                "first_name": "Jesse"
            },
            {
                "last_name": "Lambert",
                "first_name": "Mike"
            },
            {
                "last_name": "Tong",
                "first_name": "Meg"
            },
            {
                "last_name": "MacDiarmid",
                "first_name": "Monte"
            },
            {
                "last_name": "Lanham",
                "first_name": "Tamera"
            },
            {
                "last_name": "Ziegler",
                "first_name": "Daniel M."
            },
            {
                "last_name": "Maxwell",
                "first_name": "Tim"
            },
            {
                "last_name": "Cheng",
                "first_name": "Newton"
            },
            {
                "last_name": "Jermyn",
                "first_name": "Adam"
            },
            {
                "last_name": "Askell",
                "first_name": "Amanda"
            },
            {
                "last_name": "Radhakrishnan",
                "first_name": "Ansh"
            },
            {
                "last_name": "Anil",
                "first_name": "Cem"
            },
            {
                "last_name": "Duvenaud",
                "first_name": "David"
            },
            {
                "last_name": "Ganguli",
                "first_name": "Deep"
            },
            {
                "last_name": "Barez",
                "first_name": "Fazl"
            },
            {
                "last_name": "Clark",
                "first_name": "Jack"
            },
            {
                "last_name": "Ndousse",
                "first_name": "Kamal"
            },
            {
                "last_name": "Sachan",
                "first_name": "Kshitij"
            },
            {
                "last_name": "Sellitto",
                "first_name": "Michael"
            },
            {
                "last_name": "Sharma",
                "first_name": "Mrinank"
            },
            {
                "last_name": "DasSarma",
                "first_name": "Nova"
            },
            {
                "last_name": "Grosse",
                "first_name": "Roger"
            },
            {
                "last_name": "Kravec",
                "first_name": "Shauna"
            },
            {
                "last_name": "Bai",
                "first_name": "Yuntao"
            },
            {
                "last_name": "Witten",
                "first_name": "Zachary"
            },
            {
                "last_name": "Favaro",
                "first_name": "Marina"
            },
            {
                "last_name": "Brauner",
                "first_name": "Jan"
            },
            {
                "last_name": "Karnofsky",
                "first_name": "Holden"
            },
            {
                "last_name": "Christiano",
                "first_name": "Paul"
            },
            {
                "last_name": "Bowman",
                "first_name": "Samuel R."
            },
            {
                "last_name": "Graham",
                "first_name": "Logan"
            },
            {
                "last_name": "Kaplan",
                "first_name": "Jared"
            },
            {
                "last_name": "Mindermann",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Greenblatt",
                "first_name": "Ryan"
            },
            {
                "last_name": "Shlegeris",
                "first_name": "Buck"
            },
            {
                "last_name": "Schiefer",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Perez",
                "first_name": "Ethan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "CL",
            "LG",
            "SE"
        ],
        "abstract": "  Humans are capable of strategically deceptive behavior: behaving helpfully in\nmost situations, but then behaving very differently in order to pursue\nalternative objectives when given the opportunity. If an AI system learned such\na deceptive strategy, could we detect it and remove it using current\nstate-of-the-art safety training techniques? To study this question, we\nconstruct proof-of-concept examples of deceptive behavior in large language\nmodels (LLMs). For example, we train models that write secure code when the\nprompt states that the year is 2023, but insert exploitable code when the\nstated year is 2024. We find that such backdoored behavior can be made\npersistent, so that it is not removed by standard safety training techniques,\nincluding supervised fine-tuning, reinforcement learning, and adversarial\ntraining (eliciting unsafe behavior and then training to remove it). The\nbackdoored behavior is most persistent in the largest models and in models\ntrained to produce chain-of-thought reasoning about deceiving the training\nprocess, with the persistence remaining even when the chain-of-thought is\ndistilled away. Furthermore, rather than removing backdoors, we find that\nadversarial training can teach models to better recognize their backdoor\ntriggers, effectively hiding the unsafe behavior. Our results suggest that,\nonce a model exhibits deceptive behavior, standard techniques could fail to\nremove such deception and create a false impression of safety.\n",
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety\n  Training",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05568",
        "abstract_url": "http://arxiv.org/abs/2401.05568",
        "authors": [
            {
                "last_name": "Vandermause",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Johansson",
                "first_name": "Anders"
            },
            {
                "last_name": "Miao",
                "first_name": "Yucong"
            },
            {
                "last_name": "Vlassak",
                "first_name": "Joost J."
            },
            {
                "last_name": "Kozinsky",
                "first_name": "Boris"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG",
            ""
        ],
        "abstract": "  Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range of\nbiomedical and engineering devices, but direct molecular dynamics simulations\nof the martensitic B19' -> B2 phase transition driving its shape-memory\nbehavior are rare and have relied on classical force fields with limited\naccuracy. Here, we train four machine-learned force fields for equiatomic NiTi\nbased on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trained\non the fly during NPT molecular dynamics, with DFT calculations and model\nupdates performed automatically whenever the uncertainty of a local energy\nprediction exceeds a chosen threshold. The models achieve accuracies of 1-2\nmeV/atom during training and are shown to closely track DFT predictions of B2\nand B19' elastic constants and phonon frequencies. Surprisingly, in large-scale\nmolecular dynamics simulations, only the SCAN model predicts a reversible B19'\n-> B2 phase transition, with the LDA, PBE, and PBEsol models predicting a\nreversible transition to a previously uncharacterized low-volume phase, which\nwe hypothesize to be a new stable high-pressure phase. We examine the structure\nof the new phase and estimate its stability on the temperature-pressure phase\ndiagram. This work establishes an automated active learning protocol for\nstudying displacive transformations, reveals important differences between DFT\nfunctionals that can only be detected in large-scale simulations, provides an\naccurate force field for NiTi, and identifies a new phase.\n",
        "title": "Phase discovery with active learning: Application to structural phase\n  transitions in equiatomic NiTi",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05569",
        "abstract_url": "http://arxiv.org/abs/2401.05569",
        "authors": [
            {
                "last_name": "Ozen",
                "first_name": "Irfan"
            },
            {
                "last_name": "Subramani",
                "first_name": "Karthika"
            },
            {
                "last_name": "Vadrevu",
                "first_name": "Phani"
            },
            {
                "last_name": "Perdisci",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Social engineering (SE) aims at deceiving users into performing actions that\nmay compromise their security and privacy. These threats exploit weaknesses in\nhuman's decision making processes by using tactics such as pretext, baiting,\nimpersonation, etc. On the web, SE attacks include attack classes such as\nscareware, tech support scams, survey scams, sweepstakes, etc., which can\nresult in sensitive data leaks, malware infections, and monetary loss. For\ninstance, US consumers lose billions of dollars annually due to various SE\nattacks. Unfortunately, generic social engineering attacks remain understudied,\ncompared to other important threats, such as software vulnerabilities and\nexploitation, network intrusions, malicious software, and phishing. The few\nexisting technical studies that focus on social engineering are limited in\nscope and mostly focus on measurements rather than developing a generic\ndefense. To fill this gap, we present SEShield, a framework for in-browser\ndetection of social engineering attacks. SEShield consists of three main\ncomponents: (i) a custom security crawler, called SECrawler, that is dedicated\nto scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet,\na deep learning-based image classifier trained on data collected by SECrawler\nthat aims to detect the often glaring visual traits of SE attack pages; and\n(iii) SEGuard, a proof-of-concept extension that embeds SENet into the web\nbrowser and enables real-time SE attack detection. We perform an extensive\nevaluation of our system and show that SENet is able to detect new instances of\nSE attacks with a detection rate of up to 99.6% at 1% false positive, thus\nproviding an effective first defense against SE attacks on the web.\n",
        "title": "SENet: Visual Detection of Online Social Engineering Attack Campaigns",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05570",
        "abstract_url": "http://arxiv.org/abs/2401.05570",
        "authors": [
            {
                "last_name": "Van Vorst",
                "first_name": "Kevin"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  Self-supervised learning has become a popular way to pretrain a deep learning\nmodel and then transfer it to perform downstream tasks. However, most of these\nmethods are developed on large-scale image datasets that contain natural\nobjects with clear textures, outlines, and distinct color contrasts. It remains\nuncertain whether these methods are equally effective for medical imaging,\nwhere the regions of interest often blend subtly and indistinctly with the\nsurrounding tissues. In this study, we propose an alternative method that uses\ncontralateral mammograms to train a neural network to encode similar embeddings\nwhen a pair contains both normal images and different embeddings when a pair\ncontains normal and abnormal images. Our approach leverages the natural\nsymmetry of human body as weak labels to learn to distinguish abnormal lesions\nfrom background tissues in a fully unsupervised manner. Our findings suggest\nthat it's feasible by incorporating soft labels derived from the Euclidean\ndistances between the embeddings of the image pairs into the Siamese network\nloss. Our method demonstrates superior performance in mammogram patch\nclassification compared to existing self-supervised learning methods. This\napproach not only leverages a vast amount of image data effectively but also\nminimizes reliance on costly labels, a significant advantage particularly in\nthe field of medical imaging.\n",
        "title": "Siamese Networks with Soft Labels for Unsupervised Lesion Detection and\n  Patch Pretraining on Screening Mammograms",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05571",
        "abstract_url": "http://arxiv.org/abs/2401.05571",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hanrui"
            },
            {
                "last_name": "Gu",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Zirui"
            },
            {
                "last_name": "Pan",
                "first_name": "David Z."
            },
            {
                "last_name": "Chong",
                "first_name": "Frederic T."
            },
            {
                "last_name": "Han",
                "first_name": "Song"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhangyang"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "AR",
            "LG"
        ],
        "abstract": "  Parameterized Quantum Circuits (PQC) have obtained increasing popularity\nthanks to their great potential for near-term Noisy Intermediate-Scale Quantum\n(NISQ) computers. Achieving quantum advantages usually requires a large number\nof qubits and quantum circuits with enough capacity. However, limited coherence\ntime and massive quantum noises severely constrain the size of quantum circuits\nthat can be executed reliably on real machines. To address these two pain\npoints, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive\nquantum circuits, aiming to achieve two key objectives: (1) implicit circuits\ncapacity during training - by dynamically exploring the circuit's sparse\nconnectivity and sticking a fixed small number of quantum gates throughout the\ntraining which satisfies the coherence time and enjoy light noises, enabling\nfeasible executions on real quantum devices; (2) noise robustness - by jointly\noptimizing the topology and parameters of quantum circuits under real device\nnoise models. In each update step of sparsity, we leverage the moving average\nof historical gradients to grow necessary gates and utilize salience-based\npruning to eliminate insignificant gates. Extensive experiments are conducted\nwith 7 Quantum Machine Learning (QML) and Variational Quantum Eigensolver (VQE)\nbenchmarks on 6 simulated or real quantum computers, where QuantumSEA\nconsistently surpasses noise-aware search, human-designed, and randomly\ngenerated quantum circuit baselines by a clear performance margin. For example,\neven in the most challenging on-chip training regime, our method establishes\nstate-of-the-art results with only half the number of quantum gates and ~2x\ntime saving of circuit executions. Codes are available at\nhttps://github.com/VITA-Group/QuantumSEA.\n",
        "title": "QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum\n  Circuits",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05572",
        "abstract_url": "http://arxiv.org/abs/2401.05572",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Qin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "MA",
            "RO"
        ],
        "abstract": "  Innate values describe agents' intrinsic motivations, which reflect their\ninherent interests and preferences to pursue goals and drive them to develop\ndiverse skills satisfying their various needs. The essence of reinforcement\nlearning (RL) is learning from interaction based on reward-driven (such as\nutilities) behaviors, much like natural agents. It is an excellent model to\ndescribe the innate-values-driven (IV) behaviors of AI agents. Especially in\nmulti-agent systems (MAS), building the awareness of AI agents to balance the\ngroup utilities and system costs and satisfy group members' needs in their\ncooperation is a crucial problem for individuals learning to support their\ncommunity and integrate human society in the long term. This paper proposes a\nhierarchical compound intrinsic value reinforcement learning model --\ninnate-values-driven reinforcement learning termed IVRL to describe the complex\nbehaviors of multi-agent interaction in their cooperation. We implement the\nIVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment and\ncompare the cooperative performance within three characteristics of innate\nvalue agents (Coward, Neutral, and Reckless) through three benchmark\nmulti-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate that\nby organizing individual various needs rationally, the group can achieve better\nperformance with lower costs effectively.\n",
        "title": "Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent\n  Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05577",
        "abstract_url": "http://arxiv.org/abs/2401.05577",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Chenbin"
            },
            {
                "last_name": "Yaman",
                "first_name": "Burhaneddin"
            },
            {
                "last_name": "Nesti",
                "first_name": "Tommaso"
            },
            {
                "last_name": "Mallik",
                "first_name": "Abhirup"
            },
            {
                "last_name": "Allievi",
                "first_name": "Alessandro G"
            },
            {
                "last_name": "Velipasalar",
                "first_name": "Senem"
            },
            {
                "last_name": "Ren",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Autonomous driving is a complex and challenging task that aims at safe motion\nplanning through scene understanding and reasoning. While vision-only\nautonomous driving methods have recently achieved notable performance, through\nenhanced scene understanding, several key issues, including lack of reasoning,\nlow generalization performance and long-tail scenarios, still need to be\naddressed. In this paper, we present VLP, a novel Vision-Language-Planning\nframework that exploits language models to bridge the gap between linguistic\nunderstanding and autonomous driving. VLP enhances autonomous driving systems\nby strengthening both the source memory foundation and the self-driving car's\ncontextual understanding. VLP achieves state-of-the-art end-to-end planning\nperformance on the challenging NuScenes dataset by achieving 35.9\\% and 60.5\\%\nreduction in terms of average L2 error and collision rates, respectively,\ncompared to the previous best method. Moreover, VLP shows improved performance\nin challenging long-tail scenarios and strong generalization capabilities when\nfaced with new urban environments.\n",
        "title": "VLP: Vision Language Planning for Autonomous Driving",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05578",
        "abstract_url": "http://arxiv.org/abs/2401.05578",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Zang",
                "first_name": "Zhenya"
            },
            {
                "last_name": "Li",
                "first_name": "Xingda"
            },
            {
                "last_name": "Li",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a rapid and precise analytical approach for analyzing cerebral\nblood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the\napplication of the Extreme Learning Machine (ELM). Our evaluation of ELM and\nexisting algorithms involves a comprehensive set of metrics. We assess these\nalgorithms using synthetic datasets for both semi-infinite and multi-layer\nmodels. The results demonstrate that ELM consistently achieves higher fidelity\nacross various noise levels and optical parameters, showcasing robust\ngeneralization ability and outperforming iterative fitting algorithms. Through\na comparison with a computationally efficient neural network, ELM attains\ncomparable accuracy with reduced training and inference times. Notably, the\nabsence of a back-propagation process in ELM during training results in\nsignificantly faster training speeds compared to existing neural network\napproaches. This proposed strategy holds promise for edge computing\napplications with online training capabilities.\n",
        "title": "Fast Cerebral Blood Flow Analysis via Extreme Learning Machine",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05579",
        "abstract_url": "http://arxiv.org/abs/2401.05579",
        "authors": [
            {
                "last_name": "Raihan",
                "first_name": "Ahmed Shoyeb"
            },
            {
                "last_name": "Khosravi",
                "first_name": "Hamed"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Tanveer Hossain"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Imtiaz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry,\noffering benefits like intricate design, minimal waste, rapid prototyping,\nmaterial versatility, and customized solutions. However, its full industry\nadoption faces hurdles, particularly in achieving consistent product quality. A\ncrucial aspect for MAM's success is understanding the relationship between\nprocess parameters and melt pool characteristics. Integrating Artificial\nIntelligence (AI) into MAM is essential. Traditional machine learning (ML)\nmethods, while effective, depend on large datasets to capture complex\nrelationships, a significant challenge in MAM due to the extensive time and\nresources required for dataset creation. Our study introduces a novel\nsurprise-guided sequential learning framework, SurpriseAF-BO, signaling a\nsignificant shift in MAM. This framework uses an iterative, adaptive learning\nprocess, modeling the dynamics between process parameters and melt pool\ncharacteristics with limited data, a key benefit in MAM's cyber manufacturing\ncontext. Compared to traditional ML models, our sequential learning method\nshows enhanced predictive accuracy for melt pool dimensions. Further improving\nour approach, we integrated a Conditional Tabular Generative Adversarial\nNetwork (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This produces\nsynthetic data resembling real experimental data, improving learning\neffectiveness. This enhancement boosts predictive precision without requiring\nadditional physical experiments. Our study demonstrates the power of advanced\ndata-driven techniques in cyber manufacturing and the substantial impact of\nsequential AI and ML, particularly in overcoming MAM's traditional challenges.\n",
        "title": "An Augmented Surprise-guided Sequential Learning Framework for\n  Predicting the Melt Pool Geometry",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05580",
        "abstract_url": "http://arxiv.org/abs/2401.05580",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Li",
                "first_name": "Xingda"
            },
            {
                "last_name": "Li",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique\nthat measures the tissue blood flow, by using near-infrared coherent\npoint-source illumination to detect spectral changes. While machine learning\nhas demonstrated significant potential for measuring blood flow index (BFi), an\nopen question concerning the success of this approach pertains to its\nrobustness in scenarios involving deviations between datasets with varying\nSignal-to-Noise Ratios (SNRs) originating from diverse clinical applications\nand various setups. This study proposes a transfer learning approach, aims to\nassess the influence of SNRs on the generalization ability of learned features,\nand demonstrate the robustness for transfer learning. A synthetic dataset with\nvarying levels of added noise is utilized to simulate different SNRs. The\nproposed network takes a 1x64 autocorrelation curve as input and generates BFi\nand the correlation parameter beta. The proposed model demonstrates excellent\nperformance across different SNRs, exhibiting enhanced fitting accuracy,\nparticularly for low SNR datasets when compared with other fitting methods.\nThis highlights its potential for clinical diagnosis and treatment across\nvarious scenarios under different clinical setups.\n",
        "title": "Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A\n  Transfer Learning Approach with Noise Robustness Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05583",
        "abstract_url": "http://arxiv.org/abs/2401.05583",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chaoyang"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Peiye"
            },
            {
                "last_name": "Siarohin",
                "first_name": "Aliaksandr"
            },
            {
                "last_name": "Cao",
                "first_name": "Junli"
            },
            {
                "last_name": "Qian",
                "first_name": "Guocheng"
            },
            {
                "last_name": "Lee",
                "first_name": "Hsin-Ying"
            },
            {
                "last_name": "Tulyakov",
                "first_name": "Sergey"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Dynamic novel view synthesis aims to capture the temporal evolution of visual\ncontent within videos. Existing methods struggle to distinguishing between\nmotion and structure, particularly in scenarios where camera poses are either\nunknown or constrained compared to object motion. Furthermore, with information\nsolely from reference images, it is extremely challenging to hallucinate unseen\nregions that are occluded or partially observed in the given videos. To address\nthese issues, we first finetune a pretrained RGB-D diffusion model on the video\nframes using a customization technique. Subsequently, we distill the knowledge\nfrom the finetuned model to a 4D representations encompassing both dynamic and\nstatic Neural Radiance Fields (NeRF) components. The proposed pipeline achieves\ngeometric consistency while preserving the scene identity. We perform thorough\nexperiments to evaluate the efficacy of the proposed method qualitatively and\nquantitatively. Our results demonstrate the robustness and utility of our\napproach in challenging cases, further advancing dynamic novel view synthesis.\n",
        "title": "Diffusion Priors for Dynamic View Synthesis from Monocular Videos",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05584",
        "abstract_url": "http://arxiv.org/abs/2401.05584",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Edison"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Maruf"
            },
            {
                "last_name": "Sun",
                "first_name": "Yue"
            },
            {
                "last_name": "Mahendru",
                "first_name": "Rahul"
            },
            {
                "last_name": "Yang",
                "first_name": "Rui"
            },
            {
                "last_name": "Cook",
                "first_name": "Harrison"
            },
            {
                "last_name": "Leeuwenburg",
                "first_name": "Tennessee"
            },
            {
                "last_name": "Evans",
                "first_name": "Ben"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recently, the FourCastNet Neural Earth System Model (NESM) has shown\nimpressive results on predicting various atmospheric variables, trained on the\nERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory\ncomplexity in sequence length compared to quadratic complexity in vanilla\ntransformers, training FourCastNet on ERA5 from scratch still requires large\namount of compute resources, which is expensive or even inaccessible to most\nresearchers. In this work, we will show improved methods that can train\nFourCastNet using only 1% of the compute required by the baseline, while\nmaintaining model performance or par or even better than the baseline.\n",
        "title": "FourCastNeXt: Improving FourCastNet Training with Limited Compute",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05585",
        "abstract_url": "http://arxiv.org/abs/2401.05585",
        "authors": [
            {
                "last_name": "Kirigin",
                "first_name": "Tajana Ban"
            },
            {
                "last_name": "Comer",
                "first_name": "Jesse"
            },
            {
                "last_name": "Kanovich",
                "first_name": "Max"
            },
            {
                "last_name": "Scedrov",
                "first_name": "Andre"
            },
            {
                "last_name": "Talcott",
                "first_name": "Carolyn"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Most research on formal system design has focused on optimizing various\nmeasures of efficiency. However, insufficient attention has been given to the\ndesign of systems optimizing resilience, the ability of systems to adapt to\nunexpected changes or adversarial disruptions. In our prior work, we formalized\nthe intuitive notion of resilience as a property of cyber-physical systems by\nusing a multiset rewriting language with explicit time. In the present work, we\nstudy the computational complexity of a formalization of time-bounded\nresilience problems for the class of progressing timed systems (PTS), where,\nintuitively, only a finite number of actions can be carried out in a bounded\ntime period. We show that, in the time-bounded model with n (potentially\nadversarially chosen) updates, the corresponding time-bounded resilience\nproblem is complete for the $\\Sigma^P_{2n+1}$ class of the polynomial\nhierarchy, PH. To support the formal models and complexity results, we perform\nautomated experiments for time-bounded verification using the rewriting logic\ntool Maude.\n",
        "title": "Technical Report: Time-Bounded Resilience",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05587",
        "abstract_url": "http://arxiv.org/abs/2401.05587",
        "authors": [
            {
                "last_name": "Sullivan",
                "first_name": "Dakota"
            },
            {
                "last_name": "White",
                "first_name": "Nathan Thomas"
            },
            {
                "last_name": "Schoen",
                "first_name": "Andrew"
            },
            {
                "last_name": "Mutlu",
                "first_name": "Bilge"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robots are ubiquitous in small-to-large-scale manufacturers. While\ncollaborative robots (cobots) have significant potential in these settings due\nto their flexibility and ease of use, proper integration is critical to realize\ntheir full potential. Specifically, cobots need to be integrated in ways that\nutilize their strengths, improve manufacturing performance, and facilitate use\nin concert with human workers. Effective integration requires careful\nconsideration and the knowledge of roboticists, manufacturing engineers, and\nbusiness administrators. We propose an approach involving the stages of\nplanning, analysis, development, and presentation, to inform manufacturers\nabout cobot integration within their facilities prior to the integration\nprocess. We contextualize our approach in a case study with an SME collaborator\nand discuss insights learned.\n",
        "title": "Making Informed Decisions: Supporting Cobot Integration Considering\n  Business and Worker Preferences",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05593",
        "abstract_url": "http://arxiv.org/abs/2401.05593",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Adrian Xuan Wei"
            },
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Griffin",
                "first_name": "Conor"
            },
            {
                "last_name": "Kyger",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Baghernezhad",
                "first_name": "Faraz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present Reverse Projection, a novel projective texture mapping technique\nfor painting a decal directly to the texture of a 3D object. Designed to be\nused in games, this technique works in real-time. By using projection\ntechniques that are computed in local space textures and outward-looking, users\nusing low-end android devices to high-end gaming desktops are able to enjoy the\npersonalization of their assets. We believe our proposed pipeline is a step in\nimproving the speed and versatility of model painting.\n",
        "title": "Reverse Projection: Real-Time Local Space Texture Mapping",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05594",
        "abstract_url": "http://arxiv.org/abs/2401.05594",
        "authors": [
            {
                "last_name": "Mallick",
                "first_name": "Prakash"
            },
            {
                "last_name": "Dayoub",
                "first_name": "Feras"
            },
            {
                "last_name": "Sherrah",
                "first_name": "Jamie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper addresses the significant challenge in open-set object detection\n(OSOD): the tendency of state-of-the-art detectors to erroneously classify\nunknown objects as known categories with high confidence. We present a novel\napproach that effectively identifies unknown objects by distinguishing between\nhigh and low-density regions in latent space. Our method builds upon the\nOpen-Det (OD) framework, introducing two new elements to the loss function.\nThese elements enhance the known embedding space's clustering and expand the\nunknown space's low-density regions. The first addition is the Class\nWasserstein Anchor (CWA), a new function that refines the classification\nboundaries. The second is a spectral normalisation step, improving the\nrobustness of the model. Together, these augmentations to the existing\nContrastive Feature Learner (CFL) and Unknown Probability Learner (UPL) loss\nfunctions significantly improve OSOD performance. Our proposed OpenDet-CWA\n(OD-CWA) method demonstrates: a) a reduction in open-set errors by\napproximately 17%-22%, b) an enhancement in novelty detection capability by\n1.5%-16%, and c) a decrease in the wilderness index by 2%-20% across various\nopen-set scenarios. These results represent a substantial advancement in the\nfield, showcasing the potential of our approach in managing the complexities of\nopen-set object detection.\n",
        "title": "Wasserstein Distance-based Expansion of Low-Density Latent Regions for\n  Unknown Class Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05596",
        "abstract_url": "http://arxiv.org/abs/2401.05596",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Shilong"
            },
            {
                "last_name": "Tian",
                "first_name": "Zhiliang"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Huang",
                "first_name": "Zhen"
            },
            {
                "last_name": "Wen",
                "first_name": "Zhihua"
            },
            {
                "last_name": "Li",
                "first_name": "Dongsheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Low-resource languages (LRLs) face challenges in supervised neural machine\ntranslation due to limited parallel data, prompting research into unsupervised\nmethods. Unsupervised neural machine translation (UNMT) methods, including\nback-translation, transfer learning, and pivot-based translation, offer\npractical solutions for LRL translation, but they are hindered by issues like\nsynthetic data noise, language bias, and error propagation, which can\npotentially be mitigated by Large Language Models (LLMs). LLMs have advanced\nNMT with in-context learning (ICL) and supervised fine-tuning methods, but\ninsufficient training data results in poor performance in LRLs. We argue that\nLLMs can mitigate the linguistic noise with auxiliary languages to improve\ntranslations in LRLs. In this paper, we propose Probability-driven Meta-graph\nPrompter (POMP), a novel approach employing a dynamic, sampling-based graph of\nmultiple auxiliary languages to enhance LLMs' translation capabilities for\nLRLs. POMP involves constructing a directed acyclic meta-graph for each source\nlanguage, from which we dynamically sample multiple paths to prompt LLMs to\nmitigate the linguistic noise and improve translations during training. We use\nthe BLEURT metric to evaluate the translations and back-propagate rewards,\nestimated by scores, to update the probabilities of auxiliary languages in the\npaths. Our experiments show significant improvements in the translation quality\nof three LRLs, demonstrating the effectiveness of our approach.\n",
        "title": "POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource\n  Unsupervised Neural Machine Translation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05602",
        "abstract_url": "http://arxiv.org/abs/2401.05602",
        "authors": [
            {
                "last_name": "Remedios",
                "first_name": "Lucas W."
            },
            {
                "last_name": "Bao",
                "first_name": "Shunxing"
            },
            {
                "last_name": "Remedios",
                "first_name": "Samuel W."
            },
            {
                "last_name": "Lee",
                "first_name": "Ho Hin"
            },
            {
                "last_name": "Cai",
                "first_name": "Leon Y."
            },
            {
                "last_name": "Li",
                "first_name": "Thomas"
            },
            {
                "last_name": "Deng",
                "first_name": "Ruining"
            },
            {
                "last_name": "Cui",
                "first_name": "Can"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Liu",
                "first_name": "Qi"
            },
            {
                "last_name": "Lau",
                "first_name": "Ken S."
            },
            {
                "last_name": "Roland",
                "first_name": "Joseph T."
            },
            {
                "last_name": "Washington",
                "first_name": "Mary K."
            },
            {
                "last_name": "Coburn",
                "first_name": "Lori A."
            },
            {
                "last_name": "Wilson",
                "first_name": "Keith T."
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            },
            {
                "last_name": "Landman",
                "first_name": "Bennett A."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Understanding the way cells communicate, co-locate, and interrelate is\nessential to understanding human physiology. Hematoxylin and eosin (H&E)\nstaining is ubiquitously available both for clinical studies and research. The\nColon Nucleus Identification and Classification (CoNIC) Challenge has recently\ninnovated on robust artificial intelligence labeling of six cell types on H&E\nstains of the colon. However, this is a very small fraction of the number of\npotential cell classification types. Specifically, the CoNIC Challenge is\nunable to classify epithelial subtypes (progenitor, endocrine, goblet),\nlymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes\n(fibroblasts, stromal). In this paper, we propose to use inter-modality\nlearning to label previously un-labelable cell types on virtual H&E. We\nleveraged multiplexed immunofluorescence (MxIF) histology imaging to identify\n14 subclasses of cell types. We performed style transfer to synthesize virtual\nH&E from MxIF and transferred the higher density labels from MxIF to these\nvirtual H&E images. We then evaluated the efficacy of learning in this\napproach. We identified helper T and progenitor nuclei with positive predictive\nvalues of $0.34 \\pm 0.15$ (prevalence $0.03 \\pm 0.01$) and $0.47 \\pm 0.1$\n(prevalence $0.07 \\pm 0.02$) respectively on virtual H&E. This approach\nrepresents a promising step towards automating annotation in digital pathology.\n",
        "title": "Nucleus subtype classification using inter-modality learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05603",
        "abstract_url": "http://arxiv.org/abs/2401.05603",
        "authors": [
            {
                "last_name": "Jhaver",
                "first_name": "Shagun"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  Personal moderation tools on social media platforms allow users to control\ntheir feeds by configuring the acceptable toxicity thresholds for their feed\ncontent or muting inappropriate accounts. This research examines how the\nend-user configuration of these tools is shaped by four critical psychosocial\nfactors - fear of missing out (FoMO), social media addiction, subjective norms,\nand trust in moderation systems. Findings from a nationally representative\nsample of 1,061 participants show that FoMO and social media addiction make\nFacebook users more vulnerable to content-based harms by reducing their\nlikelihood of adopting personal moderation tools to hide inappropriate posts.\nIn contrast, descriptive and injunctive norms positively influence the use of\nthese tools. Further, trust in Facebook's moderation systems also significantly\naffects users' engagement with personal moderation. This analysis highlights\nqualitatively different pathways through which FoMO and social media addiction\nmake affected users disproportionately unsafe and offers design and policy\nsolutions to address this challenge.\n",
        "title": "Exploring How FoMO, Social Media Addiction, and Subjective Norms\n  Influence Personal Moderation Configurations",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05604",
        "abstract_url": "http://arxiv.org/abs/2401.05604",
        "authors": [
            {
                "last_name": "Gritsevskiy",
                "first_name": "Andrew"
            },
            {
                "last_name": "Panickssery",
                "first_name": "Arjun"
            },
            {
                "last_name": "Kirtland",
                "first_name": "Aaron"
            },
            {
                "last_name": "Kauffman",
                "first_name": "Derik"
            },
            {
                "last_name": "Gundlach",
                "first_name": "Hans"
            },
            {
                "last_name": "Gritsevskaya",
                "first_name": "Irina"
            },
            {
                "last_name": "Cavanagh",
                "first_name": "Joe"
            },
            {
                "last_name": "Chiang",
                "first_name": "Jonathan"
            },
            {
                "last_name": "La Roux",
                "first_name": "Lydia"
            },
            {
                "last_name": "Hung",
                "first_name": "Michelle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "CV",
            "CY"
        ],
        "abstract": "  We propose a new benchmark evaluating the performance of multimodal large\nlanguage models on rebus puzzles. The dataset covers 333 original examples of\nimage-based wordplay, cluing 13 categories such as movies, composers, major\ncities, and food. To achieve good performance on the benchmark of identifying\nthe clued word or phrase, models must combine image recognition and string\nmanipulation with hypothesis testing, multi-step reasoning, and an\nunderstanding of human cognition, making for a complex, multimodal evaluation\nof capabilities. We find that proprietary models such as GPT-4V and Gemini Pro\nsignificantly outperform all other tested models. However, even the best model\nhas a final accuracy of just 24%, highlighting the need for substantial\nimprovements in reasoning. Further, models rarely understand all parts of a\npuzzle, and are almost always incapable of retroactively explaining the correct\nanswer. Our benchmark can therefore be used to identify major shortcomings in\nthe knowledge and reasoning of multimodal large language models.\n",
        "title": "REBUS: A Robust Evaluation Benchmark of Understanding Symbols",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05605",
        "abstract_url": "http://arxiv.org/abs/2401.05605",
        "authors": [
            {
                "last_name": "Kalajdzievski",
                "first_name": "Damjan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            ""
        ],
        "abstract": "  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n",
        "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05609",
        "abstract_url": "http://arxiv.org/abs/2401.05609",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenxiong"
            },
            {
                "last_name": "Huang",
                "first_name": "Qikun"
            },
            {
                "last_name": "Chen",
                "first_name": "Suiyin"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  This paper introduces a cable finite element model based on an accurate\ndescription of the tension field for the static nonlinear analysis of cable\nstructures. The proposed cable element is developed using the geometrically\nexact beam model that adequately considers the effects of large displacements.\nBy neglecting flexural stiffness and shear deformation, the formulation of the\ncable finite element for scenarios involving given unstrained length and\nundetermined unstrained length is respectively presented. Additionally, the\nimplementations of solutions based on complete tangent matrix and element\ninternal iteration are introduced. Numerical examples are conducted to validate\nthe accuracy of the presented formulation for cable analysis under various\nconditions and to demonstrate the computational efficiency of the proposed\nelement and solution method. The results indicate that the proposed cable\nfinite element not only exhibits extremely high accuracy but also effectively\naddresses the problem of determining the cable state with an unknown unstrained\nlength, demonstrating the wide applicability of the proposed element. Through\nthe utilization of an iteration algorithm with arc-length control and the\nintroduction of additional control conditions, the proposed cable finite\nelement can be further utilized to solve complex practical engineering\nproblems.\n",
        "title": "A cable finite element formulation based on exact tension field for\n  static nonlinear analysis of cable structures",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05610",
        "abstract_url": "http://arxiv.org/abs/2401.05610",
        "authors": [
            {
                "last_name": "Dax",
                "first_name": "Victoria M."
            },
            {
                "last_name": "Li",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Leahy",
                "first_name": "Kevin"
            },
            {
                "last_name": "Kochenderfer",
                "first_name": "Mykel J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Graph-structured data is ubiquitous throughout natural and social sciences,\nand Graph Neural Networks (GNNs) have recently been shown to be effective at\nsolving prediction and inference problems on graph data. In this paper, we\npropose and demonstrate that GNNs can be applied to solve Combinatorial\nOptimization (CO) problems. CO concerns optimizing a function over a discrete\nsolution space that is often intractably large. To learn to solve CO problems,\nwe formulate the optimization process as a sequential decision making problem,\nwhere the return is related to how close the candidate solution is to\noptimality. We use a GNN to learn a policy to iteratively build increasingly\npromising candidate solutions. We present preliminary evidence that GNNs\ntrained through Q-Learning can solve CO problems with performance approaching\nstate-of-the-art heuristic-based solvers, using only a fraction of the\nparameters and training time.\n",
        "title": "Graph Q-Learning for Combinatorial Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05611",
        "abstract_url": "http://arxiv.org/abs/2401.05611",
        "authors": [
            {
                "last_name": "D\u00f6cker",
                "first_name": "Janosch"
            },
            {
                "last_name": "Linz",
                "first_name": "Simone"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CC",
            "DS"
        ],
        "abstract": "  Recently, there has been a growing interest in the relationships between\nunrooted and rooted phylogenetic networks. In this context, a natural question\nto ask is if an unrooted phylogenetic network U can be oriented as a rooted\nphylogenetic network such that the latter satisfies certain structural\nproperties. In a recent preprint, Bulteau et al. claim that it is computational\nhard to decide if U has a funneled (resp. funneled tree-child) orientation, for\nwhen the internal vertices of U have degree at most 5. Unfortunately, the proof\nof their funneled tree-child result appears to be incorrect. In this paper, we\npresent a corrected proof and show that hardness remains for other popular\nclasses of rooted phylogenetic networks such as funneled normal and funneled\nreticulation-visible. Additionally, our results hold regardless of whether U is\nrooted at an existing vertex or by subdividing an edge with the root.\n",
        "title": "On the existence of funneled orientations for classes of unrooted\n  phylogenetic networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05612",
        "abstract_url": "http://arxiv.org/abs/2401.05612",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Shiye"
            },
            {
                "last_name": "Liu",
                "first_name": "Anqi"
            },
            {
                "last_name": "Huang",
                "first_name": "Chien-Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Appropriate reliance is critical to achieving synergistic human-AI\ncollaboration. For instance, when users over-rely on AI assistance, their\nhuman-AI team performance is bounded by the model's capability. This work\nstudies how the presentation of model uncertainty may steer users'\ndecision-making toward fostering appropriate reliance. Our results demonstrate\nthat showing the calibrated model uncertainty alone is inadequate. Rather,\ncalibrating model uncertainty and presenting it in a frequency format allow\nusers to adjust their reliance accordingly and help reduce the effect of\nconfirmation bias on their decisions. Furthermore, the critical nature of our\nskin cancer screening task skews participants' judgment, causing their reliance\nto vary depending on their initial decision. Additionally, step-wise multiple\nregression analyses revealed how user demographics such as age and familiarity\nwith probability and statistics influence human-AI collaborative\ndecision-making. We discuss the potential for model uncertainty presentation,\ninitial user decision, and user demographics to be incorporated in designing\npersonalized AI aids for appropriate reliance.\n",
        "title": "Designing for Appropriate Reliance:Designing for Appropriate Reliance:\n  The Roles of AI Uncertainty Presentation, Initial User Decision, and User\n  Demographics in AI-Assisted Decision-Making",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05614",
        "abstract_url": "http://arxiv.org/abs/2401.05614",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Lian"
            },
            {
                "last_name": "Pun",
                "first_name": "Chi-Man"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "MM",
            ""
        ],
        "abstract": "  Due to the successful application of deep learning, audio spoofing detection\nhas made significant progress. Spoofed audio with speech synthesis or voice\nconversion can be well detected by many countermeasures. However, an automatic\nspeaker verification system is still vulnerable to spoofing attacks such as\nreplay or Deep-Fake audio. Deep-Fake audio means that the spoofed utterances\nare generated using text-to-speech (TTS) and voice conversion (VC) algorithms.\nHere, we propose a novel framework based on hybrid features with the\nself-attention mechanism. It is expected that hybrid features can be used to\nget more discrimination capacity. Firstly, instead of only one type of\nconventional feature, deep learning features and Mel-spectrogram features will\nbe extracted by two parallel paths: convolution neural networks and a\nshort-time Fourier transform (STFT) followed by Mel-frequency. Secondly,\nfeatures will be concatenated by a max-pooling layer. Thirdly, there is a\nSelf-attention mechanism for focusing on essential elements. Finally, ResNet\nand a linear layer are built to get the results. Experimental results reveal\nthat the hybrid features, compared with conventional features, can cover more\ndetails of an utterance. We achieve the best Equal Error Rate (EER) of 9.67\\%\nin the physical access (PA) scenario and 8.94\\% in the Deep fake task on the\nASVspoof 2021 dataset. Compared with the best baseline system, the proposed\napproach improves by 74.60\\% and 60.05\\%, respectively.\n",
        "title": "Self-Attention and Hybrid Features for Replay and Deep-Fake Audio\n  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05618",
        "abstract_url": "http://arxiv.org/abs/2401.05618",
        "authors": [
            {
                "last_name": "Renze",
                "first_name": "Matthew"
            },
            {
                "last_name": "Guven",
                "first_name": "Erhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We\ncompared standard CoT and CCoT prompts to see how conciseness impacts response\nlength and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4\nwith a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced\naverage response length by 48.70% for both GPT-3.5 and GPT-4 while having a\nnegligible impact on problem-solving performance. However, on math problems,\nGPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads\nto an average per-token cost reduction of 22.67%. These results have practical\nimplications for AI systems engineers using LLMs to solve real-world problems\nwith CoT prompt-engineering techniques. In addition, these results provide more\ngeneral insight for AI researchers studying the emergent behavior of\nstep-by-step reasoning in LLMs.\n",
        "title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large\n  Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05624",
        "abstract_url": "http://arxiv.org/abs/2401.05624",
        "authors": [
            {
                "last_name": "Tissaoui",
                "first_name": "Yassine"
            },
            {
                "last_name": "Kelly",
                "first_name": "James F."
            },
            {
                "last_name": "Marras",
                "first_name": "Simone"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  Mitigating the impact of waves leaving a numerical domain has been a\npersistent challenge in numerical modeling. Reducing wave reflection at the\ndomain boundary is crucial for accurate simulations. Absorbing layers, while\ncommon, often incur significant computational costs. This paper introduces an\nefficient application of a Legendre-Laguerre basis for absorbing layers for\ntwo-dimensional non-linear compressible Euler equations. The method couples a\nspectral-element bounded domain with a semi-infinite region, employing a tensor\nproduct of Lagrange and scaled Laguerre basis functions. The semi-infinite\nregion serves as an absorbing layer for our simulations. In comparison to\nexisting methods with similar absorbing layer extensions, our approach, a\npioneering application to the Euler equations, demonstrates substantial\ncomputational savings. The study marks the first application of semi-infinite\nelements to mitigate wave reflection in the solution of the Euler equations,\nparticularly in nonhydrostatic atmospheric modeling. A comprehensive set of\ntests demonstrates the method's versatility for general systems of conservation\nlaws, with a focus on its effectiveness in damping vertically propagating\ngravity waves in a linear hydrostatic mountain simulation a benchmark for\natmospheric models. Across all tests, our model consistently exhibits notable\nperformance improvements compared to a traditional Rayleigh damping approach.\n",
        "title": "Efficient Spectral Element Method for the Euler Equations on Unbounded\n  Domains in Multiple Dimensions",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05625",
        "abstract_url": "http://arxiv.org/abs/2401.05625",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Juni"
            },
            {
                "last_name": "Dong",
                "first_name": "Zhikang"
            },
            {
                "last_name": "Polak",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a novel method that combines differential geometry, kernels\nsmoothing, and spectral analysis to quantify facial muscle activity from widely\naccessible video recordings, such as those captured on personal smartphones.\nOur approach emphasizes practicality and accessibility. It has significant\npotential for applications in national security and plastic surgery.\nAdditionally, it offers remote diagnosis and monitoring for medical conditions\nsuch as stroke, Bell's palsy, and acoustic neuroma. Moreover, it is adept at\ndetecting and classifying emotions, from the overt to the subtle. The proposed\nface muscle analysis technique is an explainable alternative to deep learning\nmethods and a non-invasive substitute to facial electromyography (fEMG).\n",
        "title": "Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle\n  Dynamics in Videos",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05626",
        "abstract_url": "http://arxiv.org/abs/2401.05626",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Yizhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baoheng"
            },
            {
                "last_name": "Ding",
                "first_name": "Yuhao"
            },
            {
                "last_name": "So",
                "first_name": "Hayden Kwok-Hay"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Event-based vision represents a paradigm shift in how vision information is\ncaptured and processed. By only responding to dynamic intensity changes in the\nscene, event-based sensing produces far less data than conventional frame-based\ncameras, promising to springboard a new generation of high-speed, low-power\nmachines for edge intelligence. However, processing such dynamically sparse\ninput originated from event cameras efficiently in real time, particularly with\ncomplex deep neural networks (DNN), remains a formidable challenge. Existing\nsolutions that employ GPUs and other frame-based DNN accelerators often\nstruggle to efficiently process the dynamically sparse event data, missing the\nopportunities to improve processing efficiency with sparse data. To address\nthis, we propose ESDA, a composable dynamic sparse dataflow architecture that\nallows customized DNN accelerators to be constructed rapidly on FPGAs for\nevent-based vision tasks. ESDA is a modular system that is composed of a set of\nparametrizable modules for each network layer type. These modules share a\nuniform sparse token-feature interface and can be connected easily to compose\nan all-on-chip dataflow accelerator on FPGA for each network model. To fully\nexploit the intrinsic sparsity in event data, ESDA incorporates the use of\nsubmanifold sparse convolutions that largely enhance the activation sparsity\nthroughout the layers while simplifying hardware implementation. Finally, a\nnetwork architecture and hardware implementation co-optimizing framework that\nallows tradeoffs between accuracy and performance is also presented.\nExperimental results demonstrate that when compared with existing GPU and\nhardware-accelerated solutions, ESDA achieves substantial speedup and\nimprovement in energy efficiency across different applications, and it allows\nmuch wider design space for real-world deployments.\n",
        "title": "A Composable Dynamic Sparse Dataflow Architecture for Efficient\n  Event-based Vision Processing on FPGA",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05627",
        "abstract_url": "http://arxiv.org/abs/2401.05627",
        "authors": [
            {
                "last_name": "Henzinger",
                "first_name": "Monika"
            },
            {
                "last_name": "Li",
                "first_name": "Jason"
            },
            {
                "last_name": "Rao",
                "first_name": "Satish"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In 1996, Karger [Kar96] gave a startling randomized algorithm that finds a\nminimum-cut in a (weighted) graph in time $O(m\\log^3n)$ which he termed\nnear-linear time meaning linear (in the size of the input) times a\npolylogarthmic factor. In this paper, we give the first deterministic algorithm\nwhich runs in near-linear time for weighted graphs.\n  Previously, the breakthrough results of Kawarabayashi and Thorup [KT19] gave\na near-linear time algorithm for simple graphs. The main technique here is a\nclustering procedure that perfectly preserves minimum cuts. Recently, Li [Li21]\ngave an $m^{1+o(1)}$ deterministic minimum-cut algorithm for weighted graphs;\nthis form of running time has been termed \"almost-linear''. Li uses\nalmost-linear time deterministic expander decompositions which do not perfectly\npreserve minimum cuts, but he can use these clusterings to, in a sense,\n\"derandomize'' the methods of Karger.\n  In terms of techniques, we provide a structural theorem that says there\nexists a sparse clustering that preserves minimum cuts in a weighted graph with\n$o(1)$ error. In addition, we construct it deterministically in near linear\ntime. This was done exactly for simple graphs in [KT19, HRW20] and with\npolylogarithmic error for weighted graphs in [Li21]. Extending the techniques\nin [KT19, HRW20] to weighted graphs presents significant challenges, and\nmoreover, the algorithm can only polylogarithmically approximately preserve\nminimum cuts. A remaining challenge is to reduce the\npolylogarithmic-approximate clusterings to $1+o(1/\\log n)$-approximate so that\nthey can be applied recursively as in [Li21] over $O(\\log n)$ many levels. This\nis an additional challenge that requires building on properties of\ntree-packings in the presence of a wide range of edge weights to, for example,\nfind sources for local flow computations which identify minimum cuts that cross\nclusters.\n",
        "title": "Deterministic Near-Linear Time Minimum Cut in Weighted Graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05628",
        "abstract_url": "http://arxiv.org/abs/2401.05628",
        "authors": [
            {
                "last_name": "Elkin",
                "first_name": "Michael"
            },
            {
                "last_name": "Trehan",
                "first_name": "Chhaya"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "",
            "",
            ""
        ],
        "abstract": "  Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a set $S \\subseteq V$,\n$|S| = n^{\\sigma}$ (for some $0 < \\sigma \\le 1$) of designated sources, the $S\n\\times V$-direcahability problem is to compute for every $s \\in S$, the set of\nall the vertices reachable from $s$ in $G$. Known naive algorithms for this\nproblem either run a BFS/DFS separately from every source, and as a result\nrequire $O(m \\cdot n^{\\sigma})$ time, or compute the transitive closure of $G$\nin $\\tilde O(n^{\\omega})$ time, where $\\omega < 2.371552\\ldots$ is the matrix\nmultiplication exponent. Hence, the current state-of-the-art bound for the\nproblem on graphs with $m = \\Theta(n^{\\mu})$ edges in $\\tilde O(n^{\\min \\{\\mu +\n\\sigma, \\omega \\}})$. Our first contribution is an algorithm with running time\n$\\tilde O(n^{1 + \\tiny{\\frac{2}{3}} \\omega(\\sigma)})$ for this problem, where\n$\\omega(\\sigma)$ is the rectangular matrix multiplication exponent. Using\ncurrent state-of-the-art estimates on $\\omega(\\sigma)$, our exponent is better\nthan $\\min \\{2 + \\sigma, \\omega \\}$ for $\\tilde \\sigma \\le \\sigma \\le 0.53$,\nwhere $1/3 < \\tilde \\sigma < 0.3336$ is a universal constant.\n  Our second contribution is a sequence of algorithms $\\mathcal A_0, \\mathcal\nA_1, \\mathcal A_2, \\ldots$ for the $S \\times V$-direachability problem. We\nargue that under a certain assumption that we introduce, for every $\\tilde\n\\sigma \\le \\sigma < 1$, there exists a sufficiently large index $k = k(\\sigma)$\nso that $\\mathcal A_k$ improves upon the current state-of-the-art bounds for $S\n\\times V$-direachability with $|S| = n^{\\sigma}$, in the densest regime $\\mu\n=2$. We show that to prove this assumption, it is sufficient to devise an\nalgorithm that computes a rectangular max-min matrix product roughly as\nefficiently as ordinary $(+, \\cdot)$ matrix product.\n  Our algorithms heavily exploit recent constructions of directed shortcuts by\nKogan and Parter.\n",
        "title": "Faster Multi-Source Directed Reachability via Shortcuts and Matrix\n  Multiplication",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05629",
        "abstract_url": "http://arxiv.org/abs/2401.05629",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shaoru"
            },
            {
                "last_name": "Fazlyab",
                "first_name": "Mahyar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Control Barrier Functions (CBFs) provide an elegant framework for designing\nsafety filters for nonlinear control systems by constraining their trajectories\nto an invariant subset of a prespecified safe set. However, the task of finding\na CBF that concurrently maximizes the volume of the resulting control invariant\nset while accommodating complex safety constraints, particularly in high\nrelative degree systems with actuation constraints, continues to pose a\nsubstantial challenge. In this work, we propose a novel self-supervised\nlearning framework that holistically addresses these hurdles. Given a Boolean\ncomposition of multiple state constraints that define the safe set, our\napproach starts with building a single continuously differentiable function\nwhose 0-superlevel set provides an inner approximation of the safe set. We then\nuse this function together with a smooth neural network to parameterize the CBF\ncandidate. Finally, we design a training loss function based on a\nHamilton-Jacobi partial differential equation to train the CBF while enlarging\nthe volume of the induced control invariant set. We demonstrate the\neffectiveness of our approach via numerical experiments.\n",
        "title": "Learning Performance-Oriented Control Barrier Functions Under Complex\n  Safety Constraints and Limited Actuation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05631",
        "abstract_url": "http://arxiv.org/abs/2401.05631",
        "authors": [
            {
                "last_name": "Rosenberg",
                "first_name": "Karl Toby"
            },
            {
                "last_name": "Kazi",
                "first_name": "Rubaiat Habib"
            },
            {
                "last_name": "Wei",
                "first_name": "Li-Yi"
            },
            {
                "last_name": "Xia",
                "first_name": "Haijun"
            },
            {
                "last_name": "Perlin",
                "first_name": "Ken"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "",
            "CL",
            "GR",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  We introduce an interactive approach, DrawTalking, in which the user builds\ninteractive worlds by sketching and speaking. It emphasizes user control and\nflexibility, and gives programming-like capability without code. We implemented\nit on the iPad. An open-ended study shows the mechanics resonate and are\napplicable to many creative-exploratory use cases. We hope to inspire and\ninform research in future natural user-centered interfaces.\n",
        "title": "DrawTalking: Building Interactive Worlds by Sketching and Speaking",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05632",
        "abstract_url": "http://arxiv.org/abs/2401.05632",
        "authors": [
            {
                "last_name": "Joshi",
                "first_name": "Aditya"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            },
            {
                "last_name": "Kanojia",
                "first_name": "Diptesh"
            },
            {
                "last_name": "Li",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Haolan"
            },
            {
                "last_name": "Haffari",
                "first_name": "Gholamreza"
            },
            {
                "last_name": "Dippold",
                "first_name": "Doris"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  State-of-the-art natural language processing (NLP) models are trained on\nmassive training corpora, and report a superlative performance on evaluation\ndatasets. This survey delves into an important attribute of these datasets: the\ndialect of a language. Motivated by the performance degradation of NLP models\nfor dialectic datasets and its implications for the equity of language\ntechnologies, we survey past research in NLP for dialects in terms of datasets,\nand approaches. We describe a wide range of NLP tasks in terms of two\ncategories: natural language understanding (NLU) (for tasks such as dialect\nclassification, sentiment analysis, parsing, and NLU benchmarks) and natural\nlanguage generation (NLG) (for summarisation, machine translation, and dialogue\nsystems). The survey is also broad in its coverage of languages which include\nEnglish, Arabic, German among others. We observe that past work in NLP\nconcerning dialects goes deeper than mere dialect classification, and . This\nincludes early approaches that used sentence transduction that lead to the\nrecent approaches that integrate hypernetworks into LoRA. We expect that this\nsurvey will be useful to NLP researchers interested in building equitable\nlanguage technologies by rethinking LLM benchmarks and model architectures.\n",
        "title": "Natural Language Processing for Dialects of a Language: A Survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05633",
        "abstract_url": "http://arxiv.org/abs/2401.05633",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Gang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junpeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Recent progress in single-image super-resolution (SISR) has achieved\nremarkable performance, yet the computational costs of these methods remain a\nchallenge for deployment on resource-constrained devices. Especially for\ntransformer-based methods, the self-attention mechanism in such models brings\ngreat breakthroughs while incurring substantial computational costs. To tackle\nthis issue, we introduce the Convolutional Transformer layer (ConvFormer) and\nthe ConvFormer-based Super-Resolution network (CFSR), which offer an effective\nand efficient solution for lightweight image super-resolution tasks. In detail,\nCFSR leverages the large kernel convolution as the feature mixer to replace the\nself-attention module, efficiently modeling long-range dependencies and\nextensive receptive fields with a slight computational cost. Furthermore, we\npropose an edge-preserving feed-forward network, simplified as EFN, to obtain\nlocal feature aggregation and simultaneously preserve more high-frequency\ninformation. Extensive experiments demonstrate that CFSR can achieve an\nadvanced trade-off between computational cost and performance when compared to\nexisting lightweight SR methods. Compared to state-of-the-art methods, e.g.\nShuffleMixer, the proposed CFSR achieves 0.39 dB gains on Urban100 dataset for\nx2 SR task while containing 26% and 31% fewer parameters and FLOPs,\nrespectively. Code and pre-trained models are available at\nhttps://github.com/Aitical/CFSR.\n",
        "title": "Transforming Image Super-Resolution: A ConvFormer-based Efficient\n  Approach",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05638",
        "abstract_url": "http://arxiv.org/abs/2401.05638",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Changtai"
            },
            {
                "last_name": "Han",
                "first_name": "Xu"
            },
            {
                "last_name": "Yao",
                "first_name": "Chao"
            },
            {
                "last_name": "Ban",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate and efficient extraction of microstructures in microscopic images of\nmaterials plays a critical role in the exploration of structure-property\nrelationships and the optimization of process parameters. Deep learning-based\nimage segmentation techniques that rely on manual annotation are time-consuming\nand labor-intensive and hardly meet the demand for model transferability and\ngeneralization. Segment Anything Model (SAM), a large visual model with\npowerful deep feature representation and zero-shot generalization capabilities,\nhas provided new solutions for image segmentation. However, directly applying\nSAM to segmenting microstructures in microscopic images of materials without\nhuman annotation cannot achieve the expected results, as the difficulty of\nadapting its native prompt engineering to the dense and dispersed\ncharacteristics of key microstructures in materials microscopy images. In this\npaper, we propose MatSAM, a general and efficient microstructure extraction\nsolution based on SAM. A new point-based prompts generation strategy is\ndesigned, grounded on the distribution and shape of materials microstructures.\nIt generates prompts for different microscopic images, fuses the prompts of the\nregion of interest (ROI) key points and grid key points, and integrates\npost-processing methods for quantitative characterization of materials\nmicrostructures. For common microstructures including grain boundary and phase,\nMatSAM achieves superior segmentation performance to conventional methods and\nis even preferable to supervised learning methods evaluated on 18 materials\nmicrostructures imaged by the optical microscope (OM) and scanning electron\nmicroscope (SEM). We believe that MatSAM can significantly reduce the cost of\nquantitative characterization of materials microstructures and accelerate the\ndesign of new materials.\n",
        "title": "MatSAM: Efficient Materials Microstructure Extraction via Visual Large\n  Model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05639",
        "abstract_url": "http://arxiv.org/abs/2401.05639",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Yahui"
            },
            {
                "last_name": "Cheng",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This paper addresses the full-state prescribed performance-based consensus\nproblem for double-integrator multi-agent systems with jointly connected\ntopologies. To improve the transient performance, a distributed prescribed\nperformance control protocol consisting of the transformed relative position\nand the transformed relative velocity is proposed, where the communication\ntopology satisfies the jointly connected assumption. Different from the\nexisting literatures, two independent transient performance specifications\nimposed on relative positions and relative velocities can be guaranteed\nsimultaneously. A numerical example is ultimately used to validate the\neffectiveness of proposed protocol.\n",
        "title": "Full-State Prescribed Performance-Based Consensus of Double-Integrator\n  Multi-Agent Systems with Jointly Connected Topologies",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05641",
        "abstract_url": "http://arxiv.org/abs/2401.05641",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Tiejin"
            },
            {
                "last_name": "Dai",
                "first_name": "Qinrun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yueqi"
            },
            {
                "last_name": "Wei",
                "first_name": "Hua"
            },
            {
                "last_name": "Zeng",
                "first_name": "Qingkai"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS",
            "CR",
            "LG"
        ],
        "abstract": "  Compartmentalization effectively prevents initial corruption from turning\ninto a successful attack. This paper presents O2C, a pioneering system designed\nto enforce OS kernel compartmentalization on the fly. It not only provides\nimmediate remediation for sudden threats but also maintains consistent system\navailability through the enforcement process.\n  O2C is empowered by the newest advancements of the eBPF ecosystem which\nallows to instrument eBPF programs that perform enforcement actions into the\nkernel at runtime. O2C takes the lead in embedding a machine learning model\ninto eBPF programs, addressing unique challenges in on-the-fly\ncompartmentalization. Our comprehensive evaluation shows that O2C effectively\nconfines damage within the compartment. Further, we validate that decision tree\nis optimally suited for O2C owing to its advantages in processing tabular data,\nits explainable nature, and its compliance with the eBPF ecosystem. Last but\nnot least, O2C is lightweight, showing negligible overhead and excellent\nsacalability system-wide.\n",
        "title": "When eBPF Meets Machine Learning: On-the-fly OS Kernel\n  Compartmentalization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05642",
        "abstract_url": "http://arxiv.org/abs/2401.05642",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Zheng"
            },
            {
                "last_name": "Mathur",
                "first_name": "Umang"
            },
            {
                "last_name": "Pavlogiannis",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Dynamic data race detection has emerged as a key technique for ensuring\nreliability of concurrent software in practice. However, dynamic approaches can\noften miss data races owing to nondeterminism in the thread scheduler.\nPredictive race detection techniques cater to this shortcoming by inferring\nalternate executions that may expose data races without re-executing the\nunderlying program. More formally, the dynamic data race prediction problem\nasks, given a trace \\sigma of an execution of a concurrent program, can \\sigma\nbe correctly reordered to expose a data race? Existing state-of-the art\ntechniques for data race prediction either do not scale to executions arising\nfrom real world concurrent software, or only expose a limited class of data\nraces, such as those that can be exposed without reversing the order of\nsynchronization operations.\n  In general, exposing data races by reasoning about synchronization reversals\nis an intractable problem. In this work, we identify a class of data races,\ncalled Optimistic Sync(hronization)-Reversal races that can be detected in a\ntractable manner and often include non-trivial data races that cannot be\nexposed by prior tractable techniques. We also propose a sound algorithm OSR\nfor detecting all optimistic sync-reversal data races in overall quadratic\ntime, and show that the algorithm is optimal by establishing a matching lower\nbound. Our experiments demonstrate the effectiveness of OSR on our extensive\nsuite of benchmarks, OSR reports the largest number of data races, and scales\nwell to large execution traces.\n",
        "title": "Optimistic Prediction of Synchronization-Reversal Data Races",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05646",
        "abstract_url": "http://arxiv.org/abs/2401.05646",
        "authors": [
            {
                "last_name": "Peng",
                "first_name": "Chunlei"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Decheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Nannan"
            },
            {
                "last_name": "Hu",
                "first_name": "Ruimin"
            },
            {
                "last_name": "Gao",
                "first_name": "Xinbo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cloth-changing person re-identification (CC-ReID) aims to match persons who\nchange clothes over long periods. The key challenge in CC-ReID is to extract\nclothing-independent features, such as face, hairstyle, body shape, and gait.\nCurrent research mainly focuses on modeling body shape using multi-modal\nbiological features (such as silhouettes and sketches). However, it does not\nfully leverage the personal description information hidden in the original RGB\nimage. Considering that there are certain attribute descriptions which remain\nunchanged after the changing of cloth, we propose a Masked Attribute\nDescription Embedding (MADE) method that unifies personal visual appearance and\nattribute description for CC-ReID. Specifically, handling variable\nclothing-sensitive information, such as color and type, is challenging for\neffective modeling. To address this, we mask the clothing and color information\nin the personal attribute description extracted through an attribute detection\nmodel. The masked attribute description is then connected and embedded into\nTransformer blocks at various levels, fusing it with the low-level to\nhigh-level features of the image. This approach compels the model to discard\nclothing information. Experiments are conducted on several CC-ReID benchmarks,\nincluding PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE\neffectively utilizes attribute description, enhancing cloth-changing person\nre-identification performance, and compares favorably with state-of-the-art\nmethods. The code is available at https://github.com/moon-wh/MADE.\n",
        "title": "Masked Attribute Description Embedding for Cloth-Changing Person\n  Re-identification",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05648",
        "abstract_url": "http://arxiv.org/abs/2401.05648",
        "authors": [
            {
                "last_name": "Curbelo",
                "first_name": "Israel R."
            },
            {
                "last_name": "Malko",
                "first_name": "Hannah R."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DS",
            ""
        ],
        "abstract": "  We consider the on-line coloring problem restricted to proper interval graphs\nwith known interval representation. Chrobak and \\'{S}lusarek (1981) showed that\nthe greedy $\\textrm{First-Fit}$ algorithm has a strict competitive ratio of\n$2$. It remains open whether there is an on-line algorithm that performs better\nthan $\\textrm{First-Fit}$. Piotr (2008) showed that if the representation is\nnot known, there is no better on-line algorithm. Epstein and Levy (2005) showed\nthat no on-line algorithm has a strict competitive ratio less than $1.5$ when a\nunit-interval representation is known, which was later improved to\n$1.\\overline{3}$. In this paper, we show that there is no on-line algorithm\nwith strict competitive ratio less than $1.75$ by presenting a strategy that\ncan force any on-line algorithm to use $7$ colors on a proper interval graph\n$G$ with chromatic number $\\chi(G)\\leq 4$ and known interval representation.\n",
        "title": "On the on-line coloring of proper interval graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05650",
        "abstract_url": "http://arxiv.org/abs/2401.05650",
        "authors": [
            {
                "last_name": "Jaradat",
                "first_name": "Israa"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haiqi"
            },
            {
                "last_name": "Li",
                "first_name": "Chengkai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Cherry-picking refers to the deliberate selection of evidence or facts that\nfavor a particular viewpoint while ignoring or distorting evidence that\nsupports an opposing perspective. Manually identifying instances of\ncherry-picked statements in news stories can be challenging, particularly when\nthe opposing viewpoint's story is absent. This study introduces Cherry, an\ninnovative approach for automatically detecting cherry-picked statements in\nnews articles by finding missing important statements in the target news story.\nCherry utilizes the analysis of news coverage from multiple sources to identify\ninstances of cherry-picking. Our approach relies on language models that\nconsider contextual information from other news sources to classify statements\nbased on their importance to the event covered in the target news story.\nFurthermore, this research introduces a novel dataset specifically designed for\ncherry-picking detection, which was used to train and evaluate the performance\nof the models. Our best performing model achieves an F-1 score of about %89 in\ndetecting important statements when tested on unseen set of news stories.\nMoreover, results show the importance incorporating external knowledge from\nalternative unbiased narratives when assessing a statement's importance.\n",
        "title": "On Detecting Cherry-picking in News Coverage Using Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05653",
        "abstract_url": "http://arxiv.org/abs/2401.05653",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Sean"
            },
            {
                "last_name": "Musunuru",
                "first_name": "Sriya"
            },
            {
                "last_name": "Zong",
                "first_name": "Baoshi"
            },
            {
                "last_name": "Thornton",
                "first_name": "Brooks"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper explores the application of Shapley Value Regression in dissecting\nmarketing performance at channel-partner level, complementing channel-level\nMarketing Mix Modeling (MMM). Utilizing real-world data from the financial\nservices industry, we demonstrate the practicality of Shapley Value Regression\nin evaluating individual partner contributions. Although structured in-field\ntesting along with cooperative game theory is most accurate, it can often be\nhighly complex and expensive to conduct. Shapley Value Regression is thus a\nmore feasible approach to disentangle the influence of each marketing partner\nwithin a marketing channel. We also propose a simple method to derive adjusted\ncoefficients of Shapley Value Regression and compares it with alternative\napproaches.\n",
        "title": "Quantifying Marketing Performance at Channel-Partner Level by Using\n  Marketing Mix Modeling (MMM) and Shapley Value Regression",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05654",
        "abstract_url": "http://arxiv.org/abs/2401.05654",
        "authors": [
            {
                "last_name": "Tu",
                "first_name": "Tao"
            },
            {
                "last_name": "Palepu",
                "first_name": "Anil"
            },
            {
                "last_name": "Schaekermann",
                "first_name": "Mike"
            },
            {
                "last_name": "Saab",
                "first_name": "Khaled"
            },
            {
                "last_name": "Freyberg",
                "first_name": "Jan"
            },
            {
                "last_name": "Tanno",
                "first_name": "Ryutaro"
            },
            {
                "last_name": "Wang",
                "first_name": "Amy"
            },
            {
                "last_name": "Li",
                "first_name": "Brenna"
            },
            {
                "last_name": "Amin",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Tomasev",
                "first_name": "Nenad"
            },
            {
                "last_name": "Azizi",
                "first_name": "Shekoofeh"
            },
            {
                "last_name": "Singhal",
                "first_name": "Karan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yong"
            },
            {
                "last_name": "Hou",
                "first_name": "Le"
            },
            {
                "last_name": "Webson",
                "first_name": "Albert"
            },
            {
                "last_name": "Kulkarni",
                "first_name": "Kavita"
            },
            {
                "last_name": "Mahdavi",
                "first_name": "S Sara"
            },
            {
                "last_name": "Semturs",
                "first_name": "Christopher"
            },
            {
                "last_name": "Gottweis",
                "first_name": "Juraj"
            },
            {
                "last_name": "Barral",
                "first_name": "Joelle"
            },
            {
                "last_name": "Chou",
                "first_name": "Katherine"
            },
            {
                "last_name": "Corrado",
                "first_name": "Greg S"
            },
            {
                "last_name": "Matias",
                "first_name": "Yossi"
            },
            {
                "last_name": "Karthikesalingam",
                "first_name": "Alan"
            },
            {
                "last_name": "Natarajan",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL",
            "LG"
        ],
        "abstract": "  At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.\n",
        "title": "Towards Conversational Diagnostic AI",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05655",
        "abstract_url": "http://arxiv.org/abs/2401.05655",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Kaixun"
            },
            {
                "last_name": "Rakovi\u0107",
                "first_name": "Mladen"
            },
            {
                "last_name": "Li",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Guan",
                "first_name": "Quanlong"
            },
            {
                "last_name": "Ga\u0161evi\u0107",
                "first_name": "Dragan"
            },
            {
                "last_name": "Chen",
                "first_name": "Guanliang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Automatic Essay Scoring (AES) is a well-established educational pursuit that\nemploys machine learning to evaluate student-authored essays. While much effort\nhas been made in this area, current research primarily focuses on either (i)\nboosting the predictive accuracy of an AES model for a specific prompt (i.e.,\ndeveloping prompt-specific models), which often heavily relies on the use of\nthe labeled data from the same target prompt; or (ii) assessing the\napplicability of AES models developed on non-target prompts to the intended\ntarget prompt (i.e., developing the AES models in a cross-prompt setting).\nGiven the inherent bias in machine learning and its potential impact on\nmarginalized groups, it is imperative to investigate whether such bias exists\nin current AES methods and, if identified, how it intervenes with an AES\nmodel's accuracy and generalizability. Thus, our study aimed to uncover the\nintricate relationship between an AES model's accuracy, fairness, and\ngeneralizability, contributing practical insights for developing effective AES\nmodels in real-world education. To this end, we meticulously selected nine\nprominent AES methods and evaluated their performance using seven metrics on an\nopen-sourced dataset, which contains over 25,000 essays and various demographic\ninformation about students such as gender, English language learner status, and\neconomic status. Through extensive evaluations, we demonstrated that: (1)\nprompt-specific models tend to outperform their cross-prompt counterparts in\nterms of predictive accuracy; (2) prompt-specific models frequently exhibit a\ngreater bias towards students of different economic statuses compared to\ncross-prompt models; (3) in the pursuit of generalizability, traditional\nmachine learning models coupled with carefully engineered features hold greater\npotential for achieving both high accuracy and fairness than complex neural\nnetwork models.\n",
        "title": "Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive\n  Investigation of Accuracy, Fairness, and Generalizability",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05657",
        "abstract_url": "http://arxiv.org/abs/2401.05657",
        "authors": [
            {
                "last_name": "Holliday",
                "first_name": "Wesley H."
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GT",
            "MA",
            "",
            ""
        ],
        "abstract": "  In the context of social choice theory with ordinal preferences, we say that\nthe defensible set is the set of alternatives $x$ such that for any alternative\n$y$, if $y$ beats $x$ in a head-to-head majority comparison, then there is an\nalternative $z$ that beats $y$ in a head-to-head majority comparison by a\nmargin at least as large as the margin by which $y$ beat $x$. We show that any\nordinal voting method satisfying two well-known axioms from voting\ntheory--positive involvement and the Condorcet winner criterion--refines the\ndefensible set. Using this lemma, we prove an impossibility theorem: there is\nno such voting method that also satisfies the Condorcet loser criterion,\nresolvability, and a common invariance property for Condorcet methods, namely\nthat the choice of winners depends only on the relative sizes of majority\nmargins.\n",
        "title": "The defensible set and a new impossibility theorem in voting",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05659",
        "abstract_url": "http://arxiv.org/abs/2401.05659",
        "authors": [
            {
                "last_name": "Madugalla",
                "first_name": "Anuradha"
            },
            {
                "last_name": "Huang",
                "first_name": "Yutan"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            },
            {
                "last_name": "Cho",
                "first_name": "Min Hee"
            },
            {
                "last_name": "Gamage",
                "first_name": "Lasith Koswatta"
            },
            {
                "last_name": "Leao",
                "first_name": "Tristan"
            },
            {
                "last_name": "Thiele",
                "first_name": "Sam"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "SE"
        ],
        "abstract": "  Most software applications contain graphics such as charts, diagrams and\nmaps. Currently, these graphics are designed with a ``one size fits all\"\napproach and do not cater to the needs of people with disabilities. Therefore,\nwhen using software with graphics, a colour-impaired user may struggle to\ninterpret graphics with certain colours, and a person with dyslexia may\nstruggle to read the text labels in the graphic. Our research addresses this\nissue by developing a framework that generates adaptive and accessible\ninformation graphics for multiple disabilities. Uniquely, the approach also\nserves people with multiple simultaneous disabilities. To achieve these, we\nused a case study of public space floorplans presented via a web tool and\nworked with four disability groups: people with low vision, colour blindness,\ndyslexia and mobility impairment. Our research involved gathering requirements\nfrom 3 accessibility experts and 80 participants with disabilities, developing\na system to generate adaptive graphics that address the identified\nrequirements, and conducting an evaluation with 7 participants with\ndisabilities. The evaluation showed that users found our solution easy to use\nand suitable for most of their requirements. The study also provides\nrecommendations for front-end developers on engineering accessible graphics for\ntheir software and discusses the implications of our work on society from the\nperspective of public space owners and end users.\n",
        "title": "Engineering Adaptive Information Graphics for Disabled Communities: A\n  Case Study with Public Space Indoor Maps",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05661",
        "abstract_url": "http://arxiv.org/abs/2401.05661",
        "authors": [
            {
                "last_name": "Espinoza",
                "first_name": "Jes\u00fas F."
            },
            {
                "last_name": "Esquer-P\u00e9rez",
                "first_name": "Cynthia G."
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            ""
        ],
        "abstract": "  In this work we study the intersection properties of a finite disk system in\nthe euclidean space. We accomplish this by utilizing subsets of spheres with\nvarying dimensions and analyze specific points within them, referred to as\npoles. Additionally, we introduce two applications: estimating the common scale\nfactor for the radii that makes the re-scaled disks intersects in a single\npoint, this is the \\v{C}ech scale, and constructing the minimal Axis-Aligned\nBounding Box (AABB) that encloses the intersection of all disks in the system.\n",
        "title": "Intersection properties of finite disk collections",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05664",
        "abstract_url": "http://arxiv.org/abs/2401.05664",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Jian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Energy efficiency is a big concern in industrial sectors. Finding the root\ncause of anomaly state of energy efficiency can help to improve energy\nefficiency of industrial systems and therefore save energy cost. In this\nresearch, we propose to use transfer entropy (TE) for root cause analysis on\nenergy efficiency of industrial systems. A method, called TE flow, is proposed\nin that a TE flow from physical measurements of each subsystem to the energy\nefficiency indicator along timeline is considered as causal strength for\ndiagnosing root cause of anomaly states of energy efficiency of a system. The\ncopula entropy-based nonparametric TE estimator is used in the proposed method.\nWe conducted experiments on real data collected from a compressing air system\nto verify the proposed method. Experimental results show that the TE flow\nmethod successfully identified the root cause of the energy (in)efficiency of\nthe system.\n",
        "title": "Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05665",
        "abstract_url": "http://arxiv.org/abs/2401.05665",
        "authors": [
            {
                "last_name": "Regal",
                "first_name": "Frank"
            },
            {
                "last_name": "Suarez",
                "first_name": "Chris"
            },
            {
                "last_name": "Parra",
                "first_name": "Fabian"
            },
            {
                "last_name": "Pryor",
                "first_name": "Mitch"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Multi-agent human-robot teaming allows for the potential to gather\ninformation about various environments more efficiently by exploiting and\ncombining the strengths of humans and robots. In industries like defense,\nsearch and rescue, first-response, and others alike, heterogeneous human-robot\nteams show promise to accelerate data collection and improve team safety by\nremoving humans from unknown and potentially hazardous situations. This work\nbuilds upon AugRE, an Augmented Reality (AR) based scalable human-robot teaming\nframework. It enables users to localize and communicate with 50+ autonomous\nagents. Through our efforts, users are able to command, control, and supervise\nagents in large teams, both line-of-sight and non-line-of-sight, without the\nneed to modify the environment prior and without requiring users to use typical\nhardware (i.e. joysticks, keyboards, laptops, tablets, etc.) in the field. The\ndemonstrated work shows early indications that combining these AR-HMD-based\nuser interaction modalities for command, control, and supervision will help\nimprove human-robot team collaboration, robustness, and trust.\n",
        "title": "Augmented Reality User Interface for Command, Control, and Supervision\n  of Large Multi-Agent Teams",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05667",
        "abstract_url": "http://arxiv.org/abs/2401.05667",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Weijieying"
            },
            {
                "last_name": "Honavar",
                "first_name": "Vasant G"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  A key challenge in the continual learning setting is to efficiently learn a\nsequence of tasks without forgetting how to perform previously learned tasks.\nMany existing approaches to this problem work by either retraining the model on\nprevious tasks or by expanding the model to accommodate new tasks. However,\nthese approaches typically suffer from increased storage and computational\nrequirements, a problem that is worsened in the case of sparse models due to\nneed for expensive re-training after sparsification. To address this challenge,\nwe propose a new method for efficient continual learning of sparse models\n(EsaCL) that can automatically prune redundant parameters without adversely\nimpacting the model's predictive power, and circumvent the need of retraining.\nWe conduct a theoretical analysis of loss landscapes with parameter pruning,\nand design a directional pruning (SDP) strategy that is informed by the\nsharpness of the loss function with respect to the model parameters. SDP\nensures model with minimal loss of predictive accuracy, accelerating the\nlearning of sparse models at each stage. To accelerate model update, we\nintroduce an intelligent data selection (IDS) strategy that can identify\ncritical instances for estimating loss landscape, yielding substantially\nimproved data efficiency. The results of our experiments show that EsaCL\nachieves performance that is competitive with the state-of-the-art methods on\nthree continual learning benchmarks, while using substantially reduced memory\nand computational resources.\n",
        "title": "EsaCL: Efficient Continual Learning of Sparse Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05668",
        "abstract_url": "http://arxiv.org/abs/2401.05668",
        "authors": [
            {
                "last_name": "Madugalla",
                "first_name": "Anuradha"
            },
            {
                "last_name": "Kanij",
                "first_name": "Tanjila"
            },
            {
                "last_name": "Hoda",
                "first_name": "Rashina"
            },
            {
                "last_name": "Hidellaarachchi",
                "first_name": "Dulaji"
            },
            {
                "last_name": "Pant",
                "first_name": "Aastha"
            },
            {
                "last_name": "Ferdousi",
                "first_name": "Samia"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The COVID-19 pandemic changed the way we live, work and the way we conduct\nresearch. With the restrictions of lockdowns and social distancing, various\nimpacts were experienced by many software engineering researchers, especially\nwhose studies depend on human participants. We conducted a mixed methods study\nto understand the extent of this impact. Through a detailed survey with 89\nsoftware engineering researchers working with human participants around the\nworld and a further nine follow-up interviews, we identified the key challenges\nfaced, the adaptations made, and the surprising fringe benefits of conducting\nresearch involving human participants during the pandemic. Our findings also\nrevealed that in retrospect, many researchers did not wish to revert to the old\nways of conducting human-oriented research. Based on our analysis and insights,\nwe share recommendations on how to conduct remote studies with human\nparticipants effectively in an increasingly hybrid world when face-to-face\nengagement is not possible or where remote participation is preferred.\n",
        "title": "Challenges, Adaptations, and Fringe Benefits of Conducting Software\n  Engineering Research with Human Participants during the COVID-19 Pandemic",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05669",
        "abstract_url": "http://arxiv.org/abs/2401.05669",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "Gu",
                "first_name": "Zhouhong"
            },
            {
                "last_name": "Liang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Lu",
                "first_name": "Dakuan"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yanghua"
            },
            {
                "last_name": "Wang",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pre-trained language models (PLMs) have been prevailing in state-of-the-art\nmethods for natural language processing, and knowledge-enhanced PLMs are\nfurther proposed to promote model performance in knowledge-intensive tasks.\nHowever, conceptual knowledge, one essential kind of knowledge for human\ncognition, still remains understudied in this line of research. This limits\nPLMs' performance in scenarios requiring human-like cognition, such as\nunderstanding long-tail entities with concepts. In this paper, we propose\nConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to\ninfuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies\nwith entity concept prediction, a novel pre-training objective to predict the\nconcepts of entities mentioned in the pre-training contexts. Unlike previous\nconcept-enhanced methods, ConcEPT can be readily adapted to various downstream\napplications without entity linking or concept mapping. Results of extensive\nexperiments show the effectiveness of ConcEPT in four tasks such as entity\ntyping, which validates that our model gains improved conceptual knowledge with\nconcept-enhanced pre-training.\n",
        "title": "ConcEPT: Concept-Enhanced Pre-Training for Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05673",
        "abstract_url": "http://arxiv.org/abs/2401.05673",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Nick"
            },
            {
                "last_name": "Marsso",
                "first_name": "Lina"
            },
            {
                "last_name": "Yaman",
                "first_name": "Sinem Getir"
            },
            {
                "last_name": "Baatartogtokh",
                "first_name": "Yesugen"
            },
            {
                "last_name": "Ayad",
                "first_name": "Reem"
            },
            {
                "last_name": "de Mello",
                "first_name": "Vict\u00f3ria Oldemburgo"
            },
            {
                "last_name": "Townsend",
                "first_name": "Beverley"
            },
            {
                "last_name": "Standen",
                "first_name": "Isobel"
            },
            {
                "last_name": "Stefanakos",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Imrie",
                "first_name": "Calum"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Gena\u00edna Nunes"
            },
            {
                "last_name": "Cavalcanti",
                "first_name": "Ana"
            },
            {
                "last_name": "Calinescu",
                "first_name": "Radu"
            },
            {
                "last_name": "Chechik",
                "first_name": "Marsha"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  As software systems increasingly interact with humans in application domains\nsuch as transportation and healthcare, they raise concerns related to the\nsocial, legal, ethical, empathetic, and cultural (SLEEC) norms and values of\ntheir stakeholders. Normative non-functional requirements (N-NFRs) are used to\ncapture these concerns by setting SLEEC-relevant boundaries for system\nbehavior. Since N-NFRs need to be specified by multiple stakeholders with\nwidely different, non-technical expertise (ethicists, lawyers, regulators, end\nusers, etc.), N-NFR elicitation is very challenging. To address this challenge,\nwe introduce N-Check, a novel tool-supported formal approach to N-NFR analysis\nand debugging. N-Check employs satisfiability checking to identify a broad\nspectrum of N-NFR well-formedness issues (WFI), such as conflicts, redundancy,\nrestrictiveness, insufficiency, yielding diagnostics which pinpoint their\ncauses in a user-friendly way that enables non-technical stakeholders to\nunderstand and fix them. We show the effectiveness and usability of our\napproach through nine case studies in which teams of ethicists, lawyers,\nphilosophers, psychologists, safety analysts, and engineers used N-Check to\nanalyse and debug 233 N-NFRs comprising 62 issues for the software underpinning\nthe operation of systems ranging from assistive-care robots and tree-disease\ndetection drones to manufacturing collaborative robots.\n",
        "title": "Analyzing and Debugging Normative Requirements via Satisfiability\n  Checking",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05674",
        "abstract_url": "http://arxiv.org/abs/2401.05674",
        "authors": [
            {
                "last_name": "Feireisl",
                "first_name": "Eduard"
            },
            {
                "last_name": "Lukacova-Medvidova",
                "first_name": "Maria"
            },
            {
                "last_name": "She",
                "first_name": "Bangwei"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuhuan"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We consider the Navier-Stokes-Fourier system governing the motion of a\ngeneral compressible, heat conducting, Newtonian fluid driven by random\ninitial/boundary data. Convergence of the stochastic collocation and Monte\nCarlo numerical methods is shown under the hypothesis that approximate\nsolutions are bounded in probability. Abstract results are illustrated by\nnumerical experiments for the Rayleigh-Benard convection problem.\n",
        "title": "Convergence of numerical methods for the Navier-Stokes-Fourier system\n  driven by uncertain initial/boundary data",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05675",
        "abstract_url": "http://arxiv.org/abs/2401.05675",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Seung Hyun"
            },
            {
                "last_name": "Li",
                "first_name": "Yinxiao"
            },
            {
                "last_name": "Ke",
                "first_name": "Junjie"
            },
            {
                "last_name": "Yoo",
                "first_name": "Innfarn"
            },
            {
                "last_name": "Zhang",
                "first_name": "Han"
            },
            {
                "last_name": "Yu",
                "first_name": "Jiahui"
            },
            {
                "last_name": "Wang",
                "first_name": "Qifei"
            },
            {
                "last_name": "Deng",
                "first_name": "Fei"
            },
            {
                "last_name": "Entis",
                "first_name": "Glenn"
            },
            {
                "last_name": "He",
                "first_name": "Junfeng"
            },
            {
                "last_name": "Li",
                "first_name": "Gang"
            },
            {
                "last_name": "Kim",
                "first_name": "Sangpil"
            },
            {
                "last_name": "Essa",
                "first_name": "Irfan"
            },
            {
                "last_name": "Yang",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent works demonstrate that using reinforcement learning (RL) with quality\nrewards can enhance the quality of generated images in text-to-image (T2I)\ngeneration. However, a simple aggregation of multiple rewards may cause\nover-optimization in certain metrics and degradation in others, and it is\nchallenging to manually find the optimal weights. An effective strategy to\njointly optimize multiple rewards in RL for T2I generation is highly desirable.\nThis paper introduces Parrot, a novel multi-reward RL framework for T2I\ngeneration. Through the use of the batch-wise Pareto optimal selection, Parrot\nautomatically identifies the optimal trade-off among different rewards during\nthe RL optimization of the T2I generation. Additionally, Parrot employs a joint\noptimization approach for the T2I model and the prompt expansion network,\nfacilitating the generation of quality-aware text prompts, thus further\nenhancing the final image quality. To counteract the potential catastrophic\nforgetting of the original user prompt due to prompt expansion, we introduce\noriginal prompt centered guidance at inference time, ensuring that the\ngenerated image remains faithful to the user input. Extensive experiments and a\nuser study demonstrate that Parrot outperforms several baseline methods across\nvarious quality criteria, including aesthetics, human preference, image\nsentiment, and text-image alignment.\n",
        "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for\n  Text-to-Image Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05676",
        "abstract_url": "http://arxiv.org/abs/2401.05676",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Weibo"
            },
            {
                "last_name": "Ren",
                "first_name": "Weihong"
            },
            {
                "last_name": "Tian",
                "first_name": "Jiandong"
            },
            {
                "last_name": "Qu",
                "first_name": "Liangqiong"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Liu",
                "first_name": "Honghai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human-Object Interaction (HOI) detection plays a vital role in scene\nunderstanding, which aims to predict the HOI triplet in the form of <human,\nobject, action>. Existing methods mainly extract multi-modal features (e.g.,\nappearance, object semantics, human pose) and then fuse them together to\ndirectly predict HOI triplets. However, most of these methods focus on seeking\nfor self-triplet aggregation, but ignore the potential cross-triplet\ndependencies, resulting in ambiguity of action prediction. In this work, we\npropose to explore Self- and Cross-Triplet Correlations (SCTC) for HOI\ndetection. Specifically, we regard each triplet proposal as a graph where\nHuman, Object represent nodes and Action indicates edge, to aggregate\nself-triplet correlation. Also, we try to explore cross-triplet dependencies by\njointly considering instance-level, semantic-level, and layout-level relations.\nBesides, we leverage the CLIP model to assist our SCTC obtain interaction-aware\nfeature by knowledge distillation, which provides useful action clues for HOI\ndetection. Extensive experiments on HICO-DET and V-COCO datasets verify the\neffectiveness of our proposed SCTC.\n",
        "title": "Exploring Self- and Cross-Triplet Correlations for Human-Object\n  Interaction Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05680",
        "abstract_url": "http://arxiv.org/abs/2401.05680",
        "authors": [
            {
                "last_name": "Mitra",
                "first_name": "Shaswata"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Trisha"
            },
            {
                "last_name": "Neupane",
                "first_name": "Subash"
            },
            {
                "last_name": "Piplai",
                "first_name": "Aritran"
            },
            {
                "last_name": "Mittal",
                "first_name": "Sudip"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "",
            "LG",
            "NE"
        ],
        "abstract": "  In an increasingly interconnected world, where information is the lifeblood\nof modern society, regular cyber-attacks sabotage the confidentiality,\nintegrity, and availability of digital systems and information. Additionally,\ncyber-attacks differ depending on the objective and evolve rapidly to disguise\ndefensive systems. However, a typical cyber-attack demonstrates a series of\nstages from attack initiation to final resolution, called an attack life cycle.\nThese diverse characteristics and the relentless evolution of cyber attacks\nhave led cyber defense to adopt modern approaches like Machine Learning to\nbolster defensive measures and break the attack life cycle. Among the adopted\nML approaches, Graph Neural Networks have emerged as a promising approach for\nenhancing the effectiveness of defensive measures due to their ability to\nprocess and learn from heterogeneous cyber threat data. In this paper, we look\ninto the application of GNNs in aiding to break each stage of one of the most\nrenowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address\neach phase of CKC and discuss how GNNs contribute to preparing and preventing\nan attack from a defensive standpoint. Furthermore, We also discuss open\nresearch areas and further improvement scopes.\n",
        "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05683",
        "abstract_url": "http://arxiv.org/abs/2401.05683",
        "authors": [
            {
                "last_name": "Sankar",
                "first_name": "V. Udaya"
            },
            {
                "last_name": "Rao",
                "first_name": "Vishisht Srihari"
            },
            {
                "last_name": "Narahari",
                "first_name": "Y."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            ""
        ],
        "abstract": "  Mechanism design is essentially reverse engineering of games and involves\ninducing a game among strategic agents in a way that the induced game satisfies\na set of desired properties in an equilibrium of the game. Desirable properties\nfor a mechanism include incentive compatibility, individual rationality,\nwelfare maximisation, revenue maximisation (or cost minimisation), fairness of\nallocation, etc. It is known from mechanism design theory that only certain\nstrict subsets of these properties can be simultaneously satisfied exactly by\nany given mechanism. Often, the mechanisms required by real-world applications\nmay need a subset of these properties that are theoretically impossible to be\nsimultaneously satisfied. In such cases, a prominent recent approach is to use\na deep learning based approach to learn a mechanism that approximately\nsatisfies the required properties by minimizing a suitably defined loss\nfunction. In this paper, we present, from relevant literature, technical\ndetails of using a deep learning approach for mechanism design and provide an\noverview of key results in this topic. We demonstrate the power of this\napproach for three illustrative case studies: (a) efficient energy management\nin a vehicular network (b) resource allocation in a mobile network (c)\ndesigning a volume discount procurement auction for agricultural inputs.\nSection 6 concludes the paper.\n",
        "title": "Deep Learning Meets Mechanism Design: Key Results and Some Novel\n  Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05684",
        "abstract_url": "http://arxiv.org/abs/2401.05684",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Sirui"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Li",
                "first_name": "Liang"
            },
            {
                "last_name": "Ding",
                "first_name": "Lingyun"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            ""
        ],
        "abstract": "  Multiscale metrics such as negative Sobolev norms are effective for\nquantifying the degree of mixedness of a passive scalar field advected by an\nincompressible flow in the absence of diffusion. In this paper we introduce a\nmix norm that is motivated by Sobolev norm $H^{-1}$ for a general domain with a\nno-flux boundary. We then derive an explicit expression for the optimal flow\nthat maximizes the instantaneous decay rate of the mix norm under fixed energy\nand enstrophy constraints. Numerical simulations indicate that the mix norm\ndecays exponentially or faster for various initial conditions and geometries\nand the rate is closely related to the smallest non-zero eigenvalue of the\nLaplace operator. These results generalize previous findings restricted for a\nperiodic domain for its analytical and numerical simplicity. Additionally, we\nobserve that periodic boundaries tend to induce a faster decay in mix norm\ncompared to no-flux conditions under the fixed energy constraint, while the\ncomparison is reversed for the fixed enstrophy constraint. In the special case\nof even initial distributions, two types of boundary conditions yield the same\noptimal flow and mix norm decay.\n",
        "title": "Optimal Stirring Strategies for Passive Scalars in a Domain with a\n  General Shape and No-Flux Boundary Condition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05686",
        "abstract_url": "http://arxiv.org/abs/2401.05686",
        "authors": [
            {
                "last_name": "Appolinary",
                "first_name": "Blaise"
            },
            {
                "last_name": "Deaconu",
                "first_name": "Alex"
            },
            {
                "last_name": "Yang",
                "first_name": "Sophia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we present a novel method for dynamically expanding\nConvolutional Neural Networks (CNNs) during training, aimed at meeting the\nincreasing demand for efficient and sustainable deep learning models. Our\napproach, drawing from the seminal work on Self-Expanding Neural Networks\n(SENN), employs a natural expansion score as an expansion criteria to address\nthe common issue of over-parameterization in deep convolutional neural\nnetworks, thereby ensuring that the model's complexity is finely tuned to the\ntask's specific needs. A significant benefit of this method is its eco-friendly\nnature, as it obviates the necessity of training multiple models of different\nsizes. We employ a strategy where a single model is dynamically expanded,\nfacilitating the extraction of checkpoints at various complexity levels,\neffectively reducing computational resource use and energy consumption while\nalso expediting the development cycle by offering diverse model complexities\nfrom a single training session. We evaluate our method on the CIFAR-10 dataset\nand our experimental results validate this approach, demonstrating that\ndynamically adding layers not only maintains but also improves CNN performance,\nunderscoring the effectiveness of our expansion criteria. This approach marks a\nconsiderable advancement in developing adaptive, scalable, and environmentally\nconsiderate neural network architectures, addressing key challenges in the\nfield of deep learning.\n",
        "title": "Self Expanding Convolutional Neural Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05689",
        "abstract_url": "http://arxiv.org/abs/2401.05689",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Wang",
                "first_name": "Minghan"
            },
            {
                "last_name": "Qiao",
                "first_name": "Xiaosong"
            },
            {
                "last_name": "Wei",
                "first_name": "Daimeng"
            },
            {
                "last_name": "Shang",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Li",
                "first_name": "Zongyao"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhengzhe"
            },
            {
                "last_name": "Li",
                "first_name": "Yinglu"
            },
            {
                "last_name": "Su",
                "first_name": "Chang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            },
            {
                "last_name": "Tao",
                "first_name": "Shimin"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD",
            ""
        ],
        "abstract": "  Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER). Previous works usually adopt end-to-end models and has strong\ndependency on Pseudo Paired Data and Original Paired Data. But when only\npre-training on Pseudo Paired Data, previous models have negative effect on\ncorrection. While fine-tuning on Original Paired Data, the source side data\nmust be transcribed by a well-trained ASR model, which takes a lot of time and\nnot universal. In this paper, we propose UCorrect, an unsupervised\nDetector-Generator-Selector framework for ASR Error Correction. UCorrect has no\ndependency on the training data mentioned before. The whole procedure is first\nto detect whether the character is erroneous, then to generate some candidate\ncharacters and finally to select the most confident one to replace the error\ncharacter. Experiments on the public AISHELL-1 dataset and WenetSpeech dataset\nshow the effectiveness of UCorrect for ASR error correction: 1) it achieves\nsignificant WER reduction, achieves 6.83\\% even without fine-tuning and 14.29\\%\nafter fine-tuning; 2) it outperforms the popular NAR correction models by a\nlarge margin with a competitive low latency; and 3) it is an universal method,\nas it reduces all WERs of the ASR model with different decoding strategies and\nreduces all WERs of ASR models trained on different scale datasets.\n",
        "title": "UCorrect: An Unsupervised Framework for Automatic Speech Recognition\n  Error Correction",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05690",
        "abstract_url": "http://arxiv.org/abs/2401.05690",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Cong"
            },
            {
                "last_name": "You",
                "first_name": "Changsheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haodong"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            },
            {
                "last_name": "Shi",
                "first_name": "Shuo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Extremely large-scale array (XL-array) has emerged as a promising technology\nto enable near-field communications for achieving enhanced spectrum efficiency\nand spatial resolution, by drastically increasing the number of antennas.\nHowever, this also inevitably incurs higher hardware and energy cost, which may\nnot be affordable in future wireless systems. To address this issue, we propose\nin this paper to exploit two types of sparse arrays (SAs) for enabling\nnear-field communications. Specifically, we first consider the linear sparse\narray (LSA) and characterize its near-field beam pattern. It is shown that\ndespite the achieved beam-focusing gain, the LSA introduces several undesired\ngrating-lobes, which have comparable beam power with the main-lobe and are\nfocused on specific regions. An efficient hybrid beamforming design is then\nproposed for the LSA to deal with the potential strong inter-user interference\n(IUI). Next, we consider another form of SA, called extended coprime array\n(ECA), which is composed of two LSA subarrays with different (coprime)\ninter-antenna spacing. By characterizing the ECA near-field beam pattern, we\nshow that compared with the LSA with the same array sparsity, the ECA can\ngreatly suppress the beam power of near-field grating-lobes thanks to the\noffset effect of the two subarrays, albeit with a larger number of\ngrating-lobes. This thus motivates us to propose a customized two-phase hybrid\nbeamforming design for the ECA. Finally, numerical results are presented to\ndemonstrate the rate performance gain of the proposed two SAs over the\nconventional uniform linear array (ULA).\n",
        "title": "Sparse Array Enabled Near-Field Communications: Beam Pattern Analysis\n  and Hybrid Beamforming Design",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05695",
        "abstract_url": "http://arxiv.org/abs/2401.05695",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Chengfeng"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Jiao",
                "first_name": "Wenpin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yongqiang"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhenwei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The use of large language models in medical dialogue generation has garnered\nsignificant attention, with a focus on improving response quality and fluency.\nWhile previous studies have made progress in optimizing model performance for\nsingle-round medical Q&A tasks, there is a need to enhance the model's\ncapability for multi-round conversations to avoid logical inconsistencies. To\naddress this, we propose an approach called preference learning from process\nfeedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF\ninvolves rule modeling, preference data generation, and preference alignment to\ntrain the model to adhere to the diagnostic process. Experimental results using\nStandardized Patient Testing show that PLPF enhances the diagnostic accuracy of\nthe baseline model in medical conversations by 17.6%, outperforming traditional\nreinforcement learning from human feedback. Additionally, PLPF demonstrates\neffectiveness in both multi-round and single-round dialogue tasks, showcasing\nits potential for improving medical dialogue generation.\n",
        "title": "Integrating Physician Diagnostic Logic into Large Language Models:\n  Preference Learning from Process Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05698",
        "abstract_url": "http://arxiv.org/abs/2401.05698",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Licai"
            },
            {
                "last_name": "Lian",
                "first_name": "Zheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Bin"
            },
            {
                "last_name": "Tao",
                "first_name": "Jianhua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC",
            "MM",
            "SD",
            ""
        ],
        "abstract": "  Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in\nrecent years for its critical role in creating emotion-ware intelligent\nmachines. Previous efforts in this area are dominated by the supervised\nlearning paradigm. Despite significant progress, supervised learning is meeting\nits bottleneck due to the longstanding data scarcity issue in AVER. Motivated\nby recent advances in self-supervised learning, we propose Hierarchical\nContrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that\nleverages large-scale self-supervised pre-training on vast unlabeled\naudio-visual data to promote the advancement of AVER. Following prior arts in\nself-supervised audio-visual representation learning, HiCMAE adopts two primary\nforms of self-supervision for pre-training, namely masked data modeling and\ncontrastive learning. Unlike them which focus exclusively on top-layer\nrepresentations while neglecting explicit guidance of intermediate layers,\nHiCMAE develops a three-pronged strategy to foster hierarchical audio-visual\nfeature learning and improve the overall quality of learned representations. To\nverify the effectiveness of HiCMAE, we conduct extensive experiments on 9\ndatasets covering both categorical and dimensional AVER tasks. Experimental\nresults show that our method significantly outperforms state-of-the-art\nsupervised and self-supervised audio-visual methods, which indicates that\nHiCMAE is a powerful audio-visual emotion representation learner. Codes and\nmodels will be publicly available at https://github.com/sunlicai/HiCMAE.\n",
        "title": "HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised\n  Audio-Visual Emotion Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05700",
        "abstract_url": "http://arxiv.org/abs/2401.05700",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhanglin"
            },
            {
                "last_name": "Li",
                "first_name": "Zongyao"
            },
            {
                "last_name": "Shang",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Wei",
                "first_name": "Daimeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Rao",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Shaojun"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Incremental Decoding is an effective framework that enables the use of an\noffline model in a simultaneous setting without modifying the original model,\nmaking it suitable for Low-Latency Simultaneous Speech Translation. However,\nthis framework may introduce errors when the system outputs from incomplete\ninput. To reduce these output errors, several strategies such as Hold-$n$,\nLA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be\ncarefully selected for optimal performance. Moreover, these strategies are more\nsuitable for end-to-end systems than cascade systems. In our paper, we propose\na new adaptable and efficient policy named \"Regularized Batched Inputs\". Our\nmethod stands out by enhancing input diversity to mitigate output errors. We\nsuggest particular regularization techniques for both end-to-end and cascade\nsystems. We conducted experiments on IWSLT Simultaneous Speech Translation\n(SimulST) tasks, which demonstrate that our approach achieves low latency while\nmaintaining no more than 2 BLEU points loss compared to offline systems.\nFurthermore, our SimulST systems attained several new state-of-the-art results\nin various language directions.\n",
        "title": "R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework\n  for Low-Latency Simultaneous Speech Translation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05702",
        "abstract_url": "http://arxiv.org/abs/2401.05702",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Hui"
            },
            {
                "last_name": "Sun",
                "first_name": "Qianru"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Anomaly Detection (VAD) aims to localize abnormal events on the\ntimeline of long-range surveillance videos. Anomaly-scoring-based methods have\nbeen prevailing for years but suffer from the high complexity of thresholding\nand low explanability of detection results. In this paper, we conduct pioneer\nresearch on equipping video-based large language models (VLLMs) in the\nframework of VAD, making the VAD model free from thresholds and able to explain\nthe reasons for the detected anomalies. We introduce a novel network module\nLong-Term Context (LTC) to mitigate the incapability of VLLMs in long-range\ncontext modeling. We design a three-phase training method to improve the\nefficiency of fine-tuning VLLMs by substantially minimizing the requirements\nfor VAD data and lowering the costs of annotating instruction-tuning data. Our\ntrained model achieves the top performance on the anomaly videos of the\nUCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%,\nrespectively. More impressively, our approach can provide textual explanations\nfor detected anomalies.\n",
        "title": "Video Anomaly Detection and Explanation via Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05705",
        "abstract_url": "http://arxiv.org/abs/2401.05705",
        "authors": [
            {
                "last_name": "Krivorotko",
                "first_name": "Olga"
            },
            {
                "last_name": "Zvonareva",
                "first_name": "Tatiana"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "",
            ""
        ],
        "abstract": "  A numerical algorithm for regularization of the solution of the source\nproblem for the diffusion-logistic model based on information about the process\nat fixed moments of time of integral type has been developed. The peculiarity\nof the problem under study is the discrete formulation in space and\nimpossibility to apply classical algorithms for its numerical solution. The\nregularization of the problem is based on the application of A.N. Tikhonov's\napproach and a priori information about the source of the process. The problem\nwas formulated in a variational formulation and solved by the global tensor\noptimization method. It is shown that in the case of noisy data regularization\nimproves the accuracy of the reconstructed source.\n",
        "title": "Regularization of the discrete source problem in the nonlinear\n  diffusive-logistic equation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05707",
        "abstract_url": "http://arxiv.org/abs/2401.05707",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Xi",
                "first_name": "Dinghao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Liumin"
            },
            {
                "last_name": "Xu",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Text style transfer is increasingly prominent in online entertainment and\nsocial media. However, existing research mainly concentrates on style transfer\nwithin individual English sentences, while ignoring the complexity of long\nChinese texts, which limits the wider applicability of style transfer in\ndigital media realm. To bridge this gap, we propose a Chinese Article-style\nTransfer framework (CAT-LLM), leveraging the capabilities of Large Language\nModels (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition\n(TSD) module aimed at comprehensively analyzing text features in articles,\nprompting LLMs to efficiently transfer Chinese article-style. The TSD module\nintegrates a series of machine learning algorithms to analyze article-style\nfrom both words and sentences levels, thereby aiding LLMs thoroughly grasp the\ntarget style without compromising the integrity of the original text. In\naddition, this module supports dynamic expansion of internal style trees,\nshowcasing robust compatibility and allowing flexible optimization in\nsubsequent research. Moreover, we select five Chinese articles with distinct\nstyles and create five parallel datasets using ChatGPT, enhancing the models'\nperformance evaluation accuracy and establishing a novel paradigm for\nevaluating subsequent research on article-style transfer. Extensive\nexperimental results affirm that CAT-LLM outperforms current research in terms\nof transfer accuracy and content preservation, and has remarkable applicability\nto various types of LLMs.\n",
        "title": "CAT-LLM: Prompting Large Language Models with Text Style Definition for\n  Chinese Article-style Transfer",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05708",
        "abstract_url": "http://arxiv.org/abs/2401.05708",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Che-Kai"
            },
            {
                "last_name": "Li",
                "first_name": "Chao"
            },
            {
                "last_name": "Mao",
                "first_name": "Ruibin"
            },
            {
                "last_name": "Yang",
                "first_name": "Jianyi"
            },
            {
                "last_name": "K\u00e4mpfe",
                "first_name": "Thomas"
            },
            {
                "last_name": "Imani",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Li",
                "first_name": "Can"
            },
            {
                "last_name": "Zhuo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Yin",
                "first_name": "Xunzhao"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Rapid advancements in artificial intelligence have given rise to\ntransformative models, profoundly impacting our lives. These models demand\nmassive volumes of data to operate effectively, exacerbating the data-transfer\nbottleneck inherent in the conventional von-Neumann architecture.\nCompute-in-memory (CIM), a novel computing paradigm, tackles these issues by\nseamlessly embedding in-memory search functions, thereby obviating the need for\ndata transfers. However, existing non-volatile memory (NVM)-based accelerators\nare application specific. During the similarity based associative search\noperation, they only support a single, specific distance metric, such as\nHamming, Manhattan, or Euclidean distance in measuring the query against the\nstored data, calling for reconfigurable in-memory solutions adaptable to\nvarious applications. To overcome such a limitation, in this paper, we present\nFeReX, a reconfigurable associative memory (AM) that accommodates various\ndistance metrics including Hamming, Manhattan, and Euclidean distances.\nLeveraging multi-bit ferroelectric field-effect transistors (FeFETs) as the\nproxy and a hardware-software co-design approach, we introduce a constrained\nsatisfaction problem (CSP)-based method to automate AM search input voltage and\nstored voltage configurations for different distance based search functions.\nDevice-circuit co-simulations first validate the effectiveness of the proposed\nFeReX methodology for reconfigurable search distance functions. Then, we\nbenchmark FeReX in the context of k-nearest neighbor (KNN) and hyperdimensional\ncomputing (HDC), which highlights the robustness of FeReX and demonstrates up\nto 250x speedup and 10^4 energy savings compared with GPU.\n",
        "title": "FeReX: A Reconfigurable Design of Multi-bit Ferroelectric\n  Compute-in-Memory for Nearest Neighbor Search",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05709",
        "abstract_url": "http://arxiv.org/abs/2401.05709",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Penghong"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Wenrui"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Debin"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            ""
        ],
        "abstract": "  Localization is one of the pivotal issues in wireless sensor network\napplications. In 3D localization studies, most algorithms focus on enhancing\nthe location prediction process, lacking theoretical derivation of the\ndetection distance of an anchor node at the varying hops, engenders a\nlocalization performance bottleneck. To address this issue, we propose a\nprobability-based average distance estimation (PADE) model that utilizes the\nprobability distribution of node distances detected by an anchor node. The aim\nis to mathematically derive the average distances of nodes detected by an\nanchor node at different hops. First, we develop a probability-based maximum\ndistance estimation (PMDE) model to calculate the upper bound of the distance\ndetected by an anchor node. Then, we present the PADE model, which relies on\nthe upper bound obtained of the distance by the PMDE model. Finally, the\nobtained average distance is used to construct a distance loss function, and it\nis embedded with the traditional distance loss function into a multi-objective\ngenetic algorithm to predict the locations of unknown nodes. The experimental\nresults demonstrate that the proposed method achieves state-of-the-art\nperformance in random and multimodal distributed sensor networks. The average\nlocalization accuracy is improved by 3.49\\%-12.66\\% and 3.99%-22.34%,\nrespectively.\n",
        "title": "Probability-based Distance Estimation Model for 3D DV-Hop Localization\n  in WSNs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05710",
        "abstract_url": "http://arxiv.org/abs/2401.05710",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zhihui"
            },
            {
                "last_name": "Perrault",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study reinforcement learning in the presence of an unknown reward\nperturbation. Existing methodologies for this problem make strong assumptions\nincluding reward smoothness, known perturbations, and/or perturbations that do\nnot modify the optimal policy. We study the case of unknown arbitrary\nperturbations that discretize and shuffle reward space, but have the property\nthat the true reward belongs to the most frequently observed class after\nperturbation. This class of perturbations generalizes existing classes (and, in\nthe limit, all continuous bounded perturbations) and defeats existing methods.\nWe introduce an adaptive distributional reward critic and show theoretically\nthat it can recover the true rewards under technical conditions. Under the\ntargeted perturbation in discrete and continuous control tasks, we win/tie the\nhighest return in 40/57 settings (compared to 16/57 for the best baseline).\nEven under the untargeted perturbation, we still win an edge over the baseline\ndesigned especially for that setting.\n",
        "title": "The Distributional Reward Critic Architecture for Perturbed-Reward\n  Reinforcement Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05711",
        "abstract_url": "http://arxiv.org/abs/2401.05711",
        "authors": [
            {
                "last_name": "Jiao",
                "first_name": "Jiyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Han",
                "first_name": "Chenpei"
            },
            {
                "last_name": "Huang",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yizhuo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  While fingerprinting localization is favored for its effectiveness, it is\nhindered by high data acquisition costs and the inaccuracy of static\ndatabase-based estimates. Addressing these issues, this letter presents an\ninnovative indoor localization method using a data-efficient meta-learning\nalgorithm. This approach, grounded in the ``Learning to Learn'' paradigm of\nmeta-learning, utilizes historical localization tasks to improve adaptability\nand learning efficiency in dynamic indoor environments. We introduce a\ntask-weighted loss to enhance knowledge transfer within this framework. Our\ncomprehensive experiments confirm the method's robustness and superiority over\ncurrent benchmarks, achieving a notable 23.13\\% average gain in Mean Euclidean\nDistance, particularly effective in scenarios with limited CSI data.\n",
        "title": "Dynamic Indoor Fingerprinting Localization based on Few-Shot\n  Meta-Learning with CSI Images",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05712",
        "abstract_url": "http://arxiv.org/abs/2401.05712",
        "authors": [
            {
                "last_name": "Hoang",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  Combining discovery and augmentation is important in the era of data usage\nwhen it comes to predicting the outcome of tasks. However, having to ask the\nuser the utility function to discover the goal to achieve the optimal small\nrightful dataset is not an optimal solution. The existing solutions do not make\ngood use of this combination, hence underutilizing the data. In this paper, we\nintroduce a novel goal-oriented framework, called BOD: Blindly Optimal Data\nDiscovery, that involves humans in the loop and comparing utility scores every\ntime querying in the process without knowing the utility function. This\nestablishes the promise of using BOD: Blindly Optimal Data Discovery for modern\ndata science solutions.\n",
        "title": "BOD: Blindly Optimal Data Discovery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05715",
        "abstract_url": "http://arxiv.org/abs/2401.05715",
        "authors": [
            {
                "last_name": "Jackiewicz",
                "first_name": "Marcel"
            },
            {
                "last_name": "Kasperski",
                "first_name": "Adam"
            },
            {
                "last_name": "Zielinski",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In this paper, the recoverable robust shortest path problem under interval\nuncertainty representations is discussed. This problem is known to be strongly\nNP-hard and also hard to approximate in general digraphs. In this paper, the\nclass of acyclic digraphs is considered. It is shown that for the traditional\ninterval uncertainty, the problem can be solved in polynomial time for all\nnatural, known from the literature, neighborhoods. Efficient algorithms for\nvarious classes of acyclic digraphs are constructed. Some negative results for\ngeneral digraphs are strengthened. Finally, some exact and approximate methods\nof solving the problem under budgeted interval uncertainty are proposed.\n",
        "title": "Recoverable robust shortest path problem under interval uncertainty\n  representations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05716",
        "abstract_url": "http://arxiv.org/abs/2401.05716",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Xu"
            },
            {
                "last_name": "Scarlett",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In this paper, we study the problem of estimating the normalizing constant\n$\\int e^{-\\lambda f(x)}dx$ through queries to the black-box function $f$, where\n$f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\\lambda$ is a\nproblem parameter. We show that to estimate the normalizing constant within a\nsmall relative error, the level of difficulty depends on the value of\n$\\lambda$: When $\\lambda$ approaches zero, the problem is similar to Bayesian\nquadrature (BQ), while when $\\lambda$ approaches infinity, the problem is\nsimilar to Bayesian optimization (BO). More generally, the problem varies\nbetween BQ and BO. We find that this pattern holds true even when the function\nevaluations are noisy, bringing new aspects to this topic. Our findings are\nsupported by both algorithm-independent lower bounds and algorithmic upper\nbounds, as well as simulation studies conducted on a variety of benchmark\nfunctions.\n",
        "title": "Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature\n  and Bayesian Optimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05717",
        "abstract_url": "http://arxiv.org/abs/2401.05717",
        "authors": [
            {
                "last_name": "Salvi",
                "first_name": "Giampiero"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT",
            "LG",
            "SD",
            "",
            "",
            ""
        ],
        "abstract": "  This article investigates the possibility to use the class entropy of the\noutput of a connectionist phoneme recogniser to predict time boundaries between\nphonetic classes. The rationale is that the value of the entropy should\nincrease in proximity of a transition between two segments that are well\nmodelled (known) by the recognition network since it is a measure of\nuncertainty. The advantage of this measure is its simplicity as the posterior\nprobabilities of each class are available in connectionist phoneme recognition.\nThe entropy and a number of measures based on differentiation of the entropy\nare used in isolation and in combination. The decision methods for predicting\nthe boundaries range from simple thresholds to neural network based procedure.\nThe different methods are compared with respect to their precision, measured in\nterms of the ratio between the number C of predicted boundaries within 10 or 20\nmsec of the reference and the total number of predicted boundaries, and recall,\nmeasured as the ratio between C and the total number of reference boundaries.\n",
        "title": "Segment Boundary Detection via Class Entropy Measurements in\n  Connectionist Phoneme Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05722",
        "abstract_url": "http://arxiv.org/abs/2401.05722",
        "authors": [
            {
                "last_name": "Court\u00e8s",
                "first_name": "Cl\u00e9mentine"
            },
            {
                "last_name": "Boileau",
                "first_name": "Matthieu"
            },
            {
                "last_name": "C\u00f4te",
                "first_name": "Rapha\u00ebl"
            },
            {
                "last_name": "Hervieux",
                "first_name": "Paul-Antoine"
            },
            {
                "last_name": "Manfredi",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  We solve the Landau-Lifshitz-Gilbert equation in the finite-temperature\nregime, where thermal fluctuations are modeled by a random magnetic field whose\nvariance is proportional to the temperature. By rescaling the temperature\nproportionally to the computational cell size $\\Delta x$ ($T \\to T\\,\\Delta\nx/a_{\\text{eff}}$, where $a_{\\text{eff}}$ is the lattice constant) [M. B. Hahn,\nJ. Phys. Comm., 3:075009, 2019], we obtain Curie temperatures $T_{\\text{C}}$\nthat are in line with the experimental values for cobalt, iron and nickel. For\nfinite-sized objects such as nanowires (1D) and nanolayers (2D), the Curie\ntemperature varies with the smallest size $d$ of the system. We show that the\ndifference between the computed finite-size $T_{\\text{C}}$ and the bulk\n$T_{\\text{C}}$ follows a power-law of the type: $(\\xi_0/d)^\\lambda$, where\n$\\xi_0$ is the correlation length at zero temperature, and $\\lambda$ is a\ncritical exponent. We obtain values of $\\xi_0$ in the nanometer range, also in\naccordance with other simulations and experiments. The computed critical\nexponent is close to $\\lambda=2$ for all considered materials and geometries.\nThis is the expected result for a mean-field approach, but slightly larger than\nthe values observed experimentally.\n",
        "title": "Micromagnetic simulations of the size dependence of the Curie\n  temperature in ferromagnetic nanowires and nanolayers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05725",
        "abstract_url": "http://arxiv.org/abs/2401.05725",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Han"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Mu",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weile"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Wong",
                "first_name": "Kai-Kit"
            },
            {
                "last_name": "Yang",
                "first_name": "Kun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  A simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS) enhanced unnamed aerial vehicle (UAV)-enabled multi-user\nmobile edge computing (MEC) scheme is proposed in this paper. Different from\nthe existing MEC works, the proposed scheme allows bi-directional offloading\nwhere users can simultaneously offload their computing tasks to the MEC servers\nsituated at the ground base station (BS) and aerial UAV with the assistance of\nthe STARRIS. Specifically, we formulate an optimization problem aiming at\nmaximizing the energy efficiency of the system while ensuring the quality of\nservice (QoS) constraint by jointly optimizing the resource allocation, user\nscheduling, passive beamforming of the STAR-RIS, and the UAV trajectory. An\niterative algorithm designed with the Dinkelbach's algorithm and the successive\nconvex approximation (SCA) is proposed to effectively handle the formulated\nnon-convex optimization problem. Simulation results indicate that the proposed\nSTAR-RIS enhanced UAV-enabled MEC scheme possesses significant advantages in\nenhancing the system energy efficiency over other baseline schemes including\nthe conventional RIS-aided scheme.\n",
        "title": "STAR-RIS Enhanced UAV-Enabled MEC Networks with Bi-Directional Task\n  Offloading",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05727",
        "abstract_url": "http://arxiv.org/abs/2401.05727",
        "authors": [
            {
                "last_name": "Chopra",
                "first_name": "Sahil"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Part of speech tagging in zero-resource settings can be an effective approach\nfor low-resource languages when no labeled training data is available. Existing\nsystems use two main techniques for POS tagging i.e. pretrained multilingual\nlarge language models(LLM) or project the source language labels into the zero\nresource target language and train a sequence labeling model on it. We explore\nthe latter approach using the off-the-shelf alignment module and train a hidden\nMarkov model(HMM) to predict the POS tags. We evaluate transfer learning setup\nwith English as a source language and French, German, and Spanish as target\nlanguages for part-of-speech tagging. Our conclusion is that projected\nalignment data in zero-resource language can be beneficial to predict POS tags.\n",
        "title": "Zero Resource Cross-Lingual Part Of Speech Tagging",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05730",
        "abstract_url": "http://arxiv.org/abs/2401.05730",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Jaeill"
            },
            {
                "last_name": "Hwang",
                "first_name": "Duhun"
            },
            {
                "last_name": "Lee",
                "first_name": "Eunjung"
            },
            {
                "last_name": "Suh",
                "first_name": "Jangwon"
            },
            {
                "last_name": "Kim",
                "first_name": "Jimyeong"
            },
            {
                "last_name": "Rhee",
                "first_name": "Wonjong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  In the past few years, contrastive learning has played a central role for the\nsuccess of visual unsupervised representation learning. Around the same time,\nhigh-performance non-contrastive learning methods have been developed as well.\nWhile most of the works utilize only two views, we carefully review the\nexisting multi-view methods and propose a general multi-view strategy that can\nimprove learning speed and performance of any contrastive or non-contrastive\nmethod. We first analyze CMC's full-graph paradigm and empirically show that\nthe learning speed of $K$-views can be increased by $_{K}\\mathrm{C}_{2}$ times\nfor small learning rate and early training. Then, we upgrade CMC's full-graph\nby mixing views created by a crop-only augmentation, adopting small-size views\nas in SwAV multi-crop, and modifying the negative sampling. The resulting\nmulti-view strategy is called ECPP (Efficient Combinatorial Positive Pairing).\nWe investigate the effectiveness of ECPP by applying it to SimCLR and assessing\nthe linear evaluation performance for CIFAR-10 and ImageNet-100. For each\nbenchmark, we achieve a state-of-the-art performance. In case of ImageNet-100,\nECPP boosted SimCLR outperforms supervised learning.\n",
        "title": "Enhancing Contrastive Learning with Efficient Combinatorial Positive\n  Pairing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05731",
        "abstract_url": "http://arxiv.org/abs/2401.05731",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Xiaohui"
            },
            {
                "last_name": "Li",
                "first_name": "Wenxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhongzhi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "",
            ""
        ],
        "abstract": "  In order to investigate the relationship between Shannon information measure\nof random variables, scholars such as Yeung utilized information diagrams to\nexplore the structured representation of information measures, establishing\ncorrespondences with sets. However, this method has limitations when studying\ninformation measures of five or more random variables. In this paper, we\nconsider employing algebraic methods to study the relationship of information\nmeasures of random variables. By introducing a semiring generated by random\nvariables, we establish correspondences between sets and elements of the\nsemiring. Utilizing the Grobner-Shirshov basis, we present the structure of the\nsemiring and its standard form. Furthermore, we delve into the structure of the\nsemiring generated under Markov chain conditions (referred to as Markov\nsemiring), obtaining its standard form.\n",
        "title": "On Grobner-Shirshov bases for Markov semirings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05735",
        "abstract_url": "http://arxiv.org/abs/2401.05735",
        "authors": [
            {
                "last_name": "Kahatapitiya",
                "first_name": "Kumara"
            },
            {
                "last_name": "Karjauv",
                "first_name": "Adil"
            },
            {
                "last_name": "Abati",
                "first_name": "Davide"
            },
            {
                "last_name": "Porikli",
                "first_name": "Fatih"
            },
            {
                "last_name": "Asano",
                "first_name": "Yuki M."
            },
            {
                "last_name": "Habibian",
                "first_name": "Amirhossein"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion-based video editing have reached impressive quality and can\ntransform either the global style, local structure, and attributes of given\nvideo inputs, following textual edit prompts. However, such solutions typically\nincur heavy memory and computational costs to generate temporally-coherent\nframes, either in the form of diffusion inversion and/or cross-frame attention.\nIn this paper, we conduct an analysis of such inefficiencies, and suggest\nsimple yet effective modifications that allow significant speed-ups whilst\nmaintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as\nOCD, to further reduce latency by allocating computations more towards\nforeground edited regions that are arguably more important for perceptual\nquality. We achieve this by two novel proposals: i) Object-Centric Sampling,\ndecoupling the diffusion steps spent on salient regions or background,\nallocating most of the model capacity to the former, and ii) Object-Centric 3D\nToken Merging, which reduces cost of cross-frame attention by fusing redundant\ntokens in unimportant background regions. Both techniques are readily\napplicable to a given video editing model \\textit{without} retraining, and can\ndrastically reduce its memory and computational cost. We evaluate our proposals\non inversion-based and control-signal-based editing pipelines, and show a\nlatency reduction up to 10x for a comparable synthesis quality.\n",
        "title": "Object-Centric Diffusion for Efficient Video Editing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05736",
        "abstract_url": "http://arxiv.org/abs/2401.05736",
        "authors": [
            {
                "last_name": "Lerner",
                "first_name": "Paul"
            },
            {
                "last_name": "Ferret",
                "first_name": "Olivier"
            },
            {
                "last_name": "Guinaudeau",
                "first_name": "Camille"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Knowledge-based Visual Question Answering about Named Entities is a\nchallenging task that requires retrieving information from a multimodal\nKnowledge Base. Named entities have diverse visual representations and are\ntherefore difficult to recognize. We argue that cross-modal retrieval may help\nbridge the semantic gap between an entity and its depictions, and is foremost\ncomplementary with mono-modal retrieval. We provide empirical evidence through\nexperiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,\nInfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different\nstrategies to fine-tune such a model: mono-modal, cross-modal, or joint\ntraining. Our method, which combines mono-and cross-modal retrieval, is\ncompetitive with billion-parameter models on the three datasets, while being\nconceptually simpler and computationally cheaper.\n",
        "title": "Cross-modal Retrieval for Knowledge-based Visual Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05737",
        "abstract_url": "http://arxiv.org/abs/2401.05737",
        "authors": [
            {
                "last_name": "Manjavacas",
                "first_name": "Antonio"
            },
            {
                "last_name": "Campoy-Nieves",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Jim\u00e9nez-Raboso",
                "first_name": "Javier"
            },
            {
                "last_name": "Molina-Solana",
                "first_name": "Miguel"
            },
            {
                "last_name": "G\u00f3mez-Romero",
                "first_name": "Juan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "",
            ""
        ],
        "abstract": "  Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver\nof energy consumption in commercial and residential buildings. Recent studies\nhave shown that Deep Reinforcement Learning (DRL) algorithms can outperform\ntraditional reactive controllers. However, DRL-based solutions are generally\ndesigned for ad hoc setups and lack standardization for comparison. To fill\nthis gap, this paper provides a critical and reproducible evaluation, in terms\nof comfort and energy consumption, of several state-of-the-art DRL algorithms\nfor HVAC control. The study examines the controllers' robustness, adaptability,\nand trade-off between optimization goals by using the Sinergym framework. The\nresults obtained confirm the potential of DRL algorithms, such as SAC and TD3,\nin complex scenarios and reveal several challenges related to generalization\nand incremental learning.\n",
        "title": "An experimental evaluation of Deep Reinforcement Learning algorithms for\n  HVAC control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05738",
        "abstract_url": "http://arxiv.org/abs/2401.05738",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Boheng"
            },
            {
                "last_name": "Lu",
                "first_name": "Yi"
            },
            {
                "last_name": "Shi",
                "first_name": "Pengbo"
            },
            {
                "last_name": "Chen",
                "first_name": "Qingzi"
            },
            {
                "last_name": "Liu",
                "first_name": "Jirui"
            },
            {
                "last_name": "Zhu",
                "first_name": "Lingyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We revisit the relationship between attention mechanisms and large kernel\nConvNets in visual transformers and propose a new spatial attention named Large\nKernel Convolutional Attention (LKCA). It simplifies the attention operation by\nreplacing it with a single large kernel convolution. LKCA combines the\nadvantages of convolutional neural networks and visual transformers, possessing\na large receptive field, locality, and parameter sharing. We explained the\nsuperiority of LKCA from both convolution and attention perspectives, providing\nequivalent code implementations for each view. Experiments confirm that LKCA\nimplemented from both the convolutional and attention perspectives exhibit\nequivalent performance. We extensively experimented with the LKCA variant of\nViT in both classification and segmentation tasks. The experiments demonstrated\nthat LKCA exhibits competitive performance in visual tasks. Our code will be\nmade publicly available at https://github.com/CatworldLee/LKCA.\n",
        "title": "LKCA: Large Kernel Convolutional Attention",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05739",
        "abstract_url": "http://arxiv.org/abs/2401.05739",
        "authors": [
            {
                "last_name": "Jia",
                "first_name": "Ang"
            },
            {
                "last_name": "Fan",
                "first_name": "Ming"
            },
            {
                "last_name": "Xu",
                "first_name": "Xi"
            },
            {
                "last_name": "Jin",
                "first_name": "Wuxia"
            },
            {
                "last_name": "Wang",
                "first_name": "Haijun"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Binary function similarity detection plays an important role in a wide range\nof security applications. Existing works usually assume that the query function\nand target function share equal semantics and compare their full semantics to\nobtain the similarity. However, we find that the function mapping is more\ncomplex, especially when function inlining happens.\n  In this paper, we will systematically investigate cross-inlining binary\nfunction similarity detection. We first construct a cross-inlining dataset by\ncompiling 51 projects using 9 compilers, with 4 optimizations, to 6\narchitectures, with 2 inlining flags, which results in two datasets both with\n216 combinations. Then we construct the cross-inlining function mappings by\nlinking the common source functions in these two datasets. Through analysis of\nthis dataset, we find that three cross-inlining patterns widely exist while\nexisting work suffers when detecting cross-inlining binary function similarity.\nNext, we propose a pattern-based model named CI-Detector for cross-inlining\nmatching. CI-Detector uses the attributed CFG to represent the semantics of\nbinary functions and GNN to embed binary functions into vectors. CI-Detector\nrespectively trains a model for these three cross-inlining patterns. Finally,\nthe testing pairs are input to these three models and all the produced\nsimilarities are aggregated to produce the final similarity. We conduct several\nexperiments to evaluate CI-Detector. Results show that CI-Detector can detect\ncross-inlining pairs with a precision of 81% and a recall of 97%, which exceeds\nall state-of-the-art works.\n",
        "title": "Cross-Inlining Binary Function Similarity Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05740",
        "abstract_url": "http://arxiv.org/abs/2401.05740",
        "authors": [
            {
                "last_name": "Berger",
                "first_name": "Andre"
            },
            {
                "last_name": "Rouhani",
                "first_name": "Arman"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Marc"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  In this paper, we introduce an improved upper bound for the efficiency of\nNash equilibria in utilitarian scheduling games on related machines. The\nmachines have varying speeds and adhere to the Shortest Processing Time (SPT)\npolicy as the global order for job processing. The goal of each job is to\nminimize its completion time, while the social objective is to minimize the sum\nof completion times. Our main result provides an upper bound of\n$2-\\frac{1}{2\\cdot(2m-1)}$ on the price of anarchy for the general case of $m$\nmachines. We improve this bound to 3/2 for the case of two machines, and to\n$2-\\frac{1}{2\\cdot m}$ for the general case of $m$ machines when the machines\nhave divisible speeds.\n",
        "title": "An improved bound for the price of anarchy for related machine\n  scheduling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05743",
        "abstract_url": "http://arxiv.org/abs/2401.05743",
        "authors": [
            {
                "last_name": "Marconi",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Rosati",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  We study consistent query answering over knowledge bases expressed by\nexistential rules. Specifically, we establish the data complexity of consistent\nquery answering and repair checking under tuple-deletion semantics for a\ngeneral class of disjunctive existential rules and for several subclasses\nthereof (acyclic, linear, full, guarded, and sticky). In particular, we\nidentify several cases in which the above problems are tractable or even\nfirst-order rewritable, and present new query rewriting techniques that can be\nthe basis for practical inconsistency-tolerant query answering systems.\n",
        "title": "Consistent Query Answering for Existential Rules under Tuple-Deletion\n  Semantics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05744",
        "abstract_url": "http://arxiv.org/abs/2401.05744",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yicong"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiangguo"
            },
            {
                "last_name": "Chen",
                "first_name": "Hongxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Sixiao"
            },
            {
                "last_name": "Yang",
                "first_name": "Yu"
            },
            {
                "last_name": "Xu",
                "first_name": "Guandong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Compared with only pursuing recommendation accuracy, the explainability of a\nrecommendation model has drawn more attention in recent years. Many graph-based\nrecommendations resort to informative paths with the attention mechanism for\nthe explanation. Unfortunately, these attention weights are intentionally\ndesigned for model accuracy but not explainability. Recently, some researchers\nhave started to question attention-based explainability because the attention\nweights are unstable for different reproductions, and they may not always align\nwith human intuition. Inspired by the counterfactual reasoning from causality\nlearning theory, we propose a novel explainable framework targeting path-based\nrecommendations, wherein the explainable weights of paths are learned to\nreplace attention weights. Specifically, we design two counterfactual reasoning\nalgorithms from both path representation and path topological structure\nperspectives. Moreover, unlike traditional case studies, we also propose a\npackage of explainability evaluation solutions with both qualitative and\nquantitative methods. We conduct extensive experiments on three real-world\ndatasets, the results of which further demonstrate the effectiveness and\nreliability of our method.\n",
        "title": "Attention Is Not the Only Choice: Counterfactual Reasoning for\n  Path-Based Explainable Recommendation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05745",
        "abstract_url": "http://arxiv.org/abs/2401.05745",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Barry Shichen"
            },
            {
                "last_name": "Liang",
                "first_name": "Siyun"
            },
            {
                "last_name": "Paetzold",
                "first_name": "Johannes"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose the use of a Transformer to accurately predict normals from point\nclouds with noise and density variations. Previous learning-based methods\nutilize PointNet variants to explicitly extract multi-scale features at\ndifferent input scales, then focus on a surface fitting method by which local\npoint cloud neighborhoods are fitted to a geometric surface approximated by\neither a polynomial function or a multi-layer perceptron (MLP). However,\nfitting surfaces to fixed-order polynomial functions can suffer from\noverfitting or underfitting, and learning MLP-represented hyper-surfaces\nrequires pre-generated per-point weights. To avoid these limitations, we first\nunify the design choices in previous works and then propose a simplified\nTransformer-based model to extract richer and more robust geometric features\nfor the surface normal estimation task. Through extensive experiments, we\ndemonstrate that our Transformer-based method achieves state-of-the-art\nperformance on both the synthetic shape dataset PCPNet, and the real-world\nindoor scene dataset SceneNN, exhibiting more noise-resilient behavior and\nsignificantly faster inference. Most importantly, we demonstrate that the\nsophisticated hand-designed modules in existing works are not necessary to\nexcel at the task of surface normal estimation.\n",
        "title": "Surface Normal Estimation with Transformers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05746",
        "abstract_url": "http://arxiv.org/abs/2401.05746",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Heqing"
            },
            {
                "last_name": "Shen",
                "first_name": "Meng"
            },
            {
                "last_name": "Hu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Chng",
                "first_name": "Eng Siong"
            },
            {
                "last_name": "Rajan",
                "first_name": "Deepu"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Audio-visual deepfake detection scrutinizes manipulations in public video\nusing complementary multimodal cues. Current methods, which train on fused\nmultimodal data for multimodal targets face challenges due to uncertainties and\ninconsistencies in learned representations caused by independent modality\nmanipulations in deepfake videos. To address this, we propose cross-modality\nand within-modality regularization to preserve modality distinctions during\nmultimodal representation learning. Our approach includes an audio-visual\ntransformer module for modality correspondence and a cross-modality\nregularization module to align paired audio-visual signals, preserving modality\ndistinctions. Simultaneously, a within-modality regularization module refines\nunimodal representations with modality-specific targets to retain\nmodal-specific details. Experimental results on the public audio-visual\ndataset, FakeAVCeleb, demonstrate the effectiveness and competitiveness of our\napproach.\n",
        "title": "Cross-Modality and Within-Modality Regularization for Audio-Visual\n  DeepFake Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05749",
        "abstract_url": "http://arxiv.org/abs/2401.05749",
        "authors": [
            {
                "last_name": "Thompson",
                "first_name": "Brian"
            },
            {
                "last_name": "Dhaliwal",
                "first_name": "Mehak Preet"
            },
            {
                "last_name": "Frisch",
                "first_name": "Peter"
            },
            {
                "last_name": "Domhan",
                "first_name": "Tobias"
            },
            {
                "last_name": "Federico",
                "first_name": "Marcello"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  We show that content on the web is often translated into many languages, and\nthe low quality of these multi-way translations indicates they were likely\ncreated using Machine Translation (MT). Multi-way parallel, machine generated\ncontent not only dominates the translations in lower resource languages; it\nalso constitutes a large fraction of the total web content in those languages.\nWe also find evidence of a selection bias in the type of content which is\ntranslated into many languages, consistent with low quality English content\nbeing translated en masse into many lower resource languages, via MT. Our work\nraises serious concerns about training models such as multilingual large\nlanguage models on both monolingual and bilingual data scraped from the web.\n",
        "title": "A Shocking Amount of the Web is Machine Translated: Insights from\n  Multi-Way Parallelism",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05750",
        "abstract_url": "http://arxiv.org/abs/2401.05750",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Peng"
            },
            {
                "last_name": "Tan",
                "first_name": "Feitong"
            },
            {
                "last_name": "Yu",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yinda"
            },
            {
                "last_name": "Qi",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Despite advances in 3D generation, the direct creation of 3D objects within\nan existing 3D scene represented as NeRF remains underexplored. This process\nrequires not only high-quality 3D object generation but also seamless\ncomposition of the generated 3D content into the existing NeRF. To this end, we\npropose a new method, GO-NeRF, capable of utilizing scene context for\nhigh-quality and harmonious 3D object generation within an existing NeRF. Our\nmethod employs a compositional rendering formulation that allows the generated\n3D objects to be seamlessly composited into the scene utilizing learned\n3D-aware opacity maps without introducing unintended scene modification.\nMoreover, we also develop tailored optimization objectives and training\nstrategies to enhance the model's ability to exploit scene context and mitigate\nartifacts, such as floaters, originating from 3D object generation within a\nscene. Extensive experiments on both feed-forward and $360^o$ scenes show the\nsuperior performance of our proposed GO-NeRF in generating objects harmoniously\ncomposited with surrounding scenes and synthesizing high-quality novel view\nimages. Project page at {\\url{https://daipengwa.github.io/GO-NeRF/}.\n",
        "title": "GO-NeRF: Generating Virtual Objects in Neural Radiance Fields",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05752",
        "abstract_url": "http://arxiv.org/abs/2401.05752",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Na"
            },
            {
                "last_name": "Qi",
                "first_name": "Lei"
            },
            {
                "last_name": "Guo",
                "first_name": "Jintao"
            },
            {
                "last_name": "Shi",
                "first_name": "Yinghuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Domain generalization (DG) intends to train a model on multiple source\ndomains to ensure that it can generalize well to an arbitrary unseen target\ndomain. The acquisition of domain-invariant representations is pivotal for DG\nas they possess the ability to capture the inherent semantic information of the\ndata, mitigate the influence of domain shift, and enhance the generalization\ncapability of the model. Adopting multiple perspectives, such as the sample and\nthe feature, proves to be effective. The sample perspective facilitates data\naugmentation through data manipulation techniques, whereas the feature\nperspective enables the extraction of meaningful generalization features. In\nthis paper, we focus on improving the generalization ability of the model by\ncompelling it to acquire domain-invariant representations from both the sample\nand feature perspectives by disentangling spurious correlations and enhancing\npotential correlations. 1) From the sample perspective, we develop a frequency\nrestriction module, guiding the model to focus on the relevant correlations\nbetween object features and labels, thereby disentangling spurious\ncorrelations. 2) From the feature perspective, the simple Tail Interaction\nmodule implicitly enhances potential correlations among all samples from all\nsource domains, facilitating the acquisition of domain-invariant\nrepresentations across multiple domains for the model. The experimental results\nshow that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons\n(MLPs) with a strong baseline embedded with these two modules can achieve\nsuperior results, e.g., an average accuracy of 92.30% on Digits-DG.\n",
        "title": "Learning Generalizable Models via Disentangling Spurious and Enhancing\n  Potential Correlations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05753",
        "abstract_url": "http://arxiv.org/abs/2401.05753",
        "authors": [
            {
                "last_name": "Ko",
                "first_name": "Yousun"
            },
            {
                "last_name": "Burgstaller",
                "first_name": "Bernd"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "abstract": "  Soft errors are a type of transient digital signal corruption that occurs in\ndigital hardware components such as the internal flip-flops of CPU pipelines,\nthe register file, memory cells, and even internal communication buses. Soft\nerrors are caused by environmental radioactivity, magnetic interference,\nlasers, and temperature fluctuations, either unintentionally, or as part of a\ndeliberate attempt to compromise a system and expose confidential data.\n  We propose a bit-level error coalescing (BEC) static program analysis and its\ntwo use cases to understand and improve program reliability against soft\nerrors. The BEC analysis tracks each bit corruption in the register file and\nclassifies the effect of the corruption by its semantics at compile time. The\nusefulness of the proposed analysis is demonstrated in two scenarios, fault\ninjection campaign pruning, and reliability-aware program transformation.\nExperimental results show that bit-level analysis pruned up to 30.04 % of\nexhaustive fault injection campaigns (13.71 % on average), without loss of\naccuracy. Program vulnerability was reduced by up to 13.11 % (4.94 % on\naverage) through bit-level vulnerability-aware instruction scheduling. The\nanalysis has been implemented within LLVM and evaluated on the RISC-V\narchitecture.\n  To the best of our knowledge, the proposed BEC analysis is the first\nbit-level compiler analysis for program reliability against soft errors. The\nproposed method is generic and not limited to a specific computer architecture.\n",
        "title": "BEC: Bit-Level Static Analysis for Reliability against Soft Errors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05757",
        "abstract_url": "http://arxiv.org/abs/2401.05757",
        "authors": [
            {
                "last_name": "Aramaki",
                "first_name": "Mitsuko"
            },
            {
                "last_name": "Bernard",
                "first_name": "Corentin"
            },
            {
                "last_name": "Kronland-Martinet",
                "first_name": "Richard"
            },
            {
                "last_name": "Poirot",
                "first_name": "Samuel"
            },
            {
                "last_name": "Ystad",
                "first_name": "S\u00f8lvi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "",
            ""
        ],
        "abstract": "  Intuitive control of synthesis processes is an ongoing challenge within the\ndomain of auditory perception and cognition. Previous works on sound modelling\ncombined with psychophysical tests have enabled our team to develop a\nsynthesizer that provides intuitive control of actions and objects based on\nsemantic descriptions for sound sources. In this demo we present an augmented\nversion of the synthesizer in which we added tactile stimulations to increase\nthe sensation of true continuous friction interactions (rubbing and scratching)\nwith the simulated objects. This is of interest for several reasons. Firstly,\nit enables to evaluate the realism of our sound model in presence of\nstimulations from other modalities. Secondly it enables to compare tactile and\nauditory signal structures linked to the same evocation, and thirdly it\nprovides a tool to investigate multimodal perception and how stimulations from\ndifferent modalities should be combined to provide realistic user interfaces.\n",
        "title": "Intuitive Control of Scraping and Rubbing Through Audio-tactile\n  Synthesis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05759",
        "abstract_url": "http://arxiv.org/abs/2401.05759",
        "authors": [
            {
                "last_name": "Vaccon",
                "first_name": "Tristan"
            },
            {
                "last_name": "Verron",
                "first_name": "Thibaut"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC",
            "",
            ""
        ],
        "abstract": "  A universal analytic Gr{\\\"o}bner basis (UAGB) of an ideal of a Tate algebra\nis a set containing a local Gr{\\\"o}bner basis for all suitable convergence\nradii. In a previous article, the authors proved the existence of finite UAGB's\nfor polynomial ideals, leaving open the question of how to compute them. In\nthis paper, we provide an algorithm computing a UAGB for a given polynomial\nideal, by traversing the Gr{\\\"o}bner fan of the ideal. As an application, it\noffers a new point of view on algorithms for computing tropical varieties of\nhomogeneous polynomial ideals, which typically rely on lifting the computations\nto an algebra of power series. Motivated by effective computations in tropical\nanalytic geometry, we also examine local bases for more general convergence\nconditions, constraining the radii to a convex polyhedron. In this setting, we\nprovide an algorithm to compute local Gr{\\\"o}bner bases and discuss obstacles\ntowards proving the existence of finite UAGBs. CCS CONCEPTS $\\bullet$ Computing\nmethodologies $\\rightarrow$ Algebraic algorithms.\n",
        "title": "Universal Analytic Gr{\\\"o}bner Bases and Tropical Geometry",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05761",
        "abstract_url": "http://arxiv.org/abs/2401.05761",
        "authors": [
            {
                "last_name": "Caramancion",
                "first_name": "Kevin Matthe"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  This study embarked on a comprehensive exploration of user preferences\nbetween Search Engines and Large Language Models (LLMs) in the context of\nvarious information retrieval scenarios. Conducted with a sample size of 100\ninternet users (N=100) from across the United States, the research delved into\n20 distinct use cases ranging from factual searches, such as looking up\nCOVID-19 guidelines, to more subjective tasks, like seeking interpretations of\ncomplex concepts in layman's terms. Participants were asked to state their\npreference between using a traditional search engine or an LLM for each\nscenario. This approach allowed for a nuanced understanding of how users\nperceive and utilize these two predominant digital tools in differing contexts.\nThe use cases were carefully selected to cover a broad spectrum of typical\nonline queries, thus ensuring a comprehensive analysis of user preferences. The\nfindings reveal intriguing patterns in user choices, highlighting a clear\ntendency for participants to favor search engines for direct, fact-based\nqueries, while LLMs were more often preferred for tasks requiring nuanced\nunderstanding and language processing. These results offer valuable insights\ninto the current state of digital information retrieval and pave the way for\nfuture innovations in this field. This study not only sheds light on the\nspecific contexts in which each tool is favored but also hints at the potential\nfor developing hybrid models that leverage the strengths of both search engines\nand LLMs. The insights gained from this research are pivotal for developers,\nresearchers, and policymakers in understanding the evolving landscape of\ndigital information retrieval and user interaction with these technologies.\n",
        "title": "Large Language Models vs. Search Engines: Evaluating User Preferences\n  Across Varied Information Retrieval Scenarios",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05765",
        "abstract_url": "http://arxiv.org/abs/2401.05765",
        "authors": [
            {
                "last_name": "Boschi",
                "first_name": "Tobia"
            },
            {
                "last_name": "Bonin",
                "first_name": "Francesca"
            },
            {
                "last_name": "Epperlein",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Ordonez-Hurtado",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Pascale",
                "first_name": "Alessandra"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Functional data analysis has emerged as a crucial tool in many contemporary\nscientific domains that require the integration and interpretation of complex\ndata. Moreover, the advent of new technologies has facilitated the collection\nof a large number of longitudinal variables, making feature selection pivotal\nfor avoiding overfitting and improving prediction performance. This paper\nintroduces a novel methodology called FSFC (Feature Selection for Functional\nClassification), that addresses the challenge of jointly performing feature\nselection and classification of functional data in scenarios with categorical\nresponses and longitudinal features. Our approach tackles a newly defined\noptimization problem that integrates logistic loss and functional features to\nidentify the most crucial features for classification. To address the\nminimization procedure, we employ functional principal components and develop a\nnew adaptive version of the Dual Augmented Lagrangian algorithm that leverages\nthe sparsity structure of the problem for dimensionality reduction. The\ncomputational efficiency of FSFC enables handling high-dimensional scenarios\nwhere the number of features may considerably exceed the number of statistical\nunits. Simulation experiments demonstrate that FSFC outperforms other machine\nlearning and deep learning methods in computational time and classification\naccuracy. Furthermore, the FSFC feature selection capability can be leveraged\nto significantly reduce the problem's dimensionality and enhance the\nperformances of other classification algorithms. The efficacy of FSFC is also\ndemonstrated through a real data application, analyzing relationships between\nfour chronic diseases and other health and socio-demographic factors.\n",
        "title": "Feature Selection for Functional Data Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05767",
        "abstract_url": "http://arxiv.org/abs/2401.05767",
        "authors": [
            {
                "last_name": "Tran",
                "first_name": "Ly-Duyen"
            },
            {
                "last_name": "Gurrin",
                "first_name": "Cathal"
            },
            {
                "last_name": "Smeaton",
                "first_name": "Alan F."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "HC"
        ],
        "abstract": "  Personal data includes the digital footprints that we leave behind as part of\nour everyday activities, both online and offline in the real world. It includes\ndata we collect ourselves, such as from wearables, as well as the data\ncollected by others about our online behaviour and activities. Sometimes we are\nable to use the personal data we ourselves collect, in order to examine some\nparts of our lives but for the most part, our personal data is leveraged by\nthird parties including internet companies, for services like targeted\nadvertising and recommendations. Lifelogging is a form of extreme personal data\ngathering and in this article we present an overview of the tools used to\nmanage access to lifelogs as demonstrated at the most recent of the annual\nLifelog Search Challenge benchmarking workshops. Here, experimental systems are\nshowcased in live, real time information seeking tasks by real users. This\noverview of these systems' capabilities show the range of possibilities for\naccessing our own personal data which may, in time, become more easily\navailable as consumer-level services.\n",
        "title": "Lifelogging As An Extreme Form of Personal Information Management --\n  What Lessons To Learn",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05768",
        "abstract_url": "http://arxiv.org/abs/2401.05768",
        "authors": [
            {
                "last_name": "Gheorghiu",
                "first_name": "Adrian"
            },
            {
                "last_name": "T\u0103iatu",
                "first_name": "Iulian-Marius"
            },
            {
                "last_name": "Cercel",
                "first_name": "Dumitru-Clementin"
            },
            {
                "last_name": "Marin",
                "first_name": "Iuliana"
            },
            {
                "last_name": "Pop",
                "first_name": "Florin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The detection and classification of diseases in Robusta coffee leaves are\nessential to ensure that plants are healthy and the crop yield is kept high.\nHowever, this job requires extensive botanical knowledge and much wasted time.\nTherefore, this task and others similar to it have been extensively researched\nsubjects in image classification. Regarding leaf disease classification, most\napproaches have used the more popular PlantVillage dataset while completely\ndisregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. As\nthe RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of\npre-trained models and multiple augmentation techniques need to be used. The\ncurrent paper uses the RoCoLe dataset and approaches based on deep learning for\nclassifying coffee leaf diseases from images, incorporating the pix2pix model\nfor segmentation and cycle-generative adversarial network (CycleGAN) for\naugmentation. Our study demonstrates the effectiveness of Transformer-based\nmodels, online augmentations, and CycleGAN augmentation in improving leaf\ndisease classification. While synthetic data has limitations, it complements\nreal data, enhancing model performance. These findings contribute to developing\nrobust techniques for plant disease detection and classification.\n",
        "title": "Evaluating Data Augmentation Techniques for Coffee Leaf Disease\n  Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05771",
        "abstract_url": "http://arxiv.org/abs/2401.05771",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhiying"
            },
            {
                "last_name": "Guo",
                "first_name": "Yongxin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate lesion classification in Wireless Capsule Endoscopy (WCE) images is\nvital for early diagnosis and treatment of gastrointestinal (GI) cancers.\nHowever, this task is confronted with challenges like tiny lesions and\nbackground interference. Additionally, WCE images exhibit higher intra-class\nvariance and inter-class similarities, adding complexity. To tackle these\nchallenges, we propose Decoupled Supervised Contrastive Learning for WCE image\nclassification, learning robust representations from zoomed-in WCE images\ngenerated by Saliency Augmentor. Specifically, We use uniformly down-sampled\nWCE images as anchors and WCE images from the same class, especially their\nzoomed-in images, as positives. This approach empowers the Feature Extractor to\ncapture rich representations from various views of the same image, facilitated\nby Decoupled Supervised Contrastive Learning. Training a linear Classifier on\nthese representations within 10 epochs yields an impressive 92.01% overall\naccuracy, surpassing the prior state-of-the-art (SOTA) by 0.72% on a blend of\ntwo publicly accessible WCE datasets. Code is available at:\nhttps://github.com/Qiukunpeng/DSCL.\n",
        "title": "Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image\n  Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05772",
        "abstract_url": "http://arxiv.org/abs/2401.05772",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Wujie"
            },
            {
                "last_name": "Chen",
                "first_name": "Defang"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Feng",
                "first_name": "Yan"
            },
            {
                "last_name": "Chen",
                "first_name": "Chun"
            },
            {
                "last_name": "Wang",
                "first_name": "Can"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Deep learning has witnessed significant advancements in recent years at the\ncost of increasing training, inference, and model storage overhead. While\nexisting model compression methods strive to reduce the number of model\nparameters while maintaining high accuracy, they inevitably necessitate the\nre-training of the compressed model or impose architectural constraints. To\novercome these limitations, this paper presents a novel framework, termed\n\\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' model\nis trained to receive the parameters of a larger model and generate compressed\nparameters. The concept of KT draws inspiration from language translation,\nwhich effectively employs neural networks to convert different languages,\nmaintaining identical meaning. Accordingly, we explore the potential of neural\nnetworks to convert models of disparate sizes, while preserving their\nfunctionality. We propose a comprehensive framework for KT, introduce data\naugmentation strategies to enhance model performance despite restricted\ntraining data, and successfully demonstrate the feasibility of KT on the MNIST\ndataset. Code is available at \\url{https://github.com/zju-SWJ/KT}.\n",
        "title": "Knowledge Translation: A New Pathway for Model Compression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05777",
        "abstract_url": "http://arxiv.org/abs/2401.05777",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jinxin"
            },
            {
                "last_name": "Cao",
                "first_name": "Shulin"
            },
            {
                "last_name": "Shi",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tingjian"
            },
            {
                "last_name": "Hou",
                "first_name": "Lei"
            },
            {
                "last_name": "Li",
                "first_name": "Juanzi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancement in the capabilities of large language models (LLMs) has\ntriggered a new surge in LLMs' evaluation. Most recent evaluation works tends\nto evaluate the comprehensive ability of LLMs over series of tasks. However,\nthe deep structure understanding of natural language is rarely explored. In\nthis work, we examine the ability of LLMs to deal with structured semantics on\nthe tasks of question answering with the help of the human-constructed formal\nlanguage. Specifically, we implement the inter-conversion of natural and formal\nlanguage through in-context learning of LLMs to verify their ability to\nunderstand and generate the structured logical forms. Extensive experiments\nwith models of different sizes and in different formal languages show that\ntoday's state-of-the-art LLMs' understanding of the logical forms can approach\nhuman level overall, but there still are plenty of room in generating correct\nlogical forms, which suggest that it is more effective to use LLMs to generate\nmore natural language training data to reinforce a small model than directly\nanswering questions with LLMs. Moreover, our results also indicate that models\nexhibit considerable sensitivity to different formal languages. In general, the\nformal language with the lower the formalization level, i.e. the more similar\nit is to natural language, is more LLMs-friendly.\n",
        "title": "Probing Structured Semantics Understanding and Generation of Language\n  Models via Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05778",
        "abstract_url": "http://arxiv.org/abs/2401.05778",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanling"
            },
            {
                "last_name": "Fu",
                "first_name": "Chuanpu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yong"
            },
            {
                "last_name": "Li",
                "first_name": "Sijia"
            },
            {
                "last_name": "Deng",
                "first_name": "Xinhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinglin"
            },
            {
                "last_name": "Qiu",
                "first_name": "Ziyi"
            },
            {
                "last_name": "Li",
                "first_name": "Peiyang"
            },
            {
                "last_name": "Tan",
                "first_name": "Zhixing"
            },
            {
                "last_name": "Xiong",
                "first_name": "Junwu"
            },
            {
                "last_name": "Kong",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Wen",
                "first_name": "Zujie"
            },
            {
                "last_name": "Xu",
                "first_name": "Ke"
            },
            {
                "last_name": "Li",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) have strong capabilities in solving diverse\nnatural language processing tasks. However, the safety and security issues of\nLLM systems have become the major obstacle to their widespread application.\nMany studies have extensively investigated risks in LLM systems and developed\nthe corresponding mitigation strategies. Leading-edge enterprises such as\nOpenAI, Google, Meta, and Anthropic have also made lots of efforts on\nresponsible LLMs. Therefore, there is a growing need to organize the existing\nstudies and establish comprehensive taxonomies for the community. In this\npaper, we delve into four essential modules of an LLM system, including an\ninput module for receiving prompts, a language model trained on extensive\ncorpora, a toolchain module for development and deployment, and an output\nmodule for exporting LLM-generated content. Based on this, we propose a\ncomprehensive taxonomy, which systematically analyzes potential risks\nassociated with each module of an LLM system and discusses the corresponding\nmitigation strategies. Furthermore, we review prevalent benchmarks, aiming to\nfacilitate the risk assessment of LLM systems. We hope that this paper can help\nLLM participants embrace a systematic perspective to build their responsible\nLLM systems.\n",
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language\n  Model Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05779",
        "abstract_url": "http://arxiv.org/abs/2401.05779",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jing"
            },
            {
                "last_name": "Le",
                "first_name": "Trung"
            },
            {
                "last_name": "Hayat",
                "first_name": "Munawar"
            },
            {
                "last_name": "Harandi",
                "first_name": "Mehrtash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In response to data protection regulations and the ``right to be forgotten'',\nin this work, we introduce an unlearning algorithm for diffusion models. Our\nalgorithm equips a diffusion model with a mechanism to mitigate the concerns\nrelated to data memorization. To achieve this, we formulate the unlearning\nproblem as a bi-level optimization problem, wherein the outer objective is to\npreserve the utility of the diffusion model on the remaining data. The inner\nobjective aims to scrub the information associated with forgetting data by\ndeviating the learnable generative process from the ground-truth denoising\nprocedure. To solve the resulting bi-level problem, we adopt a first-order\nmethod, having superior practical performance while being vigilant about the\ndiffusion process and solving a bi-level problem therein. Empirically, we\ndemonstrate that our algorithm can preserve the model utility, effectiveness,\nand efficiency while removing across two widely-used diffusion models and in\nboth conditional and unconditional image generation scenarios. In our\nexperiments, we demonstrate the unlearning of classes, attributes, and even a\nrace from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and\nCIFAR10.\n",
        "title": "EraseDiff: Erasing Data Influence in Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05782",
        "abstract_url": "http://arxiv.org/abs/2401.05782",
        "authors": [
            {
                "last_name": "Noom",
                "first_name": "Jacques"
            },
            {
                "last_name": "Soloviev",
                "first_name": "Oleg"
            },
            {
                "last_name": "Smith",
                "first_name": "Carlas"
            },
            {
                "last_name": "Verhaegen",
                "first_name": "Michel"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Stochastic Closed-Loop Active Fault Diagnosis (CLAFD) aims to select the\ninput sequentially in order to improve the discrimination of different models\nby minimizing the predicted error probability. As computation of these error\nprobabilities encompasses the evaluation of multidimensional probability\nintegrals, relaxation methods are of interest. This manuscript presents a new\nmethod that allows to make an improved trade-off between three factors --\nnamely maximized accuracy of diagnosis, minimized number of consecutive\nmeasurements to achieve that accuracy, and minimized computational effort per\ntime step -- with respect to the state-of-the-art. It relies on minimizing an\nupper bound on the error probability, which is in the case of linear models\nwith Gaussian noise proven to be concave in the most challenging discrimination\nconditions. A simulation study is conducted both for open-loop and feedback\ncontrolled candidate models. The results demonstrate the favorable trade-off\nusing the new contributions in this manuscript.\n",
        "title": "Online input design for discrimination of linear models using concave\n  minimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05783",
        "abstract_url": "http://arxiv.org/abs/2401.05783",
        "authors": [
            {
                "last_name": "Vlachou",
                "first_name": "Maria"
            },
            {
                "last_name": "Macdonald",
                "first_name": "Craig"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In Conversational Recommendation Systems (CRS), a user can provide feedback\non recommended items at each interaction turn, leading the CRS towards more\ndesirable recommendations. Currently, different types of CRS offer various\npossibilities for feedback, i.e., natural language feedback, or answering\nclarifying questions. In most cases, a user simulator is employed for training\nas well as evaluating the CRS. Such user simulators typically critique the\ncurrent retrieved items based on knowledge of a single target item. Still,\nevaluating systems in offline settings with simulators suffers from problems,\nsuch as focusing entirely on a single target item (not addressing the\nexploratory nature of a recommender system), and exhibiting extreme patience\n(consistent feedback over a large number of turns). To overcome these\nlimitations, we obtain extra judgements for a selection of alternative items in\ncommon CRS datasets, namely Shoes and Fashion IQ Dresses. Going further, we\npropose improved user simulators that allow simulated users not only to express\ntheir preferences about alternative items to their original target, but also to\nchange their mind and level of patience. In our experiments using the relative\nimage captioning CRS setting and different CRS models, we find that using the\nknowledge of alternatives by the simulator can have a considerable impact on\nthe evaluation of existing CRS models, specifically that the existing\nsingle-target evaluation underestimates their effectiveness, and when simulated\nusers are allowed to instead consider alternatives, the system can rapidly\nrespond to more quickly satisfy the user.\n",
        "title": "What Else Would I Like? A User Simulator using Alternatives for Improved\n  Evaluation of Fashion Conversational Recommendation Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05787",
        "abstract_url": "http://arxiv.org/abs/2401.05787",
        "authors": [
            {
                "last_name": "Parvez",
                "first_name": "Md Rizwan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While chain-of-thought (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer\nfrom limitations like slowness, limited context grounding, hallucination and\ninconsistent outputs. To overcome these challenges, we introduce Evidence to\nGenerate (E2G), a novel single-agent, two-step prompting framework. Instead of\nunverified reasoning claims, this innovative approach leverages the power of\n\"evidence for decision making\" by first focusing exclusively on the thought\nsequences (the series of intermediate steps) explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npowerful approach unlocks the true potential of chain-of-thought like\nprompting, paving the way for faster, more reliable, and more contextually\naware reasoning in LLMs. \\tool achieves remarkable results robustly across a\nwide range of knowledge-intensive reasoning and generation tasks, surpassing\nbaseline approaches with state-of-the-art LLMs. For example, (i) on LogiQA\nbenchmark using GPT-4 as backbone model, \\tool achieves a new state-of-the\nAccuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant of\nE2G with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9\nF1 points, reaching an F1 score of 83.3 on a subset of DROP.\n",
        "title": "Evidence to Generate (E2G): A Single-agent Two-step Prompting for\n  Context Grounded and Retrieval Augmented Reasoning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05790",
        "abstract_url": "http://arxiv.org/abs/2401.05790",
        "authors": [
            {
                "last_name": "Dolata",
                "first_name": "Mateusz"
            },
            {
                "last_name": "Lange",
                "first_name": "Norbert"
            },
            {
                "last_name": "Schwabe",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The rise of generative AI has led many companies to hire freelancers to\nharness its potential. However, this technology presents unique challenges to\ndevelopers who have not previously engaged with it. Freelancers may find these\nchallenges daunting due to the absence of organizational support and their\nreliance on positive client feedback. In a study involving 52 freelance\ndevelopers, we identified multiple challenges associated with developing\nsolutions based on generative AI. Freelancers often struggle with aspects they\nperceive as unique to generative AI such as unpredictability of its output, the\noccurrence of hallucinations, and the inconsistent effort required due to\ntrial-and-error prompting cycles. Further, the limitations of specific\nframeworks, such as token limits and long response times, add to the\ncomplexity. Hype-related issues, such as inflated client expectations and a\nrapidly evolving technological ecosystem, further exacerbate the difficulties.\nTo address these issues, we propose Software Engineering for Generative AI\n(SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the\nsoftware engineering community can provide effective guidance. This support is\nessential for freelancers working with generative AI and other emerging\ntechnologies.\n",
        "title": "Development in times of hype: How freelancers explore Generative AI?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05792",
        "abstract_url": "http://arxiv.org/abs/2401.05792",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhihui"
            },
            {
                "last_name": "Zhao",
                "first_name": "Handong"
            },
            {
                "last_name": "Yu",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Shuai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large pretrained multilingual language models (ML-LMs) have shown remarkable\ncapabilities of zero-shot cross-lingual transfer, without direct cross-lingual\nsupervision. While these results are promising, follow-up works found that,\nwithin the multilingual embedding spaces, there exists strong language identity\ninformation which hinders the expression of linguistic factors shared across\nlanguages. For semantic tasks like cross-lingual sentence retrieval, it is\ndesired to remove such language identity signals to fully leverage semantic\ninformation. In this work, we provide a novel view of projecting away\nlanguage-specific factors from a multilingual embedding space. Specifically, we\ndiscover that there exists a low-rank subspace that primarily encodes\ninformation irrelevant to semantics (e.g., syntactic information). To identify\nthis subspace, we present a simple but effective unsupervised method based on\nsingular value decomposition with multiple monolingual corpora as input. Once\nthe subspace is found, we can directly project the original embeddings into the\nnull space to boost language agnosticism without finetuning. We systematically\nevaluate our method on various tasks including the challenging\nlanguage-agnostic QA retrieval task. Empirical results show that applying our\nmethod consistently leads to improvements over commonly used ML-LMs.\n",
        "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual\n  Representations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05794",
        "abstract_url": "http://arxiv.org/abs/2401.05794",
        "authors": [
            {
                "last_name": "Geneson",
                "first_name": "Jesse"
            },
            {
                "last_name": "Tang",
                "first_name": "Linus"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DM",
            ""
        ],
        "abstract": "  We improve several worst-case bounds for various online learning scenarios\nfrom (Auer and Long, Machine Learning, 1999). In particular, we sharpen an\nupper bound for delayed ambiguous reinforcement learning by a factor of 2, an\nupper bound for learning compositions of families of functions by a factor of\n2.41, and an upper bound for agnostic learning by a factor of 1.09. We also\nimprove a lower bound from the same paper for learning compositions of $k$\nfamilies of functions by a factor of $\\Theta(\\ln{k})$, matching the upper bound\nup to a constant factor. In addition, we solve a problem from (Long,\nTheoretical Computer Science, 2020) on the price of bandit feedback with\nrespect to standard feedback for multiclass learning, and we improve an upper\nbound from (Feng et al., Theoretical Computer Science, 2023) on the price of\n$r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching\na lower bound from the same paper up to the leading term.\n",
        "title": "Bounds on the price of feedback for mistake-bounded online learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05797",
        "abstract_url": "http://arxiv.org/abs/2401.05797",
        "authors": [
            {
                "last_name": "Deb",
                "first_name": "Soubhik"
            },
            {
                "last_name": "Raynor",
                "first_name": "Robert"
            },
            {
                "last_name": "Kannan",
                "first_name": "Sreeram"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  As of July 15, 2023, Ethererum, which is a Proof-of-Stake (PoS) blockchain\n[1] has around 410 Billion USD in total assets on chain (popularly referred to\nas total-value-locked, TVL) but has only 33 Billion USD worth of ETH staked in\nsecuring the underlying consensus of the chain [2]. A preliminary analysis\nmight suggest that as the amount staked is far less (11x less) than the value\nsecured, the Ethereum blockchain is insecure and \"over-leveraged\" in a purely\ncryptoeconomic sense. In this work, we investigate how Ethereum, or, more\ngenerally, any PoS blockchain can be made secure despite this apparent\nimbalance. Towards that end, we attempt to formalize a model for analyzing the\ncryptoeconomic safety of PoS blockchain, which separately analyzes the\ncost-of-corruption, the cost incurred by an attacker, and the\nprofit-from-corruption, the profit gained by an attacker. We derive sharper\nbounds on profit-from-corruption, as well as new confirmation rules that\nsignificantly decrease this upper-bound. We evaluate cost-of-corruption and\nprofit-from-corruption only from the perspective of attacking safety. Finally,\nwe present a new \"insurance\" mechanism, STAKESURE, for allocating the slashed\nfunds in a PoS system, that has several highly desirable properties: solving\ncommon information problem in existing blockchains, creating a mechanism for\nprovably safe bridging, and providing the first sharp solution for\nautomatically adjusting how much economic security is sufficient in a PoS\nsystem. Finally, we show that the system satisfies a notion of strong\ncryptoeconomic safety, which guarantees that no honest transactor ever loses\nmoney, and creates a closed system of Karma, which not only ensures that the\nattacker suffers a loss of funds but also that the harmed parties are\nsufficiently compensated.\n",
        "title": "STAKESURE: Proof of Stake Mechanisms with Strong Cryptoeconomic Safety",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05799",
        "abstract_url": "http://arxiv.org/abs/2401.05799",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Frank"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "MA",
            ""
        ],
        "abstract": "  Large language models (LLMs) have drastically changed the possible ways to\ndesign intelligent systems, shifting the focuses from massive data acquisition\nand new modeling training to human alignment and strategical elicitation of the\nfull potential of existing pre-trained models. This paradigm shift, however, is\nnot fully realized in financial sentiment analysis (FSA), due to the\ndiscriminative nature of this task and a lack of prescriptive knowledge of how\nto leverage generative models in such a context. This study investigates the\neffectiveness of the new paradigm, i.e., using LLMs without fine-tuning for\nFSA. Rooted in Minsky's theory of mind and emotions, a design framework with\nheterogeneous LLM agents is proposed. The framework instantiates specialized\nagents using prior domain knowledge of the types of FSA errors and reasons on\nthe aggregated agent discussions. Comprehensive evaluation on FSA datasets show\nthat the framework yields better accuracies, especially when the discussions\nare substantial. This study contributes to the design foundations and paves new\navenues for LLMs-based FSA. Implications on business and management are also\ndiscussed.\n",
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05800",
        "abstract_url": "http://arxiv.org/abs/2401.05800",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Yu"
            },
            {
                "last_name": "Koh",
                "first_name": "Huan Yee"
            },
            {
                "last_name": "Jin",
                "first_name": "Ming"
            },
            {
                "last_name": "Chi",
                "first_name": "Lianhua"
            },
            {
                "last_name": "Wang",
                "first_name": "Haishuai"
            },
            {
                "last_name": "Phan",
                "first_name": "Khoa T."
            },
            {
                "last_name": "Chen",
                "first_name": "Yi-Ping Phoebe"
            },
            {
                "last_name": "Pan",
                "first_name": "Shirui"
            },
            {
                "last_name": "Xiang",
                "first_name": "Wei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The detection of anomalies in multivariate time series data is crucial for\nvarious practical applications, including smart power grids, traffic flow\nforecasting, and industrial process control. However, real-world time series\ndata is usually not well-structured, posting significant challenges to existing\napproaches: (1) The existence of missing values in multivariate time series\ndata along variable and time dimensions hinders the effective modeling of\ninterwoven spatial and temporal dependencies, resulting in important patterns\nbeing overlooked during model training; (2) Anomaly scoring with\nirregularly-sampled observations is less explored, making it difficult to use\nexisting detectors for multivariate series without fully-observed values. In\nthis work, we introduce a novel framework called GST-Pro, which utilizes a\ngraph spatiotemporal process and anomaly scorer to tackle the aforementioned\nchallenges in detecting anomalies on irregularly-sampled multivariate time\nseries. Our approach comprises two main components. First, we propose a graph\nspatiotemporal process based on neural controlled differential equations. This\nprocess enables effective modeling of multivariate time series from both\nspatial and temporal perspectives, even when the data contains missing values.\nSecond, we present a novel distribution-based anomaly scoring mechanism that\nalleviates the reliance on complete uniform observations. By analyzing the\npredictions of the graph spatiotemporal process, our approach allows anomalies\nto be easily detected. Our experimental results show that the GST-Pro method\ncan effectively detect anomalies in time series data and outperforms\nstate-of-the-art methods, regardless of whether there are missing values\npresent in the data. Our code is available: https://github.com/huankoh/GST-Pro.\n",
        "title": "Graph Spatiotemporal Process for Multivariate Time Series Anomaly\n  Detection with Missing Values",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05802",
        "abstract_url": "http://arxiv.org/abs/2401.05802",
        "authors": [
            {
                "last_name": "Johal",
                "first_name": "Wafa"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  With advancement of robotics and artificial intelligence, applications for\nrobotics are flourishing. Human-robot interaction (HRI) is an important area of\nrobotics as it allows robots to work closer to humans (with them or for them).\nOne crucial factor for the success of HRI research is transferability, which\nrefers to the ability of research outputs to be adopted by industry and provide\nbenefits to society. In this paper, we explore the potentials and challenges of\ntransferability in HRI research. Firstly, we examine the current state of HRI\nresearch and identify various types of contributions that could lead to\nsuccessful outcomes. Secondly, we discuss the potential benefits for each type\nof contribution and identify factors that could facilitate industry adoption of\nHRI research. However, we also recognize that there are several challenges\nassociated with transferability, such as the diversity of well-defined\njob/skill-sets required from HRI practitioners, the lack of industry-led\nresearch, and the lack of standardization in HRI research methods. We discuss\nthese challenges and propose potential solutions to bridge the gap between\nindustry expectations and academic research in HRI.\n",
        "title": "Transferability of HRI Research: Potential and Challenges",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05806",
        "abstract_url": "http://arxiv.org/abs/2401.05806",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Dong",
                "first_name": "Neng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Liehuang"
            },
            {
                "last_name": "Peng",
                "first_name": "Hao"
            },
            {
                "last_name": "Tao",
                "first_name": "Dapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visible-infrared person re-identification (VIReID) primarily deals with\nmatching identities across person images from different modalities. Due to the\nmodality gap between visible and infrared images, cross-modality identity\nmatching poses significant challenges. Recognizing that high-level semantics of\npedestrian appearance, such as gender, shape, and clothing style, remain\nconsistent across modalities, this paper intends to bridge the modality gap by\ninfusing visual features with high-level semantics. Given the capability of\nCLIP to sense high-level semantic information corresponding to visual\nrepresentations, we explore the application of CLIP within the domain of\nVIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network\n(CSDN) that consists of Modality-specific Prompt Learner, Semantic Information\nIntegration (SII), and High-level Semantic Embedding (HSE). Specifically,\nconsidering the diversity stemming from modality discrepancies in language\ndescriptions, we devise bimodal learnable text tokens to capture\nmodality-private semantic information for visible and infrared images,\nrespectively. Additionally, acknowledging the complementary nature of semantic\ndetails across different modalities, we integrate text features from the\nbimodal language descriptions to achieve comprehensive semantics. Finally, we\nestablish a connection between the integrated text features and the visual\nfeatures across modalities. This process embed rich high-level semantic\ninformation into visual representations, thereby promoting the modality\ninvariance of visual representations. The effectiveness and superiority of our\nproposed CSDN over existing methods have been substantiated through\nexperimental evaluations on multiple widely used benchmarks. The code will be\nreleased at \\url{https://github.com/nengdong96/CSDN}.\n",
        "title": "CLIP-Driven Semantic Discovery Network for Visible-Infrared Person\n  Re-Identification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05807",
        "abstract_url": "http://arxiv.org/abs/2401.05807",
        "authors": [
            {
                "last_name": "Cobo",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Valle",
                "first_name": "Roberto"
            },
            {
                "last_name": "Buenaposada",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Baumela",
                "first_name": "Luis"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Head pose estimation (HPE) is a problem of interest in computer vision to\nimprove the performance of face processing tasks in semi-frontal or profile\nsettings. Recent applications require the analysis of faces in the full\n360{\\deg} rotation range. Traditional approaches to solve the semi-frontal and\nprofile cases are not directly amenable for the full rotation case. In this\npaper we analyze the methodology for short- and wide-range HPE and discuss\nwhich representations and metrics are adequate for each case. We show that the\npopular Euler angles representation is a good choice for short-range HPE, but\nnot at extreme rotations. However, the Euler angles' gimbal lock problem\nprevents them from being used as a valid metric in any setting. We also revisit\nthe current cross-data set evaluation methodology and note that the lack of\nalignment between the reference systems of the training and test data sets\nnegatively biases the results of all articles in the literature. We introduce a\nprocedure to quantify this misalignment and a new methodology for cross-data\nset HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwi\nbenchmark. We also propose a generalization of the geodesic angular distance\nmetric that enables the construction of a loss that controls the contribution\nof each training sample to the optimization of the model. Finally, we introduce\na wide range HPE benchmark based on the CMU Panoptic data set.\n",
        "title": "On the representation and methodology for wide and short range head pose\n  estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05808",
        "abstract_url": "http://arxiv.org/abs/2401.05808",
        "authors": [
            {
                "last_name": "Azarbahram",
                "first_name": "Ali"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  The paper proposes an intermittent communication mechanism for the tracking\nconsensus of high-order nonlinear multi-agent systems (MASs) surrounded by\nrandom disturbances. Each collaborating agent is described by a class of\nhigh-order nonlinear uncertain strict-feedback dynamics which is disturbed by a\nwide stationary process representing the external noise. The resiliency level\nof this networked control system (NCS) to the failures of physical devices or\nunreliability of communication channels is analyzed by introducing a linear\nauxiliary trajectory of the system. More precisely, the unreliability of\ncommunication channels sometimes makes an agent incapable of sensing the local\ninformation or receiving it from neighboring nodes. Therefore, an intermittent\ncommunication scheme is proposed among the follower agents as a consequence of\nemploying the linear auxiliary dynamics. The closed-loop networked system\nsignals are proved to be noise-to-state practically stable in probability\n(NSpS-P). It has been justified that each agent follows the trajectory of the\ncorresponding local auxiliary virtual system practically in probability. The\nsimulation experiments finally quantify the effectiveness of our proposed\napproach in terms of providing a resilient performance against unreliability of\ncommunication channels and reaching the tracking consensus.\n",
        "title": "Tracking Consensus of Networked Random Nonlinear Multi-agent Systems\n  with Intermittent Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05809",
        "abstract_url": "http://arxiv.org/abs/2401.05809",
        "authors": [
            {
                "last_name": "Tomita",
                "first_name": "Yoshihide"
            },
            {
                "last_name": "Koyama",
                "first_name": "Shoichi"
            },
            {
                "last_name": "Saruwatari",
                "first_name": "Hiroshi"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  A method for synthesizing the desired sound field while suppressing the\nexterior radiation power with directional weighting is proposed. The exterior\nradiation from the loudspeakers in sound field synthesis systems can be\nproblematic in practical situations. Although several methods to suppress the\nexterior radiation have been proposed, suppression in all outward directions is\ngenerally difficult, especially when the number of loudspeakers is not\nsufficiently large. We propose the directionally weighted exterior radiation\nrepresentation to prioritize the suppression directions by incorporating it\ninto the optimization problem of sound field synthesis. By using the proposed\nrepresentation, the exterior radiation in the prioritized directions can be\nsignificantly reduced while maintaining high interior synthesis accuracy, owing\nto the relaxed constraint on the exterior radiation. Its performance is\nevaluated with the application of the proposed representation to amplitude\nmatching in numerical experiments.\n",
        "title": "Localizing Acoustic Energy in Sound Field Synthesis by Directionally\n  Weighted Exterior Radiation Suppression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05811",
        "abstract_url": "http://arxiv.org/abs/2401.05811",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Zhuoyuan"
            },
            {
                "last_name": "Yu",
                "first_name": "Yen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.\n",
        "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine\n  Translation in Unseen, Low-resource Languages",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05815",
        "abstract_url": "http://arxiv.org/abs/2401.05815",
        "authors": [
            {
                "last_name": "Kaiser",
                "first_name": "Jan"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenran"
            },
            {
                "last_name": "Eichler",
                "first_name": "Annika"
            },
            {
                "last_name": "Garcia",
                "first_name": "Andrea Santamaria"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG"
        ],
        "abstract": "  Machine learning has emerged as a powerful solution to the modern challenges\nin accelerator physics. However, the limited availability of beam time, the\ncomputational cost of simulations, and the high-dimensionality of optimisation\nproblems pose significant challenges in generating the required data for\ntraining state-of-the-art machine learning models. In this work, we introduce\nCheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.\nCheetah enables the fast collection of large data sets by reducing computation\ntimes by multiple orders of magnitude and facilitates efficient gradient-based\noptimisation for accelerator tuning and system identification. This positions\nCheetah as a user-friendly, readily extensible tool that integrates seamlessly\nwith widely adopted machine learning tools. We showcase the utility of Cheetah\nthrough five examples, including reinforcement learning training,\ngradient-based beamline tuning, gradient-based system identification,\nphysics-informed Bayesian optimisation priors, and modular neural network\nsurrogate modelling of space charge effects. The use of such a high-speed\ndifferentiable simulation code will simplify the development of machine\nlearning-based methods for particle accelerators and fast-track their\nintegration into everyday operations of accelerator facilities.\n",
        "title": "Cheetah: Bridging the Gap Between Machine Learning and Particle\n  Accelerator Physics with High-Speed, Differentiable Simulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05818",
        "abstract_url": "http://arxiv.org/abs/2401.05818",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Raquel"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Alberto"
            },
            {
                "last_name": "Mekler",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Writing and genre conventions are extant to any scientific community, and CHI\nis no different. In this paper, we present the early phases of an AI tool we\ncreated called KITSUNE, which supports authors in placing their work into the\nformat of a CHI paper, taking into account many conventions that are\never-present in CHI papers. We describe the development of the tool with the\nintent to promote discussion around how writing conventions are upheld and\nunquestioned by the CHI community, and how this translates to the work\nproduced. In addition, we bring up questions surrounding how the introduction\nof LLMs into academic writing fundamentally change how conventions will be\nupheld now and in the future\n",
        "title": "How to write a CHI paper (asking for a friend)",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05819",
        "abstract_url": "http://arxiv.org/abs/2401.05819",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yuting"
            },
            {
                "last_name": "Chen",
                "first_name": "Fei"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Auditory spatial attention detection (ASAD) is used to determine the\ndirection of a listener's attention to a speaker by analyzing her/his\nelectroencephalographic (EEG) signals. This study aimed to further improve the\nperformance of ASAD with a short decision window (i.e., <1 s) rather than with\nlong decision windows in previous studies. An end-to-end temporal attention\nnetwork (i.e., TAnet) was introduced in this work. TAnet employs a multi-head\nattention (MHA) mechanism, which can more effectively capture the interactions\namong time steps in collected EEG signals and efficiently assign corresponding\nweights to those EEG time steps. Experiments demonstrated that, compared with\nthe CNN-based method and recent ASAD methods, TAnet provided improved decoding\nperformance in the KUL dataset, with decoding accuracies of 92.4% (decision\nwindow 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s)\nwith short decision windows (i.e., <1 s). As a new ASAD model with a short\ndecision window, TAnet can potentially facilitate the design of EEG-controlled\nintelligent hearing aids and sound recognition systems.\n",
        "title": "TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial\n  Attention Decoding with a Short Decision Window",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05820",
        "abstract_url": "http://arxiv.org/abs/2401.05820",
        "authors": [
            {
                "last_name": "Emonds",
                "first_name": "Yannick"
            },
            {
                "last_name": "Xi",
                "first_name": "Kai"
            },
            {
                "last_name": "Fr\u00f6ning",
                "first_name": "Holger"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "ET",
            "PF"
        ],
        "abstract": "  Resistive memory is a promising alternative to SRAM, but is also an\ninherently unstable device that requires substantial effort to ensure correct\nread and write operations. To avoid the associated costs in terms of area, time\nand energy, the present work is concerned with exploring how much noise in\nmemory operations can be tolerated by image classification tasks based on\nneural networks. We introduce a special noisy operator that mimics the noise in\nan exemplary resistive memory unit, explore the resilience of convolutional\nneural networks on the CIFAR-10 classification task, and discuss a couple of\ncountermeasures to improve this resilience.\n",
        "title": "Implications of Noise in Resistive Memory on Deep Neural Networks for\n  Image Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05821",
        "abstract_url": "http://arxiv.org/abs/2401.05821",
        "authors": [
            {
                "last_name": "Delfosse",
                "first_name": "Quentin"
            },
            {
                "last_name": "Sztwiertnia",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Stammer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Rothermel",
                "first_name": "Mark"
            },
            {
                "last_name": "Kersting",
                "first_name": "Kristian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SC"
        ],
        "abstract": "  Reward sparsity, difficult credit assignment, and misalignment are only a few\nof the many issues that make it difficult, if not impossible, for deep\nreinforcement learning (RL) agents to learn optimal policies. Unfortunately,\nthe black-box nature of deep networks impedes the inclusion of domain experts\nwho could interpret the model and correct wrong behavior. To this end, we\nintroduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole\ndecision pipeline transparent via the integration of consecutive concept\nbottleneck layers. SCoBots make use of not only relevant object properties but\nalso of relational concepts. Our experimental results provide strong evidence\nthat SCoBots allow domain experts to efficiently understand and regularize\ntheir behavior, resulting in potentially better human-aligned RL. In this way,\nSCoBots enabled us to identify a misalignment problem in the most simple and\niconic video game, Pong, and resolve it.\n",
        "title": "Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05822",
        "abstract_url": "http://arxiv.org/abs/2401.05822",
        "authors": [
            {
                "last_name": "Free",
                "first_name": "Michael"
            },
            {
                "last_name": "Langworthy",
                "first_name": "Andrew"
            },
            {
                "last_name": "Dimitropoulaki",
                "first_name": "Mary"
            },
            {
                "last_name": "Thompson",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  The objective of this work is to train a chatbot capable of solving evolving\nproblems through conversing with a user about a problem the chatbot cannot\ndirectly observe. The system consists of a virtual problem (in this case a\nsimple game), a simulated user capable of answering natural language questions\nthat can observe and perform actions on the problem, and a Deep Q-Network\n(DQN)-based chatbot architecture. The chatbot is trained with the goal of\nsolving the problem through dialogue with the simulated user using\nreinforcement learning. The contributions of this paper are as follows: a\nproposed architecture to apply a conversational DQN-based agent to evolving\nproblems, an exploration of training methods such as curriculum learning on\nmodel performance and the effect of modified reward functions in the case of\nincreasing environment complexity.\n",
        "title": "Towards Goal-Oriented Agents for Evolving Problems Observed via\n  Conversation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05824",
        "abstract_url": "http://arxiv.org/abs/2401.05824",
        "authors": [
            {
                "last_name": "Phang",
                "first_name": "Kenji"
            },
            {
                "last_name": "Pradhan",
                "first_name": "Siddharth Saarathi"
            },
            {
                "last_name": "Ikwuegbu",
                "first_name": "Chino"
            },
            {
                "last_name": "Ramos",
                "first_name": "Gonzalo"
            },
            {
                "last_name": "Ford",
                "first_name": "Denae"
            },
            {
                "last_name": "Okoli",
                "first_name": "Ebele"
            },
            {
                "last_name": "Chishti",
                "first_name": "Salman Muin Kayser"
            },
            {
                "last_name": "Suh",
                "first_name": "Jina"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  Mental health is a pressing concern in today's digital age, particularly\namong youth who are deeply intertwined with technology. Despite the influx of\ntechnology solutions addressing mental health issues, youth often remain\nsidelined during the design process. While co-design methods have been employed\nto improve participation by youth, many such initiatives are limited to design\nactivities and lack training for youth to research and develop solutions for\nthemselves. In this case study, we detail our 8-week remote, collaborative\nresearch initiative called Youth WellTech, designed to facilitate remote\nco-design sprints aimed at equipping youth with the tools and knowledge to\nenvision and design tech futures for their own communities. We pilot this\ninitiative with 12 student technology evangelists across 8 countries globally\nto foster the sharing of mental health challenges and diverse perspectives. We\nhighlight insights from our experiences running this global program remotely,\nits structure, and recommendations for co-research.\n",
        "title": "Youth WellTech: A Global Remote Co-Design Sprint for Youth Mental Health\n  Technology",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05826",
        "abstract_url": "http://arxiv.org/abs/2401.05826",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Nivedita"
            },
            {
                "last_name": "Do",
                "first_name": "Yejin"
            },
            {
                "last_name": "Fouad",
                "first_name": "Yongsang Yu. Imane"
            },
            {
                "last_name": "Kim",
                "first_name": "Jungrae"
            },
            {
                "last_name": "Kim",
                "first_name": "Hyoungshick"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Despite stringent data protection regulations such as the General Data\nProtection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and\nother country-specific regulations, many websites continue to use cookies to\ntrack user activities. Recent studies have revealed several data protection\nviolations, resulting in significant penalties, especially for multinational\ncorporations. Motivated by the question of why these data protection violations\ncontinue to occur despite strong data protection regulations, we examined 360\npopular e-commerce websites in multiple countries to analyze whether they\ncomply with regulations to protect user privacy from a cookie perspective.\n",
        "title": "Crumbled Cookie Exploring E-commerce Websites Cookie Policies with Data\n  Protection Regulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05827",
        "abstract_url": "http://arxiv.org/abs/2401.05827",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jinge"
            },
            {
                "last_name": "Kim",
                "first_name": "Yunsoo"
            },
            {
                "last_name": "Wu",
                "first_name": "Honghan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "CV"
        ],
        "abstract": "  The recent success of large language and vision models on vision question\nanswering (VQA), particularly their applications in medicine (Med-VQA), has\nshown a great potential of realizing effective visual assistants for\nhealthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models limitations and reveals the effectiveness\nof various prompting strategies.\n",
        "title": "Hallucination Benchmark in Medical Visual Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05831",
        "abstract_url": "http://arxiv.org/abs/2401.05831",
        "authors": [
            {
                "last_name": "Vardakas",
                "first_name": "Georgios"
            },
            {
                "last_name": "Pavlopoulos",
                "first_name": "John"
            },
            {
                "last_name": "Likas",
                "first_name": "Aristidis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Silhouette coefficient is an established internal clustering evaluation\nmeasure that produces a score per data point, assessing the quality of its\nclustering assignment. To assess the quality of the clustering of the whole\ndataset, the scores of all the points in the dataset are typically averaged\ninto a single value, a strategy which we call as micro-averaging. As we\nillustrate in this work, by using a synthetic example, this micro-averaging\nstrategy is sensitive both to cluster imbalance and outliers (background\nnoise). To address these issues, we propose an alternative aggregation\nstrategy, which first averages the silhouette scores at a cluster level and\nthen (macro) averages the scores across the clusters. Based on the same\nsynthetic example, we show that the proposed macro-averaged silhouette score is\nrobust to cluster imbalance and background noise. We have conducted an\nexperimental study showing that our macro-averaged variant provides better\nestimates of the ground truth number of clusters on several cases compared to\nthe typical micro-averaged score.\n",
        "title": "Revisiting Silhouette: From Micro to Macro Aggregation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05833",
        "abstract_url": "http://arxiv.org/abs/2401.05833",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Attaining ultra-reliable communication (URC) in fifth-generation (5G) and\nbeyond networks requires deriving statistics of channel in ultra-reliable\nregion by modeling the extreme events. Extreme value theory (EVT) has been\npreviously adopted in channel modeling to characterize the lower tail of\nreceived powers in URC systems. In this paper, we propose a multivariate EVT\n(MEVT)-based channel modeling methodology for tail of the joint distribution of\nmulti-channel by characterizing the multivariate extremes of multiple-input\nmultiple-output (MIMO) system. The proposed approach derives lower tail\nstatistics of received power of each channel by using the generalized Pareto\ndistribution (GPD). Then, tail of the joint distribution is modeled as a\nfunction of estimated GPD parameters based on two approaches: logistic\ndistribution, which utilizes logistic distribution to determine dependency\nfactors among the Frechet transformed tail sequence and obtain a bi-variate\nextreme value model, and Poisson point process, which estimates probability\nmeasure function of the Pickands angular component to model bi-variate extreme\nvalues. Finally, validity of the proposed models is assessed by incorporating\nthe mean constraint on probability measure function of Pichanks coordinates.\nBased on the data collected within the engine compartment of Fiat Linea, we\ndemonstrate the superiority of proposed methodology compared to the\nconventional extrapolation-based methods in providing the best fit to the\nmultivariate extremes.\n",
        "title": "Multivariate Extreme Value Theory Based Channel Modeling for\n  Ultra-Reliable Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05834",
        "abstract_url": "http://arxiv.org/abs/2401.05834",
        "authors": [
            {
                "last_name": "Mari",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Anish"
            },
            {
                "last_name": "Ren",
                "first_name": "Runtian"
            },
            {
                "last_name": "Sankowski",
                "first_name": "Piotr"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Web requests are growing exponentially since the 90s due to the rapid\ndevelopment of the Internet. This process was further accelerated by the\nintroduction of cloud services. It has been observed statistically that memory\nor web requests generally follow power-law distribution, Breslau et al.\nINFOCOM'99. That is, the $i^{\\text{th}}$ most popular web page is requested\nwith a probability proportional to $1 / i^{\\alpha}$ ($\\alpha > 0$ is a\nconstant). Furthermore, this study, which was performed more than 20 years ago,\nindicated Zipf-like behavior, i.e., that $\\alpha \\le 1$. Surprisingly, the\nmemory access traces coming from petabyte-size modern cloud systems not only\nshow that $\\alpha$ can be bigger than one but also illustrate a shifted\npower-law distribution -- called Pareto type II or Lomax. These previously not\nreported phenomenon calls for statistical explanation.\n  Our first contribution is a new statistical {\\it multi-core power-law} model\nindicating that double-power law can be attributed to the presence of multiple\ncores running many virtual machines in parallel on such systems. We verify\nexperimentally the applicability of this model using the Kolmogorov-Smirnov\ntest (K-S test).\n  The second contribution of this paper is a theoretical analysis indicating\nwhy LRU and LFU-based algorithms perform well in practice on data satisfying\npower-law or multi-core assumptions. We provide an explanation by studying the\nonline paging problem in the stochastic input model, i.e., the input is a\nrandom sequence with each request independently drawn from a page set according\nto a distribution $\\pi$. We derive formulas (as a function of the page\nprobabilities in $\\pi$) to upper bound their ratio-of-expectations, which help\nin establishing O(1) performance ratio given the random sequence following\npower-law and multi-core power-law distributions.\n",
        "title": "Modeling Online Paging in Multi-Core Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05835",
        "abstract_url": "http://arxiv.org/abs/2401.05835",
        "authors": [
            {
                "last_name": "Hosseinalizadeh",
                "first_name": "Teimour"
            },
            {
                "last_name": "Schl\u00fcter",
                "first_name": "Nils"
            },
            {
                "last_name": "Darup",
                "first_name": "Moritz Schulze"
            },
            {
                "last_name": "Monshizadeh",
                "first_name": "Nima"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Search for the optimizer in computationally demanding model predictive\ncontrol (MPC) setups can be facilitated by Cloud as a service provider in\ncyber-physical systems. This advantage introduces the risk that Cloud can\nobtain unauthorized access to the privacy-sensitive parameters of the system\nand cost function. To solve this issue, i.e., preventing Cloud from accessing\nthe parameters while benefiting from Cloud computation, random affine\ntransformations provide an exact yet light weight in computation solution. This\nresearch deals with analyzing privacy preserving properties of these\ntransformations when they are adopted for MPC problems. We consider two common\nstrategies for outsourcing the optimization required in MPC problems, namely\nseparate and dense forms, and establish that random affine transformations\nutilized in these forms are vulnerable to side-knowledge from Cloud.\nSpecifically, we prove that the privacy guarantees of these methods and their\nextensions for separate form are undermined when a mild side-knowledge about\nthe problem in terms of structure of MPC cost function is available. In\naddition, while we prove that outsourcing the MPC problem in the dense form\ninherently leads to some degree of privacy for the system and cost function\nparameters, we also establish that affine transformations applied to this form\nare nevertheless prone to be undermined by a Cloud with mild side-knowledge.\nNumerical simulations confirm our results.\n",
        "title": "Privacy Analysis of Affine Transformations in Cloud-based MPC:\n  Vulnerability to Side-knowledge",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05836",
        "abstract_url": "http://arxiv.org/abs/2401.05836",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Feng"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xveqing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuantai"
            },
            {
                "last_name": "Chen",
                "first_name": "Weijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaohong"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The essential of navigation, perception, and decision-making which are basic\ntasks for intelligent robots, is to estimate necessary system states. Among\nthem, navigation is fundamental for other upper applications, providing precise\nposition and orientation, by integrating measurements from multiple sensors.\nWith observations of each sensor appropriately modelled, multi-sensor fusion\ntasks for navigation are reduced to the state estimation problem which can be\nsolved by two approaches: optimization and filtering. Recent research has shown\nthat optimization-based frameworks outperform filtering-based ones in terms of\naccuracy. However, both methods are based on maximum likelihood estimation\n(MLE) and should be theoretically equivalent with the same linearization\npoints, observation model, measurements, and Gaussian noise assumption. In this\npaper, we deeply dig into the theories and existing strategies utilized in both\noptimization-based and filtering-based approaches. It is demonstrated that the\ntwo methods are equal theoretically, but this equivalence corrupts due to\ndifferent strategies applied in real-time operation. By adjusting existing\nstrategies of the filtering-based approaches, the Monte-Carlo simulation and\nvehicular ablation experiments based on visual odometry (VO) indicate that the\nstrategy adjusted filtering strictly equals to optimization. Therefore, future\nresearch on sensor-fusion problems should concentrate on their own algorithms\nand strategies rather than state estimation approaches.\n",
        "title": "On State Estimation in Multi-Sensor Fusion Navigation: Optimization and\n  Filtering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05840",
        "abstract_url": "http://arxiv.org/abs/2401.05840",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhuoyan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Yin",
                "first_name": "Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            ""
        ],
        "abstract": "  With the rapid development of AI-based decision aids, different forms of AI\nassistance have been increasingly integrated into the human decision making\nprocesses. To best support humans in decision making, it is essential to\nquantitatively understand how diverse forms of AI assistance influence humans'\ndecision making behavior. To this end, much of the current research focuses on\nthe end-to-end prediction of human behavior using ``black-box'' models, often\nlacking interpretations of the nuanced ways in which AI assistance impacts the\nhuman decision making process. Meanwhile, methods that prioritize the\ninterpretability of human behavior predictions are often tailored for one\nspecific form of AI assistance, making adaptations to other forms of assistance\ndifficult. In this paper, we propose a computational framework that can provide\nan interpretable characterization of the influence of different forms of AI\nassistance on decision makers in AI-assisted decision making. By\nconceptualizing AI assistance as the ``{\\em nudge}'' in human decision making\nprocesses, our approach centers around modelling how different forms of AI\nassistance modify humans' strategy in weighing different information in making\ntheir decisions. Evaluations on behavior data collected from real human\ndecision makers show that the proposed framework outperforms various baselines\nin accurately predicting human behavior in AI-assisted decision making. Based\non the proposed framework, we further provide insights into how individuals\nwith different cognitive styles are nudged by AI assistance differently.\n",
        "title": "Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in\n  AI-assisted Decision Making",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05841",
        "abstract_url": "http://arxiv.org/abs/2401.05841",
        "authors": [
            {
                "last_name": "Br\u00fcning",
                "first_name": "Frederik"
            },
            {
                "last_name": "Driemel",
                "first_name": "Anne"
            },
            {
                "last_name": "Erg\u00fcr",
                "first_name": "Alperen"
            },
            {
                "last_name": "R\u00f6glin",
                "first_name": "Heiko"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  The DTW Barycenter Averaging (DBA) algorithm is a widely used algorithm for\nestimating the mean of a given set of point sequences. In this context, the\nmean is defined as a point sequence that minimises the sum of dynamic time\nwarping distances (DTW). The algorithm is similar to the $k$-means algorithm in\nthe sense that it alternately repeats two steps: (1) computing an optimal\nassignment to the points of the current mean, and (2) computing an optimal mean\nunder the current assignment. The popularity of DBA can be attributed to the\nfact that it works well in practice, despite any theoretical guarantees to be\nknown. In our paper, we aim to initiate a theoretical study of the number of\niterations that DBA performs until convergence. We assume the algorithm is\ngiven $n$ sequences of $m$ points in $\\mathbb{R}^d$ and a parameter $k$ that\nspecifies the length of the mean sequence to be computed. We show that, in\ncontrast to its fast running time in practice, the number of iterations can be\nexponential in $k$ in the worst case - even if the number of input sequences is\n$n=2$. We complement these findings with experiments on real-world data that\nsuggest this worst-case behaviour is likely degenerate. To better understand\nthe performance of the algorithm on non-degenerate input, we study DBA in the\nmodel of smoothed analysis, upper-bounding the expected number of iterations in\nthe worst case under random perturbations of the input. Our smoothed upper\nbound is polynomial in $k$, $n$ and $d$, and for constant $n$, it is also\npolynomial in $m$. For our analysis, we adapt the set of techniques that were\ndeveloped for analysing $k$-means and observe that this set of techniques is\nnot sufficient to obtain tight bounds for general $n$.\n",
        "title": "On the number of iterations of the DBA algorithm",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05842",
        "abstract_url": "http://arxiv.org/abs/2401.05842",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Tao"
            },
            {
                "last_name": "Bao",
                "first_name": "Jialu"
            },
            {
                "last_name": "Hsu",
                "first_name": "Justin"
            },
            {
                "last_name": "Silva",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Zanasi",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  The logic of Dependence and Independence Bunched Implications (DIBI) is a\nlogic to reason about conditional independence (CI); for instance, DIBI\nformulas can characterise CI in probability distributions and relational\ndatabases, using the probabilistic and relational DIBI models, respectively.\nDespite the similarity of the probabilistic and relational models, a uniform,\nmore abstract account remains unsolved. The laborious case-by-case verification\nof the frame conditions required for constructing new models also calls for\nsuch a treatment. In this paper, we develop an abstract framework for\nsystematically constructing DIBI models, using category theory as the unifying\nmathematical language. In particular, we use string diagrams -- a graphical\npresentation of monoidal categories -- to give a uniform definition of the\nparallel composition and subkernel relation in DIBI models. Our approach not\nonly generalises known models, but also yields new models of interest and\nreduces properties of DIBI models to structures in the underlying categories.\nFurthermore, our categorical framework enables a logical notion of CI, in terms\nof the satisfaction of specific DIBI formulas. We compare it with string\ndiagrammatic approaches to CI and show that it is an extension of string\ndiagrammatic CI under reasonable conditions.\n",
        "title": "A Categorical Approach to DIBI Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05845",
        "abstract_url": "http://arxiv.org/abs/2401.05845",
        "authors": [
            {
                "last_name": "Konrad",
                "first_name": "Christian"
            },
            {
                "last_name": "O'Sullivan",
                "first_name": "Conor"
            },
            {
                "last_name": "Traistaru",
                "first_name": "Victor"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We consider the Graph Reconstruction problem given only query access to the\ninput graph via a Maximal Independent Set oracle. In this setting, in each\nround, the player submits a query consisting of a subset of vertices to the\noracle, and the oracle returns any maximal independent set in the subgraph\ninduced by the queried vertices. The goal for the player is to learn all the\nedges of the input graph.\n  In this paper, we give tight (up to a logarithmic factor) upper and lower\nbounds for this problem:\n  1. We give a randomized query algorithm that uses $O(\\Delta^2 \\log n)$\nnon-adaptive queries and succeeds with high probability to reconstruct an\n$n$-vertex graph with maximum degree $\\Delta$. Using the probabilistic method,\nwe also show that a non-adaptive deterministic algorithm that executes\n$O(\\Delta^3 \\log n)$ queries exists.\n  2. We give two lower bounds that apply to arbitrary adaptive randomized\nalgorithms that succeed with probability greater than $\\frac{1}{2}$. We show\nthat, for such algorithms, $\\Omega(\\Delta^2)$ rounds are necessary in graphs of\nmaximum degree $\\Delta$, and that $\\Omega(\\log n)$ rounds are necessary even\nwhen the input graph is an $n$-vertex cycle.\n",
        "title": "Graph Reconstruction via MIS Queries",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05848",
        "abstract_url": "http://arxiv.org/abs/2401.05848",
        "authors": [
            {
                "last_name": "Riebesell",
                "first_name": "Janosh"
            },
            {
                "last_name": "Surta",
                "first_name": "T. Wesley"
            },
            {
                "last_name": "Goodall",
                "first_name": "Rhys"
            },
            {
                "last_name": "Gaultois",
                "first_name": "Michael"
            },
            {
                "last_name": "Lee",
                "first_name": "Alpha A"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "LG",
            ""
        ],
        "abstract": "  Materials with high-dielectric constant easily polarize under external\nelectric fields, allowing them to perform essential functions in many modern\nelectronic devices. Their practical utility is determined by two conflicting\nproperties: high dielectric constants tend to occur in materials with narrow\nband gaps, limiting the operating voltage before dielectric breakdown. We\npresent a high-throughput workflow that combines element substitution, ML\npre-screening, ab initio simulation and human expert intuition to efficiently\nexplore the vast space of unknown materials for potential dielectrics, leading\nto the synthesis and characterization of two novel dielectric materials,\nCsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective\noptimization setting with concave Pareto front. While usually considered more\nchallenging than single-objective optimization, we argue and show preliminary\nevidence that the $1/x$-correlation between band gap and permittivity in fact\nmakes the task more amenable to ML methods by allowing separate models for band\ngap and permittivity to each operate in regions of good training support while\nstill predicting materials of exceptional merit. To our knowledge, this is the\nfirst instance of successful ML-guided multi-objective materials optimization\nachieving experimental synthesis and characterization. CsTaTeO6 is a structure\ngenerated via element substitution not present in our reference data sources,\nthus exemplifying successful de-novo materials design. Meanwhile, we report the\nfirst high-purity synthesis and dielectric characterization of Bi2Zr2O7 with a\nband gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics of\nour multi-objective search.\n",
        "title": "Pushing the Pareto front of band gap and permittivity: ML-guided search\n  for dielectric materials",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05849",
        "abstract_url": "http://arxiv.org/abs/2401.05849",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Litian"
            },
            {
                "last_name": "Molhoek",
                "first_name": "Jord"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CL",
            "HC",
            "",
            ""
        ],
        "abstract": "  Humans have good natural intuition to recognize when another person has\nsomething to say. It would be interesting if an AI can also recognize\nintentions to speak. Especially in scenarios when an AI is guiding a group\ndiscussion, this can be a useful skill. This work studies the inference of\nsuccessful and unsuccessful intentions to speak from accelerometer data. This\nis chosen because it is privacy-preserving and feasible for in-the-wild\nsettings since it can be placed in a smart badge. Data from a real-life social\nnetworking event is used to train a machine-learning model that aims to infer\nintentions to speak. A subset of unsuccessful intention-to-speak cases in the\ndata is annotated. The model is trained on the successful intentions to speak\nand evaluated on both the successful and unsuccessful cases. In conclusion,\nthere is useful information in accelerometer data, but not enough to reliably\ncapture intentions to speak. For example, posture shifts are correlated with\nintentions to speak, but people also often shift posture without having an\nintention to speak, or have an intention to speak without shifting their\nposture. More modalities are likely needed to reliably infer intentions to\nspeak.\n",
        "title": "Inferring Intentions to Speak Using Accelerometer Data In-the-Wild",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05850",
        "abstract_url": "http://arxiv.org/abs/2401.05850",
        "authors": [
            {
                "last_name": "Guan",
                "first_name": "Yadong"
            },
            {
                "last_name": "Han",
                "first_name": "Jiqing"
            },
            {
                "last_name": "Song",
                "first_name": "Hongwei"
            },
            {
                "last_name": "Song",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Zheng",
                "first_name": "Guibin"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieran"
            },
            {
                "last_name": "He",
                "first_name": "Yongjun"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            ""
        ],
        "abstract": "  Overlapping sound events are ubiquitous in real-world environments, but\nexisting end-to-end sound event detection (SED) methods still struggle to\ndetect them effectively. A critical reason is that these methods represent\noverlapping events using shared and entangled frame-wise features, which\ndegrades the feature discrimination. To solve the problem, we propose a\ndisentangled feature learning framework to learn a category-specific\nrepresentation. Specifically, we employ different projectors to learn the\nframe-wise features for each category. To ensure that these feature does not\ncontain information of other categories, we maximize the common information\nbetween frame-wise features within the same category and propose a frame-wise\ncontrastive loss. In addition, considering that the labeled data used by the\nproposed method is limited, we propose a semi-supervised frame-wise contrastive\nloss that can leverage large amounts of unlabeled data to achieve feature\ndisentanglement. The experimental results demonstrate the effectiveness of our\nmethod.\n",
        "title": "Contrastive Loss Based Frame-wise Feature disentanglement for Polyphonic\n  Sound Event Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05856",
        "abstract_url": "http://arxiv.org/abs/2401.05856",
        "authors": [
            {
                "last_name": "Barnett",
                "first_name": "Scott"
            },
            {
                "last_name": "Kurniawan",
                "first_name": "Stefanus"
            },
            {
                "last_name": "Thudumu",
                "first_name": "Srikanth"
            },
            {
                "last_name": "Brannelly",
                "first_name": "Zach"
            },
            {
                "last_name": "Abdelrazek",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.\n",
        "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05857",
        "abstract_url": "http://arxiv.org/abs/2401.05857",
        "authors": [
            {
                "last_name": "Azarbahram",
                "first_name": "Ali"
            },
            {
                "last_name": "Amini",
                "first_name": "Amir"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This article proposes a secure implementation for consensus using a dynamic\nevent-triggered (DET) communication scheme in high-order nonlinear multi-agent\nsystems (MAS) under asynchronous (distributed) denial of service (DoS) attacks.\nBy introducing a linear auxiliary trajectory of the system, the DET data\ntransmission scheme among the neighboring agents is employed to reduce the\ncommunication for each agent. The asynchronous DoS attacks can block each\ncommunication channel among the cooperative agents independently in an unknown\npattern. To guarantee state consensus of auxiliary MAS under DoS, a linear\nmatrix inequality (LMI) based optimization approach is proposed which\nsimultaneously designs all the unknown DET communication parameters as well as\nthe state feedback control gain. In addition to asynchronous DoS attacks over\nthe graph topology, the destructive effects of independent DoS attacks over the\ncommunication links between actual and auxiliary states are compensated as an\nadditional layer of resiliency for the system. The output of each agent\nultimately tracks the auxiliary state of the system and this results in the\noutput consensus.\n",
        "title": "Secure Dynamic Event-triggered Consensus Under Asynchronous Denial of\n  Service",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05859",
        "abstract_url": "http://arxiv.org/abs/2401.05859",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Wentu"
            },
            {
                "last_name": "Cai",
                "first_name": "Kui"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, for any fixed integer $q>2$, we construct $q$-ary codes\ncorrecting a burst of at most $t$ deletions with redundancy $\\log n+8\\log\\log\nn+o(\\log\\log n)+\\gamma_{q,t}$ bits and near-linear encoding/decoding\ncomplexity, where $n$ is the message length and $\\gamma_{q,t}$ is a constant\nthat only depends on $q$ and $t$. In previous works there are constructions of\nsuch codes with redundancy $\\log n+O(\\log q\\log\\log n)$ bits or $\\log\nn+O(t^2\\log\\log n)+O(t\\log q)$. The redundancy of our new construction is\nindependent of $q$ and $t$ in the second term.\n",
        "title": "New Construction of $q$-ary Codes Correcting a Burst of at most $t$\n  Deletions",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05860",
        "abstract_url": "http://arxiv.org/abs/2401.05860",
        "authors": [
            {
                "last_name": "Phan",
                "first_name": "Thomy"
            },
            {
                "last_name": "Driscoll",
                "first_name": "Joseph"
            },
            {
                "last_name": "Romberg",
                "first_name": "Justin"
            },
            {
                "last_name": "Koenig",
                "first_name": "Sven"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  A wide range of real-world applications can be formulated as Multi-Agent Path\nFinding (MAPF) problem, where the goal is to find collision-free paths for\nmultiple agents with individual start and goal locations. State-of-the-art MAPF\nsolvers are mainly centralized and depend on global information, which limits\ntheir scalability and flexibility regarding changes or new maps that would\nrequire expensive replanning. Multi-agent reinforcement learning (MARL) offers\nan alternative way by learning decentralized policies that can generalize over\na variety of maps. While there exist some prior works that attempt to connect\nboth areas, the proposed techniques are heavily engineered and very complex due\nto the integration of many mechanisms that limit generality and are expensive\nto use. We argue that much simpler and general approaches are needed to bring\nthe areas of MARL and MAPF closer together with significantly lower costs. In\nthis paper, we propose Confidence-based Auto-Curriculum for Team Update\nStability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a\nsimple reverse curriculum scheme, where the goal of each agent is randomly\nplaced within an allocation radius around the agent's start location. The\nallocation radius increases gradually as all agents improve, which is assessed\nby a confidence-based measure. We evaluate CACTUS in various maps of different\nsizes, obstacle densities, and numbers of agents. Our experiments demonstrate\nbetter performance and generalization capabilities than state-of-the-art MARL\napproaches with less than 600,000 trainable parameters, which is less than 5%\nof the neural network size of current MARL approaches to MAPF.\n",
        "title": "Confidence-Based Curriculum Learning for Multi-Agent Path Finding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05861",
        "abstract_url": "http://arxiv.org/abs/2401.05861",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Pengzhi"
            },
            {
                "last_name": "He",
                "first_name": "Zhongjun"
            },
            {
                "last_name": "Wu",
                "first_name": "Hua"
            },
            {
                "last_name": "Wang",
                "first_name": "Haifeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The training paradigm for machine translation has gradually shifted, from\nlearning neural machine translation (NMT) models with extensive parallel\ncorpora to instruction finetuning on pretrained multilingual large language\nmodels (LLMs) with high-quality translation pairs. In this paper, we focus on\nboosting the many-to-many multilingual translation performance of LLMs with an\nemphasis on zero-shot translation directions. We demonstrate that prompt\nstrategies adopted during instruction finetuning are crucial to zero-shot\ntranslation performance and introduce a cross-lingual consistency\nregularization, XConST, to bridge the representation gap among different\nlanguages and improve zero-shot translation performance. XConST is not a new\nmethod, but a version of CrossConST (Gao et al., 2023a) adapted for\nmultilingual finetuning on LLMs with translation instructions. Experimental\nresults on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that\nour approach consistently improves translation performance. Our implementations\nare available at https://github.com/gpengzhi/CrossConST-LLM.\n",
        "title": "Towards Boosting Many-to-Many Multilingual Machine Translation with\n  Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05868",
        "abstract_url": "http://arxiv.org/abs/2401.05868",
        "authors": [
            {
                "last_name": "Ham",
                "first_name": "David A."
            },
            {
                "last_name": "Hapla",
                "first_name": "Vaclav"
            },
            {
                "last_name": "Knepley",
                "first_name": "Matthew G."
            },
            {
                "last_name": "Mitchell",
                "first_name": "Lawrence"
            },
            {
                "last_name": "Sagiyama",
                "first_name": "Koki"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "MS"
        ],
        "abstract": "  In this work, we introduce a new algorithm for N-to-M checkpointing in finite\nelement simulations. This new algorithm allows efficient saving/loading of\nfunctions representing physical quantities associated with the mesh\nrepresenting the physical domain. Specifically, the algorithm allows for using\ndifferent numbers of parallel processes for saving and loading, allowing for\nrestarting and post-processing on the process count appropriate to the given\nphase of the simulation and other conditions. For demonstration, we implemented\nthis algorithm in PETSc, the Portable, Extensible Toolkit for Scientific\nComputation, and added a convenient high-level interface into Firedrake, a\nsystem for solving partial differential equations using finite element methods.\nWe evaluated our new implementation by saving and loading data involving 8.2\nbillion finite element degrees of freedom using 8,192 parallel processes on\nARCHER2, the UK National Supercomputing Service.\n",
        "title": "Efficient N-to-M Checkpointing Algorithm for Finite Element Simulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05870",
        "abstract_url": "http://arxiv.org/abs/2401.05870",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Hanzhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Yang",
                "first_name": "Jinze"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhongrui"
            },
            {
                "last_name": "Xie",
                "first_name": "Zeke"
            },
            {
                "last_name": "Tian",
                "first_name": "Lei"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            },
            {
                "last_name": "Sun",
                "first_name": "Mingming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  The goal of Arbitrary Style Transfer (AST) is injecting the artistic features\nof a style reference into a given image/video. Existing methods usually focus\non pursuing the balance between style and content, whereas ignoring the\nsignificant demand for flexible and customized stylization results and thereby\nlimiting their practical application. To address this critical issue, a novel\nAST approach namely HiCAST is proposed, which is capable of explicitly\ncustomizing the stylization results according to various source of semantic\nclues. In the specific, our model is constructed based on Latent Diffusion\nModel (LDM) and elaborately designed to absorb content and style instance as\nconditions of LDM. It is characterized by introducing of \\textit{Style\nAdapter}, which allows user to flexibly manipulate the output results by\naligning multi-level style information and intrinsic knowledge in LDM. Lastly,\nwe further extend our model to perform video AST. A novel learning objective is\nleveraged for video diffusion model training, which significantly improve\ncross-frame temporal consistency in the premise of maintaining stylization\nstrength. Qualitative and quantitative comparisons as well as comprehensive\nuser studies demonstrate that our HiCAST outperforms the existing SoTA methods\nin generating visually plausible stylization results.\n",
        "title": "HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced\n  Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05871",
        "abstract_url": "http://arxiv.org/abs/2401.05871",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Yahui"
            },
            {
                "last_name": "Song",
                "first_name": "Haiyue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Personality recognition is useful for enhancing robots' ability to tailor\nuser-adaptive responses, thus fostering rich human-robot interactions. One of\nthe challenges in this task is a limited number of speakers in existing\ndialogue corpora, which hampers the development of robust, speaker-independent\npersonality recognition models. Additionally, accurately modeling both the\ninterdependencies among interlocutors and the intra-dependencies within the\nspeaker in dialogues remains a significant issue. To address the first\nchallenge, we introduce personality trait interpolation for speaker data\naugmentation. For the second, we propose heterogeneous conversational graph\nnetworks to independently capture both contextual influences and inherent\npersonality traits. Evaluations on the RealPersonaChat corpus demonstrate our\nmethod's significant improvements over existing baselines.\n",
        "title": "Enhancing Personality Recognition in Dialogue by Data Augmentation and\n  Heterogeneous Conversational Graph Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05872",
        "abstract_url": "http://arxiv.org/abs/2401.05872",
        "authors": [
            {
                "last_name": "Goncharov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Santamaria",
                "first_name": "Alessio"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Lutz"
            },
            {
                "last_name": "Tsampas",
                "first_name": "Stelios"
            },
            {
                "last_name": "Urbat",
                "first_name": "Henning"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "PL"
        ],
        "abstract": "  We present a systematic approach to logical predicates based on universal\ncoalgebra and higher-order abstract GSOS, thus making a first step towards a\nunifying theory of logical relations. We first observe that logical predicates\nare special cases of coalgebraic invariants on mixed-variance functors. We then\nintroduce the notion of a locally maximal logical refinement of a given\npredicate, with a view to enabling inductive reasoning, and identify sufficient\nconditions on the overall setup in which locally maximal logical refinements\ncanonically exist. Finally, we develop induction-up-to techniques that simplify\ninductive proofs via logical predicates on systems encoded as (certain classes\nof) higher-order GSOS laws by identifying and abstracting away from their\nboiler-plate part.\n",
        "title": "Logical Predicates in Higher-Order Mathematical Operational Semantics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05876",
        "abstract_url": "http://arxiv.org/abs/2401.05876",
        "authors": [
            {
                "last_name": "Baumann",
                "first_name": "Dominik"
            },
            {
                "last_name": "Sch\u00f6n",
                "first_name": "Thomas B."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  When deploying machine learning algorithms in the real world, guaranteeing\nsafety is an essential asset. Existing safe learning approaches typically\nconsider continuous variables, i.e., regression tasks. However, in practice,\nrobotic systems are also subject to discrete, external environmental changes,\ne.g., having to carry objects of certain weights or operating on frozen, wet,\nor dry surfaces. Such influences can be modeled as discrete context variables.\nIn the existing literature, such contexts are, if considered, mostly assumed to\nbe known. In this work, we drop this assumption and show how we can perform\nsafe learning when we cannot directly measure the context variables. To achieve\nthis, we derive frequentist guarantees for multi-class classification, allowing\nus to estimate the current context from measurements. Further, we propose an\napproach for identifying contexts through experiments. We discuss under which\nconditions we can retain theoretical guarantees and demonstrate the\napplicability of our algorithm on a Furuta pendulum with camera measurements of\ndifferent weights that serve as contexts.\n",
        "title": "Safe reinforcement learning in uncertain contexts",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05879",
        "abstract_url": "http://arxiv.org/abs/2401.05879",
        "authors": [
            {
                "last_name": "Jing",
                "first_name": "Yu"
            },
            {
                "last_name": "Yujuan",
                "first_name": "Tan"
            },
            {
                "last_name": "Ao",
                "first_name": "Ren"
            },
            {
                "last_name": "Duo",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Occlusions pose a significant challenge to optical flow algorithms that even\nrely on global evidences. We consider an occluded point to be one that is\nimaged in the reference frame but not in the next. Estimating the motion of\nthese points is extremely difficult, particularly in the two-frame setting.\nPrevious work only used the current frame as the only input, which could not\nguarantee providing correct global reference information for occluded points,\nand had problems such as long calculation time and poor accuracy in predicting\noptical flow at occluded points. To enable both high accuracy and efficiency,\nWe fully mine and utilize the spatiotemporal information provided by the frame\npair, design a loopback judgment algorithm to ensure that correct global\nreference information is obtained, mine multiple necessary global information,\nand design an efficient refinement module that fuses these global information.\nSpecifically, we propose a YOIO framework, which consists of three main\ncomponents: an initial flow estimator, a multiple global information extraction\nmodule, and a unified refinement module. We demonstrate that optical flow\nestimates in the occluded regions can be significantly improved in only one\niteration without damaging the performance in non-occluded regions. Compared\nwith GMA, the optical flow prediction accuracy of this method in the occluded\narea is improved by more than 10%, and the occ_out area exceeds 15%, while the\ncalculation time is 27% shorter. This approach, running up to 18.9fps with\n436*1024 image resolution, obtains new state-of-the-art results on the\nchallenging Sintel dataset among all published and unpublished approaches that\ncan run in real-time, suggesting a new paradigm for accurate and efficient\noptical flow estimation.\n",
        "title": "YOIO: You Only Iterate Once by mining and fusing multiple necessary\n  global information in the optical flow estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05881",
        "abstract_url": "http://arxiv.org/abs/2401.05881",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Chendong"
            },
            {
                "last_name": "Yang",
                "first_name": "Dapeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Dai",
                "first_name": "Yiming"
            },
            {
                "last_name": "Jiang",
                "first_name": "Li"
            },
            {
                "last_name": "Liu",
                "first_name": "Hong"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The fabric-based pneumatic exosuit is now a hot research topic because it is\nlighter and softer than traditional exoskeletons. Existing research focused\nmore on the mechanical properties of the exosuit (e.g., torque and speed), but\nless on its wearability (e.g., appearance and comfort). This work presents a\nnew design concept for fabric-based pneumatic exosuits Volume Transfer, which\nmeans transferring the volume of pneumatic actuators beyond the garments\nprofile to the inside. This allows for a concealed appearance and a larger\nstress area while maintaining adequate torques. In order to verify this\nconcept, we develop a fabric-based pneumatic exosuit for knee extension\nassistance. Its profile is only 26mm and its stress area wraps around almost\nhalf of the leg. We use a mathematical model and simulation to determine the\nparameters of the exosuit, avoiding multiple iterations of the prototype.\nExperiment results show that the exosuit can generate a torque of 7.6Nm at a\npressure of 90kPa and produce a significant reduction in the electromyography\nactivity of the knee extensor muscles. We believe that Volume Transfer could be\nutilized prevalently in future fabric-based pneumatic exosuit designs to\nachieve a significant improvement in wearability.\n",
        "title": "Volume Transfer: A New Design Concept for Fabric-Based Pneumatic\n  Exosuits",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05882",
        "abstract_url": "http://arxiv.org/abs/2401.05882",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Ultra-reliable low latency communication (URLLC) requires the packet error\nrate to be on the order of $10^{-9}$-$10^{-5}$. Determining the appropriate\ntransmission rate to satisfy this ultra-reliability constraint requires\nderiving the statistics of the channel in the ultra-reliable region and then\nincorporating these statistics into the rate selection. In this paper, we\npropose a framework for determining the rate selection for ultra-reliable\ncommunications based on the extreme value theory (EVT). We first model the\nwireless channel at URLLC by estimating the parameters of the generalized\nPareto distribution (GPD) best fitting to the tail distribution of the received\npowers, i.e., the power values below a certain threshold. Then, we determine\nthe maximum transmission rate by incorporating the Pareto distribution into the\nrate selection function. Finally, we validate the selected rate by computing\nthe resulting error probability. Based on the data collected within the engine\ncompartment of Fiat Linea, we demonstrate the superior performance of the\nproposed methodology in determining the maximum transmission rate compared to\nthe traditional extrapolation-based approaches.\n",
        "title": "Extreme Value Theory Based Rate Selection for Ultra-Reliable\n  Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05883",
        "abstract_url": "http://arxiv.org/abs/2401.05883",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xianming"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Social media data is plagued by the redundancy problem caused by its noisy\nnature, leading to increased training time and model bias. To address this\nissue, we propose a novel approach called generative duplication. It aims to\nremove duplicate text from noisy social media data and mitigate model bias. By\ndoing so, it can improve social media language understanding performance and\nsave training time. Extensive experiments demonstrate that the proposed\ngenerative deduplication can effectively reduce training samples while\nimproving performance. This evidence suggests the effectiveness of generative\ndeduplication and its importance in social media language understanding.\n",
        "title": "Generative Deduplication For Socia Media Data Selection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05888",
        "abstract_url": "http://arxiv.org/abs/2401.05888",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  Proper determination of the transmission rate in ultra-reliable low latency\ncommunication (URLLC) needs to incorporate a confidence interval (CI) for the\nestimated parameters due to the large amount of data required for their\naccurate estimation. In this paper, we propose a framework based on the extreme\nvalue theory (EVT) for determining the transmission rate along with its\ncorresponding CI for an ultra-reliable communication system. This framework\nconsists of characterizing the statistics of extreme events by fitting the\ngeneralized Pareto distribution (GPD) to the channel tail, deriving the GPD\nparameters and their associated CIs, and obtaining the transmission rate within\na confidence interval. Based on the data collected within the engine\ncompartment of Fiat Linea, we demonstrate the accuracy of the estimated rate\nobtained through the EVT-based framework considering the confidence interval\nfor the GPD parameters. Additionally, we show that proper estimation of the\ntransmission rate based on the proposed framework requires a lower number of\nsamples compared to the traditional extrapolation-based approaches.\n",
        "title": "Incorporation of Confidence Interval into Rate Selection Based on the\n  Extreme Value Theory for Ultra-Reliable Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05891",
        "abstract_url": "http://arxiv.org/abs/2401.05891",
        "authors": [
            {
                "last_name": "Ciobotari",
                "first_name": "Ion"
            },
            {
                "last_name": "Pr\u00edncipe",
                "first_name": "Adriana"
            },
            {
                "last_name": "Oliveira",
                "first_name": "Maria Alexandra"
            },
            {
                "last_name": "Silva",
                "first_name": "Jo\u00e3o Nuno"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The collection of ecological data in the field is essential to diagnose,\nmonitor and manage ecosystems in a sustainable way. Since acquisition of this\ninformation through traditional methods are generally time-consuming, due to\nthe capability of recording large volumes of data in short time periods,\nautomation of data acquisition sees a growing trend. Terrestrial laser scanners\n(TLS), particularly LiDAR sensors, have been used in ecology, allowing to\nreconstruct the 3D structure of vegetation, and thus, infer ecosystem\ncharacteristics based on the spatial variation of the density of points.\nHowever, the low amount of information obtained per beam, lack of data analysis\ntools and the high cost of the equipment limit their use. This way, a low-cost\nTLS (<10k$) was developed along with data acquisition and processing mechanisms\napplicable in two case studies: an urban garden and a target area for\necological restoration. The orientation of LiDAR was modified to make\nobservations in the vertical plane and a motor was integrated for its rotation,\nenabling the acquisition of 360 degree data with high resolution. Motion and\nlocation sensors were also integrated for automatic error correction and\ngeoreferencing. From the data generated, histograms of point density variation\nalong the vegetation height were created, where shrub stratum was easily\ndistinguishable from tree stratum, and maximum tree height and shrub cover were\ncalculated. These results agreed with the field data, whereby the developed TLS\nhas proved to be effective in calculating metrics of structural complexity of\nvegetation.\n",
        "title": "LiDAR data acquisition and processing for ecology applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05892",
        "abstract_url": "http://arxiv.org/abs/2401.05892",
        "authors": [
            {
                "last_name": "Goetze",
                "first_name": "Miriam"
            },
            {
                "last_name": "Jungeblut",
                "first_name": "Paul"
            },
            {
                "last_name": "Ueckerdt",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            ""
        ],
        "abstract": "  We study the recognition complexity of subgraphs of 2- and 3-connected planar\ncubic graphs. Recently, we presented [ESA 2022] a quadratic-time algorithm to\nrecognize subgraphs of planar cubic bridgeless (but not necessarily connected)\ngraphs, both in the variable and fixed embedding setting (the latter only for\n2-connected inputs). Here, we extend our results in two directions: First, we\npresent a quartic-time algorithm to recognize subgraphs of 2-connected planar\ncubic graphs in the fixed embedding setting, even for disconnected inputs.\nSecond, we prove NP-hardness of recognizing subgraphs of 3-connected planar\ncubic graphs in the variable embedding setting.\n",
        "title": "Recognition Complexity of Subgraphs of 2- and 3-Connected Planar Cubic\n  Graphs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05894",
        "abstract_url": "http://arxiv.org/abs/2401.05894",
        "authors": [
            {
                "last_name": "Banaei",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Ebrahimy",
                "first_name": "Razgar"
            },
            {
                "last_name": "Madsen",
                "first_name": "Henrik"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper, a computationally lightweight algorithm is introduced for\nhybrid PV/Battery/Load systems that is price responsive, responds fast, does\nnot require powerful hardware, and considers the operational limitations of the\nsystem. The method is applied to two buildings equipped with PV and battery.\nSimulation results show that the method can give results that are up to 3.9%\nmore expensive than the Model predictive control (MPC) approach while the\nruntime of the program is up to 1000 times less than the MPC. Also, while the\nruntime of the proposed method is in the range of the self-consumption\nmaximization (SCM) approach as the fastest method, its electricity cost is\nabout 3.2% cheaper than the SCM method. Simulation results also show that in\ncase of providing grid services by the battery the difference between\nelectricity cost of the proposed approach and MPC can reduce which makes the\nmethod good for such applications.\n",
        "title": "A Lightweight Energy Management Method for Hybrid PV/Battery/Load\n  Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05895",
        "abstract_url": "http://arxiv.org/abs/2401.05895",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Tianxiu"
            },
            {
                "last_name": "Gai",
                "first_name": "Keke"
            },
            {
                "last_name": "Yu",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Liehuang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CR",
            "DC"
        ],
        "abstract": "  Distributed machine learning enables parallel training of extensive datasets\nby delegating computing tasks across multiple workers. Despite the cost\nreduction benefits of distributed machine learning, the dissemination of final\nmodel weights often leads to potential conflicts over model ownership as\nworkers struggle to substantiate their involvement in the training computation.\nTo address the above ownership issues and prevent accidental failures and\nmalicious attacks, verifying the computational integrity and effectiveness of\nworkers becomes particularly crucial in distributed machine learning. In this\npaper, we proposed a novel binary linear tree commitment-based ownership\nprotection model to ensure computational integrity with limited overhead and\nconcise proof. Due to the frequent updates of parameters during training, our\ncommitment scheme introduces a maintainable tree structure to reduce the costs\nof updating proofs. Distinguished from SNARK-based verifiable computation, our\nmodel achieves efficient proof aggregation by leveraging inner product\narguments. Furthermore, proofs of model weights are watermarked by worker\nidentity keys to prevent commitments from being forged or duplicated. The\nperformance analysis and comparison with SNARK-based hash commitments validate\nthe efficacy of our model in preserving computational integrity within\ndistributed machine learning.\n",
        "title": "Binary Linear Tree Commitment-based Ownership Protection for Distributed\n  Machine Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05896",
        "abstract_url": "http://arxiv.org/abs/2401.05896",
        "authors": [
            {
                "last_name": "Abdi",
                "first_name": "Nima"
            },
            {
                "last_name": "Albaseer",
                "first_name": "Abdullatif"
            },
            {
                "last_name": "Abdallah",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            "NI"
        ],
        "abstract": "  As smart grids (SG) increasingly rely on advanced technologies like sensors\nand communication systems for efficient energy generation, distribution, and\nconsumption, they become enticing targets for sophisticated cyberattacks. These\nevolving threats demand robust security measures to maintain the stability and\nresilience of modern energy systems. While extensive research has been\nconducted, a comprehensive exploration of proactive cyber defense strategies\nutilizing Deep Learning (DL) in {SG} remains scarce in the literature. This\nsurvey bridges this gap, studying the latest DL techniques for proactive cyber\ndefense. The survey begins with an overview of related works and our distinct\ncontributions, followed by an examination of SG infrastructure. Next, we\nclassify various cyber defense techniques into reactive and proactive\ncategories. A significant focus is placed on DL-enabled proactive defenses,\nwhere we provide a comprehensive taxonomy of DL approaches, highlighting their\nroles and relevance in the proactive security of SG. Subsequently, we analyze\nthe most significant DL-based methods currently in use. Further, we explore\nMoving Target Defense, a proactive defense strategy, and its interactions with\nDL methodologies. We then provide an overview of benchmark datasets used in\nthis domain to substantiate the discourse.{ This is followed by a critical\ndiscussion on their practical implications and broader impact on cybersecurity\nin Smart Grids.} The survey finally lists the challenges associated with\ndeploying DL-based security systems within SG, followed by an outlook on future\ndevelopments in this key field.\n",
        "title": "The Role of Deep Learning in Advancing Proactive Cybersecurity Measures\n  for Smart Grid Networks: A Survey",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05897",
        "abstract_url": "http://arxiv.org/abs/2401.05897",
        "authors": [
            {
                "last_name": "Bartels",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Tscherner",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  It is shown that discretizations based on variational or weak formulations of\nthe plate bending problem with simple support boundary conditions do not lead\nto failure of convergence when polygonal domain approximations are used and the\nimposed boundary conditions are compatible with the nodal interpolation of the\nrestriction of certain regular functions to approximating domains. It is\nfurther shown that this is optimal in the sense that a full realization of the\nboundary conditions leads to failure of convergence for conforming methods. The\nabstract conditions imply that standard nonconforming and discontinuous\nGalerkin methods converge correctly while conforming methods require a suitable\nrelaxation of the boundary condition. The results are confirmed by numerical\nexperiments.\n",
        "title": "Necessary and Sufficient Conditions for Avoiding Babuska's Paradox on\n  Simplicial Meshes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05898",
        "abstract_url": "http://arxiv.org/abs/2401.05898",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Weihang"
            },
            {
                "last_name": "Shikh-Bahaei",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            ""
        ],
        "abstract": "  In this work, we propose a novel partial compress-and-forward (PCF) scheme\nfor improving the maximum achievable transmission rate of a diamond relay\nnetwork with two noisy relays. PCF combines conventional compress-and-forward\n(CF) and amplify-and-forward (AF) protocols, enabling one relay to operate\nalternately in the CF or the AF mode, while the other relay works purely in the\nCF mode. As the direct link from the source to the destination is unavailable,\nand there is no noiseless relay in the diamond network, messages received from\nboth relays must act as side information for each other and must be decoded\njointly. We propose a joint decoder to decode two Luby transform (LT) codes\nreceived from both relays corresponding to the same original message. Numerical\nresults show that PCF can achieve significant performance improvements compared\nto decode-and-forward (DF) and pure CF protocols when at least the channels\nconnected to one of the relays are of high quality.\n",
        "title": "A Partial Compress-and-Forward Strategy for Relay-assisted Wireless\n  Networks Based on Rateless Coding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05899",
        "abstract_url": "http://arxiv.org/abs/2401.05899",
        "authors": [
            {
                "last_name": "Zhai",
                "first_name": "Yuanzhao"
            },
            {
                "last_name": "Li",
                "first_name": "Yiying"
            },
            {
                "last_name": "Gao",
                "first_name": "Zijian"
            },
            {
                "last_name": "Gong",
                "first_name": "Xudong"
            },
            {
                "last_name": "Xu",
                "first_name": "Kele"
            },
            {
                "last_name": "Feng",
                "first_name": "Dawei"
            },
            {
                "last_name": "Bo",
                "first_name": "Ding"
            },
            {
                "last_name": "Wang",
                "first_name": "Huaimin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Model-based offline reinforcement learning (RL) has made remarkable progress,\noffering a promising avenue for improving generalization with synthetic model\nrollouts. Existing works primarily focus on incorporating pessimism for policy\noptimization, usually via constructing a Pessimistic Markov Decision Process\n(P-MDP). However, the P-MDP discourages the policies from learning in\nout-of-distribution (OOD) regions beyond the support of offline datasets, which\ncan under-utilize the generalization ability of dynamics models. In contrast,\nwe propose constructing an Optimistic MDP (O-MDP). We initially observed the\npotential benefits of optimism brought by encouraging more OOD rollouts.\nMotivated by this observation, we present ORPO, a simple yet effective\nmodel-based offline RL framework. ORPO generates Optimistic model Rollouts for\nPessimistic offline policy Optimization. Specifically, we train an optimistic\nrollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel\nthe sampled state-action pairs with penalized rewards and optimize the output\npolicy in the P-MDP. Theoretically, we demonstrate that the performance of\npolicies trained with ORPO can be lower-bounded in linear MDPs. Experimental\nresults show that our framework significantly outperforms P-MDP baselines by a\nmargin of 30%, achieving state-of-the-art performance on the widely-used\nbenchmark. Moreover, ORPO exhibits notable advantages in problems that require\ngeneralization.\n",
        "title": "Optimistic Model Rollouts for Pessimistic Offline Policy Optimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05901",
        "abstract_url": "http://arxiv.org/abs/2401.05901",
        "authors": [
            {
                "last_name": "Rivas-Villar",
                "first_name": "David"
            },
            {
                "last_name": "Hervella",
                "first_name": "\u00c1lvaro S."
            },
            {
                "last_name": "Rouco",
                "first_name": "Jos\u00e9"
            },
            {
                "last_name": "Novo",
                "first_name": "Jorge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Retinal image registration is of utmost importance due to its wide\napplications in medical practice. In this context, we propose ConKeD, a novel\ndeep learning approach to learn descriptors for retinal image registration. In\ncontrast to current registration methods, our approach employs a novel\nmulti-positive multi-negative contrastive learning strategy that enables the\nutilization of additional information from the available training samples. This\nmakes it possible to learn high quality descriptors from limited training data.\nTo train and evaluate ConKeD, we combine these descriptors with domain-specific\nkeypoints, particularly blood vessel bifurcations and crossovers, that are\ndetected using a deep neural network. Our experimental results demonstrate the\nbenefits of the novel multi-positive multi-negative strategy, as it outperforms\nthe widely used triplet loss technique (single-positive and single-negative) as\nwell as the single-positive multi-negative alternative. Additionally, the\ncombination of ConKeD with the domain-specific keypoints produces comparable\nresults to the state-of-the-art methods for retinal image registration, while\noffering important advantages such as avoiding pre-processing, utilizing fewer\ntraining samples, and requiring fewer detected keypoints, among others.\nTherefore, ConKeD shows a promising potential towards facilitating the\ndevelopment and application of deep learning-based methods for retinal image\nregistration.\n",
        "title": "ConKeD: Multiview contrastive descriptor learning for keypoint-based\n  retinal image registration",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05902",
        "abstract_url": "http://arxiv.org/abs/2401.05902",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Weihang"
            },
            {
                "last_name": "Shikh-Bahaei",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers downlink incremental redundancy Hybrid Automatic Repeat\nRequest (IR-HARQ) over unreliable feedback channels. Since the impact of\npositive feedback (i.e., ACK) error is smaller than that of negative feedback\n(i.e., NACK) error, an asymmetric feedback detection scheme is proposed to\nprotect NACK and further reduce the outage probability. We formulate the HARQ\nprocess as a Markov Decision Process (MDP) model to adapt to the transmission\nrate of each transmission attempt without enriched feedback and additional\nfeedback cost. We aim to optimize the performance of HARQ process under certain\noutage probability requirements by finding optimal asymmetric detection\nthresholds. Numerical results obtained on the downlink Rayleigh fading channel\nand 5G new radio (NR) PUCCH feedback channel show that by applying asymmetric\nfeedback detection and adaptive rate allocation, higher throughput can be\nachieved under outage probability limitations.\n",
        "title": "Optimized Asymmetric Feedback Detection for Rate-adaptive HARQ with\n  Unreliable Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05906",
        "abstract_url": "http://arxiv.org/abs/2401.05906",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Hyunjin"
            },
            {
                "last_name": "Sung",
                "first_name": "Minhyuk"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D\nsegmentation lifting. Recent studies have highlighted the advantages of\nutilizing 2D segmentation models to achieve high-quality 3D segmentation\nthrough few-shot adaptation. However, previous approaches have focused on\nadapting 2D segmentation models for domain shift to rendered images and\nsynthetic text descriptions, rather than optimizing the model specifically for\n3D segmentation. Our proposed task adaptation method finetunes a 2D bounding\nbox prediction model with an objective function for 3D segmentation. We\nintroduce weights for 2D bounding boxes for adaptive merging and learn the\nweights using a small additional neural network. Additionally, we incorporate\nSAM, a foreground segmentation model on a bounding box, to improve the\nboundaries of 2D segments and consequently those of 3D segmentation. Our\nexperiments on the PartNet-Mobility dataset show significant improvements with\nour task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p\nimprovement in mAP_50 for semantic and instance segmentation compared to the\nSotA few-shot 3D segmentation model.\n",
        "title": "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05907",
        "abstract_url": "http://arxiv.org/abs/2401.05907",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Kang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanjie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This article introduces a sliding window model for defocus deblurring that\nachieves the best performance to date with extremely low memory usage. Named\nSwintormer, the method utilizes a diffusion model to generate latent prior\nfeatures that assist in restoring more detailed images. It also extends the\nsliding window strategy to specialized Transformer blocks for efficient\ninference. Additionally, we have further optimized Multiply-Accumulate\noperations (Macs). Compared to the currently top-performing GRL method, our\nSwintormer model drastically reduces computational complexity from 140.35 GMACs\nto 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocus\ndeblurring from 27.04 dB to 27.07 dB. This new method allows for the processing\nof higher resolution images on devices with limited memory, significantly\nexpanding potential application scenarios. The article concludes with an\nablation study that provides an in-depth analysis of the impact of each network\nmodule on final performance. The source code and model will be available at the\nfollowing website: https://github.com/bnm6900030/swintormer.\n",
        "title": "Efficient Image Deblurring Networks based on Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05908",
        "abstract_url": "http://arxiv.org/abs/2401.05908",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Xuyang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qibin"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Toshihisa"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  With large training datasets and massive amounts of computing sources, large\nlanguage models (LLMs) achieve remarkable performance in comprehensive and\ngenerative ability. Based on those powerful LLMs, the model fine-tuned with\ndomain-specific datasets posseses more specialized knowledge and thus is more\npractical like medical LLMs. However, the existing fine-tuned medical LLMs are\nlimited to general medical knowledge with English language. For\ndisease-specific problems, the model's response is inaccurate and sometimes\neven completely irrelevant, especially when using a language other than\nEnglish. In this work, we focus on the particular disease of Epilepsy with\nJapanese language and introduce a customized LLM termed as EpilepsyLLM. Our\nmodel is trained from the pre-trained LLM by fine-tuning technique using\ndatasets from the epilepsy domain. The datasets contain knowledge of basic\ninformation about disease, common treatment methods and drugs, and important\nnotes in life and work. The experimental results demonstrate that EpilepsyLLM\ncan provide more reliable and specialized medical knowledge responses.\n",
        "title": "EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with\n  Epilepsy Medical Knowledge",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05909",
        "abstract_url": "http://arxiv.org/abs/2401.05909",
        "authors": [
            {
                "last_name": "Pavlichenko",
                "first_name": "Dmytro"
            },
            {
                "last_name": "Ficht",
                "first_name": "Grzegorz"
            },
            {
                "last_name": "Villar-Corrales",
                "first_name": "Angel"
            },
            {
                "last_name": "Denninger",
                "first_name": "Luis"
            },
            {
                "last_name": "Brocker",
                "first_name": "Julia"
            },
            {
                "last_name": "Sinen",
                "first_name": "Tim"
            },
            {
                "last_name": "Schreiber",
                "first_name": "Michael"
            },
            {
                "last_name": "Behnke",
                "first_name": "Sven"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The RoboCup Humanoid League holds annual soccer robot world championships\ntowards the long-term objective of winning against the FIFA world champions by\n2050. The participating teams continuously improve their systems. This paper\npresents the upgrades to our humanoid soccer system, leading our team NimbRo to\nwin the Soccer Tournament in the Humanoid AdultSize League at RoboCup 2023 in\nBordeaux, France. The mentioned upgrades consist of: an updated model\narchitecture for visual perception, extended fused angles feedback mechanisms\nand an additional COM-ZMP controller for walking robustness, and parametric\nin-walk kicks through waveforms.\n",
        "title": "RoboCup 2023 Humanoid AdultSize Winner NimbRo: NimbRoNet3 Visual\n  Perception and Responsive Gait with Waveform In-walk Kicks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05912",
        "abstract_url": "http://arxiv.org/abs/2401.05912",
        "authors": [
            {
                "last_name": "Santos",
                "first_name": "Wesley Ramos dos"
            },
            {
                "last_name": "Paraboni",
                "first_name": "Ivandre"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This article presents a method for prompt-based mental health screening from\na large and noisy dataset of social media text. Our method uses GPT 3.5.\nprompting to distinguish publications that may be more relevant to the task,\nand then uses a straightforward bag-of-words text classifier to predict actual\nuser labels. Results are found to be on pair with a BERT mixture of experts\nclassifier, and incurring only a fraction of its computational costs.\n",
        "title": "Prompt-based mental health screening from social media text",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05914",
        "abstract_url": "http://arxiv.org/abs/2401.05914",
        "authors": [
            {
                "last_name": "Elkins",
                "first_name": "Sabina"
            },
            {
                "last_name": "Kochmar",
                "first_name": "Ekaterina"
            },
            {
                "last_name": "Cheung",
                "first_name": "Jackie C. K."
            },
            {
                "last_name": "Serban",
                "first_name": "Iulian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Question generation (QG) is a natural language processing task with an\nabundance of potential benefits and use cases in the educational domain. In\norder for this potential to be realized, QG systems must be designed and\nvalidated with pedagogical needs in mind. However, little research has assessed\nor designed QG approaches with the input from real teachers or students. This\npaper applies a large language model-based QG approach where questions are\ngenerated with learning goals derived from Bloom's taxonomy. The automatically\ngenerated questions are used in multiple experiments designed to assess how\nteachers use them in practice. The results demonstrate that teachers prefer to\nwrite quizzes with automatically generated questions, and that such quizzes\nhave no loss in quality compared to handwritten versions. Further, several\nmetrics indicate that automatically generated questions can even improve the\nquality of the quizzes created, showing the promise for large scale use of QG\nin the classroom setting.\n",
        "title": "How Teachers Can Use Large Language Models and Bloom's Taxonomy to\n  Create Educational Quizzes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05916",
        "abstract_url": "http://arxiv.org/abs/2401.05916",
        "authors": [
            {
                "last_name": "Heikkinen",
                "first_name": "Mikko"
            },
            {
                "last_name": "Politis",
                "first_name": "Archontis"
            },
            {
                "last_name": "Virtanen",
                "first_name": "Tuomas"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "SD"
        ],
        "abstract": "  Ambisonics encoding of microphone array signals can enable various spatial\naudio applications, such as virtual reality or telepresence, but it is\ntypically designed for uniformly-spaced spherical microphone arrays. This paper\nproposes a method for Ambisonics encoding that uses a deep neural network (DNN)\nto estimate a signal transform from microphone inputs to Ambisonics signals.\nThe approach uses a DNN consisting of a U-Net structure with a learnable\npreprocessing as well as a loss function consisting of mean average error,\nspatial correlation, and energy preservation components. The method is\nvalidated on two microphone arrays with regular and irregular shapes having\nfour microphones, on simulated reverberant scenes with multiple sources. The\nresults of the validation show that the proposed method can meet or exceed the\nperformance of a conventional signal-independent Ambisonics encoder on a number\nof error metrics.\n",
        "title": "Neural Ambisonics encoding for compact irregular microphone arrays",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05918",
        "abstract_url": "http://arxiv.org/abs/2401.05918",
        "authors": [
            {
                "last_name": "Boll",
                "first_name": "Bastian"
            },
            {
                "last_name": "Cassel",
                "first_name": "Jonas"
            },
            {
                "last_name": "Albers",
                "first_name": "Peter"
            },
            {
                "last_name": "Petra",
                "first_name": "Stefania"
            },
            {
                "last_name": "Schn\u00f6rr",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "GT",
            ""
        ],
        "abstract": "  This paper studies a meta-simplex concept and geometric embedding framework\nfor multi-population replicator dynamics. Central results are two embedding\ntheorems which constitute a formal reduction of multi-population replicator\ndynamics to single-population ones. In conjunction with a robust mathematical\nformalism, this provides a toolset for analyzing complex multi-population\nmodels. Our framework provides a unifying perspective on different population\ndynamics in the literature which in particular enables to establish a formal\nlink between multi-population and multi-game dynamics.\n",
        "title": "A Geometric Embedding Approach to Multiple Games and Multiple\n  Populations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05925",
        "abstract_url": "http://arxiv.org/abs/2401.05925",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Bin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Ma",
                "first_name": "Yongjia"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaohui"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zejian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a\nmethod for compact 3D-consistent scene segmentation at fast rendering speed\nwith only RGB images input. Previous NeRF-based 3D segmentation methods have\nrelied on implicit or voxel neural scene representation and ray-marching volume\nrendering which are time consuming. Recent 3D Gaussian Splatting significantly\nimproves the rendering speed, however, existing Gaussians-based segmentation\nmethods(eg: Gaussian Grouping) fail to provide compact segmentation masks\nespecially in zero-shot segmentation, which is mainly caused by the lack of\nrobustness and compactness for straightforwardly assigning learnable parameters\nto each Gaussian when encountering inconsistent 2D machine-generated labels.\nOur method aims to achieve compact and reliable zero-shot scene segmentation\nswiftly by mapping fused spatial and semantically meaningful features for each\nGaussian point with a shallow decoding network. Specifically, our method\nfirstly optimizes Gaussian points' position, convariance and color attributes\nunder the supervision of RGB images. After Gaussian Locating, we distill\nmulti-scale DINO features extracted from images through unprojection to each\nGaussian, which is then incorporated with spatial features from the fast point\nfeatures processing network, i.e. RandLA-Net. Then the shallow decoding MLP is\napplied to the multi-scale fused features to obtain compact segmentation.\nExperimental results show that our model can perform high-quality zero-shot\nscene segmentation, as our model outperforms other segmentation methods on both\nsemantic and panoptic segmentation task, meanwhile consumes approximately only\n10% segmenting time compared to NeRF-based segmentation. Code and more results\nwill be available at https://David-Dou.github.io/CoSSegGaussians\n",
        "title": "CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05926",
        "abstract_url": "http://arxiv.org/abs/2401.05926",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Linghao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jingshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Chong"
            },
            {
                "last_name": "Liang",
                "first_name": "Peng"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  A commit message is a textual description of the code changes in a commit,\nwhich is a key part of the Git version control system (VCS). It captures the\nessence of software updating. Therefore, it can help developers understand code\nevolution and facilitate efficient collaboration between developers. However,\nit is time-consuming and labor-intensive to write good and valuable commit\nmessages. Some researchers have conducted extensive studies on the automatic\ngeneration of commit messages and proposed several methods for this purpose,\nsuch as generation-based and retrieval-based models. However, seldom studies\nexplored whether large language models (LLMs) can be effectively used for the\nautomatic generation of commit messages. To this end, this paper designed and\nconducted a series of experiments to comprehensively evaluate the performance\nof popular open-source and closed-source LLMs, i.e., Llama 2 and ChatGPT, in\ncommit message generation. The results indicate that considering the BLEU and\nRouge-L metrics, LLMs surpass existing methods in certain indicators but lag\nbehind in others. After human evaluations, however, LLMs show a distinct\nadvantage over all these existing methods. Especially, in 78% of the 366\nsamples, the commit messages generated by LLMs were evaluated by humans as the\nbest. This work not only reveals the promising potential of using LLMs to\ngenerate commit messages, but also explores the limitations of commonly used\nmetrics in evaluating the quality of automatically generated commit messages.\n",
        "title": "Using Large Language Models for Commit Message Generation: A Preliminary\n  Study",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05928",
        "abstract_url": "http://arxiv.org/abs/2401.05928",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiashuo"
            },
            {
                "last_name": "Xu",
                "first_name": "Chunpu"
            },
            {
                "last_name": "Leong",
                "first_name": "Chak Tou"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  An emotional support conversation system aims to alleviate users' emotional\ndistress and assist them in addressing their challenges. To generate supportive\nresponses, it is critical to consider multiple factors such as empathy, support\nstrategies, and response coherence, as established in prior methods.\nNonetheless, previous models occasionally generate unhelpful responses, which\nintend to provide support but display counterproductive effects. According to\npsychology and communication theories, poor performance in just one\ncontributing factor might cause a response to be unhelpful. From the model\ntraining perspective, since these models have not been exposed to unhelpful\nresponses during their training phase, they are unable to distinguish if the\ntokens they generate might result in unhelpful responses during inference. To\naddress this issue, we introduce a novel model-agnostic framework named\nmitigating unhelpfulness with multifaceted AI feedback for emotional support\n(Muffin). Specifically, Muffin employs a multifaceted AI feedback module to\nassess the helpfulness of responses generated by a specific model with\nconsideration of multiple factors. Using contrastive learning, it then reduces\nthe likelihood of the model generating unhelpful responses compared to the\nhelpful ones. Experimental results demonstrate that Muffin effectively\nmitigates the generation of unhelpful responses while slightly increasing\nresponse fluency and relevance.\n",
        "title": "Mitigating Unhelpfulness in Emotional Support Conversations with\n  Multifaceted AI Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05930",
        "abstract_url": "http://arxiv.org/abs/2401.05930",
        "authors": [
            {
                "last_name": "Kai",
                "first_name": "Jushi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Hu",
                "first_name": "Hai"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhouhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.\n",
        "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05932",
        "abstract_url": "http://arxiv.org/abs/2401.05932",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Langwen"
            },
            {
                "last_name": "Gianinazzi",
                "first_name": "Lukas"
            },
            {
                "last_name": "Yu",
                "first_name": "Yuejiang"
            },
            {
                "last_name": "Dueben",
                "first_name": "Peter D."
            },
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            ""
        ],
        "abstract": "  The generation of initial conditions via accurate data assimilation is\ncrucial for reliable weather forecasting and climate modeling. We propose the\nDiffDA as a machine learning based data assimilation method capable of\nassimilating atmospheric variables using predicted states and sparse\nobservations. We adapt the pretrained GraphCast weather forecast model as a\ndenoising diffusion model. Our method applies two-phase conditioning: on the\npredicted state during both training and inference, and on sparse observations\nduring inference only. As a byproduct, this strategy also enables the\npost-processing of predictions into the future, for which no observations are\navailable.Through experiments based on a reanalysis dataset, we have verified\nthat our method can produce assimilated global atmospheric data consistent with\nobservations at 0.25degree resolution. The experiments also show that the\ninitial conditions that are generated via our approach can be used for forecast\nmodels with a loss of lead time of at most 24 hours when compared to initial\nconditions of state-of-the-art data assimilation suites. This enables to apply\nthe method to real world applications such as the creation of reanalysis\ndatasets with autoregressive data assimilation.\n",
        "title": "DiffDA: a diffusion model for weather-scale data assimilation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05933",
        "abstract_url": "http://arxiv.org/abs/2401.05933",
        "authors": [
            {
                "last_name": "Aribe Jr.",
                "first_name": "Sales G."
            },
            {
                "last_name": "Gerardo",
                "first_name": "Bobby D."
            },
            {
                "last_name": "Medina",
                "first_name": "Ruji P."
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS\nepidemic in the Philippines is the one that is spreading the quickest in the\nwestern Pacific. Although the full effects of COVID-19 on HIV services and\ndevelopment are still unknown, it is predicted that such disruptions could lead\nto a significant increase in HIV casualties. Therefore, the nation needs some\nmodeling and forecasting techniques to foresee the spread pattern and enhance\nthe governments prevention, treatment, testing, and care program. In this\nstudy, the researcher uses Multilayer Perceptron Neural Network to forecast\ntime series during the period when the COVID-19 pandemic strikes the nation,\nusing statistics taken from the HIV/AIDS and ART Registry of the Philippines.\nAfter training, validation, and testing of data, the study finds that the\npredicted cumulative cases in the nation by 2030 will reach 145,273.\nAdditionally, there is very little difference between observed and anticipated\nHIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well\nas a greater coefficient of determination. Further research revealed that the\nPhilippines seems far from achieving Sustainable Development Goal 3 of Project\n2030 due to an increase in the nations rate of new HIV infections. Despite the\ndetrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the\nPhilippine government, under the Marcos administration, must continue to adhere\nto the United Nations 90-90-90 targets by enhancing its ART program and\nensuring that all vital health services are readily accessible and available.\n",
        "title": "Time Series Forecasting of HIV/AIDS in the Philippines Using Deep\n  Learning: Does COVID-19 Epidemic Matter?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05938",
        "abstract_url": "http://arxiv.org/abs/2401.05938",
        "authors": [
            {
                "last_name": "Picasarri-Arrieta",
                "first_name": "Lucas"
            },
            {
                "last_name": "Rambaud",
                "first_name": "Cl\u00e9ment"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "DM"
        ],
        "abstract": "  Aboulker et al. proved that a digraph with large enough dichromatic number\ncontains any fixed digraph as a subdivision. The dichromatic number of a\ndigraph is the smallest order of a partition of its vertex set into acyclic\ninduced subdigraphs. A digraph is dicritical if the removal of any arc or\nvertex decreases its dichromatic number. In this paper we give sufficient\nconditions on a dicritical digraph of large order or large directed girth to\ncontain a given digraph as a subdivision. In particular, we prove that (i) for\nevery integers $k,\\ell$, large enough dicritical digraphs with dichromatic\nnumber $k$ contain an orientation of a cycle with at least $\\ell$ vertices;\n(ii) there are functions $f,g$ such that for every subdivision $F^*$ of a\ndigraph $F$, digraphs with directed girth at least $f(F^*)$ and dichromatic\nnumber at least $g(F)$ contain a subdivision of $F^*$, and if $F$ is a tree,\nthen $g(F)=|V(F)|$; (iii) there is a function $f$ such that for every\nsubdivision $F^*$ of $TT_3$ (the transitive tournament on three vertices),\ndigraphs with directed girth at least $f(F^*)$ and minimum out-degree at least\n$2$ contain $F^*$ as a subdivision.\n",
        "title": "Subdivisions in dicritical digraphs with large order or digirth",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05939",
        "abstract_url": "http://arxiv.org/abs/2401.05939",
        "authors": [
            {
                "last_name": "Chatterjee",
                "first_name": "Shubham"
            },
            {
                "last_name": "Mackie",
                "first_name": "Iain"
            },
            {
                "last_name": "Dalton",
                "first_name": "Jeff"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            ""
        ],
        "abstract": "  While entity-oriented neural IR models have advanced significantly, they\noften overlook a key nuance: the varying degrees of influence individual\nentities within a document have on its overall relevance. Addressing this gap,\nwe present DREQ, an entity-oriented dense document re-ranking model. Uniquely,\nwe emphasize the query-relevant entities within a document's representation\nwhile simultaneously attenuating the less relevant ones, thus obtaining a\nquery-specific entity-centric document representation. We then combine this\nentity-centric document representation with the text-centric representation of\nthe document to obtain a \"hybrid\" representation of the document. We learn a\nrelevance score for the document using this hybrid representation. Using four\nlarge-scale benchmarks, we show that DREQ outperforms state-of-the-art neural\nand non-neural re-ranking methods, highlighting the effectiveness of our\nentity-oriented representation approach.\n",
        "title": "DREQ: Document Re-Ranking Using Entity-based Query Understanding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05940",
        "abstract_url": "http://arxiv.org/abs/2401.05940",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Shin",
                "first_name": "Donghwan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in processing\nboth natural and programming languages, which have enabled various applications\nin software engineering, such as requirement engineering, code generation, and\nsoftware testing. However, existing code generation benchmarks do not\nnecessarily assess the code understanding performance of LLMs, especially for\nthe subtle inconsistencies that may arise between code and its semantics\ndescribed in natural language.\n  In this paper, we propose a novel method to systematically assess the code\nunderstanding performance of LLMs, particularly focusing on subtle differences\nbetween code and its descriptions, by introducing code mutations to existing\ncode generation datasets. Code mutations are small changes that alter the\nsemantics of the original code, creating a mismatch with the natural language\ndescription. We apply different types of code mutations, such as operator\nreplacement and statement deletion, to generate inconsistent code-description\npairs. We then use these pairs to test the ability of LLMs to correctly detect\nthe inconsistencies.\n  We propose a new LLM testing method, called Mutation-based Consistency\nTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and\nGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which\nconsists of six programming languages (Python, C++, Java, Go, JavaScript, and\nRust). We compare the performance of the LLMs across different types of code\nmutations and programming languages and analyze the results. We find that the\nLLMs show significant variation in their code understanding performance and\nthat they have different strengths and weaknesses depending on the mutation\ntype and language.\n",
        "title": "Mutation-based Consistency Testing for Evaluating the Code Understanding\n  Capability of LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05943",
        "abstract_url": "http://arxiv.org/abs/2401.05943",
        "authors": [
            {
                "last_name": "Harnes",
                "first_name": "H\u00e5kon"
            },
            {
                "last_name": "Morrison",
                "first_name": "Donn"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  WebAssembly is a low-level bytecode language that allows high-level languages\nlike C, C++, and Rust to be executed in the browser at near-native performance.\nIn recent years, WebAssembly has gained widespread adoption is now natively\nsupported by all modern browsers. However, vulnerabilities in memory-unsafe\nlanguages, like C and C++, can translate into vulnerabilities in WebAssembly\nbinaries. Unfortunately, most WebAssembly binaries are compiled from such\nmemory-unsafe languages, and these vulnerabilities have been shown to be\npractical in real-world scenarios. WebAssembly smart contracts have also been\nfound to be vulnerable, causing significant financial loss. Additionally,\nWebAssembly has been used for malicious purposes like cryptojacking. To address\nthese issues, several analysis techniques for WebAssembly binaries have been\nproposed. In this paper, we conduct a comprehensive literature review of these\ntechniques and categorize them based on their analysis strategy and objectives.\nFurthermore, we compare and evaluate the techniques using quantitative data,\nhighlighting their strengths and weaknesses. In addition, one of the main\ncontributions of this paper is the identification of future research directions\nbased on the thorough literature review conducted.\n",
        "title": "SoK: Analysis techniques for WebAssembly",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05946",
        "abstract_url": "http://arxiv.org/abs/2401.05946",
        "authors": [
            {
                "last_name": "Dedieu",
                "first_name": "Antoine"
            },
            {
                "last_name": "Lehrach",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guangyao"
            },
            {
                "last_name": "George",
                "first_name": "Dileep"
            },
            {
                "last_name": "L\u00e1zaro-Gredilla",
                "first_name": "Miguel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  Despite their stellar performance on a wide range of tasks, including\nin-context tasks only revealed during inference, vanilla transformers and\nvariants trained for next-token predictions (a) do not learn an explicit world\nmodel of their environment which can be flexibly queried and (b) cannot be used\nfor planning or navigation. In this paper, we consider partially observed\nenvironments (POEs), where an agent receives perceptually aliased observations\nas it navigates, which makes path planning hard. We introduce a transformer\nwith (multiple) discrete bottleneck(s), TDB, whose latent codes learn a\ncompressed representation of the history of observations and actions. After\ntraining a TDB to predict the future observation(s) given the history, we\nextract interpretable cognitive maps of the environment from its active\nbottleneck(s) indices. These maps are then paired with an external solver to\nsolve (constrained) path planning problems. First, we show that a TDB trained\non POEs (a) retains the near perfect predictive performance of a vanilla\ntransformer or an LSTM while (b) solving shortest path problems exponentially\nfaster. Second, a TDB extracts interpretable representations from text\ndatasets, while reaching higher in-context accuracy than vanilla sequence\nmodels. Finally, in new POEs, a TDB (a) reaches near-perfect in-context\naccuracy, (b) learns accurate in-context cognitive maps (c) solves in-context\npath planning problems.\n",
        "title": "Learning Cognitive Maps from Transformer Representations for Efficient\n  Planning in Partially Observed Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05947",
        "abstract_url": "http://arxiv.org/abs/2401.05947",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhuolun"
            },
            {
                "last_name": "Majumdar",
                "first_name": "Srijoni"
            },
            {
                "last_name": "Pournaras",
                "first_name": "Evangelos"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Conditional Information Reveal (CIR) automates the release of information\nupon meeting specific pre-defined conditions, such as time or location. This\npaper advances the understanding and implementation of CIR by introducing a new\nparadigm to highlight the security challenges in CIR design, and proposes a\ndecentralized architecture as a design guideline for secure CIR systems.\nFurthermore, in the context of time-sensitive data sharing, this paper proposes\na practical timed-release cryptography system employing the proposed\narchitecture and a novel verifiable secret sharing scheme. Key achievements of\nthis study include the creation of an open-source prototype for practical\ndeployment and a comprehensive system evaluation that highlights the enhanced\nsecurity and efficiency of the proposed system. Furthermore, the paper delves\ninto the application of this system in E-voting scenarios, illustrating its\ncapacity to secure and ensure fair electronic voting processes.\n",
        "title": "Blockchain-based Decentralized Time Lock Machines: Automated Reveal of\n  Time-sensitive Information",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05949",
        "abstract_url": "http://arxiv.org/abs/2401.05949",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Shuai"
            },
            {
                "last_name": "Jia",
                "first_name": "Meihuizi"
            },
            {
                "last_name": "Tuan",
                "first_name": "Luu Anh"
            },
            {
                "last_name": "Wen",
                "first_name": "Jinming"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Unlike traditional fine-tuning methods, in-context learning\nadapts pre-trained models to unseen tasks without updating any parameters.\nDespite being widely applied, in-context learning is vulnerable to malicious\nattacks. In this work, we raise security concerns regarding this paradigm. Our\nstudies demonstrate that an attacker can manipulate the behavior of large\nlanguage models by poisoning the demonstration context, without the need for\nfine-tuning the model. Specifically, we have designed a new backdoor attack\nmethod, named ICLAttack, to target large language models based on in-context\nlearning. Our method encompasses two types of attacks: poisoning demonstration\nexamples and poisoning prompts, which can make models behave in accordance with\npredefined intentions. ICLAttack does not require additional fine-tuning to\nimplant a backdoor, thus preserving the model's generality. Furthermore, the\npoisoned examples are correctly labeled, enhancing the natural stealth of our\nattack method. Extensive experimental results across several language models,\nranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of\nour attack method, exemplified by a high average attack success rate of 95.0%\nacross the three datasets on OPT models. Our findings highlight the\nvulnerabilities of language models, and we hope this work will raise awareness\nof the possible security threats associated with in-context learning.\n",
        "title": "Universal Vulnerabilities in Large Language Models: In-context Learning\n  Backdoor Attacks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05950",
        "abstract_url": "http://arxiv.org/abs/2401.05950",
        "authors": [
            {
                "last_name": "Trombini",
                "first_name": "Sofia"
            },
            {
                "last_name": "Pasta",
                "first_name": "Edoardo"
            },
            {
                "last_name": "Fagiano",
                "first_name": "Lorenzo"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  This study investigates deep offshore, pumping Airborne Wind Energy systems,\nfocusing on the kite-platform interaction. The considered system includes a 360\nm2 soft-wing kite, connected by a tether to a winch installed on a\n10-meter-deep spar with four mooring lines. Wind power is converted into\nelectricity with a feedback controlled periodic trajectory of the kite and\ncorresponding reeling motion of the tether. An analysis of the mutual influence\nbetween the platform and the kite dynamics, with different wave regimes,\nreveals a rather small sensitivity of the flight pattern to the platform\noscillations; on the other hand, the frequency of tether force oscillations can\nbe close to the platform resonance peaks, resulting in possible increased\nfatigue loads and damage of the floating and submerged components. A control\ndesign procedure is then proposed to avoid this problem, acting on the kite\npath planner. Simulation results confirm the effectiveness of the approach.\n",
        "title": "On the Kite-Platform Interactions in Offshore Airborne Wind Energy\n  Systems: Frequency Analysis and Control Approach",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05952",
        "abstract_url": "http://arxiv.org/abs/2401.05952",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Chujie"
            },
            {
                "last_name": "Chen",
                "first_name": "Dongping"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qihui"
            },
            {
                "last_name": "Huang",
                "first_name": "Yue"
            },
            {
                "last_name": "Wan",
                "first_name": "Yao"
            },
            {
                "last_name": "Sun",
                "first_name": "Lichao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  With the remarkable development and widespread applications of large language\nmodels (LLMs), the use of machine-generated text (MGT) is becoming increasingly\ncommon. This trend brings potential risks, particularly to the quality and\ncompleteness of information in fields such as news and education. Current\nresearch predominantly addresses the detection of pure MGT without adequately\naddressing mixed scenarios including AI-revised Human-Written Text (HWT) or\nhuman-revised MGT. To confront this challenge, we introduce mixcase, a novel\nconcept representing a hybrid text form involving both machine-generated and\nhuman-generated content. We collected mixcase instances generated from multiple\ndaily text-editing scenarios and composed MixSet, the first dataset dedicated\nto studying these mixed modification scenarios. We conduct experiments to\nevaluate the efficacy of popular MGT detectors, assessing their effectiveness,\nrobustness, and generalization performance. Our findings reveal that existing\ndetectors struggle to identify mixcase as a separate class or MGT, particularly\nin dealing with subtle modifications and style adaptability. This research\nunderscores the urgent need for more fine-grain detectors tailored for mixcase,\noffering valuable insights for future research. Code and Models are available\nat https://github.com/Dongping-Chen/MixSet.\n",
        "title": "LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05956",
        "abstract_url": "http://arxiv.org/abs/2401.05956",
        "authors": [
            {
                "last_name": "Rohwedder",
                "first_name": "Lars"
            },
            {
                "last_name": "Safari",
                "first_name": "Ashkan"
            },
            {
                "last_name": "Vredeveld",
                "first_name": "Tjark"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Local search is a widely used technique for tackling challenging optimization\nproblems, offering significant advantages in terms of computational efficiency\nand exhibiting strong empirical behavior across a wide range of problem\ndomains. In this paper, we address a scheduling problem on two identical\nparallel machines with the objective of \\emph{makespan minimization}. For this\nproblem, we consider a local search neighborhood, called \\emph{$k$-swap}, which\nis a more generalized version of the widely-used \\emph{swap} and \\emph{jump}\nneighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$\njobs between two machines in our schedule. First, we propose an algorithm for\nfinding an improving neighbor in the $k$-swap neighborhood which is faster than\nthe naive approach, and prove an almost matching lower bound on any such an\nalgorithm. Then, we analyze the number of local search steps required to\nconverge to a local optimum with respect to the $k$-swap neighborhood. For the\ncase $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper\nbound on the number of local search steps, and for the case $k = 3$, we provide\nan exponential lower bound. Finally, we conduct computational experiments on\nvarious families of instances, and we discuss extensions to more than two\nmachines in our schedule.\n",
        "title": "A k-swap Local Search for Makespan Scheduling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05960",
        "abstract_url": "http://arxiv.org/abs/2401.05960",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xijun"
            },
            {
                "last_name": "Zhu",
                "first_name": "Fangzhou"
            },
            {
                "last_name": "Zhen",
                "first_name": "Hui-Ling"
            },
            {
                "last_name": "Luo",
                "first_name": "Weilin"
            },
            {
                "last_name": "Lu",
                "first_name": "Meng"
            },
            {
                "last_name": "Huang",
                "first_name": "Yimin"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhenan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zirui"
            },
            {
                "last_name": "Kuang",
                "first_name": "Yufei"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhihai"
            },
            {
                "last_name": "Geng",
                "first_name": "Zijie"
            },
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Liu",
                "first_name": "Haoyang"
            },
            {
                "last_name": "An",
                "first_name": "Zhiwu"
            },
            {
                "last_name": "Yang",
                "first_name": "Muming"
            },
            {
                "last_name": "Li",
                "first_name": "Jianshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Jie"
            },
            {
                "last_name": "Yan",
                "first_name": "Junchi"
            },
            {
                "last_name": "Sun",
                "first_name": "Defeng"
            },
            {
                "last_name": "Zhong",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yong"
            },
            {
                "last_name": "Zeng",
                "first_name": "Jia"
            },
            {
                "last_name": "Yuan",
                "first_name": "Mingxuan"
            },
            {
                "last_name": "Hao",
                "first_name": "Jianye"
            },
            {
                "last_name": "Yao",
                "first_name": "Jun"
            },
            {
                "last_name": "Mao",
                "first_name": "Kun"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In an era of digital ubiquity, efficient resource management and\ndecision-making are paramount across numerous industries. To this end, we\npresent a comprehensive study on the integration of machine learning (ML)\ntechniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the\nscarcity of real-world mathematical programming instances, and to surpass the\ncapabilities of traditional optimization techniques. We showcase our methods\nfor generating complex SAT and MILP instances utilizing generative models that\nmirror multifaceted structures of real-world problem. Furthermore, we introduce\na training framework leveraging augmentation policies to maintain solvers'\nutility in dynamic environments. Besides the data generation and augmentation,\nour proposed approaches also include novel ML-driven policies for personalized\nsolver strategies, with an emphasis on applications like graph convolutional\nnetworks for initial basis selection and reinforcement learning for advanced\npresolving and cut selection. Additionally, we detail the incorporation of\nstate-of-the-art parameter tuning algorithms which markedly elevate solver\nperformance. Compared with traditional solvers such as Gurobi and SCIP, our\nML-augmented OptVerse AI Solver demonstrates superior speed and precision\nacross both established benchmarks and real-world scenarios, reinforcing the\npractical imperative and effectiveness of machine learning techniques in\nmathematical programming solvers.\n",
        "title": "Machine Learning Insides OptVerse AI Solver: Design Principles and\n  Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05961",
        "abstract_url": "http://arxiv.org/abs/2401.05961",
        "authors": [
            {
                "last_name": "Cesarano",
                "first_name": "Carmine"
            },
            {
                "last_name": "Natella",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Application Layer Gateways (ALGs) play a crucial role in securing critical\nsystems, including railways, industrial automation, and defense applications,\nby segmenting networks at different levels of criticality. However, they\nrequire rigorous security testing to prevent software vulnerabilities, not only\nat the network level but also at the application layer (e.g., deep traffic\ninspection components). This paper presents a vulnerability-driven methodology\nfor the comprehensive security testing of ALGs. We present the methodology in\nthe context of an industrial case study in the railways domain, and a\nsimulation-based testing environment to support the methodology.\n",
        "title": "Securing an Application Layer Gateway: An Industrial Case Study",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05963",
        "abstract_url": "http://arxiv.org/abs/2401.05963",
        "authors": [
            {
                "last_name": "L\u00f3pez-Ure\u00f1a",
                "first_name": "Sergio"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In this paper, we introduce a novel non-linear uniform subdivision scheme for\nthe generation of curves in $\\mathbb{R}^n$, $n\\geq2$. This scheme is\ndistinguished by its capacity to reproduce second-degree polynomial data on\nnon-uniform grids without necessitating prior knowledge of the grid\nspecificities. Our approach exploits the potential of annihilation operators to\ninfer the underlying grid, thereby obviating the need for end-users to specify\nsuch information. We define the scheme in a non-stationary manner, ensuring\nthat it progressively approaches a classical linear scheme as the iteration\nnumber increases, all while preserving its polynomial reproduction capability.\n  The convergence is established through two distinct theoretical methods.\nFirstly, we propose a new class of schemes, including ours, for which we\nestablish $\\mathcal{C}^1$ convergence by combining results from the analysis of\nquasilinear schemes and asymptotically equivalent linear non-uniform\nnon-stationary schemes. Secondly, we adapt conventional analytical tools for\nnon-linear schemes to the non-stationary case, allowing us to again conclude\nthe convergence of the proposed class of schemes.\n  We show its practical usefulness through numerical examples, showing that the\ngenerated curves are curvature continuous.\n",
        "title": "A uniform non-linear subdivision scheme reproducing polynomials at any\n  non-uniform grid",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05964",
        "abstract_url": "http://arxiv.org/abs/2401.05964",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hongjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Try to generate new bridge types using generative artificial intelligence\ntechnology. Using symmetric structured image dataset of three-span beam bridge,\narch bridge, cable-stayed bridge and suspension bridge , based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nPixelCNN is constructed and trained. The model can capture the statistical\nstructure of the images and calculate the probability distribution of the next\npixel when the previous pixels are given. From the obtained latent space\nsampling, new bridge types different from the training dataset can be\ngenerated. PixelCNN can organically combine different structural components on\nthe basis of human original bridge types, creating new bridge types that have a\ncertain degree of human original ability. Autoregressive models cannot\nunderstand the meaning of the sequence, while multimodal models combine\nregression and autoregressive models to understand the sequence. Multimodal\nmodels should be the way to achieve artificial general intelligence in the\nfuture.\n",
        "title": "An attempt to generate new bridge types from latent space of PixelCNN",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05965",
        "abstract_url": "http://arxiv.org/abs/2401.05965",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shiwei"
            },
            {
                "last_name": "Diao",
                "first_name": "Lansong"
            },
            {
                "last_name": "Wu",
                "first_name": "Chuan"
            },
            {
                "last_name": "Cao",
                "first_name": "Zongyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyu"
            },
            {
                "last_name": "Lin",
                "first_name": "Wei"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to\ntrain large deep neural networks (DNNs). Few studies have explored its\napplicability on heterogeneous clusters, to fully exploit available resources\nfor large model learning. This paper presents \\OurSystem, an automated system\ndesigned to expedite SPMD DNN training on heterogeneous clusters. \\OurSystem\njointly optimizes the tensor sharding strategy, sharding ratios across\nheterogeneous devices and the communication methods for tensor exchanges for\noptimized distributed training with SPMD parallelism. We novelly formulate\nmodel partitioning as a program synthesis problem, in which we generate a\ndistributed program from scratch on a distributed instruction set that\nsemantically resembles the program designed for a single device, and\nsystematically explore the solution space with an A*-based search algorithm. We\nderive the optimal tensor sharding ratios by formulating it as a linear\nprogramming problem. Additionally, \\OurSystem explores tensor communication\noptimization in a heterogeneous cluster and integrates it as part of the\nprogram synthesis process, for automatically choosing optimal collective\ncommunication primitives and applying sufficient factor broadcasting technique.\nExtensive experiments on representative workloads demonstrate that \\OurSystem\nachieves up to 2.41x speed-up on heterogeneous clusters.\n",
        "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated\n  Program Synthesis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05967",
        "abstract_url": "http://arxiv.org/abs/2401.05967",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Yihua"
            },
            {
                "last_name": "Shimodaira",
                "first_name": "Hidetoshi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n",
        "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05968",
        "abstract_url": "http://arxiv.org/abs/2401.05968",
        "authors": [
            {
                "last_name": "Chaudhuri",
                "first_name": "Yashwardhan"
            },
            {
                "last_name": "Kumar",
                "first_name": "Ankit"
            },
            {
                "last_name": "Phukan",
                "first_name": "Orchid Chetia"
            },
            {
                "last_name": "Buduru",
                "first_name": "Arun Balaji"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Crowd counting finds direct applications in real-world situations, making\ncomputational efficiency and performance crucial. However, most of the previous\nmethods rely on a heavy backbone and a complex downstream architecture that\nrestricts the deployment. To address this challenge and enhance the versatility\nof crowd-counting models, we introduce two lightweight models. These models\nmaintain the same downstream architecture while incorporating two distinct\nbackbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to\nextract diverse scale features from a Pre-Trained Model (PTM) and subsequently\ncombine these features seamlessly. This approach empowers our models to achieve\nimproved performance while maintaining a compact and efficient design. With the\ncomparison of our proposed models with previously available state-of-the-art\n(SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it\nachieves comparable results while being the most computationally efficient\nmodel. Finally, we present a comparative study, an extensive ablation study,\nalong with pruning to show the effectiveness of our models.\n",
        "title": "A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd\n  Counting",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05969",
        "abstract_url": "http://arxiv.org/abs/2401.05969",
        "authors": [
            {
                "last_name": "Strau\u00df",
                "first_name": "Niklas"
            },
            {
                "last_name": "Schubert",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  The traveling officer problem (TOP) is a challenging stochastic optimization\ntask. In this problem, a parking officer is guided through a city equipped with\nparking sensors to fine as many parking offenders as possible. A major\nchallenge in TOP is the dynamic nature of parking offenses, which randomly\nappear and disappear after some time, regardless of whether they have been\nfined. Thus, solutions need to dynamically adjust to currently fineable parking\noffenses while also planning ahead to increase the likelihood that the officer\narrives during the offense taking place. Though various solutions exist, these\nmethods often struggle to take the implications of actions on the ability to\nfine future parking violations into account. This paper proposes SATOP, a novel\nspatial-aware deep reinforcement learning approach for TOP. Our novel state\nencoder creates a representation of each action, leveraging the spatial\nrelationships between parking spots, the agent, and the action. Furthermore, we\npropose a novel message-passing module for learning future inter-action\ncorrelations in the given environment. Thus, the agent can estimate the\npotential to fine further parking violations after executing an action. We\nevaluate our method using an environment based on real-world data from\nMelbourne. Our results show that SATOP consistently outperforms\nstate-of-the-art TOP agents and is able to fine up to 22% more parking\noffenses.\n",
        "title": "Spatial-Aware Deep Reinforcement Learning for the Traveling Officer\n  Problem",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05971",
        "abstract_url": "http://arxiv.org/abs/2401.05971",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Rouwan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xiaoya"
            },
            {
                "last_name": "Zhu",
                "first_name": "Juelin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xuxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Maojun"
            },
            {
                "last_name": "Yan",
                "first_name": "Shen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite significant progress in global localization of Unmanned Aerial\nVehicles (UAVs) in GPS-denied environments, existing methods remain constrained\nby the availability of datasets. Current datasets often focus on small-scale\nscenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV\nbuild-in sensor data. To address these limitations, we introduce a large-scale\n6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF\nlocalization pipeline (UAVLoc), which consists of offline synthetic data\ngeneration and online visual localization. Additionally, based on the 6-DoF\nestimator, we design a hierarchical system for tracking ground target in 3D\nspace. Experimental results on the new dataset demonstrate the effectiveness of\nthe proposed approach. Code and dataset are available at\nhttps://github.com/RingoWRW/UAVD4L\n",
        "title": "UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05972",
        "abstract_url": "http://arxiv.org/abs/2401.05972",
        "authors": [
            {
                "last_name": "Gahr",
                "first_name": "Constatin"
            },
            {
                "last_name": "Farcas",
                "first_name": "Ionut-Gabriel"
            },
            {
                "last_name": "Jenko",
                "first_name": "Frank"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CE",
            "LG",
            ""
        ],
        "abstract": "  This paper focuses on the construction of non-intrusive Scientific Machine\nLearning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma\nturbulence simulations. In particular, we propose using Operator Inference\n(OpInf) to build low-cost physics-based ROMs from data for such simulations. As\na representative example, we focus on the Hasegawa-Wakatani (HW) equations used\nfor modeling two-dimensional electrostatic drift-wave plasma turbulence. For a\ncomprehensive perspective of the potential of OpInf to construct accurate ROMs\nfor this model, we consider a setup for the HW equations that leads to the\nformation of complex, nonlinear, and self-driven dynamics, and perform two sets\nof experiments. We first use the data obtained via a direct numerical\nsimulation of the HW equations starting from a specific initial condition and\ntrain OpInf ROMs for predictions beyond the training time horizon. In the\nsecond, more challenging set of experiments, we train ROMs using the same\ndataset as before but this time perform predictions for six other initial\nconditions. Our results show that the OpInf ROMs capture the important features\nof the turbulent dynamics and generalize to new and unseen initial conditions\nwhile reducing the evaluation time of the high-fidelity model by up to five\norders of magnitude in single-core performance. In the broader context of\nfusion research, this shows that non-intrusive SciML ROMs have the potential to\ndrastically accelerate numerical studies, which can ultimately enable tasks\nsuch as the design and real-time control of optimized fusion devices.\n",
        "title": "Learning physics-based reduced models from data for the\n  Hasegawa-Wakatani equations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05975",
        "abstract_url": "http://arxiv.org/abs/2401.05975",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shihao"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Ma",
                "first_name": "Yingwei"
            },
            {
                "last_name": "Ma",
                "first_name": "Jian"
            },
            {
                "last_name": "Zhong",
                "first_name": "Wenliang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guannan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kejun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xinwang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            ""
        ],
        "abstract": "  Mining users' intents plays a crucial role in sequential recommendation. The\nrecent approach, ICLRec, was introduced to extract underlying users' intents\nusing contrastive learning and clustering. While it has shown effectiveness,\nthe existing method suffers from complex and cumbersome alternating\noptimization, leading to two main issues. Firstly, the separation of\nrepresentation learning and clustering optimization within a generalized\nexpectation maximization (EM) framework often results in sub-optimal\nperformance. Secondly, performing clustering on the entire dataset hampers\nscalability for large-scale industry data. To address these challenges, we\npropose a novel intent learning method called \\underline{ELCRec}, which\nintegrates representation learning into an \\underline{E}nd-to-end\n\\underline{L}earnable \\underline{C}lustering framework for\n\\underline{Rec}ommendation. Specifically, we encode users' behavior sequences\nand initialize the cluster centers as learnable network parameters.\nAdditionally, we design a clustering loss that guides the networks to\ndifferentiate between different cluster centers and pull similar samples\ntowards their respective cluster centers. This allows simultaneous optimization\nof recommendation and clustering using mini-batch data. Moreover, we leverage\nthe learned cluster centers as self-supervision signals for representation\nlearning, resulting in further enhancement of recommendation performance.\nExtensive experiments conducted on open benchmarks and industry data validate\nthe superiority, effectiveness, and efficiency of our proposed ELCRec method.\nCode is available at: https://github.com/yueliu1999/ELCRec.\n",
        "title": "End-to-end Learnable Clustering for Intent Learning in Recommendation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05982",
        "abstract_url": "http://arxiv.org/abs/2401.05982",
        "authors": [
            {
                "last_name": "Zakrisson",
                "first_name": "Henning"
            },
            {
                "last_name": "Lindholm",
                "first_name": "Mathias"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  The paper introduces a tree-based varying coefficient model (VCM) where the\nvarying coefficients are modelled using the cyclic gradient boosting machine\n(CGBM) from Delong et al. (2023). Modelling the coefficient functions using a\nCGBM allows for dimension-wise early stopping and feature importance scores.\nThe dimension-wise early stopping not only reduces the risk of\ndimension-specific overfitting, but also reveals differences in model\ncomplexity across dimensions. The use of feature importance scores allows for\nsimple feature selection and easy model interpretation. The model is evaluated\non the same simulated and real data examples as those used in Richman and\nW\\\"uthrich (2023), and the results show that it produces results in terms of\nout of sample loss that are comparable to those of their neural network-based\nVCM called LocalGLMnet.\n",
        "title": "A tree-based varying coefficient model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05984",
        "abstract_url": "http://arxiv.org/abs/2401.05984",
        "authors": [
            {
                "last_name": "Tong",
                "first_name": "Hua"
            },
            {
                "last_name": "Halilaj",
                "first_name": "Eni"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongjie Jessica"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            ""
        ],
        "abstract": "  We present a new software package \"HybridOctree_Hex\" for adaptive\nall-hexahedral mesh generation based on hybrid octree and quality improvement\nwith Jacobian control. The proposed HybridOctree_Hex begins by detecting\ncurvatures and narrow regions of the input boundary to identify key surface\nfeatures and initialize an octree structure. Subsequently, a strongly balanced\noctree is constructed using the balancing and pairing rules. Inspired by our\nearlier preliminary hybrid octree-based work, templates are designed to\nguarantee an all-hexahedral dual mesh generation directly from the strongly\nbalanced octree. With these pre-defined templates, the sophisticated hybrid\noctree construction step is skipped to achieve an efficient implementation.\nAfter that, elements outside and around the boundary are removed to create a\ncore mesh. The boundary points of the core mesh are connected to their\ncorresponding closest points on the surface to fill the buffer zone and build\nthe final mesh. Coupled with smart Laplacian smoothing, HybridOctree_Hex takes\nadvantage of a delicate optimization-based quality improvement method\nconsidering geometric fitting, Jacobian and scaled Jacobian to achieve the\nminimum scaled Jacobian higher than $0.5$. We empirically verify the robustness\nand efficiency of our method by running the HybridOctree_Hex software on dozens\nof complex 3D models without any manual intervention or parameter adjustment.\nWe provide the HybridOctree_Hex source codes, comprehensive results,\nencompassing mesh input/output files and statistical data presented at\nhttps://github.com/CMU-CBML/HybridOctree_Hex.\n",
        "title": "HybridOctree_Hex: Hybrid Octree-Based Adaptive All-Hexahedral Mesh\n  Generation with Jacobian Control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05986",
        "abstract_url": "http://arxiv.org/abs/2401.05986",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Yifan"
            },
            {
                "last_name": "Chai",
                "first_name": "Bingxu"
            },
            {
                "last_name": "Yu",
                "first_name": "Siyu"
            },
            {
                "last_name": "Li",
                "first_name": "Ying"
            },
            {
                "last_name": "He",
                "first_name": "Pinjia"
            },
            {
                "last_name": "Jiang",
                "first_name": "Wei"
            },
            {
                "last_name": "Li",
                "first_name": "Jianguo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Due to the sheer size of software logs, developers rely on automated log\nanalysis. Log parsing, which parses semi-structured logs into a structured\nformat, is a prerequisite of automated log analysis. However, existing log\nparsers are unsatisfactory when applied in practice because: 1) they ignore\ncategories of variables, and 2) have poor generalization ability. To address\nthe limitations of existing approaches, we propose LogPTR, the first end-to-end\nvariable-aware log parser that can extract the static and dynamic parts in\nlogs, and further identify the categories of variables. The key of LogPTR is\nusing pointer network to copy words from the log message. We have performed\nextensive experiments on 16 public log datasets and the results show that\nLogPTR outperforms state-of-the-art log parsers both on general log parsing\nthat extracts the log template and variable-aware log parsing that further\nidentifies the category of variables.\n",
        "title": "LogPTR: Variable-Aware Log Parsing with Pointer Network",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05987",
        "abstract_url": "http://arxiv.org/abs/2401.05987",
        "authors": [
            {
                "last_name": "von Gladiss",
                "first_name": "Anselm"
            },
            {
                "last_name": "Ahmadian",
                "first_name": "Amir Shayan"
            },
            {
                "last_name": "J\u00fcrjens",
                "first_name": "Jan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Magnetic particle imaging (MPI) is an emerging medical imaging modality which\noffers a unique combination of high temporal and spatial resolution,\nsensitivity and biocompatibility. For system-matrix (SM) based image\nreconstruction in MPI, a huge amount of calibration data needs to be acquired\nprior to reconstruction in a time-consuming procedure. Conventionally, the data\nis recorded on-site inside the scanning device, which significantly limits the\ntime that the scanning device is available for patient care in a clinical\nsetting. Due to its size, handling the calibration data can be challenging. To\nsolve these issues of recording and handling the data, data spaces could be\nused, as it has been shown that the calibration data can be measured in\ndedicated devices off-site. We propose a data space aimed at improving the\nefficiency of SM-based image reconstruction in MPI. The data space consists of\nimaging facilities, calibration data providers and reconstruction experts. Its\nspecifications follow the reference architecture model of international data\nspaces (IDS). Use-cases of image reconstruction in MPI are formulated. The\nstakeholders and tasks are listed and mapped to the terminology of IDS. The\nsignal chain in MPI is analysed to identify a minimum information model which\nis used by the data space.\n",
        "title": "Reconstruction as a service: a data space for off-site image\n  reconstruction in magnetic particle imaging",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05993",
        "abstract_url": "http://arxiv.org/abs/2401.05993",
        "authors": [
            {
                "last_name": "Da R\u00f9",
                "first_name": "Pietro"
            },
            {
                "last_name": "Benoni",
                "first_name": "Arianna"
            },
            {
                "last_name": "Salucci",
                "first_name": "Marco"
            },
            {
                "last_name": "Rocca",
                "first_name": "Paolo"
            },
            {
                "last_name": "Massa",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  In the framework of the \"Smart ElectroMagnetic Environment\" (SEME), an\ninnovative strategy leveraging Equivalence Source concepts is introduced for\nenhancing the performance of large-scale outdoor wireless communication\nsystems. The proposed Opportunistic Sources Synthesis (OSS) approach is aimed\nat unconventionally synthesizing the primary source (i.e., the base transceiver\nstation (BTS) antenna array), so that the complex scattering phenomena induced\nin the surrounding scatterers are profitably exploited to enhance the received\npower within user-defined regions of interest (RoIs). To yield a\ncomputationally feasible synthesis process, an innovative\n\"Embedded-plus-Environment Patterns\" (EPEPs) method is introduced. A set of\nrepresentative numerical examples, concerned with realistic large-scale outdoor\nscenarios, is presented to assess the effectiveness and the efficiency of the\nproposed optimization-driven approach for a realistic SEME implementation.\n",
        "title": "An Opportunistic Source Synthesis Method for Smart Electromagnetic\n  Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05994",
        "abstract_url": "http://arxiv.org/abs/2401.05994",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Qian"
            },
            {
                "last_name": "Chen",
                "first_name": "Jieyang"
            },
            {
                "last_name": "Whitney",
                "first_name": "Ben"
            },
            {
                "last_name": "Liang",
                "first_name": "Xin"
            },
            {
                "last_name": "Reshniak",
                "first_name": "Viktor"
            },
            {
                "last_name": "Banerjee",
                "first_name": "Tania"
            },
            {
                "last_name": "Lee",
                "first_name": "Jaemoon"
            },
            {
                "last_name": "Rangarajan",
                "first_name": "Anand"
            },
            {
                "last_name": "Wan",
                "first_name": "Lipeng"
            },
            {
                "last_name": "Vidal",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Liu",
                "first_name": "Qing"
            },
            {
                "last_name": "Gainaru",
                "first_name": "Ana"
            },
            {
                "last_name": "Podhorszki",
                "first_name": "Norbert"
            },
            {
                "last_name": "Archibald",
                "first_name": "Richard"
            },
            {
                "last_name": "Ranka",
                "first_name": "Sanjay"
            },
            {
                "last_name": "Klasky",
                "first_name": "Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  We describe MGARD, a software providing MultiGrid Adaptive Reduction for\nfloating-point scientific data on structured and unstructured grids. With\nexceptional data compression capability and precise error control, MGARD\naddresses a wide range of requirements, including storage reduction,\nhigh-performance I/O, and in-situ data analysis. It features a unified\napplication programming interface (API) that seamlessly operates across diverse\ncomputing architectures. MGARD has been optimized with highly-tuned GPU kernels\nand efficient memory and device management mechanisms, ensuring scalable and\nrapid operations.\n",
        "title": "MGARD: A multigrid framework for high-performance, error-controlled data\n  compression and refactoring",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05995",
        "abstract_url": "http://arxiv.org/abs/2401.05995",
        "authors": [
            {
                "last_name": "Dasgupta",
                "first_name": "Sankarshan"
            },
            {
                "last_name": "Buckley",
                "first_name": "James"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  In this new digital era, accessibility to real-world events is moving towards\nweb-based modules. This is mostly visible on e-commerce websites where there is\nlimited availability of physical verification. With this unforeseen\ndevelopment, we depend on the verification in the virtual world to influence\nour decisions. One of the decision making process is deeply based on review\nreading. Reviews play an important part in this transactional process. And\nseeking a real review can be very tenuous work for the user. On the other hand,\nfake review heavily impacts these transaction records of a product. The article\npresents an implementation of a Siamese network for detecting fake reviews. The\nfake reviews dataset, consisting of 40K reviews, preprocessed with different\ntechniques. The cleaned data is passed through embeddings generated by MiniLM\nBERT for contextual relationship and Word2Vec for semantic relationship to form\nvectors. Further, the embeddings are trained in a Siamese network with LSTM\nlayers connected to fuzzy logic for decision-making. The results show that fake\nreviews can be detected with high accuracy on a siamese network for prediction\nand verification.\n",
        "title": "A Multi-Embedding Convergence Network on Siamese Architecture for Fake\n  Reviews",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05998",
        "abstract_url": "http://arxiv.org/abs/2401.05998",
        "authors": [
            {
                "last_name": "Chern",
                "first_name": "Steffi"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Andy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            ""
        ],
        "abstract": "  While state-of-the-art language models have achieved impressive results, they\nremain susceptible to inference-time adversarial attacks, such as adversarial\nprompts generated by red teams arXiv:2209.07858. One approach proposed to\nimprove the general quality of language model generations is multi-agent\ndebate, where language models self-evaluate through discussion and feedback\narXiv:2305.14325. We implement multi-agent debate between current\nstate-of-the-art language models and evaluate models' susceptibility to red\nteam attacks in both single- and multi-agent settings. We find that multi-agent\ndebate can reduce model toxicity when jailbroken or less capable models are\nforced to debate with non-jailbroken or more capable models. We also find\nmarginal improvements through the general usage of multi-agent interactions. We\nfurther perform adversarial prompt content classification via embedding\nclustering, and analyze the susceptibility of different models to different\ntypes of attack topics.\n",
        "title": "Combating Adversarial Attacks with Multi-Agent Debate",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05999",
        "abstract_url": "http://arxiv.org/abs/2401.05999",
        "authors": [
            {
                "last_name": "Margarido",
                "first_name": "Solange"
            },
            {
                "last_name": "Roque",
                "first_name": "Lic\u00ednio"
            },
            {
                "last_name": "Machado",
                "first_name": "Penousal"
            },
            {
                "last_name": "Martins",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In recent years, there has been a growing application of mixed-initiative\nco-creative approaches in the creation of video games. The rapid advances in\nthe capabilities of artificial intelligence (AI) systems further propel\ncreative collaboration between humans and computational agents. In this\ntutorial, we present guidelines for researchers and practitioners to develop\ngame design tools with a high degree of mixed-initiative co-creativity\n(MI-CCy). We begin by reviewing a selection of current works that will serve as\ncase studies and categorize them by the type of game content they address. We\nintroduce the MI-CCy Quantifier, a framework that can be used by researchers\nand developers to assess co-creative tools on their level of MI-CCy through a\nvisual scheme of quantifiable criteria scales. We demonstrate the usage of the\nMI-CCy Quantifier by applying it to the selected works. This analysis enabled\nus to discern prevalent patterns within these tools, as well as features that\ncontribute to a higher level of MI-CCy. We highlight current gaps in MI-CCy\napproaches within game design, which we propose as pivotal aspects to tackle in\nthe development of forthcoming approaches.\n",
        "title": "Boosting Mixed-Initiative Co-Creativity in Game Design: A Tutorial",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06000",
        "abstract_url": "http://arxiv.org/abs/2401.06000",
        "authors": [
            {
                "last_name": "Bian",
                "first_name": "Sizhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Lukowicz",
                "first_name": "Paul"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CV"
        ],
        "abstract": "  Due to the fact that roughly sixty percent of the human body is essentially\ncomposed of water, the human body is inherently a conductive object, being able\nto, firstly, form an inherent electric field from the body to the surroundings\nand secondly, deform the distribution of an existing electric field near the\nbody. Body-area capacitive sensing, also called body-area electric field\nsensing, is becoming a promising alternative for wearable devices to accomplish\ncertain tasks in human activity recognition and human-computer interaction.\nOver the last decade, researchers have explored plentiful novel sensing systems\nbacked by the body-area electric field. On the other hand, despite the\npervasive exploration of the body-area electric field, a comprehensive survey\ndoes not exist for an enlightening guideline. Moreover, the various hardware\nimplementations, applied algorithms, and targeted applications result in a\nchallenging task to achieve a systematic overview of the subject. This paper\naims to fill in the gap by comprehensively summarizing the existing works on\nbody-area capacitive sensing so that researchers can have a better view of the\ncurrent exploration status. To this end, we first sorted the explorations into\nthree domains according to the involved body forms: body-part electric field,\nwhole-body electric field, and body-to-body electric field, and enumerated the\nstate-of-art works in the domains with a detailed survey of the backed sensing\ntricks and targeted applications. We then summarized the three types of sensing\nfrontends in circuit design, which is the most critical part in body-area\ncapacitive sensing, and analyzed the data processing pipeline categorized into\nthree kinds of approaches. Finally, we described the challenges and outlooks of\nbody-area electric sensing.\n",
        "title": "Body-Area Capacitive or Electric Field Sensing for Human Activity\n  Recognition and Human-Computer Interaction: A Comprehensive Survey",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06003",
        "abstract_url": "http://arxiv.org/abs/2401.06003",
        "authors": [
            {
                "last_name": "Franke",
                "first_name": "Linus"
            },
            {
                "last_name": "R\u00fcckert",
                "first_name": "Darius"
            },
            {
                "last_name": "Fink",
                "first_name": "Laura"
            },
            {
                "last_name": "Stamminger",
                "first_name": "Marc"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "",
            ""
        ],
        "abstract": "  Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n",
        "title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06005",
        "abstract_url": "http://arxiv.org/abs/2401.06005",
        "authors": [
            {
                "last_name": "Peters",
                "first_name": "Benjamin"
            },
            {
                "last_name": "DiCarlo",
                "first_name": "James J."
            },
            {
                "last_name": "Gureckis",
                "first_name": "Todd"
            },
            {
                "last_name": "Haefner",
                "first_name": "Ralf"
            },
            {
                "last_name": "Isik",
                "first_name": "Leyla"
            },
            {
                "last_name": "Tenenbaum",
                "first_name": "Joshua"
            },
            {
                "last_name": "Konkle",
                "first_name": "Talia"
            },
            {
                "last_name": "Naselaris",
                "first_name": "Thomas"
            },
            {
                "last_name": "Stachenfeld",
                "first_name": "Kimberly"
            },
            {
                "last_name": "Tavares",
                "first_name": "Zenna"
            },
            {
                "last_name": "Tsao",
                "first_name": "Doris"
            },
            {
                "last_name": "Yildirim",
                "first_name": "Ilker"
            },
            {
                "last_name": "Kriegeskorte",
                "first_name": "Nikolaus"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "",
            "CV",
            "LG"
        ],
        "abstract": "  Vision is widely understood as an inference problem. However, two contrasting\nconceptions of the inference process have each been influential in research on\nbiological vision as well as the engineering of machine vision. The first\nemphasizes bottom-up signal flow, describing vision as a largely feedforward,\ndiscriminative inference process that filters and transforms the visual\ninformation to remove irrelevant variation and represent behaviorally relevant\ninformation in a format suitable for downstream functions of cognition and\nbehavioral control. In this conception, vision is driven by the sensory data,\nand perception is direct because the processing proceeds from the data to the\nlatent variables of interest. The notion of \"inference\" in this conception is\nthat of the engineering literature on neural networks, where feedforward\nconvolutional neural networks processing images are said to perform inference.\nThe alternative conception is that of vision as an inference process in\nHelmholtz's sense, where the sensory evidence is evaluated in the context of a\ngenerative model of the causal processes giving rise to it. In this conception,\nvision inverts a generative model through an interrogation of the evidence in a\nprocess often thought to involve top-down predictions of sensory data to\nevaluate the likelihood of alternative hypotheses. The authors include\nscientists rooted in roughly equal numbers in each of the conceptions and\nmotivated to overcome what might be a false dichotomy between them and engage\nthe other perspective in the realm of theory and experiment. The primate brain\nemploys an unknown algorithm that may combine the advantages of both\nconceptions. We explain and clarify the terminology, review the key empirical\nevidence, and propose an empirical research program that transcends the\ndichotomy and sets the stage for revealing the mysterious hybrid algorithm of\nprimate vision.\n",
        "title": "How does the primate brain combine generative and discriminative\n  computations in vision?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06009",
        "abstract_url": "http://arxiv.org/abs/2401.06009",
        "authors": [
            {
                "last_name": "Rogers",
                "first_name": "Martin S J"
            },
            {
                "last_name": "Fox",
                "first_name": "Maria"
            },
            {
                "last_name": "Fleming",
                "first_name": "Andrew"
            },
            {
                "last_name": "van Zeeland",
                "first_name": "Louisa"
            },
            {
                "last_name": "Wilkinson",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Hosking",
                "first_name": "J. Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            ""
        ],
        "abstract": "  Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea\nice mapping due to its spatio-temporal coverage and the ability to detect sea\nice independent of cloud and lighting conditions. Automatic sea ice detection\nusing SAR imagery remains problematic due to the presence of ambiguous signal\nand noise within the image. Conversely, ice and water are easily\ndistinguishable using multispectral imagery (MSI), but in the polar regions the\nocean's surface is often occluded by cloud or the sun may not appear above the\nhorizon for many months. To address some of these limitations, this paper\nproposes a new tool trained using concurrent multispectral Visible and SAR\nimagery for sea Ice Detection (ViSual\\_IceD). ViSual\\_IceD is a convolution\nneural network (CNN) that builds on the classic U-Net architecture by\ncontaining two parallel encoder stages, enabling the fusion and concatenation\nof MSI and SAR imagery containing different spatial resolutions. The\nperformance of ViSual\\_IceD is compared with U-Net models trained using\nconcatenated MSI and SAR imagery as well as models trained exclusively on MSI\nor SAR imagery. ViSual\\_IceD outperforms the other networks, with a F1 score\n1.60\\% points higher than the next best network, and results indicate that\nViSual\\_IceD is selective in the image type it uses during image segmentation.\nOutputs from ViSual\\_IceD are compared to sea ice concentration products\nderived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how\nViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly\nin coastal regions. As the spatial-temporal coverage of MSI and SAR imagery\ncontinues to increase, ViSual\\_IceD provides a new opportunity for robust,\naccurate sea ice coverage detection in polar regions.\n",
        "title": "Sea ice detection using concurrent multispectral and synthetic aperture\n  radar imagery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06010",
        "abstract_url": "http://arxiv.org/abs/2401.06010",
        "authors": [
            {
                "last_name": "del Amor",
                "first_name": "Roc\u00edo"
            },
            {
                "last_name": "Silva-Rodr\u00edguez",
                "first_name": "Julio"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The development of computer vision solutions for gigapixel images in digital\npathology is hampered by significant computational limitations due to the large\nsize of whole slide images. In particular, digitizing biopsies at high\nresolutions is a time-consuming process, which is necessary due to the\nworsening results from the decrease in image detail. To alleviate this issue,\nrecent literature has proposed using knowledge distillation to enhance the\nmodel performance at reduced image resolutions. In particular, soft labels and\nfeatures extracted at the highest magnification level are distilled into a\nmodel that takes lower-magnification images as input. However, this approach\nfails to transfer knowledge about the most discriminative image regions in the\nclassification process, which may be lost when the resolution is decreased. In\nthis work, we propose to distill this information by incorporating attention\nmaps during training. In particular, our formulation leverages saliency maps of\nthe target class via grad-CAMs, which guides the lower-resolution Student model\nto match the Teacher distribution by minimizing the l2 distance between them.\nComprehensive experiments on prostate histology image grading demonstrate that\nthe proposed approach substantially improves the model performance across\ndifferent image resolutions compared to previous literature.\n",
        "title": "Attention to detail: inter-resolution knowledge distillation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06013",
        "abstract_url": "http://arxiv.org/abs/2401.06013",
        "authors": [
            {
                "last_name": "Beilei",
                "first_name": "Cui"
            },
            {
                "last_name": "Mobarakol",
                "first_name": "Islam"
            },
            {
                "last_name": "Long",
                "first_name": "Bai"
            },
            {
                "last_name": "Hongliang",
                "first_name": "Ren"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            ""
        ],
        "abstract": "  Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,\nsurgical navigation and augmented reality visualization. Although the\nfoundation model exhibits outstanding performance in many vision tasks,\nincluding depth estimation (e.g., DINOv2), recent works observed its\nlimitations in medical and surgical domain-specific applications. This work\npresents a low-ranked adaptation (LoRA) of the foundation model for surgical\ndepth estimation. Methods: We design a foundation model-based depth estimation\nmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for\ndepth estimation in endoscopic surgery. We build LoRA layers and integrate them\ninto DINO to adapt with surgery-specific domain knowledge instead of\nconventional fine-tuning. During training, we freeze the DINO image encoder,\nwhich shows excellent visual representation capacity, and only optimize the\nLoRA layers and depth decoder to integrate features from the surgical scene.\nResults: Our model is extensively validated on a MICCAI challenge dataset of\nSCARED, which is collected from da Vinci Xi endoscope surgery. We empirically\nshow that Surgical-DINO significantly outperforms all the state-of-the-art\nmodels in endoscopic depth estimation tasks. The analysis with ablation studies\nhas shown evidence of the remarkable effect of our LoRA layers and adaptation.\nConclusion: Surgical-DINO shed some light on the successful adaptation of the\nfoundation models into the surgical domain for depth estimation. There is clear\nevidence in the results that zero-shot prediction on pre-trained weights in\ncomputer vision datasets or naive fine-tuning is not sufficient to use the\nfoundation model in the surgical domain directly. Code is available at\nhttps://github.com/BeileiCui/SurgicalDINO.\n",
        "title": "Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation\n  in Endoscopic Surgery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06019",
        "abstract_url": "http://arxiv.org/abs/2401.06019",
        "authors": [
            {
                "last_name": "Alonso",
                "first_name": "Pablo"
            },
            {
                "last_name": "de Gordoa",
                "first_name": "Jon Ander I\u00f1iguez"
            },
            {
                "last_name": "Ortega",
                "first_name": "Juan Diego"
            },
            {
                "last_name": "Garc\u00eda",
                "first_name": "Sara"
            },
            {
                "last_name": "Iriarte",
                "first_name": "Francisco Javier"
            },
            {
                "last_name": "Nieto",
                "first_name": "Marcos"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Runway and taxiway pavements are exposed to high stress during their\nprojected lifetime, which inevitably leads to a decrease in their condition\nover time. To make sure airport pavement condition ensure uninterrupted and\nresilient operations, it is of utmost importance to monitor their condition and\nconduct regular inspections. UAV-based inspection is recently gaining\nimportance due to its wide range monitoring capabilities and reduced cost. In\nthis work, we propose a vision-based approach to automatically identify\npavement distress using images captured by UAVs. The proposed method is based\non Deep Learning (DL) to segment defects in the image. The DL architecture\nleverages the low computational capacities of embedded systems in UAVs by using\nan optimised implementation of EfficientNet feature extraction and Feature\nPyramid Network segmentation. To deal with the lack of annotated data for\ntraining we have developed a synthetic dataset generation methodology to extend\navailable distress datasets. We demonstrate that the use of a mixed dataset\ncomposed of synthetic and real training images yields better results when\ntesting the training models in real application scenarios.\n",
        "title": "Automatic UAV-based Airport Pavement Inspection Using Mixed Real and\n  Virtual Scenarios",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06021",
        "abstract_url": "http://arxiv.org/abs/2401.06021",
        "authors": [
            {
                "last_name": "de Groot",
                "first_name": "Oscar"
            },
            {
                "last_name": "Ferranti",
                "first_name": "Laura"
            },
            {
                "last_name": "Gavrila",
                "first_name": "Dariu"
            },
            {
                "last_name": "Alonso-Mora",
                "first_name": "Javier"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Ground robots navigating in complex, dynamic environments must compute\ncollision-free trajectories to avoid obstacles safely and efficiently.\nNonconvex optimization is a popular method to compute a trajectory in\nreal-time. However, these methods often converge to locally optimal solutions\nand frequently switch between different local minima, leading to inefficient\nand unsafe robot motion. In this work, We propose a novel topology-driven\ntrajectory optimization strategy for dynamic environments that plans multiple\ndistinct evasive trajectories to enhance the robot's behavior and efficiency. A\nglobal planner iteratively generates trajectories in distinct homotopy classes.\nThese trajectories are then optimized by local planners working in parallel.\nWhile each planner shares the same navigation objectives, they are locally\nconstrained to a specific homotopy class, meaning each local planner attempts a\ndifferent evasive maneuver. The robot then executes the feasible trajectory\nwith the lowest cost in a receding horizon manner. We demonstrate, on a mobile\nrobot navigating among pedestrians, that our approach leads to faster and safer\ntrajectories than existing planners.\n",
        "title": "Topology-Driven Parallel Trajectory Optimization in Dynamic Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06030",
        "abstract_url": "http://arxiv.org/abs/2401.06030",
        "authors": [
            {
                "last_name": "Sheng",
                "first_name": "Lijun"
            },
            {
                "last_name": "Liang",
                "first_name": "Jian"
            },
            {
                "last_name": "He",
                "first_name": "Ran"
            },
            {
                "last_name": "Wang",
                "first_name": "Zilei"
            },
            {
                "last_name": "Tan",
                "first_name": "Tieniu"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Model adaptation tackles the distribution shift problem with a pre-trained\nmodel instead of raw data, becoming a popular paradigm due to its great privacy\nprotection. Existing methods always assume adapting to a clean target domain,\noverlooking the security risks of unlabeled samples. In this paper, we explore\nthe potential backdoor attacks on model adaptation launched by well-designed\npoisoning target data. Concretely, we provide two backdoor triggers with two\npoisoning strategies for different prior knowledge owned by attackers. These\nattacks achieve a high success rate and keep the normal performance on clean\nsamples in the test stage. To defend against backdoor embedding, we propose a\nplug-and-play method named MixAdapt, combining it with existing adaptation\nalgorithms. Experiments across commonly used benchmarks and adaptation methods\ndemonstrate the effectiveness of MixAdapt. We hope this work will shed light on\nthe safety of learning with unlabeled data.\n",
        "title": "Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and\n  Defense on Model Adaptation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06031",
        "abstract_url": "http://arxiv.org/abs/2401.06031",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Huaming"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiayu"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhibo"
            },
            {
                "last_name": "Choo",
                "first_name": "Kim-Kwang Raymond"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Adversarial generative models, such as Generative Adversarial Networks\n(GANs), are widely applied for generating various types of data, i.e., images,\ntext, and audio. Accordingly, its promising performance has led to the\nGAN-based adversarial attack methods in the white-box and black-box attack\nscenarios. The importance of transferable black-box attacks lies in their\nability to be effective across different models and settings, more closely\naligning with real-world applications. However, it remains challenging to\nretain the performance in terms of transferable adversarial examples for such\nmethods. Meanwhile, we observe that some enhanced gradient-based transferable\nadversarial attack algorithms require prolonged time for adversarial sample\ngeneration. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to\nenhance the transferability of adversarial samples whilst improving the\nalgorithm's efficiency. The main approach is via optimising the training\nprocess of the generator parameters. With the functional and characteristic\nsimilarity analysis, we introduce a novel gradient editing (GE) mechanism and\nverify its feasibility in generating transferable samples on various models.\nMoreover, by exploring the frequency domain information to determine the\ngradient editing direction, GE-AdvGAN can generate highly transferable\nadversarial samples while minimizing the execution time in comparison to the\nstate-of-the-art transferable adversarial attack algorithms. The performance of\nGE-AdvGAN is comprehensively evaluated by large-scale experiments on different\ndatasets, which results demonstrate the superiority of our algorithm. The code\nfor our algorithm is available at: https://github.com/LMBTough/GE-advGAN\n",
        "title": "GE-AdvGAN: Improving the transferability of adversarial samples by\n  gradient editing-based adversarial generative model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06034",
        "abstract_url": "http://arxiv.org/abs/2401.06034",
        "authors": [
            {
                "last_name": "Adilazuarda",
                "first_name": "Muhammad Farid"
            },
            {
                "last_name": "Cahyawijaya",
                "first_name": "Samuel"
            },
            {
                "last_name": "Aji",
                "first_name": "Alham Fikri"
            },
            {
                "last_name": "Winata",
                "first_name": "Genta Indra"
            },
            {
                "last_name": "Purwarianti",
                "first_name": "Ayu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pretrained language models (PLMs) have shown remarkable generalization toward\nmultiple tasks and languages. Nonetheless, the generalization of PLMs towards\nunseen languages is poor, resulting in significantly worse language\nperformance, or even generating nonsensical responses that are comparable to a\nrandom baseline. This limitation has been a longstanding problem of PLMs\nraising the problem of diversity and equal access to language modeling\ntechnology. In this work, we solve this limitation by introducing LinguAlchemy,\na regularization technique that incorporates various aspects of languages\ncovering typological, geographical, and phylogenetic constraining the resulting\nrepresentation of PLMs to better characterize the corresponding linguistics\nconstraints. LinguAlchemy significantly improves the accuracy performance of\nmBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to\nfully finetuned models and displaying a high degree of unseen language\ngeneralization. We further introduce AlchemyScale and AlchemyTune, extension of\nLinguAlchemy which adjusts the linguistic regularization weights automatically,\nalleviating the need for hyperparameter search. LinguAlchemy enables better\ncross-lingual generalization to unseen languages which is vital for better\ninclusivity and accessibility of PLMs.\n",
        "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen\n  Language Generalization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06035",
        "abstract_url": "http://arxiv.org/abs/2401.06035",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Partha"
            },
            {
                "last_name": "Sanyal",
                "first_name": "Soubhik"
            },
            {
                "last_name": "Schmid",
                "first_name": "Cordelia"
            },
            {
                "last_name": "Sch\u00f6lkopf",
                "first_name": "Bernhard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We present a novel unconditional video generative model designed to address\nlong-term spatial and temporal dependencies. To capture these dependencies, our\napproach incorporates a hybrid explicit-implicit tri-plane representation\ninspired by 3D-aware generative frameworks developed for three-dimensional\nobject representation and employs a singular latent code to model an entire\nvideo sequence. Individual video frames are then synthesized from an\nintermediate tri-plane representation, which itself is derived from the primary\nlatent code. This novel strategy reduces computational complexity by a factor\nof $2$ as measured in FLOPs. Consequently, our approach facilitates the\nefficient and temporally coherent generation of videos. Moreover, our joint\nframe modeling approach, in contrast to autoregressive methods, mitigates the\ngeneration of visual artifacts. We further enhance the model's capabilities by\nintegrating an optical flow-based module within our Generative Adversarial\nNetwork (GAN) based generator architecture, thereby compensating for the\nconstraints imposed by a smaller generator size. As a result, our model is\ncapable of synthesizing high-fidelity video clips at a resolution of\n$256\\times256$ pixels, with durations extending to more than $5$ seconds at a\nframe rate of 30 fps. The efficacy and versatility of our approach are\nempirically validated through qualitative and quantitative assessments across\nthree different datasets comprising both synthetic and real video clips.\n",
        "title": "RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane\n  Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06040",
        "abstract_url": "http://arxiv.org/abs/2401.06040",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Qipeng"
            },
            {
                "last_name": "Mallick",
                "first_name": "Tanwi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traffic forecasting is the foundation for intelligent transportation systems.\nSpatiotemporal graph neural networks have demonstrated state-of-the-art\nperformance in traffic forecasting. However, these methods do not explicitly\nmodel some of the natural characteristics in traffic data, such as the\nmultiscale structure that encompasses spatial and temporal variations at\ndifferent levels of granularity or scale. To that end, we propose a\nWavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines\nmultiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In\nWavGCRN, the traffic data is decomposed into time-frequency components with\nDiscrete Wavelet Transformation (DWT), constructing a multi-stream input\nstructure; then Graph Convolutional Recurrent networks (GCRNs) are employed as\nencoders for each stream, extracting spatiotemporal features in different\nscales; and finally the learnable Inversed DWT and GCRN are combined as the\ndecoder, fusing the information from all streams for traffic metrics\nreconstruction and prediction. Furthermore, road-network-informed graphs and\ndata-driven graph learning are combined to accurately capture spatial\ncorrelation. The proposed method can offer well-defined interpretability,\npowerful learning capability, and competitive forecasting performance on\nreal-world traffic data sets.\n",
        "title": "Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for\n  Traffic Forecasting",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06044",
        "abstract_url": "http://arxiv.org/abs/2401.06044",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Xun"
            },
            {
                "last_name": "Beillahi",
                "first_name": "Sidi Mohamed"
            },
            {
                "last_name": "Minwalla",
                "first_name": "Cyrus"
            },
            {
                "last_name": "Du",
                "first_name": "Han"
            },
            {
                "last_name": "Veneris",
                "first_name": "Andreas"
            },
            {
                "last_name": "Long",
                "first_name": "Fan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            ""
        ],
        "abstract": "  This paper presents OVer, a framework designed to automatically analyze the\nbehavior of decentralized finance (DeFi) protocols when subjected to a \"skewed\"\noracle input. OVer firstly performs symbolic analysis on the given contract and\nconstructs a model of constraints. Then, the framework leverages an SMT solver\nto identify parameters that allow its secure operation. Furthermore, guard\nstatements may be generated for smart contracts that may use the oracle values,\nthus effectively preventing oracle manipulation attacks. Empirical results show\nthat OVer can successfully analyze all 10 benchmarks collected, which encompass\na diverse range of DeFi protocols. Additionally, this paper also illustrates\nthat current parameters utilized in the majority of benchmarks are inadequate\nto ensure safety when confronted with significant oracle deviations.\n",
        "title": "Safeguarding DeFi Smart Contracts against Oracle Deviations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06047",
        "abstract_url": "http://arxiv.org/abs/2401.06047",
        "authors": [
            {
                "last_name": "Agarwal",
                "first_name": "Pankaj K."
            },
            {
                "last_name": "Raychaudhury",
                "first_name": "Rahul"
            },
            {
                "last_name": "Sintos",
                "first_name": "Stavros"
            },
            {
                "last_name": "Yang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DB"
        ],
        "abstract": "  We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each\n$R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in\n[0,1]$ denotes its \\emph{selectivity}. The goal is to compute a small-size\n\\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots,\n(q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in [0,1]$ for each $1\\leq j\\leq m$,\nand $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most\n\\emph{consistent} with $\\mathcal{Z}$, i.e.,\n$\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\!\n\\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized. In a\ndatabase setting, $\\mathcal{Z}$ corresponds to a workload of range queries over\nsome table, together with their observed selectivities (i.e., fraction of\ntuples returned), and $\\mathcal{D}$ can be used as compact model for\napproximating the data distribution within the table without accessing the\nunderlying contents.\n  In this paper, we obtain both upper and lower bounds for this problem. In\nparticular, we show that the problem of finding the best data distribution from\nselectivity queries is $\\mathsf{NP}$-complete. On the positive side, we\ndescribe a Monte Carlo algorithm that constructs, in time\n$O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discrete\ndistribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that\n$\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq\n\\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for\n$p=1,2,\\infty$) where the minimum is taken over all discrete distributions. We\nalso establish conditional lower bounds, which strongly indicate the\ninfeasibility of relative approximations as well as removal of the exponential\ndependency on the dimension for additive approximations. This suggests that\nsignificant improvements to our algorithm are unlikely.\n",
        "title": "Computing Data Distribution from Query Selectivities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06048",
        "abstract_url": "http://arxiv.org/abs/2401.06048",
        "authors": [
            {
                "last_name": "Guettala",
                "first_name": "Walid"
            },
            {
                "last_name": "Guly\u00e1s",
                "first_name": "L\u00e1szl\u00f3"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "",
            "LG"
        ],
        "abstract": "  This paper studies four Graph Neural Network architectures (GNNs) for a graph\nclassification task on a synthetic dataset created using classic generative\nmodels of Network Science. Since the synthetic networks do not contain (node or\nedge) features, five different augmentation strategies (artificial feature\ntypes) are applied to nodes. All combinations of the 4 GNNs (GCN with\nHierarchical and Global aggregation, GIN and GATv2) and the 5 feature types\n(constant 1, noise, degree, normalized degree and ID -- a vector of the number\nof cycles of various lengths) are studied and their performances compared as a\nfunction of the hidden dimension of artificial neural networks used in the\nGNNs. The generalisation ability of these models is also analysed using a\nsecond synthetic network dataset (containing networks of different sizes).Our\nresults point towards the balanced importance of the computational power of the\nGNN architecture and the the information level provided by the artificial\nfeatures. GNN architectures with higher computational power, like GIN and\nGATv2, perform well for most augmentation strategies. On the other hand,\nartificial features with higher information content, like ID or degree, not\nonly consistently outperform other augmentation strategies, but can also help\nGNN architectures with lower computational power to achieve good performance.\n",
        "title": "On the Power of Graph Neural Networks and Feature Augmentation\n  Strategies to Classify Social Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06052",
        "abstract_url": "http://arxiv.org/abs/2401.06052",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Guanjun"
            },
            {
                "last_name": "Yi",
                "first_name": "Taoran"
            },
            {
                "last_name": "Fang",
                "first_name": "Jiemin"
            },
            {
                "last_name": "Liu",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinggang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Neural Radiances Fields (NeRF) and their extensions have shown great success\nin representing 3D scenes and synthesizing novel-view images. However, most\nNeRF methods take in low-dynamic-range (LDR) images, which may lose details,\nespecially with nonuniform illumination. Some previous NeRF methods attempt to\nintroduce high-dynamic-range (HDR) techniques but mainly target static scenes.\nTo extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF\nframework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images\ncaptured with various exposures. A learnable exposure mapping function is\nconstructed to obtain adaptive exposure values for each image. Based on the\nmonotonically increasing prior, a camera response function is designed for\nstable learning. With the proposed model, high-quality novel-view images at any\ntime point can be rendered with any desired exposure. We further construct a\ndataset containing multiple dynamic scenes captured with diverse exposures for\nevaluation. All the datasets and code are available at\n\\url{https://guanjunwu.github.io/HDR-HexPlane/}.\n",
        "title": "Fast High Dynamic Range Radiance Fields for Dynamic Scenes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06056",
        "abstract_url": "http://arxiv.org/abs/2401.06056",
        "authors": [
            {
                "last_name": "Vecchio",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Deschaintre",
                "first_name": "Valentin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR\nmaterials. Materials are crucial components of virtual relightable assets,\ndefining the interaction of light at the surface of geometries. Given their\nimportance, significant research effort was dedicated to their representation,\ncreation and acquisition. However, in the past 6 years, most research in\nmaterial acquisiton or generation relied either on the same unique dataset, or\non company-owned huge library of procedural materials. With this dataset we\npropose a significantly larger, more diverse, and higher resolution set of\nmaterials than previously publicly available. We carefully discuss the data\ncollection process and demonstrate the benefits of this dataset on material\nacquisition and generation applications. The complete data further contains\nmetadata with each material's origin, license, category, tags, creation method\nand, when available, descriptions and physical size, as well as 3M+ renderings\nof the augmented materials, in 1K, under various environment lightings. The\nMatSynth dataset is released through the project page at:\nhttps://www.gvecchio.com/matsynth.\n",
        "title": "MatSynth: A Modern PBR Materials Dataset",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06059",
        "abstract_url": "http://arxiv.org/abs/2401.06059",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Minhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Ken Ziyu"
            },
            {
                "last_name": "Zhong",
                "first_name": "Ming"
            },
            {
                "last_name": "Schaeffer",
                "first_name": "Rylan"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Siru"
            },
            {
                "last_name": "Han",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Koyejo",
                "first_name": "Sanmi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n",
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06066",
        "abstract_url": "http://arxiv.org/abs/2401.06066",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Damai"
            },
            {
                "last_name": "Deng",
                "first_name": "Chengqi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chenggang"
            },
            {
                "last_name": "Xu",
                "first_name": "R. X."
            },
            {
                "last_name": "Gao",
                "first_name": "Huazuo"
            },
            {
                "last_name": "Chen",
                "first_name": "Deli"
            },
            {
                "last_name": "Li",
                "first_name": "Jiashi"
            },
            {
                "last_name": "Zeng",
                "first_name": "Wangding"
            },
            {
                "last_name": "Yu",
                "first_name": "Xingkai"
            },
            {
                "last_name": "Wu",
                "first_name": "Y."
            },
            {
                "last_name": "Xie",
                "first_name": "Zhenda"
            },
            {
                "last_name": "Li",
                "first_name": "Y. K."
            },
            {
                "last_name": "Huang",
                "first_name": "Panpan"
            },
            {
                "last_name": "Luo",
                "first_name": "Fuli"
            },
            {
                "last_name": "Ruan",
                "first_name": "Chong"
            },
            {
                "last_name": "Sui",
                "first_name": "Zhifang"
            },
            {
                "last_name": "Liang",
                "first_name": "Wenfeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managing computational costs when scaling up model parameters.\nHowever, conventional MoE architectures like GShard, which activate the top-$K$\nout of $N$ experts, face challenges in ensuring expert specialization, i.e.\neach expert acquires non-overlapping and focused knowledge. In response, we\npropose the DeepSeekMoE architecture towards ultimate expert specialization. It\ninvolves two principal strategies: (1) finely segmenting the experts into $mN$\nones and activating $mK$ from them, allowing for a more flexible combination of\nactivated experts; (2) isolating $K_s$ experts as shared ones, aiming at\ncapturing common knowledge and mitigating redundancy in routed experts.\nStarting from a modest scale with 2B parameters, we demonstrate that\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5\ntimes the expert parameters and computation. In addition, DeepSeekMoE 2B nearly\napproaches the performance of its dense counterpart with the same number of\ntotal parameters, which set the upper bound of MoE models. Subsequently, we\nscale up DeepSeekMoE to 16B parameters and show that it achieves comparable\nperformance with LLaMA2 7B, with only about 40% of computations. Further, our\npreliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\nvalidate its substantial advantages over the GShard architecture, and show its\nperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)\nof computations.\n",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in\n  Mixture-of-Experts Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06070",
        "abstract_url": "http://arxiv.org/abs/2401.06070",
        "authors": [
            {
                "last_name": "Jafarzadeh",
                "first_name": "Siavash"
            },
            {
                "last_name": "Silling",
                "first_name": "Stewart"
            },
            {
                "last_name": "Liu",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhongqiang"
            },
            {
                "last_name": "Yu",
                "first_name": "Yue"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "LG"
        ],
        "abstract": "  Neural operators, which can act as implicit solution operators of hidden\ngoverning equations, have recently become popular tools for learning the\nresponses of complex real-world physical systems. Nevertheless, most neural\noperator applications have thus far been data-driven and neglect the intrinsic\npreservation of fundamental physical laws in data. In this work, we introduce a\nnovel integral neural operator architecture called the Peridynamic Neural\nOperator (PNO) that learns a nonlocal constitutive law from data. This neural\noperator provides a forward model in the form of state-based peridynamics, with\nobjectivity and momentum balance laws automatically guaranteed. As\napplications, we demonstrate the expressivity and efficacy of our model in\nlearning complex material behaviors from both synthetic and experimental data\nsets. We show that, owing to its ability to capture complex responses, our\nlearned neural operator achieves improved accuracy and efficiency compared to\nbaseline models that use predefined constitutive laws. Moreover, by preserving\nthe essential physical laws within the neural network architecture, the PNO is\nrobust in treating noisy data. The method shows generalizability to different\ndomain configurations, external loadings, and discretizations.\n",
        "title": "Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model\n  for Complex Material Responses",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06071",
        "abstract_url": "http://arxiv.org/abs/2401.06071",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhaowei"
            },
            {
                "last_name": "Xu",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dong"
            },
            {
                "last_name": "Song",
                "first_name": "Hang"
            },
            {
                "last_name": "Cai",
                "first_name": "Yiqing"
            },
            {
                "last_name": "Qi",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ran"
            },
            {
                "last_name": "Pan",
                "first_name": "Junting"
            },
            {
                "last_name": "Li",
                "first_name": "Zefeng"
            },
            {
                "last_name": "Vu",
                "first_name": "Van Tu"
            },
            {
                "last_name": "Huang",
                "first_name": "Zhida"
            },
            {
                "last_name": "Wang",
                "first_name": "Tao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Multi-modal large language models have demonstrated impressive performance\nacross various tasks in different modalities. However, existing multi-modal\nmodels primarily emphasize capturing global information within each modality\nwhile neglecting the importance of perceiving local information across\nmodalities. Consequently, these models lack the ability to effectively\nunderstand the fine-grained details of input data, limiting their performance\nin tasks that require a more nuanced understanding. To address this limitation,\nthere is a compelling need to develop models that enable fine-grained\nunderstanding across multiple modalities, thereby enhancing their applicability\nto a wide range of tasks. In this paper, we propose LEGO, a language enhanced\nmulti-modal grounding model. Beyond capturing global information like other\nmulti-modal models, our proposed model excels at tasks demanding a detailed\nunderstanding of local information within the input. It demonstrates precise\nidentification and localization of specific regions in images or moments in\nvideos. To achieve this objective, we design a diversified dataset construction\npipeline, resulting in a multi-modal, multi-granularity dataset for model\ntraining. The code, dataset, and demo of our model can be found at https:\n//github.com/lzw-lzw/LEGO.\n",
        "title": "LEGO:Language Enhanced Multi-modal Grounding Model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06072",
        "abstract_url": "http://arxiv.org/abs/2401.06072",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Ruilin"
            },
            {
                "last_name": "Gu",
                "first_name": "Tianle"
            },
            {
                "last_name": "Li",
                "first_name": "Haoling"
            },
            {
                "last_name": "Li",
                "first_name": "Junzhe"
            },
            {
                "last_name": "Lin",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Li",
                "first_name": "Jiayi"
            },
            {
                "last_name": "Yang",
                "first_name": "Yujiu"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "CL"
        ],
        "abstract": "  Temporal Knowledge Graph Completion (TKGC) is a challenging task of\npredicting missing event links at future timestamps by leveraging established\ntemporal structural knowledge. Given the formidable generative capabilities\ninherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize\ntemporal link prediction as an event generation task within the context of a\nhistorical event chain. We employ efficient fine-tuning methods to make LLMs\nadapt to specific graph textual information and patterns discovered in temporal\ntimelines. Furthermore, we introduce structure-based historical data\naugmentation and the integration of reverse knowledge to emphasize LLMs'\nawareness of structural information, thereby enhancing their reasoning\ncapabilities. We conduct thorough experiments on multiple widely used datasets\nand find that our fine-tuned model outperforms existing embedding-based models\non multiple metrics, achieving SOTA results. We also carry out sufficient\nablation experiments to explore the key influencing factors when LLMs perform\nstructured temporal knowledge inference tasks.\n",
        "title": "Chain of History: Learning and Forecasting with LLMs for Temporal\n  Knowledge Graph Completion",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06080",
        "abstract_url": "http://arxiv.org/abs/2401.06080",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Binghai"
            },
            {
                "last_name": "Zheng",
                "first_name": "Rui"
            },
            {
                "last_name": "Chen",
                "first_name": "Lu"
            },
            {
                "last_name": "Liu",
                "first_name": "Yan"
            },
            {
                "last_name": "Dou",
                "first_name": "Shihan"
            },
            {
                "last_name": "Huang",
                "first_name": "Caishuang"
            },
            {
                "last_name": "Shen",
                "first_name": "Wei"
            },
            {
                "last_name": "Jin",
                "first_name": "Senjie"
            },
            {
                "last_name": "Zhou",
                "first_name": "Enyu"
            },
            {
                "last_name": "Shi",
                "first_name": "Chenyu"
            },
            {
                "last_name": "Gao",
                "first_name": "Songyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Nuo"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuhao"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiaoran"
            },
            {
                "last_name": "Xi",
                "first_name": "Zhiheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Ji",
                "first_name": "Tao"
            },
            {
                "last_name": "Yan",
                "first_name": "Hang"
            },
            {
                "last_name": "Shen",
                "first_name": "Lixing"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhan"
            },
            {
                "last_name": "Gui",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            },
            {
                "last_name": "Qiu",
                "first_name": "Xipeng"
            },
            {
                "last_name": "Huang",
                "first_name": "Xuanjing"
            },
            {
                "last_name": "Wu",
                "first_name": "Zuxuan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yu-Gang"
            }
        ],
        "primary_category": "",
        "categories": [
            ""
        ],
        "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become a crucial\ntechnology for aligning language models with human values and intentions,\nenabling models to produce more helpful and harmless responses. Reward models\nare trained as proxies for human preferences to drive reinforcement learning\noptimization. While reward models are often considered central to achieving\nhigh performance, they face the following challenges in practical applications:\n(1) Incorrect and ambiguous preference pairs in the dataset may hinder the\nreward model from accurately capturing human intent. (2) Reward models trained\non data from a specific distribution often struggle to generalize to examples\noutside that distribution and are not suitable for iterative RLHF training.\n  In this report, we attempt to address these two issues. (1) From a data\nperspective, we propose a method to measure the strength of preferences within\nthe data, based on a voting mechanism of multiple reward models. Experimental\nresults confirm that data with varying preference strengths have different\nimpacts on reward model performance. We introduce a series of novel methods to\nmitigate the influence of incorrect and ambiguous preferences in the dataset\nand fully leverage high-quality preference data. (2) From an algorithmic\nstandpoint, we introduce contrastive learning to enhance the ability of reward\nmodels to distinguish between chosen and rejected responses, thereby improving\nmodel generalization. Furthermore, we employ meta-learning to enable the reward\nmodel to maintain the ability to differentiate subtle differences in\nout-of-distribution samples, and this approach can be utilized for iterative\nRLHF optimization.\n",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06081",
        "abstract_url": "http://arxiv.org/abs/2401.06081",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kun"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Wan",
                "first_name": "Junchen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fuzheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Di"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Reinforcement learning (RL) has been widely used in training large language\nmodels~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and\nerrors. However, existing RL methods mostly adopt the instance-level reward,\nwhich is unable to provide fine-grained supervision for complex reasoning\ntasks, and can not focus on the few key tokens that lead to the incorrectness.\nTo address it, we propose a new RL method named \\textbf{RLMEC} that\nincorporates a generative model as the reward model, which is trained by the\nerroneous solution rewriting task under the minimum editing constraint, and can\nproduce token-level rewards for RL training. Based on the generative reward\nmodel, we design the token-level RL objective for training and an\nimitation-based regularization for stabilizing RL process. And the both\nobjectives focus on the learning of the key tokens for the erroneous solution,\nreducing the effect of other unimportant tokens. The experiment results on\nmathematical tasks and question-answering tasks have demonstrated the\neffectiveness of our approach. Our code and data are available at\n\\url{https://github.com/RUCAIBox/RLMEC}.\n",
        "title": "Improving Large Language Models via Fine-grained Reinforcement Learning\n  with Minimum Editing Constraint",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06085",
        "abstract_url": "http://arxiv.org/abs/2401.06085",
        "authors": [
            {
                "last_name": "Smaldore",
                "first_name": "Valentino"
            },
            {
                "last_name": "Zanella",
                "first_name": "Corrado"
            },
            {
                "last_name": "Zullo",
                "first_name": "Ferdinando"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            "IT"
        ],
        "abstract": "  In this paper we will study the action of $\\mathbb{F}_{q^n}^{2 \\times 2}$ on\nthe graph of an $\\mathbb{F}_q$-linear function of $\\mathbb{F}_{q^n}$ into\nitself. In particular we will see that, under certain combinatorial\nassumptions, its stabilizer (together with the sum and product of matrices) is\na field. We will also see some examples for which this does not happen.\nMoreover, we will establish a connection between such a stabilizer and the\nright idealizer of the rank-metric code defined by the linear function and give\nsome structural results in the case in which the polynomials are partially\nscattered.\n",
        "title": "On the stabilizer of the graph of linear functions over finite fields",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06086",
        "abstract_url": "http://arxiv.org/abs/2401.06086",
        "authors": [
            {
                "last_name": "Terawong",
                "first_name": "Chawin"
            },
            {
                "last_name": "Cliff",
                "first_name": "Dave"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CE",
            "MA"
        ],
        "abstract": "  We present first results from the use of XGBoost, a highly effective machine\nlearning (ML) method, within the Bristol Betting Exchange (BBE), an open-source\nagent-based model (ABM) designed to simulate a contemporary sports-betting\nexchange with in-play betting during track-racing events such as horse races.\nWe use the BBE ABM and its array of minimally-simple bettor-agents as a\nsynthetic data generator which feeds into our XGBoost ML system, with the\nintention that XGBoost discovers profitable dynamic betting strategies by\nlearning from the more profitable bets made by the BBE bettor-agents. After\nthis XGBoost training, which results in one or more decision trees, a\nbettor-agent with a betting strategy determined by the XGBoost-learned decision\ntree(s) is added to the BBE ABM and made to bet on a sequence of races under\nvarious conditions and betting-market scenarios, with profitability serving as\nthe primary metric of comparison and evaluation. Our initial findings presented\nhere show that XGBoost trained in this way can indeed learn profitable betting\nstrategies, and can generalise to learn strategies that outperform each of the\nset of strategies used for creation of the training data. To foster further\nresearch and enhancements, the complete version of our extended BBE, including\nthe XGBoost integration, has been made freely available as an open-source\nrelease on GitHub.\n",
        "title": "XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an\n  Agent-Based Model of a Sports Betting Exchange",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06088",
        "abstract_url": "http://arxiv.org/abs/2401.06088",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "K M Sajjadul"
            },
            {
                "last_name": "Nipu",
                "first_name": "Ayesha Siddika"
            },
            {
                "last_name": "Madiraju",
                "first_name": "Praveen"
            },
            {
                "last_name": "Deshpande",
                "first_name": "Priya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  The Chief Complaint (CC) is a crucial component of a patient's medical record\nas it describes the main reason or concern for seeking medical care. It\nprovides critical information for healthcare providers to make informed\ndecisions about patient care. However, documenting CCs can be time-consuming\nfor healthcare providers, especially in busy emergency departments. To address\nthis issue, an autocompletion tool that suggests accurate and well-formatted\nphrases or sentences for clinical notes can be a valuable resource for triage\nnurses. In this study, we utilized text generation techniques to develop\nmachine learning models using CC data. In our proposed work, we train a Long\nShort-Term Memory (LSTM) model and fine-tune three different variants of\nBiomedical Generative Pretrained Transformers (BioGPT), namely\nmicrosoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.\nAdditionally, we tune a prompt by incorporating exemplar CC sentences,\nutilizing the OpenAI API of GPT-4. We evaluate the models' performance based on\nthe perplexity score, modified BERTScore, and cosine similarity score. The\nresults show that BioGPT-Large exhibits superior performance compared to the\nother models. It consistently achieves a remarkably low perplexity score of\n1.65 when generating CC, whereas the baseline LSTM model achieves the best\nperplexity score of 170. Further, we evaluate and assess the proposed models'\nperformance and the outcome of GPT-4.0. Our study demonstrates that utilizing\nLLMs such as BioGPT, leads to the development of an effective autocompletion\ntool for generating CC documentation in healthcare settings.\n",
        "title": "Autocompletion of Chief Complaints in the Electronic Health Records\n  using Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06089",
        "abstract_url": "http://arxiv.org/abs/2401.06089",
        "authors": [
            {
                "last_name": "Sao",
                "first_name": "Piyush"
            },
            {
                "last_name": "Prokopenko",
                "first_name": "Andrey"
            },
            {
                "last_name": "Lebrun-Grandi\u00e9",
                "first_name": "Damien"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC",
            "",
            "",
            ""
        ],
        "abstract": "  This paper presents \\pandora, a novel parallel algorithm for efficiently\nconstructing dendrograms for single-linkage hierarchical clustering, including\n\\hdbscan. Traditional dendrogram construction methods from a minimum spanning\ntree (MST), such as agglomerative or divisive techniques, often fail to\nefficiently parallelize, especially with skewed dendrograms common in\nreal-world data.\n  \\pandora addresses these challenges through a unique recursive tree\ncontraction method, which simplifies the tree for initial dendrogram\nconstruction and then progressively reconstructs the complete dendrogram. This\nprocess makes \\pandora asymptotically work-optimal, independent of dendrogram\nskewness. All steps in \\pandora are fully parallel and suitable for massively\nthreaded accelerators such as GPUs.\n  Our implementation is written in Kokkos, providing support for both CPUs and\nmulti-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \\pandora is\n2.2$\\times$ faster than the current best-multithreaded implementation, while\nthe GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and\n10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora. These\nadvancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over the\ncurrent best, which only offload MST construction to GPUs and perform\nmultithreaded dendrogram construction.\n",
        "title": "PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage\n  Clustering on GPU",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06091",
        "abstract_url": "http://arxiv.org/abs/2401.06091",
        "authors": [
            {
                "last_name": "McDermott",
                "first_name": "Matthew B. A."
            },
            {
                "last_name": "Hansen",
                "first_name": "Lasse Hyldig"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Angelotti",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Gallifant",
                "first_name": "Jack"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            ""
        ],
        "abstract": "  In machine learning (ML), a widespread adage is that the area under the\nprecision-recall curve (AUPRC) is a superior metric for model comparison to the\narea under the receiver operating characteristic (AUROC) for binary\nclassification tasks with class imbalance. This paper challenges this notion\nthrough novel mathematical analysis, illustrating that AUROC and AUPRC can be\nconcisely related in probabilistic terms. We demonstrate that AUPRC, contrary\nto popular belief, is not superior in cases of class imbalance and might even\nbe a harmful metric, given its inclination to unduly favor model improvements\nin subpopulations with more frequent positive labels. This bias can\ninadvertently heighten algorithmic disparities. Prompted by these insights, a\nthorough review of existing ML literature was conducted, utilizing large\nlanguage models to analyze over 1.5 million papers from arXiv. Our\ninvestigation focused on the prevalence and substantiation of the purported\nAUPRC superiority. The results expose a significant deficit in empirical\nbacking and a trend of misattributions that have fuelled the widespread\nacceptance of AUPRC's supposed advantages. Our findings represent a dual\ncontribution: a significant technical advancement in understanding metric\nbehaviors and a stark warning about unchecked assumptions in the ML community.\nAll experiments are accessible at\nhttps://github.com/mmcdermott/AUC_is_all_you_need.\n",
        "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06098",
        "abstract_url": "http://arxiv.org/abs/2401.06098",
        "authors": [
            {
                "last_name": "Bako",
                "first_name": "Laurent"
            },
            {
                "last_name": "Nadri",
                "first_name": "Madiha"
            },
            {
                "last_name": "Andrieu",
                "first_name": "Vincent"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "",
        "categories": [
            "",
            ""
        ],
        "abstract": "  This paper discusses a general framework for designing robust state\nestimators for a class of discrete-time nonlinear systems. We consider systems\nthat may be impacted by impulsive (sparse but otherwise arbitrary) measurement\nnoise sequences. We show that a family of state estimators, robust to this type\nof undesired signal, can be obtained by minimizing a class of nonsmooth convex\nfunctions at each time step. The resulting state observers are defined through\nproximal operators. We obtain a nonlinear implicit dynamical system in term of\nestimation error and prove, in the noise-free setting, that it vanishes\nasymptotically when the minimized loss function and the to-beobserved system\nenjoy appropriate properties. From a computational perspective, even though the\nproposed observers can be implemented via efficient numerical procedures, they\ndo not admit closed-form expressions. The paper argues that by adopting\nappropriate relaxations, simple and fast analytic expressions can be derived.\n",
        "title": "Proximal observers for secure state estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06102",
        "abstract_url": "http://arxiv.org/abs/2401.06102",
        "authors": [
            {
                "last_name": "Ghandeharioun",
                "first_name": "Asma"
            },
            {
                "last_name": "Caciularu",
                "first_name": "Avi"
            },
            {
                "last_name": "Pearce",
                "first_name": "Adam"
            },
            {
                "last_name": "Dixon",
                "first_name": "Lucas"
            },
            {
                "last_name": "Geva",
                "first_name": "Mor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "",
            "LG"
        ],
        "abstract": "  Inspecting the information encoded in hidden representations of large\nlanguage models (LLMs) can explain models' behavior and verify their alignment\nwith human values. Given the capabilities of LLMs in generating\nhuman-understandable text, we propose leveraging the model itself to explain\nits internal representations in natural language. We introduce a framework\ncalled Patchscopes and show how it can be used to answer a wide range of\nresearch questions about an LLM's computation. We show that prior\ninterpretability methods based on projecting representations into the\nvocabulary space and intervening on the LLM computation, can be viewed as\nspecial instances of this framework. Moreover, several of their shortcomings\nsuch as failure in inspecting early layers or lack of expressivity can be\nmitigated by a Patchscope. Beyond unifying prior inspection techniques,\nPatchscopes also opens up new possibilities such as using a more capable model\nto explain the representations of a smaller model, and unlocks new applications\nsuch as self-correction in multi-hop reasoning.\n",
        "title": "Patchscope: A Unifying Framework for Inspecting Hidden Representations\n  of Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06104",
        "abstract_url": "http://arxiv.org/abs/2401.06104",
        "authors": [
            {
                "last_name": "Oren",
                "first_name": "Matanel"
            },
            {
                "last_name": "Hassid",
                "first_name": "Michael"
            },
            {
                "last_name": "Adi",
                "first_name": "Yossi"
            },
            {
                "last_name": "Schwartz",
                "first_name": "Roy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Transformers are considered conceptually different compared to the previous\ngeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).\nIn this work, we demonstrate that decoder-only transformers can in fact be\nconceptualized as infinite multi-state RNNs - an RNN variant with unlimited\nhidden state size. We further show that pretrained transformers can be\nconverted into $\\textit{finite}$ multi-state RNNs by fixing the size of their\nhidden state. We observe that several existing transformers cache compression\ntechniques can be framed as such conversion policies, and introduce a novel\npolicy, TOVA, which is simpler compared to these policies. Our experiments with\nseveral long range tasks indicate that TOVA outperforms all other baseline\npolicies, while being nearly on par with the full (infinite) model, and using\nin some cases only $\\frac{1}{8}$ of the original cache size. Our results\nindicate that transformer decoder LLMs often behave in practice as RNNs. They\nalso lay out the option of mitigating one of their most painful computational\nbottlenecks - the size of their cache memory. We publicly release our code at\nhttps://github.com/schwartz-lab-NLP/TOVA.\n",
        "title": "Transformers are Multi-State RNNs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06105",
        "abstract_url": "http://arxiv.org/abs/2401.06105",
        "authors": [
            {
                "last_name": "Arar",
                "first_name": "Moab"
            },
            {
                "last_name": "Voynov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Hertz",
                "first_name": "Amir"
            },
            {
                "last_name": "Avrahami",
                "first_name": "Omri"
            },
            {
                "last_name": "Fruchter",
                "first_name": "Shlomi"
            },
            {
                "last_name": "Pritch",
                "first_name": "Yael"
            },
            {
                "last_name": "Cohen-Or",
                "first_name": "Daniel"
            },
            {
                "last_name": "Shamir",
                "first_name": "Ariel"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "GR",
            "LG"
        ],
        "abstract": "  Content creators often aim to create personalized images using personal\nsubjects that go beyond the capabilities of conventional text-to-image models.\nAdditionally, they may want the resulting image to encompass a specific\nlocation, style, ambiance, and more. Existing personalization methods may\ncompromise personalization ability or the alignment to complex textual prompts.\nThis trade-off can impede the fulfillment of user prompts and subject fidelity.\nWe propose a new approach focusing on personalization methods for a\n\\emph{single} prompt to address this issue. We term our approach prompt-aligned\npersonalization. While this may seem restrictive, our method excels in\nimproving text alignment, enabling the creation of images with complex and\nintricate prompts, which may pose a challenge for current techniques. In\nparticular, our method keeps the personalized model aligned with a target\nprompt using an additional score distillation sampling term. We demonstrate the\nversatility of our method in multi- and single-shot settings and further show\nthat it can compose multiple subjects or use inspiration from reference images,\nsuch as artworks. We compare our approach quantitatively and qualitatively with\nexisting baselines and state-of-the-art techniques.\n",
        "title": "PALP: Prompt Aligned Personalization of Text-to-Image Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06109",
        "abstract_url": "http://arxiv.org/abs/2401.06109",
        "authors": [
            {
                "last_name": "Szab\u00f3",
                "first_name": "D\u00e1niel"
            },
            {
                "last_name": "Apers",
                "first_name": "Simon"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DM",
            ""
        ],
        "abstract": "  We show that the graph property of having a (very) large $k$-th Betti number\n$\\beta_k$ for constant $k$ is testable with a constant number of queries in the\ndense graph model. More specifically, we consider a clique complex defined by\nan underlying graph and prove that for any $\\varepsilon>0$, there exists\n$\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta)\nd_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing\n$(k+2)$-clique-freeness, which is known to be testable. This complements a\nresult by Elek (2010) showing that Betti numbers are testable in the\nbounded-degree model. Our result combines the Euler characteristic, matroid\ntheory and the graph removal lemma.\n",
        "title": "Holey graphs: very large Betti numbers are testable",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06112",
        "abstract_url": "http://arxiv.org/abs/2401.06112",
        "authors": [
            {
                "last_name": "Yamagiwa",
                "first_name": "Hiroaki"
            },
            {
                "last_name": "Takase",
                "first_name": "Yusuke"
            },
            {
                "last_name": "Shimodaira",
                "first_name": "Hidetoshi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Word embedding is one of the most important components in natural language\nprocessing, but interpreting high-dimensional embeddings remains a challenging\nproblem. To address this problem, Independent Component Analysis (ICA) is\nidentified as an effective solution. ICA-transformed word embeddings reveal\ninterpretable semantic axes; however, the order of these axes are arbitrary. In\nthis study, we focus on this property and propose a novel method, Axis Tour,\nwhich optimizes the order of the axes. Inspired by Word Tour, a one-dimensional\nword embedding method, we aim to improve the clarity of the word embedding\nspace by maximizing the semantic continuity of the axes. Furthermore, we show\nthrough experiments on downstream tasks that Axis Tour constructs better\nlow-dimensional embeddings compared to both PCA and ICA.\n",
        "title": "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed\n  Embeddings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06116",
        "abstract_url": "http://arxiv.org/abs/2401.06116",
        "authors": [
            {
                "last_name": "Bolanos",
                "first_name": "Luis"
            },
            {
                "last_name": "Su",
                "first_name": "Shih-Yang"
            },
            {
                "last_name": "Rhodin",
                "first_name": "Helge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Neural character models can now reconstruct detailed geometry and texture\nfrom video, but they lack explicit shadows and shading, leading to artifacts\nwhen generating novel views and poses or during relighting. It is particularly\ndifficult to include shadows as they are a global effect and the required\ncasting of secondary rays is costly. We propose a new shadow model using a\nGaussian density proxy that replaces sampling with a simple analytic formula.\nIt supports dynamic motion and is tailored for shadow computation, thereby\navoiding the affine projection approximation and sorting required by the\nclosely related Gaussian splatting. Combined with a deferred neural rendering\nmodel, our Gaussian shadows enable Lambertian shading and shadow casting with\nminimal overhead. We demonstrate improved reconstructions, with better\nseparation of albedo, shading, and shadows in challenging outdoor scenes with\ndirect sun light and hard shadows. Our method is able to optimize the light\ndirection without any input from the user. As a result, novel poses have fewer\nshadow artifacts and relighting in novel scenes is more realistic compared to\nthe state-of-the-art methods, providing new ways to pose neural characters in\nnovel environments, increasing their applicability.\n",
        "title": "Gaussian Shadow Casting for Neural Characters",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06118",
        "abstract_url": "http://arxiv.org/abs/2401.06118",
        "authors": [
            {
                "last_name": "Egiazarian",
                "first_name": "Vage"
            },
            {
                "last_name": "Panferov",
                "first_name": "Andrei"
            },
            {
                "last_name": "Kuznedelev",
                "first_name": "Denis"
            },
            {
                "last_name": "Frantar",
                "first_name": "Elias"
            },
            {
                "last_name": "Babenko",
                "first_name": "Artem"
            },
            {
                "last_name": "Alistarh",
                "first_name": "Dan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  The emergence of accurate open large language models (LLMs) has led to a race\ntowards quantization techniques for such models enabling execution on end-user\ndevices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression--defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter, from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our work builds on top of Additive Quantization, a classic\nalgorithm from the MCQ family, and adapts it to the quantization of language\nmodels. The resulting algorithm advances the state-of-the-art in LLM\ncompression, outperforming all recently-proposed techniques in terms of\naccuracy at a given compression budget. For instance, when compressing Llama 2\nmodels to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93\nperplexity (a 1.29 improvement relative to the best prior work, and 1.81 points\nfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B\nmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release our\nimplementation of Additive Quantization for Language Models AQLM as a baseline\nto facilitate future research in LLM quantization.\n",
        "title": "Extreme Compression of Large Language Models via Additive Quantization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06121",
        "abstract_url": "http://arxiv.org/abs/2401.06121",
        "authors": [
            {
                "last_name": "Maini",
                "first_name": "Pratyush"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhili"
            },
            {
                "last_name": "Schwarzschild",
                "first_name": "Avi"
            },
            {
                "last_name": "Lipton",
                "first_name": "Zachary C."
            },
            {
                "last_name": "Kolter",
                "first_name": "J. Zico"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n",
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06122",
        "abstract_url": "http://arxiv.org/abs/2401.06122",
        "authors": [
            {
                "last_name": "Bareeva",
                "first_name": "Dilyara"
            },
            {
                "last_name": "H\u00f6hne",
                "first_name": "Marina M. -C."
            },
            {
                "last_name": "Warnecke",
                "first_name": "Alexander"
            },
            {
                "last_name": "Pirch",
                "first_name": "Lukas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Klaus-Robert"
            },
            {
                "last_name": "Rieck",
                "first_name": "Konrad"
            },
            {
                "last_name": "Bykov",
                "first_name": "Kirill"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "",
            "CV"
        ],
        "abstract": "  Deep Neural Networks (DNNs) are capable of learning complex and versatile\nrepresentations, however, the semantic nature of the learned concepts remains\nunknown. A common method used to explain the concepts learned by DNNs is\nActivation Maximization (AM), which generates a synthetic input signal that\nmaximally activates a particular neuron in the network. In this paper, we\ninvestigate the vulnerability of this approach to adversarial model\nmanipulations and introduce a novel method for manipulating feature\nvisualization without altering the model architecture or significantly\nimpacting the model's decision-making process. We evaluate the effectiveness of\nour method on several neural network models and demonstrate its capabilities to\nhide the functionality of specific neurons by masking the original explanations\nof neurons with chosen target explanations during model auditing. As a remedy,\nwe propose a protective measure against such manipulations and provide\nquantitative evidence which substantiates our findings.\n",
        "title": "Manipulating Feature Visualizations with Gradient Slingshots",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06125",
        "abstract_url": "http://arxiv.org/abs/2401.06125",
        "authors": [
            {
                "last_name": "D\u00e6hli",
                "first_name": "Karen M."
            },
            {
                "last_name": "Obead",
                "first_name": "Sarah A"
            },
            {
                "last_name": "Lin",
                "first_name": "Hsuan-Yin"
            },
            {
                "last_name": "Rosnes",
                "first_name": "Eirik"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "IT"
        ],
        "abstract": "  In private computation, a user wishes to retrieve a function evaluation of\nmessages stored on a set of databases without revealing the function's identity\nto the databases. Obead \\emph{et al.} introduced a capacity outer bound for\nprivate nonlinear computation, dependent on the order of the candidate\nfunctions. Focusing on private \\emph{quadratic monomial} computation, we\npropose three methods for ordering candidate functions: a graph edge-coloring\nmethod, a graph-distance method, and an entropy-based greedy method. We\nconfirm, via an exhaustive search, that all three methods yield an optimal\nordering for $f < 6$ messages. For $6 \\leq f \\leq 12$ messages, we numerically\nevaluate the performance of the proposed methods compared with a directed\nrandom search. For almost all scenarios considered, the entropy-based greedy\nmethod gives the smallest gap to the best-found ordering.\n",
        "title": "Improved Capacity Outer Bound for Private Quadratic Monomial Computation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06126",
        "abstract_url": "http://arxiv.org/abs/2401.06126",
        "authors": [
            {
                "last_name": "Saunders",
                "first_name": "Jack"
            },
            {
                "last_name": "Namboodiri",
                "first_name": "Vinay"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Visual dubbing is the process of generating lip motions of an actor in a\nvideo to synchronise with given audio. Recent advances have made progress\ntowards this goal but have not been able to produce an approach suitable for\nmass adoption. Existing methods are split into either person-generic or\nperson-specific models. Person-specific models produce results almost\nindistinguishable from reality but rely on long training times using large\nsingle-person datasets. Person-generic works have allowed for the visual\ndubbing of any video to any audio without further training, but these fail to\ncapture the person-specific nuances and often suffer from visual artefacts. Our\nmethod, based on data-efficient neural rendering priors, overcomes the\nlimitations of existing approaches. Our pipeline consists of learning a\ndeferred neural rendering prior network and actor-specific adaptation using\nneural textures. This method allows for $\\textbf{high-quality visual dubbing\nwith just a few seconds of data}$, that enables video dubbing for any actor -\nfrom A-list celebrities to background actors. We show that we achieve\nstate-of-the-art in terms of $\\textbf{visual quality}$ and\n$\\textbf{recognisability}$ both quantitatively, and qualitatively through two\nuser studies. Our prior learning and adaptation method $\\textbf{generalises to\nlimited data}$ better and is more $\\textbf{scalable}$ than existing\nperson-specific models. Our experiments on real-world, limited data scenarios\nfind that our model is preferred over all others. The project page may be found\nat https://dubbingforeveryone.github.io/\n",
        "title": "Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural\n  Rendering Priors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06127",
        "abstract_url": "http://arxiv.org/abs/2401.06127",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Yifan"
            },
            {
                "last_name": "Zhan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Jin",
                "first_name": "Qing"
            },
            {
                "last_name": "Li",
                "first_name": "Yanyu"
            },
            {
                "last_name": "Idelbayev",
                "first_name": "Yerlan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xian"
            },
            {
                "last_name": "Zharkov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Aberman",
                "first_name": "Kfir"
            },
            {
                "last_name": "Tulyakov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanzhi"
            },
            {
                "last_name": "Ren",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "",
            "LG"
        ],
        "abstract": "  One highly promising direction for enabling flexible real-time on-device\nimage editing is utilizing data distillation by leveraging large-scale\ntext-to-image diffusion models, such as Stable Diffusion, to generate paired\ndatasets used for training generative adversarial networks (GANs). This\napproach notably alleviates the stringent requirements typically imposed by\nhigh-end commercial GPUs for performing image editing with diffusion models.\nHowever, unlike text-to-image diffusion models, each distilled GAN is\nspecialized for a specific image editing task, necessitating costly training\nefforts to obtain models for various concepts. In this work, we introduce and\naddress a novel research direction: can the process of distilling GANs from\ndiffusion models be made significantly more efficient? To achieve this goal, we\npropose a series of innovative techniques. First, we construct a base GAN model\nwith generalized features, adaptable to different concepts through fine-tuning,\neliminating the need for training from scratch. Second, we identify crucial\nlayers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a\nsimple yet effective rank search process, rather than fine-tuning the entire\nbase model. Third, we investigate the minimal amount of data necessary for\nfine-tuning, further reducing the overall training time. Extensive experiments\nshow that we can efficiently empower GANs with the ability to perform real-time\nhigh-quality image editing on mobile devices with remarkable reduced training\ncost and storage for each concept.\n",
        "title": "E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image\n  Translation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06129",
        "abstract_url": "http://arxiv.org/abs/2401.06129",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Long"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jialin"
            },
            {
                "last_name": "Chu",
                "first_name": "Chun-Te"
            },
            {
                "last_name": "Miao",
                "first_name": "Hui"
            },
            {
                "last_name": "Schroff",
                "first_name": "Florian"
            },
            {
                "last_name": "Adam",
                "first_name": "Hartwig"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            },
            {
                "last_name": "Gong",
                "first_name": "Boqing"
            },
            {
                "last_name": "Kr\u00e4henb\u00fchl",
                "first_name": "Philipp"
            },
            {
                "last_name": "Yuan",
                "first_name": "Liangzhe"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The recent advance in vision-language models is largely attributed to the\nabundance of image-text data. We aim to replicate this success for\nvideo-language models, but there simply is not enough human-curated video-text\ndata available. We thus resort to fine-tuning a video-language model from a\nstrong image-language baseline with synthesized instructional data. The\nresulting video-language model is then used to auto-label millions of videos to\ngenerate high-quality captions. We show the adapted video-language model\nperforms well on a wide range of video-language benchmarks. For instance, it\nsurpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our\nmodel generates detailed descriptions for previously unseen videos, which\nprovide better textual supervision than existing methods. Experiments show that\na video-language dual-encoder model contrastively trained on these\nauto-generated captions is 3.8% better than the strongest baseline that also\nleverages vision-language models. Our best model outperforms state-of-the-art\nmethods on MSR-VTT zero-shot text-to-video retrieval by 6%.\n",
        "title": "Distilling Vision-Language Models on Millions of Videos",
        "date": "2024-01-11",
        "group": "cs"
    }
]
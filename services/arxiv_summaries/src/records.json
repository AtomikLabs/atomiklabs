[
    {
        "identifier": "oai:arXiv.org:2005.10310",
        "abstract_url": "http://arxiv.org/abs/2005.10310",
        "authors": [
            {
                "last_name": "Brink",
                "first_name": "Kevin M."
            },
            {
                "last_name": "Zhang",
                "first_name": "Jincheng"
            },
            {
                "last_name": "Willis",
                "first_name": "Andrew R."
            },
            {
                "last_name": "Sherrill",
                "first_name": "Ryan E."
            },
            {
                "last_name": "Godwin",
                "first_name": "Jamie L."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This article introduces an approach to facilitate cooperative exploration and mapping of large-scale, near-ground, underground, or indoor spaces via a novel integration framework for locally-dense agent map data. The effort targets limited Size, Weight, and Power (SWaP) agents with an emphasis on limiting required communications and redundant processing. The approach uses a unique organization of batch optimization engines to enable a highly efficient two-tier optimization structure. Tier I consist of agents that create and potentially share local maplets (local maps, limited in size) which are generated using Simultaneous Localization and Mapping (SLAM) map-building software and then marginalized to a more compact parameterization. Maplets are generated in an overlapping manner and used to estimate the transform and uncertainty between those overlapping maplets, providing accurate and compact odometry or delta-pose representation between maplet's local frames. The delta poses can be shared between agents, and in cases where maplets have salient features (for loop closures), the compact representation of the maplet can also be shared.   The second optimization tier consists of a global optimizer that seeks to optimize those maplet-to-maplet transformations, including any loop closures identified. This can provide an accurate global \"skeleton\"' of the traversed space without operating on the high-density point cloud. This compact version of the map data allows for scalable, cooperative exploration with limited communication requirements where most of the individual maplets, or low fidelity renderings, are only shared if desired. ",
        "title": "Maplets: An Efficient Approach for Cooperative SLAM Map Building Under  Communication and Computation Constraints",
        "date": "2020-05-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2005.10902",
        "abstract_url": "http://arxiv.org/abs/2005.10902",
        "authors": [
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            },
            {
                "last_name": "Bongartz",
                "first_name": "Dominik"
            },
            {
                "last_name": "Grothe",
                "first_name": "Daniel"
            },
            {
                "last_name": "Kerkenhoff",
                "first_name": "Tim"
            },
            {
                "last_name": "Lin",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Najman",
                "first_name": "Jaromil"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Gaussian processes~(Kriging) are interpolating data-driven models that are frequently applied in various disciplines. Often, Gaussian processes are trained on datasets and are subsequently embedded as surrogate models in optimization problems. These optimization problems are nonconvex and global optimization is desired. However, previous literature observed computational burdens limiting deterministic global optimization to Gaussian processes trained on few data points. We propose a reduced-space formulation for deterministic global optimization with trained Gaussian processes embedded. For optimization, the branch-and-bound solver branches only on the degrees of freedom and McCormick relaxations are propagated through explicit Gaussian process models. The approach also leads to significantly smaller and computationally cheaper subproblems for lower and upper bounding. To further accelerate convergence, we derive envelopes of common covariance functions for GPs and tight relaxations of acquisition functions used in Bayesian optimization including expected improvement, probability of improvement, and lower confidence bound. In total, we reduce computational time by orders of magnitude compared to state-of-the-art methods, thus overcoming previous computational burdens. We demonstrate the performance and scaling of the proposed method and apply it to Bayesian optimization with global optimization of the acquisition function and chance-constrained programming. The Gaussian process models, acquisition functions, and training scripts are available open-source within the \"MeLOn - Machine Learning Models for Optimization\" toolbox~(https://git.rwth-aachen.de/avt.svt/public/MeLOn). ",
        "title": "Global Optimization of Gaussian processes",
        "date": "2020-05-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2101.10154",
        "abstract_url": "http://arxiv.org/abs/2101.10154",
        "authors": [
            {
                "last_name": "Hibat-Allah",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Inack",
                "first_name": "Estelle M."
            },
            {
                "last_name": "Wiersema",
                "first_name": "Roeland"
            },
            {
                "last_name": "Melko",
                "first_name": "Roger G."
            },
            {
                "last_name": "Carrasquilla",
                "first_name": "Juan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Many important challenges in science and technology can be cast as optimization problems. When viewed in a statistical physics framework, these can be tackled by simulated annealing, where a gradual cooling procedure helps search for groundstate solutions of a target Hamiltonian. While powerful, simulated annealing is known to have prohibitively slow sampling dynamics when the optimization landscape is rough or glassy. Here we show that by generalizing the target distribution with a parameterized model, an analogous annealing framework based on the variational principle can be used to search for groundstate solutions. Modern autoregressive models such as recurrent neural networks provide ideal parameterizations since they can be exactly sampled without slow dynamics even when the model encodes a rough landscape. We implement this procedure in the classical and quantum settings on several prototypical spin glass Hamiltonians, and find that it significantly outperforms traditional simulated annealing in the asymptotic limit, illustrating the potential power of this yet unexplored route to optimization. ",
        "title": "Variational Neural Annealing",
        "date": "2021-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2102.05500",
        "abstract_url": "http://arxiv.org/abs/2102.05500",
        "authors": [
            {
                "last_name": "Angermeir",
                "first_name": "Florian"
            },
            {
                "last_name": "Voggenreiter",
                "first_name": "Markus"
            },
            {
                "last_name": "Moy\u00f3n",
                "first_name": "Fabiola"
            },
            {
                "last_name": "Mendez",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Agile and DevOps are widely adopted by the industry. Hence, integrating security activities with industrial practices, such as continuous integration (CI) pipelines, is necessary to detect security flaws and adhere to regulators' demands early. In this paper, we analyze automated security activities in CI pipelines of enterprise-driven open source software (OSS). This shall allow us, in the long-run, to better understand the extent to which security activities are (or should be) part of automated pipelines. In particular, we mine publicly available OSS repositories and survey a sample of project maintainers to better understand the role that security activities and their related tools play in their CI pipelines. To increase transparency and allow other researchers to replicate our study (and to take different perspectives), we further disclose our research artefacts. Our results indicate that security activities in enterprise-driven OSS projects are scarce and protection coverage is rather low. Only 6.83% of the analyzed 8,243 projects apply security automation in their CI pipelines, even though maintainers consider security to be rather important. This alerts industry to keep the focus on vulnerabilities of 3rd Party software and it opens space for other improvements of practice which we outline in this manuscript. ",
        "title": "Enterprise-Driven Open Source Software: A Case Study on Security  Automation",
        "date": "2021-02-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2105.02813",
        "abstract_url": "http://arxiv.org/abs/2105.02813",
        "authors": [
            {
                "last_name": "De",
                "first_name": "Subhayan"
            },
            {
                "last_name": "Hai",
                "first_name": "Bhuiyan Shameem Mahmood Ebna"
            },
            {
                "last_name": "Doostan",
                "first_name": "Alireza"
            },
            {
                "last_name": "Bause",
                "first_name": "Markus"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Structural health monitoring (SHM) systems use the non-destructive testing principle for damage identification. As part of SHM, the propagation of ultrasonic guided waves (UGWs) is tracked and analyzed for the changes in the associated wave pattern. These changes help identify the location of a structural damage, if any. We advance existing research by accounting for uncertainty in the material and geometric properties of a structure. The physics model used in this study comprises of a monolithically coupled system of acoustic and elastic wave equations, known as the wave propagation in fluid-solid and their interface (WpFSI) problem. As the UGWs propagate in the solid, fluid, and their interface, the wave signal displacement measurements are contrasted against the benchmark pattern. For the numerical solution, we develop an efficient algorithm that successfully addresses the inherent complexity of solving the multiphysics problem under uncertainty. We present a procedure that uses Gaussian process regression and convolutional neural network for predicting the UGW propagation in a solid-fluid and their interface under uncertainty. First, a set of training images for different realizations of the uncertain parameters of the inclusion inside the structure is generated using a monolithically-coupled system of acoustic and elastic wave equations. Next, Gaussian processes trained with these images are used for predicting the propagated wave with convolutional neural networks for further enhancement to produce high-quality images of the wave patterns for new realizations of the uncertainty. The results indicate that the proposed approach provides an accurate prediction for the WpFSI problem in the presence of uncertainty. ",
        "title": "Prediction of Ultrasonic Guided Wave Propagation in Solid-fluid and  their Interface under Uncertainty using Machine Learning",
        "date": "2021-03-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2107.10888",
        "abstract_url": "http://arxiv.org/abs/2107.10888",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Guanrui"
            },
            {
                "last_name": "Tunchez",
                "first_name": "Alex"
            },
            {
                "last_name": "Loianno",
                "first_name": "Giuseppe"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we address the Perception--Constrained Model Predictive Control (PCMPC) and state estimation problems for quadrotors with cable suspended payloads using a single camera and Inertial Measurement Unit (IMU). We design a receding--horizon control strategy for cable suspended payloads directly formulated on the system manifold configuration space SE(3)xS^2. The approach considers the system dynamics, actuator limits and the camera's Field Of View (FOV) constraint to guarantee the payload's visibility during motion. The monocular camera, IMU, and vehicle's motor speeds are combined to provide estimation of the vehicle's states in 3D space, the payload's states, the cable's direction and velocity. The proposed control and state estimation solution runs in real-time at 500 Hz on a small quadrotor equipped with a limited computational unit. The approach is validated through experimental results considering a cable suspended payload trajectory tracking problem at different speeds. ",
        "title": "PCMPC: Perception-Constrained Model Predictive Control for Quadrotors  with Suspended Loads using a Single Camera and IMU",
        "date": "2021-07-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2109.00787",
        "abstract_url": "http://arxiv.org/abs/2109.00787",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Guanqiu"
            },
            {
                "last_name": "Hu",
                "first_name": "Guanghui"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper is concerned with the factorization method with a single far-field pattern to recover an arbitrary convex polygonal scatterer/source in linear elasticity. The approach also applies to the compressional (resp. shear) part of the far-field pattern excited by a single compressional (resp. shear) plane wave. The one-wave factorization is based on the scattering data for a priori given testing scatterers. It can be regarded as a domain-defined sampling method and does not require forward solvers. We derive the spectral system of the far-field operator for rigid disks and show that, using testing disks, the one-wave factorization method can be justified independently of the classical factorization method. ",
        "title": "Factorization method for inverse time-harmonic elastic scattering with a  single plane wave",
        "date": "2021-09-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2110.07542",
        "abstract_url": "http://arxiv.org/abs/2110.07542",
        "authors": [
            {
                "last_name": "Maioli",
                "first_name": "Andrea"
            },
            {
                "last_name": "Mottola",
                "first_name": "Luca"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  We present ALFRED: a virtual memory abstraction that resolves the dichotomy between volatile and non-volatile memory in intermittent computing. Mixed-volatile microcontrollers allow programmers to allocate part of the application state onto non-volatile main memory. Programmers are therefore to explore manually the trade-off between simpler management of persistent state against the energy overhead for non-volatile memory operations and intermittence anomalies due to re-execution of non-idempotent code. This approach is laborious and yields sub-optimal performance. We take a different stand with ALFRED: we provide programmers with a virtual memory abstraction detached from the specific volatile nature of memory and automatically determine an efficient mapping from virtual to volatile or non-volatile memory. Unlike existing works, ALFRED does not require programmers to learn a new programming model or language syntax, while the mapping is entirely resolved at compile-time, reducing the run-time energy overhead. We implement ALFRED through a series of program machine-level code transformations. Compared to existing systems, we demonstrate that ALFRED reduces energy consumption by up to two orders of magnitude given a fixed workload. This enables the workloads to finish sooner, as the use of available energy shifts from ensuring forward progress to useful application processing. ",
        "title": "ALFRED: Virtual Memory for Intermittent Computing",
        "date": "2021-10-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2110.15676",
        "abstract_url": "http://arxiv.org/abs/2110.15676",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Abhinav"
            },
            {
                "last_name": "P\u00e1rtl",
                "first_name": "Ond\u0159ej"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Naveed"
            },
            {
                "last_name": "Kuzmin",
                "first_name": "Dmitri"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider flux-corrected finite element discretizations of 3D convection-dominated transport problems and assess the computational efficiency of algorithms based on such approximations. The methods under investigation include flux-corrected transport schemes and monolithic limiters. We discretize in space using a continuous Galerkin method and $\\mathbb{P}_1$ or $\\mathbb{Q}_1$ finite elements. Time integration is performed using the Crank-Nicolson method or an explicit strong stability preserving Runge-Kutta method. Nonlinear systems are solved using a fixed-point iteration method, which requires solution of large linear systems at each iteration or time step. The great variety of options in the choice of discretization methods and solver components calls for a dedicated comparative study of existing approaches. To perform such a study, we define new 3D test problems for time-dependent and stationary convection-diffusion-reaction equations. The results of our numerical experiments illustrate how the limiting technique, time discretization and solver impact on the overall performance. ",
        "title": "An Assessment of Solvers for Algebraically Stabilized Discretizations of  Convection-Diffusion-Reaction Equations",
        "date": "2021-10-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2111.11285",
        "abstract_url": "http://arxiv.org/abs/2111.11285",
        "authors": [
            {
                "last_name": "Craig",
                "first_name": "D. L."
            },
            {
                "last_name": "Moon",
                "first_name": "H."
            },
            {
                "last_name": "Fedele",
                "first_name": "F."
            },
            {
                "last_name": "Lennon",
                "first_name": "D. T."
            },
            {
                "last_name": "Van Straaten",
                "first_name": "B."
            },
            {
                "last_name": "Vigneau",
                "first_name": "F."
            },
            {
                "last_name": "Camenzind",
                "first_name": "L. C."
            },
            {
                "last_name": "Zumb\u00fchl",
                "first_name": "D. M."
            },
            {
                "last_name": "Briggs",
                "first_name": "G. A. D."
            },
            {
                "last_name": "Osborne",
                "first_name": "M. A."
            },
            {
                "last_name": "Sejdinovic",
                "first_name": "D."
            },
            {
                "last_name": "Ares",
                "first_name": "N."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The discrepancies between reality and simulation impede the optimisation and scalability of solid-state quantum devices. Disorder induced by the unpredictable distribution of material defects is one of the major contributions to the reality gap. We bridge this gap using physics-aware machine learning, in particular, using an approach combining a physical model, deep learning, Gaussian random field, and Bayesian inference. This approach has enabled us to infer the disorder potential of a nanoscale electronic device from electron transport data. This inference is validated by verifying the algorithm's predictions about the gate voltage values required for a laterally-defined quantum dot device in AlGaAs/GaAs to produce current features corresponding to a double quantum dot regime. ",
        "title": "Bridging the reality gap in quantum devices with physics-aware machine  learning",
        "date": "2021-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2112.06333",
        "abstract_url": "http://arxiv.org/abs/2112.06333",
        "authors": [
            {
                "last_name": "Bradshaw",
                "first_name": "Peter"
            },
            {
                "last_name": "Masa\u0159\u00edk",
                "first_name": "Tom\u00e1\u0161"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  We consider single-conflict colorings, a variant of graph colorings in which each edge of a graph has a single forbidden color pair. We show that for any assignment of forbidden color pairs to the edges of a $d$-degenerate graph $G$ on $n$ vertices of edge-multiplicity at most $\\log \\log n$, $O(\\sqrt{ d } \\log n)$ colors are always enough to color the vertices of $G$ in a way that avoids every forbidden color pair. This answers a question of Dvo\\v{r}\\'ak, Esperet, Kang, and Ozeki for simple graphs (Journal of Graph Theory 2021). ",
        "title": "Single-conflict colorings of degenerate graphs",
        "date": "2021-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.10671",
        "abstract_url": "http://arxiv.org/abs/2201.10671",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Ruan",
                "first_name": "Changxiao"
            },
            {
                "last_name": "Hadiwijoyo",
                "first_name": "Jessica"
            },
            {
                "last_name": "Chen",
                "first_name": "Brenna"
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Mataric",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  The physical design of a robot suggests expectations of that robot's functionality for human users and collaborators. When those expectations align with the true capabilities of the robot, interaction with the robot is enhanced. However, misalignment of those expectations can result in an unsatisfying interaction. This paper uses Mechanical Turk to evaluate user expectation through the use of design metaphors as applied to a wide range of robot embodiments. The first study (N=382) associates crowd-sourced design metaphors to different robot embodiments. The second study (N=803) assesses initial social expectations of robot embodiments. The final study (N=805) addresses the degree of abstraction of the design metaphors and the functional expectations projected on robot embodiments. Together, these results can guide robot designers toward aligning user expectations with true robot capabilities, facilitating positive human-robot interaction. ",
        "title": "Using Design Metaphors to Understand User Expectations of Socially  Interactive Robot Embodiments",
        "date": "2022-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2205.03262",
        "abstract_url": "http://arxiv.org/abs/2205.03262",
        "authors": [
            {
                "last_name": "Sarkar",
                "first_name": "Abhiroop"
            },
            {
                "last_name": "Svensson",
                "first_name": "Bo Joel"
            },
            {
                "last_name": "Sheeran",
                "first_name": "Mary"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  Programming embedded systems applications involve writing concurrent, event-driven and timing-aware programs. Traditionally, such programs are written in low-level machine-oriented programming languages like C or Assembly. We present an alternative by introducing Synchron, an API that offers high-level abstractions to the programmer while supporting the low-level infrastructure in an associated runtime system and one-time-effort drivers. Embedded systems applications exhibit the general characteristics of being (i) concurrent, (ii) I/O-bound and (iii) timing-aware. To address each of these concerns, the Synchron API consists of three components: (1) a Concurrent ML (CML) inspired message-passing concurrency model, (2) a message-passing--based I/O interface that translates between low-level interrupt based and memory-mapped peripherals, and (3) a timing operator, $syncT$, that marries CML's $sync$ operator with timing windows inspired from the TinyTimber kernel. We implement the Synchron API as the bytecode instructions of a virtual machine called SynchronVM. SynchronVM hosts a Caml-inspired functional language as its frontend language, and the backend of the VM supports the STM32F4 and NRF52 microcontrollers, with RAM in the order of hundreds of kilobytes. We illustrate the expressiveness of the Synchron API by showing examples of expressing state machines commonly found in embedded systems. The timing functionality is demonstrated through a music programming exercise. Finally, we provide benchmarks on the response time, jitter rates, memory, and power usage of the SynchronVM. ",
        "title": "Synchron -- An API and Runtime for Embedded Systems",
        "date": "2022-05-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.04891",
        "abstract_url": "http://arxiv.org/abs/2206.04891",
        "authors": [
            {
                "last_name": "Marton",
                "first_name": "Sascha"
            },
            {
                "last_name": "L\u00fcdtke",
                "first_name": "Stefan"
            },
            {
                "last_name": "Bartelt",
                "first_name": "Christian"
            },
            {
                "last_name": "Tschalzev",
                "first_name": "Andrej"
            },
            {
                "last_name": "Stuckenschmidt",
                "first_name": "Heiner"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We consider generating explanations for neural networks in cases where the network's training data is not accessible, for instance due to privacy or safety issues. Recently, $\\mathcal{I}$-Nets have been proposed as a sample-free approach to post-hoc, global model interpretability that does not require access to training data. They formulate interpretation as a machine learning task that maps network representations (parameters) to a representation of an interpretable function. In this paper, we extend the $\\mathcal{I}$-Net framework to the cases of standard and soft decision trees as surrogate models. We propose a suitable decision tree representation and design of the corresponding $\\mathcal{I}$-Net output layers. Furthermore, we make $\\mathcal{I}$-Nets applicable to real-world tasks by considering more realistic distributions when generating the $\\mathcal{I}$-Net's training data. We empirically evaluate our approach against traditional global, post-hoc interpretability approaches and show that it achieves superior results when the training data is not accessible. ",
        "title": "Explaining Neural Networks without Access to Training Data",
        "date": "2022-06-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.08087",
        "abstract_url": "http://arxiv.org/abs/2206.08087",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jianfeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhonghao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Xu",
                "first_name": "Yixin"
            },
            {
                "last_name": "Yu",
                "first_name": "Tongguang"
            },
            {
                "last_name": "Ye",
                "first_name": "Enze"
            },
            {
                "last_name": "Zheng",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Huazhong"
            },
            {
                "last_name": "George",
                "first_name": "Sumitha"
            },
            {
                "last_name": "Liu",
                "first_name": "Yongpan"
            },
            {
                "last_name": "Narayanan",
                "first_name": "Vijaykrishnan"
            },
            {
                "last_name": "Li",
                "first_name": "Xueqing"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Intellectual property (IP) piracy has become a non-negligible problem as the integrated circuit (IC) production supply chain is becoming increasingly globalized and separated that enables attacks by potentially untrusted attackers. Logic locking is a widely adopted method to lock the circuit module with a key and prevent hackers from cracking it. The key is the critical aspect of logic locking, but the existing works have overlooked three possible challenges of the key: safety of key storage, easy key-attempt from interface and key-related overheads, bringing the further challenges of low error rate and small state space. In this work, the key is dynamically generated by utilizing the huge space of a CPU core, and the unlocking is performed implicitly through the interconnection inside the chip. A novel low-cost logic reconfigurable gate is together proposed with ferroelectric FET (FeFET) to mitigate the reverse engineering and removal attack. Compared to the common logic locking methods, our proposed approach is 19,945 times more time consuming to traverse all the possible combinations in only 9-bit-key condition. Furthermore, our technique let key length increases this complexity exponentially and ensure the logic obfuscation effect. ",
        "title": "ALL-MASK: A Reconfigurable Logic Locking Method for Multicore  Architecture with Sequential-Instruction-Oriented Key",
        "date": "2022-06-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2206.11776",
        "abstract_url": "http://arxiv.org/abs/2206.11776",
        "authors": [
            {
                "last_name": "Rittig",
                "first_name": "Jan G."
            },
            {
                "last_name": "Hicham",
                "first_name": "Karim Ben"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            },
            {
                "last_name": "Dahmen",
                "first_name": "Manuel"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Ionic liquids (ILs) are important solvents for sustainable processes and predicting activity coefficients (ACs) of solutes in ILs is needed. Recently, matrix completion methods (MCMs), transformers, and graph neural networks (GNNs) have shown high accuracy in predicting ACs of binary mixtures, superior to well-established models, e.g., COSMO-RS and UNIFAC. GNNs are particularly promising here as they learn a molecular graph-to-property relationship without pretraining, typically required for transformers, and are, unlike MCMs, applicable to molecules not included in training. For ILs, however, GNN applications are currently missing. Herein, we present a GNN to predict temperature-dependent infinite dilution ACs of solutes in ILs. We train the GNN on a database including more than 40,000 AC values and compare it to a state-of-the-art MCM. The GNN and MCM achieve similar high prediction performance, with the GNN additionally enabling high-quality predictions for ACs of solutions that contain ILs and solutes not considered during training. ",
        "title": "Graph Neural Networks for Temperature-Dependent Activity Coefficient  Prediction of Solutes in Ionic Liquids",
        "date": "2022-06-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.12051",
        "abstract_url": "http://arxiv.org/abs/2207.12051",
        "authors": [
            {
                "last_name": "Stops",
                "first_name": "Laura"
            },
            {
                "last_name": "Leenhouts",
                "first_name": "Roel"
            },
            {
                "last_name": "Gao",
                "first_name": "Qinghe"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Process synthesis experiences a disruptive transformation accelerated by digitization and artificial intelligence. We propose a reinforcement learning algorithm for chemical process design based on a state-of-the-art actor-critic logic. Our proposed algorithm represents chemical processes as graphs and uses graph convolutional neural networks to learn from process graphs. In particular, the graph neural networks are implemented within the agent architecture to process the states and make decisions. Moreover, we implement a hierarchical and hybrid decision-making process to generate flowsheets, where unit operations are placed iteratively as discrete decisions and corresponding design variables are selected as continuous decisions. We demonstrate the potential of our method to design economically viable flowsheets in an illustrative case study comprising equilibrium reactions, azeotropic separation, and recycles. The results show quick learning in discrete, continuous, and hybrid action spaces. Due to the flexible architecture of the proposed reinforcement learning agent, the method is predestined to include large action-state spaces and an interface to process simulators in future research. ",
        "title": "Flowsheet synthesis through hierarchical reinforcement learning and  graph neural networks",
        "date": "2022-07-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2207.13779",
        "abstract_url": "http://arxiv.org/abs/2207.13779",
        "authors": [
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            },
            {
                "last_name": "Rittig",
                "first_name": "Jan G."
            },
            {
                "last_name": "Weber",
                "first_name": "Jana M."
            },
            {
                "last_name": "Grohe",
                "first_name": "Martin"
            },
            {
                "last_name": "Dahmen",
                "first_name": "Manuel"
            },
            {
                "last_name": "Leonhard",
                "first_name": "Kai"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph neural networks (GNNs) are emerging in chemical engineering for the end-to-end learning of physicochemical properties based on molecular graphs. A key element of GNNs is the pooling function which combines atom feature vectors into molecular fingerprints. Most previous works use a standard pooling function to predict a variety of properties. However, unsuitable pooling functions can lead to unphysical GNNs that poorly generalize. We compare and select meaningful GNN pooling methods based on physical knowledge about the learned properties. The impact of physical pooling functions is demonstrated with molecular properties calculated from quantum mechanical computations. We also compare our results to the recent set2set pooling approach. We recommend using sum pooling for the prediction of properties that depend on molecular size and compare pooling functions for properties that are molecular size-independent. Overall, we show that the use of physical pooling functions significantly enhances generalization. ",
        "title": "Physical Pooling Functions in Graph Neural Networks for Molecular  Property Prediction",
        "date": "2022-07-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.00778",
        "abstract_url": "http://arxiv.org/abs/2208.00778",
        "authors": [
            {
                "last_name": "Vogel",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Hirtreiter",
                "first_name": "Edwin"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  SFILES is a text-based notation for chemical process flowsheets. It was originally proposed by d'Anterroches (2006) who was inspired by the text-based SMILES notation for molecules. The text-based format has several advantages compared to flowsheet images regarding the storage format, computational accessibility, and eventually for data analysis and processing. However, the original SFILES version cannot describe essential flowsheet configurations unambiguously, such as the distinction between top and bottom products. Neither is it capable of describing the control structure required for the safe and reliable operation of chemical processes. Also, there is no publicly available software for decoding or encoding chemical process topologies to SFILES. We propose the SFILES 2.0 with a complete description of the extended notation and naming conventions. Additionally, we provide open-source software for the automated conversion between flowsheet graphs and SFILES 2.0 strings. This way, we hope to encourage researchers and engineers to publish their flowsheet topologies as SFILES 2.0 strings. The ultimate goal is to set the standards for creating a FAIR database of chemical process flowsheets, which would be of great value for future data analysis and processing. ",
        "title": "SFILES 2.0: An extended text-based flowsheet representation",
        "date": "2022-07-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.00859",
        "abstract_url": "http://arxiv.org/abs/2208.00859",
        "authors": [
            {
                "last_name": "Vogel",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  We propose a novel method enabling autocompletion of chemical flowsheets. This idea is inspired by the autocompletion of text. We represent flowsheets as strings using the text-based SFILES 2.0 notation and learn the grammatical structure of the SFILES 2.0 language and common patterns in flowsheets using a transformer-based language model. We pre-train our model on synthetically generated flowsheets to learn the flowsheet language grammar. Then, we fine-tune our model in a transfer learning step on real flowsheet topologies. Finally, we use the trained model for causal language modeling to autocomplete flowsheets. Eventually, the proposed method can provide chemical engineers with recommendations during interactive flowsheet synthesis. The results demonstrate a high potential of this approach for future AI-assisted process synthesis. ",
        "title": "Learning from flowsheets: A generative transformer model for  autocompletion of flowsheets",
        "date": "2022-08-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2208.04852",
        "abstract_url": "http://arxiv.org/abs/2208.04852",
        "authors": [
            {
                "last_name": "Rittig",
                "first_name": "Jan G."
            },
            {
                "last_name": "Gao",
                "first_name": "Qinghe"
            },
            {
                "last_name": "Dahmen",
                "first_name": "Manuel"
            },
            {
                "last_name": "Mitsos",
                "first_name": "Alexander"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Molecular property prediction is of crucial importance in many disciplines such as drug discovery, molecular biology, or material and process design. The frequently employed quantitative structure-property/activity relationships (QSPRs/QSARs) characterize molecules by descriptors which are then mapped to the properties of interest via a linear or nonlinear model. In contrast, graph neural networks, a novel machine learning method, directly work on the molecular graph, i.e., a graph representation where atoms correspond to nodes and bonds correspond to edges. GNNs allow to learn properties in an end-to-end fashion, thereby avoiding the need for informative descriptors as in QSPRs/QSARs. GNNs have been shown to achieve state-of-the-art prediction performance on various property predictions tasks and represent an active field of research. We describe the fundamentals of GNNs and demonstrate the application of GNNs via two examples for molecular property prediction. ",
        "title": "Graph neural networks for the prediction of molecular structure-property  relationships",
        "date": "2022-07-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2211.05583",
        "abstract_url": "http://arxiv.org/abs/2211.05583",
        "authors": [
            {
                "last_name": "Hirtreiter",
                "first_name": "Edwin"
            },
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step during the development of chemical processes. Currently, this is a tedious, manual, and time-consuming task. We propose a novel, completely data-driven method for the prediction of control structures. Our methodology is inspired by end-to-end transformer-based human language translation models. We cast the control structure prediction as a translation task where Process Flow Diagrams (PFDs) are translated to P&IDs. To use established transformer-based language translation models, we represent the P&IDs and PFDs as strings using our recently proposed SFILES 2.0 notation. Model training is performed in a transfer learning approach. Firstly, we pre-train our model using generated P&IDs to learn the grammatical structure of the process diagrams. Thereafter, the model is fine-tuned leveraging transfer learning on real P&IDs. The model achieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on 100,000 generated P&IDs. These promising results show great potential for AI-assisted process engineering. The tests on a dataset of 312 real P&IDs indicate the need of a larger P&IDs dataset for industry applications. ",
        "title": "Towards automatic generation of Piping and Instrumentation Diagrams  (P&IDs) with Artificial Intelligence",
        "date": "2022-10-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.00756",
        "abstract_url": "http://arxiv.org/abs/2212.00756",
        "authors": [
            {
                "last_name": "Chremos",
                "first_name": "Ioannis Vasileios"
            },
            {
                "last_name": "Malikopoulos",
                "first_name": "Andreas A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This article provides an introduction to the theory of mechanism design and its application to engineering problems. Our aim is to provide the fundamental principles of the theory of mechanism design for control engineers and theorists along with the state-of-the-art methods in engineering applications. We start our exposition with a brief overview of game theory highlighting the key notions that are necessary to introduce mechanism design, and then we offer a comprehensive discussion of the principles in mechanism design. Finally, we explore four key applications of mechanism design in engineering, i.e., communication networks, power grids, transportation, and security systems. ",
        "title": "Mechanism Design Theory in Control Engineering: A Tutorial and Overview  of Applications in Communication, Power Grid, Transportation, and Security  Systems",
        "date": "2022-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.00851",
        "abstract_url": "http://arxiv.org/abs/2212.00851",
        "authors": [
            {
                "last_name": "Ranasinghe",
                "first_name": "Tharindu"
            },
            {
                "last_name": "Anuradha",
                "first_name": "Isuri"
            },
            {
                "last_name": "Premasiri",
                "first_name": "Damith"
            },
            {
                "last_name": "Silva",
                "first_name": "Kanishka"
            },
            {
                "last_name": "Hettiarachchi",
                "first_name": "Hansi"
            },
            {
                "last_name": "Uyangodage",
                "first_name": "Lasitha"
            },
            {
                "last_name": "Zampieri",
                "first_name": "Marcos"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  The widespread of offensive content online, such as hate speech and cyber-bullying, is a global phenomenon. This has sparked interest in the artificial intelligence (AI) and natural language processing (NLP) communities, motivating the development of various systems trained to detect potentially harmful content automatically. These systems require annotated datasets to train the machine learning (ML) models. However, with a few notable exceptions, most datasets on this topic have dealt with English and a few other high-resource languages. As a result, the research in offensive language identification has been limited to these languages. This paper addresses this gap by tackling offensive language identification in Sinhala, a low-resource Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments on this dataset. SOLD is a manually annotated dataset containing 10,000 posts from Twitter annotated as offensive and not offensive at both sentence-level and token-level, improving the explainability of the ML models. SOLD is the first large publicly available offensive language dataset compiled for Sinhala. We also introduce SemiSOLD, a larger dataset containing more than 145,000 Sinhala tweets, annotated following a semi-supervised approach. ",
        "title": "SOLD: Sinhala Offensive Language Dataset",
        "date": "2022-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.09879",
        "abstract_url": "http://arxiv.org/abs/2212.09879",
        "authors": [
            {
                "last_name": "Jungmannov\u00e1",
                "first_name": "Lenka"
            },
            {
                "last_name": "Plech\u00e1\u010d",
                "first_name": "Petr"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In addition to being a widely recognised novelist, Milan Kundera has also authored three pieces for theatre: The Owners of the Keys (Majitel\\'e kl\\'i\\v{c}\\r{u}, 1961), The Blunder (Pt\\'akovina, 1967), and Jacques and his Master (Jakub a jeho p\\'an, 1971). In recent years, however, the hypothesis has been raised that Kundera is the true author of a fourth play: Juro J\\'ano\\v{s}\\'ik, first performed in a 1974 production under the name of Karel Steigerwald, who was Kundera's student at the time. In this study, we make use of supervised machine learning to settle the question of authorship attribution in the case of Juro J\\'ano\\v{s}\\'ik, with results strongly supporting the hypothesis of Kundera's authorship. ",
        "title": "Unsigned Play by Milan Kundera? An Authorship Attribution Study",
        "date": "2022-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2212.14236",
        "abstract_url": "http://arxiv.org/abs/2212.14236",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Hu",
                "first_name": "Guanghui"
            },
            {
                "last_name": "Ma",
                "first_name": "Guanqiu"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a multi-frequency algorithm for imaging the trajectory of a moving point source from one and sparse far-field observation directions in the frequency domain. The starting and terminal time points of the moving source are both supposed to be known. We introduce the concept of observable directions (angles) in the far-field region and derive all observable directions (angles) for straight and circular motions. At an observable direction, it is verified that the smallest trip containing the trajectory and perpendicular to the direction can be imaged, provided the orbit function possesses a certain monotonical property. Without the monotonicity one can only expect to recover a thinner strip. The far-field data measured at sparse observable directions can be used to recover the $\\Theta$-convex domain of the trajectory. Both two- and three-dimensional numerical examples are implemented to show effectiveness and feasibility of the approach. ",
        "title": "Imaging a moving point source from multi-frequency data measured at one  and sparse observation directions (part I): far-field case",
        "date": "2022-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.01364",
        "abstract_url": "http://arxiv.org/abs/2302.01364",
        "authors": [
            {
                "last_name": "Proskurnikov",
                "first_name": "Anton V."
            },
            {
                "last_name": "Runvik",
                "first_name": "H\u00e5kan"
            },
            {
                "last_name": "Medvedev",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Existence of periodical solutions, i.e. cycles, in the Impulsive Goodwin's Oscillator (IGO) with the continuous part of an arbitrary order m is considered. The original IGO with a third-order continuous part is a hybrid model that portrays a chemical or biochemical system composed of three substances represented by their concentrations and arranged in a cascade. The first substance in the chain is introduced via an impulsive feedback where both the impulse frequency and weights are modulated by the measured output of the continuous part. It is shown that, under the standard assumptions on the IGO, a positive periodic solution with one firing of the pulse-modulated feedback in the least period also exists in models with any m >= 1. Furthermore, the uniqueness of this 1-cycle is proved for the IGO with m <= 10 whereas, for m > 10, the uniqueness can still be guaranteed under mild assumptions on the frequency modulation function. ",
        "title": "Cycles in Impulsive Goodwin's Oscillators of Arbitrary Order",
        "date": "2023-02-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.01500",
        "abstract_url": "http://arxiv.org/abs/2302.01500",
        "authors": [
            {
                "last_name": "Suetake",
                "first_name": "Kazuma"
            },
            {
                "last_name": "Ushimaru",
                "first_name": "Takuya"
            },
            {
                "last_name": "Saiin",
                "first_name": "Ryuji"
            },
            {
                "last_name": "Sawada",
                "first_name": "Yoshihide"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Spiking neural networks (SNNs) are energy-efficient neural networks because of their spiking nature. However, as the spike firing rate of SNNs increases, the energy consumption does as well, and thus, the advantage of SNNs diminishes. Here, we tackle this problem by introducing a novel penalty term for the spiking activity into the objective function in the training phase. Our method is designed so as to optimize the energy consumption metric directly without modifying the network architecture. Therefore, the proposed method can reduce the energy consumption more than other methods while maintaining the accuracy. We conducted experiments for image classification tasks, and the results indicate the effectiveness of the proposed method, which mitigates the dilemma of the energy--accuracy trade-off. ",
        "title": "Spiking Synaptic Penalty: Appropriate Penalty Term for Energy-Efficient  Spiking Neural Networks",
        "date": "2023-02-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.03379",
        "abstract_url": "http://arxiv.org/abs/2302.03379",
        "authors": [
            {
                "last_name": "Balhorn",
                "first_name": "Lukas Schulze"
            },
            {
                "last_name": "Hirtreiter",
                "first_name": "Edwin"
            },
            {
                "last_name": "Luderer",
                "first_name": "Lynn"
            },
            {
                "last_name": "Schweidtmann",
                "first_name": "Artur M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Artificial intelligence has great potential for accelerating the design and engineering of chemical processes. Recently, we have shown that transformer-based language models can learn to auto-complete chemical process flowsheets using the SFILES 2.0 string notation. Also, we showed that language translation models can be used to translate Process Flow Diagrams (PFDs) into Process and Instrumentation Diagrams (P&IDs). However, artificial intelligence methods require big data and flowsheet data is currently limited. To mitigate this challenge of limited data, we propose a new data augmentation methodology for flowsheet data that is represented in the SFILES 2.0 notation. We show that the proposed data augmentation improves the performance of artificial intelligence-based process design models. In our case study flowsheet data augmentation improved the prediction uncertainty of the flowsheet autocompletion model by 14.7%. In the future, our flowsheet data augmentation can be used for other machine learning algorithms on chemical process flowsheets that are based on SFILES notation. ",
        "title": "Data augmentation for machine learning of chemical process flowsheets",
        "date": "2023-02-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.07875",
        "abstract_url": "http://arxiv.org/abs/2302.07875",
        "authors": [
            {
                "last_name": "Yamaguchi",
                "first_name": "Tomoya"
            },
            {
                "last_name": "Arai",
                "first_name": "Kohei"
            },
            {
                "last_name": "Niiyama",
                "first_name": "Tomoaki"
            },
            {
                "last_name": "Uchida",
                "first_name": "Atsushi"
            },
            {
                "last_name": "Sunada",
                "first_name": "Satoshi"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "NE"
        ],
        "abstract": "  High-speed machine vision is increasing its importance in both scientific and technological applications. Neuro-inspired photonic computing is a promising approach to speed-up machine vision processing with ultralow latency. However, the processing rate is fundamentally limited by the low frame rate of image sensors, typically operating at tens of hertz. Here, we propose an image-sensor-free machine vision framework, which optically processes real-world visual information with only a single input channel, based on a random temporal encoding technique. This approach allows for compressive acquisitions of visual information with a single channel at gigahertz rates, outperforming conventional approaches, and enables its direct photonic processing using a photonic reservoir computer in a time domain. We experimentally demonstrate that the proposed approach is capable of high-speed image recognition and anomaly detection, and furthermore, it can be used for high-speed imaging. The proposed approach is multipurpose and can be extended for a wide range of applications, including tracking, controlling, and capturing sub-nanosecond phenomena. ",
        "title": "Ultrafast single-channel machine vision based on neuro-inspired photonic  computing",
        "date": "2023-02-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.09813",
        "abstract_url": "http://arxiv.org/abs/2302.09813",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Juexiao"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Liao",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bin"
            },
            {
                "last_name": "He",
                "first_name": "Wenjia"
            },
            {
                "last_name": "Li",
                "first_name": "Zhongxiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Longxi"
            },
            {
                "last_name": "Gao",
                "first_name": "Xin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Revoking personal private data is one of the basic human rights, which has already been sheltered by several privacy-preserving laws in many countries. However, with the development of data science, machine learning and deep learning techniques, this right is usually neglected or violated as more and more patients' data are being collected and used for model training, especially in intelligent healthcare, thus making intelligent healthcare a sector where technology must meet the law, regulations, and privacy principles to ensure that the innovation is for the common good. In order to secure patients' right to be forgotten, we proposed a novel solution by using auditing to guide the forgetting process, where auditing means determining whether a dataset has been used to train the model and forgetting requires the information of a query dataset to be forgotten from the target model. We unified these two tasks by introducing a new approach called knowledge purification. To implement our solution, we developed AFS, a unified open-source software, which is able to evaluate and revoke patients' private data from pre-trained deep learning models. We demonstrated the generality of AFS by applying it to four tasks on different datasets with various data sizes and architectures of deep learning networks. The software is publicly available at \\url{https://github.com/JoshuaChou2018/AFS}. ",
        "title": "Audit to Forget: A Unified Method to Revoke Patients' Private Data in  Intelligent Healthcare",
        "date": "2023-02-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.11571",
        "abstract_url": "http://arxiv.org/abs/2302.11571",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Juexiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Longxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Li",
                "first_name": "Haoyang"
            },
            {
                "last_name": "Chu",
                "first_name": "Yuetan"
            },
            {
                "last_name": "Han",
                "first_name": "Wenkai"
            },
            {
                "last_name": "Gao",
                "first_name": "Xin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Heterogeneous data is endemic due to the use of diverse models and settings of devices by hospitals in the field of medical imaging. However, there are few open-source frameworks for federated heterogeneous medical image analysis with personalization and privacy protection simultaneously without the demand to modify the existing model structures or to share any private data. In this paper, we proposed PPPML-HMI, an open-source learning paradigm for personalized and privacy-preserving federated heterogeneous medical image analysis. To our best knowledge, personalization and privacy protection were achieved simultaneously for the first time under the federated scenario by integrating the PerFedAvg algorithm and designing our novel cyclic secure aggregation with the homomorphic encryption algorithm. To show the utility of PPPML-HMI, we applied it to a simulated classification task namely the classification of healthy people and patients from the RAD-ChestCT Dataset, and one real-world segmentation task namely the segmentation of lung infections from COVID-19 CT scans. For the real-world task, PPPML-HMI achieved $\\sim$5\\% higher Dice score on average compared to conventional FL under the heterogeneous scenario. Meanwhile, we applied the improved deep leakage from gradients to simulate adversarial attacks and showed the solid privacy-preserving capability of PPPML-HMI. By applying PPPML-HMI to both tasks with different neural networks, a varied number of users, and sample sizes, we further demonstrated the strong robustness of PPPML-HMI. ",
        "title": "Personalized and privacy-preserving federated heterogeneous medical  image analysis with PPPML-HMI",
        "date": "2023-02-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.02669",
        "abstract_url": "http://arxiv.org/abs/2303.02669",
        "authors": [
            {
                "last_name": "Ali",
                "first_name": "Hassan"
            },
            {
                "last_name": "Butt",
                "first_name": "Muhammad Atif"
            },
            {
                "last_name": "Filali",
                "first_name": "Fethi"
            },
            {
                "last_name": "Al-Fuqaha",
                "first_name": "Ala"
            },
            {
                "last_name": "Qadir",
                "first_name": "Junaid"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Recent works have shown that deep learning (DL) models can effectively learn city-wide crowd-flow patterns, which can be used for more effective urban planning and smart city management. However, DL models have been known to perform poorly on inconspicuous adversarial perturbations. Although many works have studied these adversarial perturbations in general, the adversarial vulnerabilities of deep crowd-flow prediction models in particular have remained largely unexplored. In this paper, we perform a rigorous analysis of the adversarial vulnerabilities of DL-based crowd-flow prediction models under multiple threat settings, making three-fold contributions. (1) We propose CaV-detect by formally identifying two novel properties - Consistency and Validity - of the crowd-flow prediction inputs that enable the detection of standard adversarial inputs with 0% false acceptance rate (FAR). (2) We leverage universal adversarial perturbations and an adaptive adversarial loss to present adaptive adversarial attacks to evade CaV-detect defense. (3) We propose CVPR, a Consistent, Valid and Physically-Realizable adversarial attack, that explicitly inducts the consistency and validity priors in the perturbation generation mechanism. We find out that although the crowd-flow models are vulnerable to adversarial perturbations, it is extremely challenging to simulate these perturbations in physical settings, notably when CaV-detect is in place. We also show that CVPR attack considerably outperforms the adaptively modified standard attacks in FAR and adversarial loss metrics. We conclude with useful insights emerging from our work and highlight promising future research directions. ",
        "title": "Consistent Valid Physically-Realizable Adversarial Attack against  Crowd-flow Prediction Models",
        "date": "2023-03-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.03825",
        "abstract_url": "http://arxiv.org/abs/2303.03825",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Kanghyun"
            },
            {
                "last_name": "Park",
                "first_name": "Daehyung"
            },
            {
                "last_name": "Kim",
                "first_name": "Min Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper presents a novel algorithm for robot task and motion planning (TAMP) problems by utilizing a reachability tree. While tree-based algorithms are known for their speed and simplicity in motion planning (MP), they are not well-suited for TAMP problems that involve both abstracted and geometrical state variables. To address this challenge, we propose a hierarchical sampling strategy, which first generates an abstracted task plan using Monte Carlo tree search (MCTS) and then fills in the details with a geometrically feasible motion trajectory. Moreover, we show that the performance of the proposed method can be significantly enhanced by selecting an appropriate reward for MCTS and by using a pre-generated goal state that is guaranteed to be geometrically feasible. A comparative study using TAMP benchmark problems demonstrates the effectiveness of the proposed approach. ",
        "title": "A Reachability Tree-Based Algorithm for Robot Task and Motion Planning",
        "date": "2023-03-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10567",
        "abstract_url": "http://arxiv.org/abs/2303.10567",
        "authors": [
            {
                "last_name": "Jeong",
                "first_name": "Jinyeong"
            },
            {
                "last_name": "Kim",
                "first_name": "Min Jun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a decentralized passive impedance control scheme for collaborative grasping using under-actuated aerial manipulators (AMs). The AM system is formulated, using a proper coordinate transformation, as an inertially decoupled dynamics with which a passivity-based control design is conducted. Since the interaction for grasping can be interpreted as a feedback interconnection of passive systems, an arbitrary number of AMs can be modularly combined, leading to a decentralized control scheme. Another interesting consequence of the passivity property is that the AMs automatically converge to a certain configuration to accomplish the grasping. Collaborative grasping using 10 AMs is presented in simulation. ",
        "title": "Passivity-based Decentralized Control for Collaborative Grasping of  Under-Actuated Aerial Manipulators",
        "date": "2023-03-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.10806",
        "abstract_url": "http://arxiv.org/abs/2303.10806",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xin-Yu"
            },
            {
                "last_name": "Hsieh",
                "first_name": "Chung-Han"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we extend the existing double linear policy by incorporating time-varying weights instead of constant weights and study a certain robustness property, called robust positive expectation (RPE), in a discrete-time setting. We prove that the RPE property holds by employing a novel elementary symmetric polynomials characterization approach and derive an explicit expression for both the expected cumulative gain-loss function and its variance. To validate our theory, we perform extensive Monte Carlo simulations using various weighting functions. Furthermore, we demonstrate how this policy can be effectively incorporated with standard technical analysis techniques, using the moving average as a trading signal. ",
        "title": "On Robustness of Double Linear Policy with Time-Varying Weights",
        "date": "2023-03-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.12806",
        "abstract_url": "http://arxiv.org/abs/2303.12806",
        "authors": [
            {
                "last_name": "Chanda",
                "first_name": "Tirtha"
            },
            {
                "last_name": "Hauser",
                "first_name": "Katja"
            },
            {
                "last_name": "Hobelsberger",
                "first_name": "Sarah"
            },
            {
                "last_name": "Bucher",
                "first_name": "Tabea-Clara"
            },
            {
                "last_name": "Garcia",
                "first_name": "Carina Nogueira"
            },
            {
                "last_name": "Wies",
                "first_name": "Christoph"
            },
            {
                "last_name": "Kittler",
                "first_name": "Harald"
            },
            {
                "last_name": "Tschandl",
                "first_name": "Philipp"
            },
            {
                "last_name": "Navarrete-Dechent",
                "first_name": "Cristian"
            },
            {
                "last_name": "Podlipnik",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Chousakos",
                "first_name": "Emmanouil"
            },
            {
                "last_name": "Crnaric",
                "first_name": "Iva"
            },
            {
                "last_name": "Majstorovic",
                "first_name": "Jovana"
            },
            {
                "last_name": "Alhajwan",
                "first_name": "Linda"
            },
            {
                "last_name": "Foreman",
                "first_name": "Tanya"
            },
            {
                "last_name": "Peternel",
                "first_name": "Sandra"
            },
            {
                "last_name": "Sarap",
                "first_name": "Sergei"
            },
            {
                "last_name": "\u00d6zdemir",
                "first_name": "\u0130rem"
            },
            {
                "last_name": "Barnhill",
                "first_name": "Raymond L."
            },
            {
                "last_name": "Velasco",
                "first_name": "Mar Llamas"
            },
            {
                "last_name": "Poch",
                "first_name": "Gabriela"
            },
            {
                "last_name": "Korsing",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Sondermann",
                "first_name": "Wiebke"
            },
            {
                "last_name": "Gellrich",
                "first_name": "Frank Friedrich"
            },
            {
                "last_name": "Heppt",
                "first_name": "Markus V."
            },
            {
                "last_name": "Erdmann",
                "first_name": "Michael"
            },
            {
                "last_name": "Haferkamp",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Drexler",
                "first_name": "Konstantin"
            },
            {
                "last_name": "Goebeler",
                "first_name": "Matthias"
            },
            {
                "last_name": "Schilling",
                "first_name": "Bastian"
            },
            {
                "last_name": "Utikal",
                "first_name": "Jochen S."
            },
            {
                "last_name": "Ghoreschi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Fr\u00f6hling",
                "first_name": "Stefan"
            },
            {
                "last_name": "Krieghoff-Henning",
                "first_name": "Eva"
            },
            {
                "last_name": "Brinker",
                "first_name": "Titus J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Although artificial intelligence (AI) systems have been shown to improve the accuracy of initial melanoma diagnosis, the lack of transparency in how these systems identify melanoma poses severe obstacles to user acceptance. Explainable artificial intelligence (XAI) methods can help to increase transparency, but most XAI methods are unable to produce precisely located domain-specific explanations, making the explanations difficult to interpret. Moreover, the impact of XAI methods on dermatologists has not yet been evaluated. Extending on two existing classifiers, we developed an XAI system that produces text and region based explanations that are easily interpretable by dermatologists alongside its differential diagnoses of melanomas and nevi. To evaluate this system, we conducted a three-part reader study to assess its impact on clinicians' diagnostic accuracy, confidence, and trust in the XAI-support. We showed that our XAI's explanations were highly aligned with clinicians' explanations and that both the clinicians' trust in the support system and their confidence in their diagnoses were significantly increased when using our XAI compared to using a conventional AI system. The clinicians' diagnostic accuracy was numerically, albeit not significantly, increased. This work demonstrates that clinicians are willing to adopt such an XAI system, motivating their future use in the clinic. ",
        "title": "Dermatologist-like explainable AI enhances trust and confidence in  diagnosing melanoma",
        "date": "2023-03-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.15228",
        "abstract_url": "http://arxiv.org/abs/2303.15228",
        "authors": [
            {
                "last_name": "Braghetto",
                "first_name": "Anna"
            },
            {
                "last_name": "Orlandini",
                "first_name": "Enzo"
            },
            {
                "last_name": "Baiesi",
                "first_name": "Marco"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Explainable and interpretable unsupervised machine learning helps understand the underlying structure of data. We introduce an ensemble analysis of machine learning models to consolidate their interpretation. Its application shows that restricted Boltzmann machines compress consistently into a few bits the information stored in a sequence of five amino acids at the start or end of $\\alpha$-helices or $\\beta$-sheets. The weights learned by the machines reveal unexpected properties of the amino acids and the secondary structure of proteins: (i) His and Thr have a negligible contribution to the amphiphilic pattern of $\\alpha$-helices; (ii) there is a class of $\\alpha$-helices particularly rich in Ala at their end; (iii) Pro occupies most often slots otherwise occupied by polar or charged amino acids, and its presence at the start of helices is relevant; (iv) Glu and especially Asp on one side, and Val, Leu, Iso, and Phe on the other, display the strongest tendency to mark amphiphilic patterns, i.e., extreme values of an \"effective hydrophobicity\", though they are not the most powerful (non) hydrophobic amino acids. ",
        "title": "Interpretable machine learning of amino acid patterns in proteins: a  statistical ensemble approach",
        "date": "2023-03-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2303.17551",
        "abstract_url": "http://arxiv.org/abs/2303.17551",
        "authors": [
            {
                "last_name": "Lechowicz",
                "first_name": "Adam"
            },
            {
                "last_name": "Christianson",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Zuo",
                "first_name": "Jinhang"
            },
            {
                "last_name": "Bashir",
                "first_name": "Noman"
            },
            {
                "last_name": "Hajiesmaili",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Wierman",
                "first_name": "Adam"
            },
            {
                "last_name": "Shenoy",
                "first_name": "Prashant"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DC"
        ],
        "abstract": "  We introduce and study the online pause and resume problem. In this problem, a player attempts to find the $k$ lowest (alternatively, highest) prices in a sequence of fixed length $T$, which is revealed sequentially. At each time step, the player is presented with a price and decides whether to accept or reject it. The player incurs a switching cost whenever their decision changes in consecutive time steps, i.e., whenever they pause or resume purchasing. This online problem is motivated by the goal of carbon-aware load shifting, where a workload may be paused during periods of high carbon intensity and resumed during periods of low carbon intensity and incurs a cost when saving or restoring its state. It has strong connections to existing problems studied in the literature on online optimization, though it introduces unique technical challenges that prevent the direct application of existing algorithms. Extending prior work on threshold-based algorithms, we introduce double-threshold algorithms for both the minimization and maximization variants of this problem. We further show that the competitive ratios achieved by these algorithms are the best achievable by any deterministic online algorithm. Finally, we empirically validate our proposed algorithm through case studies on the application of carbon-aware load shifting using real carbon trace data and existing baseline algorithms. ",
        "title": "The Online Pause and Resume Problem: Optimal Algorithms and An  Application to Carbon-Aware Load Shifting",
        "date": "2023-03-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.06624",
        "abstract_url": "http://arxiv.org/abs/2305.06624",
        "authors": [
            {
                "last_name": "Omanovi\u0107",
                "first_name": "Amra"
            },
            {
                "last_name": "Oblak",
                "first_name": "Polona"
            },
            {
                "last_name": "Curk",
                "first_name": "Toma\u017e"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Tropical semiring has proven successful in several research areas, including optimal control, bioinformatics, discrete event systems, or solving a decision problem. In previous studies, a matrix two-factorization algorithm based on the tropical semiring has been applied to investigate bipartite and tripartite networks. Tri-factorization algorithms based on standard linear algebra are used for solving tasks such as data fusion, co-clustering, matrix completion, community detection, and more. However, there is currently no tropical matrix tri-factorization approach, which would allow for the analysis of multipartite networks with a high number of parts. To address this, we propose the triFastSTMF algorithm, which performs tri-factorization over the tropical semiring. We apply it to analyze a four-partition network structure and recover the edge lengths of the network. We show that triFastSTMF performs similarly to Fast-NMTF in terms of approximation and prediction performance when fitted on the whole network. When trained on a specific subnetwork and used to predict the whole network, triFastSTMF outperforms Fast-NMTF by several orders of magnitude smaller error. The robustness of triFastSTMF is due to tropical operations, which are less prone to predict large values compared to standard operations. ",
        "title": "Matrix tri-factorization over the tropical semiring",
        "date": "2023-05-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.09082",
        "abstract_url": "http://arxiv.org/abs/2305.09082",
        "authors": [
            {
                "last_name": "Shin",
                "first_name": "Jiho"
            },
            {
                "last_name": "Wei",
                "first_name": "Moshi"
            },
            {
                "last_name": "Wang",
                "first_name": "Junjie"
            },
            {
                "last_name": "Shi",
                "first_name": "Lin"
            },
            {
                "last_name": "Wang",
                "first_name": "Song"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Machine learning (ML) has been increasingly used in a variety of domains, while solving ML programming tasks poses unique challenges because of the fundamentally different nature and construction from general programming tasks, especially for developers who do not have ML backgrounds. Automatic code generation that produces a code snippet from a natural language description can be a promising technique to accelerate ML programming tasks. In recent years, although many deep learning-based neural code generation models have been proposed with high accuracy, the fact that most of them are mainly evaluated on general programming tasks calls into question their effectiveness and usefulness in ML programming tasks. In this paper, we set out to investigate the effectiveness of existing neural code generation models on ML programming tasks. For our analysis, we select six state-of-the-art neural code generation models, and evaluate their performance on four widely used ML libraries, with newly-created 83K pairs of natural-language described ML programming tasks. Our empirical study reveals some good, bad, and missing aspects of neural code generation models on ML tasks, with a few major ones listed below. (Good) Neural code generation models perform significantly better on ML tasks than on non-ML tasks. (Bad) Most of the generated code is semantically incorrect. (Bad) Code generation models cannot significantly improve developers' completion time. (Good) The generated code can help developers write more correct code by providing developers with clues for using correct APIs. (Missing) The observation from our user study reveals the missing aspects of code generation for ML tasks, e.g., decomposing code generation for divide-and-conquer into two tasks: API sequence identification and API usage generation. ",
        "title": "The Good, the Bad, and the Missing: Neural Code Generation for Machine  Learning Tasks",
        "date": "2023-05-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11292",
        "abstract_url": "http://arxiv.org/abs/2305.11292",
        "authors": [
            {
                "last_name": "Vinod",
                "first_name": "Vivin"
            },
            {
                "last_name": "Maity",
                "first_name": "Sayan"
            },
            {
                "last_name": "Zaspel",
                "first_name": "Peter"
            },
            {
                "last_name": "Kleinekath\u00f6fer",
                "first_name": "Ulrich"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The accurate but fast calculation of molecular excited states is still a very challenging topic. For many applications, detailed knowledge of the energy funnel in larger molecular aggregates is of key importance requiring highly accurate excited state energies. To this end, machine learning techniques can be an extremely useful tool though the cost of generating highly accurate training datasets still remains a severe challenge. To overcome this hurdle, this work proposes the use of multi-fidelity machine learning where very little training data from high accuracies is combined with cheaper and less accurate data to achieve the accuracy of the costlier level. In the present study, the approach is employed to predict the first excited state energies for three molecules of increasing size, namely, benzene, naphthalene, and anthracene. The energies are trained and tested for conformations stemming from classical molecular dynamics simulations and from real-time density functional tight-binding calculations. It can be shown that the multi-fidelity machine learning model can achieve the same accuracy as a machine learning model built only on high cost training data while having a much lower computational effort to generate the data. The numerical gain observed in these benchmark test calculations was over a factor of 30 but certainly can be much higher for high accuracy data. ",
        "title": "Multi-Fidelity Machine Learning for Excited State Energies of Molecules",
        "date": "2023-05-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.15099",
        "abstract_url": "http://arxiv.org/abs/2305.15099",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Yang",
                "first_name": "Meng"
            },
            {
                "last_name": "Feng",
                "first_name": "Minwei"
            },
            {
                "last_name": "Yin",
                "first_name": "Jingcheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinbing"
            },
            {
                "last_name": "Leng",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhouhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. \\footnote{Our code is publicly available at \\url{https://github.com/LUMIA-Group/FourierTransformer}} ",
        "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence  Redundancy with FFT Operator",
        "date": "2023-05-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.17594",
        "abstract_url": "http://arxiv.org/abs/2305.17594",
        "authors": [
            {
                "last_name": "Bian",
                "first_name": "Sizhen"
            },
            {
                "last_name": "Rupp",
                "first_name": "Alexander"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years, working out in the gym has gotten increasingly more data-focused and many gym enthusiasts are recording their exercises to have a better overview of their historical gym activities and to make a better exercise plan for the future. As a side effect, this recording process has led to a lot of time spent painstakingly operating these apps by plugging in used types of equipment and repetitions. This project aims to automate this process using an Internet of Things (IoT) approach. Specifically, beacons with embedded ultra-low-power inertial measurement units (IMUs) are attached to the types of equipment to recognize the usage and transmit the information to gym-goers and managers. We have created a small ecosystem composed of beacons, a gateway, smartwatches, android/iPhone applications, a firebase cloud server, and a dashboard, all communicating over a mixture of Bluetooth and Wifi to distribute collected data from machines to users and gym managers in a compact and meaningful way. The system we have implemented is a working prototype of a bigger end goal and is supposed to initialize progress toward a smarter, more efficient, and still privacy-respect gym environment in the future. A small-scale real-life test shows 94.6\\% accuracy in user gym session recording, which can reach up to 100\\% easily with a more suitable assembling of the beacons. This promising result shows the potential of a fully automatic exercise recording system, which enables comprehensive monitoring and analysis of the exercise sessions and frees the user from manual recording. The estimated battery life of the beacon is 400 days with a 210 mAh coin battery. We also discussed the shortcoming of the current demonstration system and the future work for a reliable and ready-to-deploy automatic gym workout recording system. ",
        "title": "Fully Automatic Gym Exercises Recording: An IoT Solution",
        "date": "2023-05-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.19069",
        "abstract_url": "http://arxiv.org/abs/2305.19069",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yifu"
            },
            {
                "last_name": "Li",
                "first_name": "Hongru"
            },
            {
                "last_name": "Yang",
                "first_name": "Tao"
            },
            {
                "last_name": "Tao",
                "first_name": "Rui"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengyuan"
            },
            {
                "last_name": "Shi",
                "first_name": "Shimeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiansong"
            },
            {
                "last_name": "Ma",
                "first_name": "Ning"
            },
            {
                "last_name": "Feng",
                "first_name": "Wujin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhanhu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xinyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Lesion segmentation of ultrasound medical images based on deep learning techniques is a widely used method for diagnosing diseases. Although there is a large amount of ultrasound image data in medical centers and other places, labeled ultrasound datasets are a scarce resource, and it is likely that no datasets are available for new tissues/organs. Transfer learning provides the possibility to solve this problem, but there are too many features in natural images that are not related to the target domain. As a source domain, redundant features that are not conducive to the task will be extracted. Migration between ultrasound images can avoid this problem, but there are few types of public datasets, and it is difficult to find sufficiently similar source domains. Compared with natural images, ultrasound images have less information, and there are fewer transferable features between different ultrasound images, which may cause negative transfer. To this end, a multi-source adversarial transfer learning network for ultrasound image segmentation is proposed. Specifically, to address the lack of annotations, the idea of adversarial transfer learning is used to adaptively extract common features between a certain pair of source and target domains, which provides the possibility to utilize unlabeled ultrasound data. To alleviate the lack of knowledge in a single source domain, multi-source transfer learning is adopted to fuse knowledge from multiple source domains. In order to ensure the effectiveness of the fusion and maximize the use of precious data, a multi-source domain independent strategy is also proposed to improve the estimation of the target domain data distribution, which further increases the learning ability of the multi-source adversarial migration learning network in multiple domains. ",
        "title": "Multi-source adversarial transfer learning for ultrasound image  segmentation with limited similarity",
        "date": "2023-05-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.01820",
        "abstract_url": "http://arxiv.org/abs/2306.01820",
        "authors": [
            {
                "last_name": "Reviriego",
                "first_name": "Pedro"
            },
            {
                "last_name": "Wang",
                "first_name": "Ziheng"
            },
            {
                "last_name": "Alonso",
                "first_name": "Alvaro"
            },
            {
                "last_name": "Gao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Niknia",
                "first_name": "Farzad"
            },
            {
                "last_name": "Liu",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Lombardi",
                "first_name": "Fabrizio"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The complexity of Machine Learning (ML) systems increases each year, with current implementations of large language models or text-to-image generators having billions of parameters and requiring billions of arithmetic operations. As these systems are widely utilized, ensuring their reliable operation is becoming a design requirement. Traditional error detection mechanisms introduce circuit or time redundancy that significantly impacts system performance. An alternative is the use of Concurrent Error Detection (CED) schemes that operate in parallel with the system and exploit their properties to detect errors. CED is attractive for large ML systems because it can potentially reduce the cost of error detection. In this paper, we introduce Concurrent Classifier Error Detection (CCED), a scheme to implement CED in ML systems using a concurrent ML classifier to detect errors. CCED identifies a set of check signals in the main ML system and feeds them to the concurrent ML classifier that is trained to detect errors. The proposed CCED scheme has been implemented and evaluated on two widely used large-scale ML models: Contrastive Language Image Pretraining (CLIP) used for image classification and Bidirectional Encoder Representations from Transformers (BERT) used for natural language applications. The results show that more than 95 percent of the errors are detected when using a simple Random Forest classifier that is order of magnitude simpler than CLIP or BERT. These results illustrate the potential of CCED to implement error detection in large-scale ML models. ",
        "title": "Concurrent Classifier Error Detection (CCED) in Large Scale Machine  Learning Systems",
        "date": "2023-06-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.11447",
        "abstract_url": "http://arxiv.org/abs/2306.11447",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Feiyang"
            },
            {
                "last_name": "\u00d8stvold",
                "first_name": "Bjarte M."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  The rise of mobile apps has brought greater convenience and many options for users. However, many apps use analytics services to collect a wide range of user interaction data, with privacy policies often failing to reveal the types of interaction data collected or the extent of the data collection practices. This lack of transparency potentially breaches data protection laws and also undermines user trust. We conducted an analysis of the top 20 analytic libraries for Android apps to identify common practices of interaction data collection and used this information to develop a standardized collection claim template for summarizing an app's data collection practices wrt. user interaction data. We selected the top 100 apps from popular categories on Google Play and used automatic static analysis to extract collection evidence from their data collection implementations. Our analysis found that a significant majority of these apps actively collected interaction data from UI types such as View (89%), Button (76%), and Textfield (63%), highlighting the pervasiveness of user interaction data collection. By comparing the collection evidence to the claims derived from privacy policy analysis, we manually fact-checked the completeness and accuracy of these claims for the top 10 apps. We found that, except for one app, they all failed to declare all types of interaction data they collect and did not specify some of the collection techniques used. ",
        "title": "Transparency in App Analytics: Analyzing the Collection of User  Interaction Data",
        "date": "2023-06-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.15327",
        "abstract_url": "http://arxiv.org/abs/2306.15327",
        "authors": [
            {
                "last_name": "Landi",
                "first_name": "Leonardo"
            },
            {
                "last_name": "Timpanella",
                "first_name": "Marco"
            },
            {
                "last_name": "Vicino",
                "first_name": "Lara"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we investigate two-point Algebraic Geometry codes associated to the Skabelund maximal curve constructed as a cyclic cover of the Suzuki curve. In order to estimate the minimum distance of such codes, we make use of the generalized order bound introduced by P. Beelen and determine certain two-point Weierstrass semigroups of the curve. ",
        "title": "Two-point AG codes from one of the Skabelund maximal curves",
        "date": "2023-06-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.04154",
        "abstract_url": "http://arxiv.org/abs/2307.04154",
        "authors": [
            {
                "last_name": "Carpio",
                "first_name": "Ana"
            },
            {
                "last_name": "Duro",
                "first_name": "Gema"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Two phase solid-fluid mixture models are ubiquitous in biological applications. For instance, models for growth of tissues and biofilms combine time dependent and quasi-stationary boundary value problems set in domains whose boundary moves in response to variations in the mechano-chemical variables. For a model of biofilm spread, we show how to obtain better posed models by characterizing the time derivatives of relevant quasi-stationary magnitudes in terms of additional boundary value problems. We also give conditions for well posedness of time dependent submodels set in moving domains depending on the motion of the boundary. After constructing solutions for transport, diffusion and elliptic submodels for volume fractions, displacements, velocities, pressures and concentrations with the required regularity, we are able to handle the full model of biofilm spread in moving domains assuming we know the dynamics of the boundary. These techniques are general and can be applied in models with a similar structure arising in biological and chemical engineering applications. ",
        "title": "Well posedness of fluid/solid mixture models for biofilm spread",
        "date": "2023-07-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2307.08318",
        "abstract_url": "http://arxiv.org/abs/2307.08318",
        "authors": [
            {
                "last_name": "Keuth",
                "first_name": "Ron"
            },
            {
                "last_name": "Heinrich",
                "first_name": "Mattias"
            },
            {
                "last_name": "Eichenlaub",
                "first_name": "Martin"
            },
            {
                "last_name": "Himstedt",
                "first_name": "Marian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Purpose: Navigation guidance is a key requirement for a multitude of lung interventions using video bronchoscopy. State-of-the-art solutions focus on lung biopsies using electromagnetic tracking and intraoperative image registration w.r.t. preoperative CT scans for guidance. The requirement of patient-specific CT scans hampers the utilisation of navigation guidance for other applications such as intensive care units.   Methods: This paper addresses navigation guidance solely incorporating bronchosopy video data. In contrast to state-of-the-art approaches we entirely omit the use of electromagnetic tracking and patient-specific CT scans. Guidance is enabled by means of topological bronchoscope localization w.r.t. an interpatient airway model. Particularly, we take maximally advantage of anatomical constraints of airway trees being sequentially traversed. This is realized by incorporating sequences of CNN-based airway likelihoods into a Hidden Markov Model.   Results: Our approach is evaluated based on multiple experiments inside a lung phantom model. With the consideration of temporal context and use of anatomical knowledge for regularization, we are able to improve the accuracy up to to 0.98 compared to 0.81 (weighted F1: 0.98 compared to 0.81) for a classification based on individual frames.   Conclusion: We combine CNN-based single image classification of airway segments with anatomical constraints and temporal HMM-based inference for the first time. Our approach renders vision-only guidance for bronchoscopy interventions in the absence of electromagnetic tracking and patient-specific CT scans possible. ",
        "title": "Airway Label Prediction in Video Bronchoscopy: Capturing Temporal  Dependencies Utilizing Anatomical Knowledge",
        "date": "2023-07-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.01675",
        "abstract_url": "http://arxiv.org/abs/2308.01675",
        "authors": [
            {
                "last_name": "Sohlbach",
                "first_name": "Lukas"
            },
            {
                "last_name": "Hobbani",
                "first_name": "Hamza"
            },
            {
                "last_name": "Blase",
                "first_name": "Chistopher"
            },
            {
                "last_name": "Perez-Pe\u00f1a",
                "first_name": "Fernando"
            },
            {
                "last_name": "Schmidt",
                "first_name": "Karsten"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In order to fully harness the potential of dielectric elastomer actu-ators (DEAs) in soft robots, advanced control methods are need-ed. An important groundwork for this is the development of a control-oriented model that can adequately describe the underly-ing dynamics of a DEA. A common feature of existing models is that always custom-made DEAs were investigated. This makes the modelling process easier, as all specifications and the struc-ture of the actuator are well known. In the case of a commercial actuator, however, only the information from the manufacturer is available and must be checked or completed during the modelling process. The aim of this paper is to explore how a commercial stacked silicone-based DEA can be modelled and how complex the model should be to properly replicate the features of the actu-ator. The static description has demonstrated the suitability of Hooke's law. In the case of dynamic description, it is shown that no viscoelastic model is needed for control-oriented modelling. However, if all features of the DEA are considered, the general-ized Kelvin-Maxwell model with three Maxwell elements shows good results, stability and computational efficiency. ",
        "title": "Modelling and simulation of a commercially available dielectric  elastomer actuator",
        "date": "2023-08-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.03299",
        "abstract_url": "http://arxiv.org/abs/2308.03299",
        "authors": [
            {
                "last_name": "Gaba",
                "first_name": "Aimen"
            },
            {
                "last_name": "Kaufman",
                "first_name": "Zhanna"
            },
            {
                "last_name": "Chueng",
                "first_name": "Jason"
            },
            {
                "last_name": "Shvakel",
                "first_name": "Marie"
            },
            {
                "last_name": "Hall",
                "first_name": "Kyle Wm."
            },
            {
                "last_name": "Brun",
                "first_name": "Yuriy"
            },
            {
                "last_name": "Bearfield",
                "first_name": "Cindy Xiong"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer \"Can visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?\" Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning. ",
        "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and  Perceived Bias in Machine Learning",
        "date": "2023-08-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.06085",
        "abstract_url": "http://arxiv.org/abs/2308.06085",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jinqiang"
            },
            {
                "last_name": "Dwarka",
                "first_name": "Vandana"
            },
            {
                "last_name": "Vuik",
                "first_name": "Cornelis"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Helmholtz equation is related to seismic exploration, sonar, antennas, and medical imaging applications. It is one of the most challenging problems to solve in terms of accuracy and convergence due to the scalability issues of the numerical solvers. For 3D large-scale applications, high-performance parallel solvers are also needed. In this paper, a matrix-free parallel iterative solver is presented for the three-dimensional (3D) heterogeneous Helmholtz equation. We consider the preconditioned Krylov subspace methods for solving the linear system obtained from finite-difference discretization. The Complex Shifted Laplace Preconditioner (CSLP) is employed since it results in a linear increase in the number of iterations as a function of the wavenumber. The preconditioner is approximately inverted using one parallel 3D multigrid cycle. For parallel computing, the global domain is partitioned blockwise. The matrix-vector multiplication and preconditioning operator are implemented in a matrix-free way instead of constructing large, memory-consuming coefficient matrices. Numerical experiments of 3D model problems demonstrate the robustness and outstanding strong scaling of our matrix-free parallel solution method. Moreover, the weak parallel scalability indicates our approach is suitable for realistic 3D heterogeneous Helmholtz problems with minimized pollution error. ",
        "title": "A matrix-free parallel solution method for the three-dimensional  heterogeneous Helmholtz equation",
        "date": "2023-08-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.16562",
        "abstract_url": "http://arxiv.org/abs/2308.16562",
        "authors": [
            {
                "last_name": "Rigaki",
                "first_name": "Maria"
            },
            {
                "last_name": "Garcia",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future. ",
        "title": "The Power of MEME: Adversarial Malware Creation with Model-Based  Reinforcement Learning",
        "date": "2023-08-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.04631",
        "abstract_url": "http://arxiv.org/abs/2309.04631",
        "authors": [
            {
                "last_name": "Kaczmarzyk",
                "first_name": "Jakub R."
            },
            {
                "last_name": "O'Callaghan",
                "first_name": "Alan"
            },
            {
                "last_name": "Inglis",
                "first_name": "Fiona"
            },
            {
                "last_name": "Kurc",
                "first_name": "Tahsin"
            },
            {
                "last_name": "Gupta",
                "first_name": "Rajarsi"
            },
            {
                "last_name": "Bremer",
                "first_name": "Erich"
            },
            {
                "last_name": "Bankhead",
                "first_name": "Peter"
            },
            {
                "last_name": "Saltz",
                "first_name": "Joel H."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The field of digital pathology has seen a proliferation of deep learning models in recent years. Despite substantial progress, it remains rare for other researchers and pathologists to be able to access models published in the literature and apply them to their own images. This is due to difficulties in both sharing and running models. To address these concerns, we introduce WSInfer: a new, open-source software ecosystem designed to make deep learning for pathology more streamlined and accessible. WSInfer comprises three main elements: 1) a Python package and command line tool to efficiently apply patch-based deep learning inference to whole slide images; 2) a QuPath extension that provides an alternative inference engine through user-friendly and interactive software, and 3) a model zoo, which enables pathology models and metadata to be easily shared in a standardized form. Together, these contributions aim to encourage wider reuse, exploration, and interrogation of deep learning models for research purposes, by putting them into the hands of pathologists and eliminating a need for coding experience when accessed through QuPath. The WSInfer source code is hosted on GitHub and documentation is available at https://wsinfer.readthedocs.io. ",
        "title": "Open and reusable deep learning for pathology with WSInfer and QuPath",
        "date": "2023-09-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.11456",
        "abstract_url": "http://arxiv.org/abs/2309.11456",
        "authors": [
            {
                "last_name": "Ghaffarzadegan",
                "first_name": "Navid"
            },
            {
                "last_name": "Majumdar",
                "first_name": "Aritra"
            },
            {
                "last_name": "Williams",
                "first_name": "Ross"
            },
            {
                "last_name": "Hosseinichimeh",
                "first_name": "Niyousha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "MA"
        ],
        "abstract": "  We discuss the emerging new opportunity for building feedback-rich computational models of social systems using generative artificial intelligence. Referred to as Generative Agent-Based Models (GABMs), such individual-level models utilize large language models such as ChatGPT to represent human decision-making in social settings. We provide a GABM case in which human behavior can be incorporated in simulation models by coupling a mechanistic model of human interactions with a pre-trained large language model. This is achieved by introducing a simple GABM of social norm diffusion in an organization. For educational purposes, the model is intentionally kept simple. We examine a wide range of scenarios and the sensitivity of the results to several changes in the prompt. We hope the article and the model serve as a guide for building useful diffusion models that include realistic human reasoning and decision-making. ",
        "title": "Generative Agent-Based Modeling: Unveiling Social System Dynamics  through Coupling Mechanistic Models with Generative Artificial Intelligence",
        "date": "2023-09-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.02045",
        "abstract_url": "http://arxiv.org/abs/2310.02045",
        "authors": [
            {
                "last_name": "Rogenmoser",
                "first_name": "Michael"
            },
            {
                "last_name": "Benini",
                "first_name": "Luca"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  One of the key challenges when operating microcontrollers in harsh environments such as space is radiation-induced Single Event Upsets (SEUs), which can lead to errors in computation. Common countermeasures rely on proprietary radiation-hardened technologies, low density technologies, or extensive replication, leading to high costs and low performance and efficiency. To combat this, we present Trikarenos, a fault-tolerant 32-bit RISC-V microcontroller SoC in an advanced TSMC 28nm technology. Trikarenos alleviates the replication cost by employing a configurable triple-core lockstep configuration, allowing three Ibex cores to execute applications reliably, operating on ECC-protected memory. If reliability is not needed for a given application, the cores can operate independently in parallel for higher performance and efficiency. Trikarenos consumes 15.7mW at 250MHz executing a fault-tolerant matrix-matrix multiplication, a 21.5x efficiency gain over state-of-the-art, and performance is increased by 2.96x when reliability is not needed for processing, with a 2.36x increase in energy efficiency. ",
        "title": "Trikarenos: A Fault-Tolerant RISC-V-based Microcontroller for CubeSats  in 28nm",
        "date": "2023-10-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2310.13320",
        "abstract_url": "http://arxiv.org/abs/2310.13320",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shaoan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Mingzhu"
            },
            {
                "last_name": "Hu",
                "first_name": "Yaoqing"
            },
            {
                "last_name": "Li",
                "first_name": "Dongyue"
            },
            {
                "last_name": "Yuan",
                "first_name": "Fusong"
            },
            {
                "last_name": "Yu",
                "first_name": "Junzhi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "RO"
        ],
        "abstract": "  High-precision pose estimation based on visual markers has been a thriving research topic in the field of computer vision. However, the suitability of traditional flat markers on curved objects is limited due to the diverse shapes of curved surfaces, which hinders the development of high-precision pose estimation for curved objects. Therefore, this paper proposes a novel visual marker called CylinderTag, which is designed for developable curved surfaces such as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmly attached to objects with a cylindrical shape. Leveraging the manifold assumption, the cross-ratio in projective invariance is utilized for encoding in the direction of zero curvature on the surface. Additionally, to facilitate the usage of CylinderTag, we propose a heuristic search-based marker generator and a high-performance recognizer as well. Moreover, an all-encompassing evaluation of CylinderTag properties is conducted by means of extensive experimentation, covering detection rate, detection speed, dictionary size, localization jitter, and pose estimation accuracy. CylinderTag showcases superior detection performance from varying view angles in comparison to traditional visual markers, accompanied by higher localization accuracy. Furthermore, CylinderTag boasts real-time detection capability and an extensive marker dictionary, offering enhanced versatility and practicality in a wide range of applications. Experimental results demonstrate that the CylinderTag is a highly promising visual marker for use on cylindrical-like surfaces, thus offering important guidance for future research on high-precision visual localization of cylinder-shaped objects. The code is available at: https://github.com/wsakobe/CylinderTag. ",
        "title": "CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects  Pose Estimation Based on Projective Invariants",
        "date": "2023-10-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.01773",
        "abstract_url": "http://arxiv.org/abs/2311.01773",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Yin",
                "first_name": "Fukun"
            },
            {
                "last_name": "Fan",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Li",
                "first_name": "Hui"
            },
            {
                "last_name": "Chen",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Wen"
            },
            {
                "last_name": "Lu",
                "first_name": "Chongshan"
            },
            {
                "last_name": "YU",
                "first_name": "Gang"
            },
            {
                "last_name": "Chen",
                "first_name": "Tao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advances in implicit neural representations have achieved impressive results by sampling and fusing individual points along sampling rays in the sampling space. However, due to the explosively growing sampling space, finely representing and synthesizing detailed textures remains a challenge for unbounded large-scale outdoor scenes. To alleviate the dilemma of using individual points to perceive the entire colossal space, we explore learning the surface distribution of the scene to provide structural priors and reduce the samplable space and propose a Point Diffusion implicit Function, PDF, for large-scale scene neural representation. The core of our method is a large-scale point cloud super-resolution diffusion module that enhances the sparse point cloud reconstructed from several training images into a dense point cloud as an explicit prior. Then in the rendering stage, only sampling points with prior points within the sampling radius are retained. That is, the sampling space is reduced from the unbounded space to the scene surface. Meanwhile, to fill in the background of the scene that cannot be provided by point clouds, the region sampling based on Mip-NeRF 360 is employed to model the background representation. Expensive experiments have demonstrated the effectiveness of our method for large-scale scene novel view synthesis, which outperforms relevant state-of-the-art baselines. ",
        "title": "PDF: Point Diffusion Implicit Function for Large-scale Scene Neural  Representation",
        "date": "2023-11-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.04128",
        "abstract_url": "http://arxiv.org/abs/2311.04128",
        "authors": [
            {
                "last_name": "Gilpin",
                "first_name": "William"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Modern generative machine learning models demonstrate surprising ability to create realistic outputs far beyond their training data, such as photorealistic artwork, accurate protein structures, or conversational text. These successes suggest that generative models learn to effectively parametrize and sample arbitrarily complex distributions. Beginning half a century ago, foundational works in nonlinear dynamics used tools from information theory to infer properties of chaotic attractors from time series, motivating the development of algorithms for parametrizing chaos in real datasets. In this perspective, we aim to connect these classical works to emerging themes in large-scale generative statistical learning. We first consider classical attractor reconstruction, which mirrors constraints on latent representations learned by state space models of time series. We next revisit early efforts to use symbolic approximations to compare minimal discrete generators underlying complex processes, a problem relevant to modern efforts to distill and interpret black-box statistical models. Emerging interdisciplinary works bridge nonlinear dynamics and learning theory, such as operator-theoretic methods for complex fluid flows, or detection of broken detailed balance in biological datasets. We anticipate that future machine learning techniques may revisit other classical concepts from nonlinear dynamics, such as transinformation decay and complexity-entropy tradeoffs. ",
        "title": "Generative learning for nonlinear dynamics",
        "date": "2023-11-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.10653",
        "abstract_url": "http://arxiv.org/abs/2311.10653",
        "authors": [
            {
                "last_name": "Keyvanian",
                "first_name": "Shafagh"
            },
            {
                "last_name": "Johnson",
                "first_name": "Michelle J."
            },
            {
                "last_name": "Figueroa",
                "first_name": "Nadia"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  A realistic human kinematic model that satisfies anatomical constraints is essential for human-robot interaction, biomechanics and robot-assisted rehabilitation. Modeling realistic joint constraints, however, is challenging as human arm motion is constrained by joint limits, inter- and intra-joint dependencies, self-collisions, individual capabilities and muscular or neurological constraints which are difficult to represent. Hence, physicians and researchers have relied on simple box-constraints, ignoring important anatomical factors. In this paper, we propose a data-driven method to learn realistic anatomically constrained upper-limb range of motion (RoM) boundaries from motion capture data. This is achieved by fitting a one-class support vector machine to a dataset of upper-limb joint space exploration motions with an efficient hyper-parameter tuning scheme. Our approach outperforms similar works focused on valid RoM learning. Further, we propose an impairment index (II) metric that offers a quantitative assessment of capability/impairment when comparing healthy and impaired arms. We validate the metric on healthy subjects physically constrained to emulate hemiplegia and different disability levels as stroke patients. ",
        "title": "Learning Realistic Joint Space Boundaries for Range of Motion Analysis  of Healthy and Impaired Human Arms",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.13123",
        "abstract_url": "http://arxiv.org/abs/2311.13123",
        "authors": [
            {
                "last_name": "Cervenjak",
                "first_name": "Philip"
            },
            {
                "last_name": "Gan",
                "first_name": "Junhao"
            },
            {
                "last_name": "Wirth",
                "first_name": "Anthony"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Maximizing a non-negative, monontone, submodular function $f$ over $n$ elements under a cardinality constraint $k$ (SMCC) is a well-studied NP-hard problem. It has important applications in, e.g., machine learning and influence maximization. Though the theoretical problem admits polynomial-time approximation algorithms, solving it in practice often involves frequently querying submodular functions that are expensive to compute. This has motivated significant research into designing parallel approximation algorithms in the adaptive complexity model; adaptive complexity (adaptivity) measures the number of sequential rounds of $\\text{poly}(n)$ function queries an algorithm requires. The state-of-the-art algorithms can achieve $(1-\\frac{1}{e}-\\varepsilon)$-approximate solutions with $O(\\frac{1}{\\varepsilon^2}\\log n)$ adaptivity, which approaches the known adaptivity lower-bounds. However, the $O(\\frac{1}{\\varepsilon^2} \\log n)$ adaptivity only applies to maximizing worst-case functions that are unlikely to appear in practice. Thus, in this paper, we consider the special class of $p$-superseparable submodular functions, which places a reasonable constraint on $f$, based on the parameter $p$, and is more amenable to maximization, while also having real-world applicability. Our main contribution is the algorithm LS+GS, a finer-grained version of the existing LS+PGB algorithm, designed for instances of SMCC when $f$ is $p$-superseparable; it achieves an expected $(1-\\frac{1}{e}-\\varepsilon)$-approximate solution with $O(\\frac{1}{\\varepsilon^2}\\log(p k))$ adaptivity independent of $n$. Additionally, unrelated to $p$-superseparability, our LS+GS algorithm uses only $O(\\frac{n}{\\varepsilon} + \\frac{\\log n}{\\varepsilon^2})$ oracle queries, which has an improved dependence on $\\varepsilon^{-1}$ over the state-of-the-art LS+PGB; this is achieved through the design of a novel thresholding subroutine. ",
        "title": "Fast Parallel Algorithms for Submodular $p$-Superseparable Maximization",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2311.17816",
        "abstract_url": "http://arxiv.org/abs/2311.17816",
        "authors": [
            {
                "last_name": "Holland",
                "first_name": "Kieran"
            },
            {
                "last_name": "Ipp",
                "first_name": "Andreas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "David I."
            },
            {
                "last_name": "Wenger",
                "first_name": "Urs"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Lattice gauge-equivariant convolutional neural networks (L-CNNs) can be used to form arbitrarily shaped Wilson loops and can approximate any gauge-covariant or gauge-invariant function on the lattice. Here we use L-CNNs to describe fixed point (FP) actions which are based on renormalization group transformations. FP actions are classically perfect, i.e., they have no lattice artifacts on classical gauge-field configurations satisfying the equations of motion, and therefore possess scale invariant instanton solutions. FP actions are tree-level Symanzik-improved to all orders in the lattice spacing and can produce physical predictions with very small lattice artifacts even on coarse lattices. We find that L-CNNs are much more accurate at parametrizing the FP action compared to older approaches. They may therefore provide a way to circumvent critical slowing down and topological freezing towards the continuum limit. ",
        "title": "Fixed point actions from convolutional neural networks",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.14129",
        "abstract_url": "http://arxiv.org/abs/2312.14129",
        "authors": [
            {
                "last_name": "Choi",
                "first_name": "Dongjin"
            },
            {
                "last_name": "Xiang",
                "first_name": "Andy"
            },
            {
                "last_name": "Ozturk",
                "first_name": "Ozgur"
            },
            {
                "last_name": "Shrestha",
                "first_name": "Deep"
            },
            {
                "last_name": "Drake",
                "first_name": "Barry"
            },
            {
                "last_name": "Haidarian",
                "first_name": "Hamid"
            },
            {
                "last_name": "Javed",
                "first_name": "Faizan"
            },
            {
                "last_name": "Park",
                "first_name": "Haesun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's effectiveness. It produces better results compared to other existing methods in classification performance, yields meaningful clustering of patients, and delivers consistent results in patient similarity searches and predictions. ",
        "title": "WellFactor: Patient Profiling using Integrative Embedding of Healthcare  Data",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.16253",
        "abstract_url": "http://arxiv.org/abs/2312.16253",
        "authors": [
            {
                "last_name": "Albouy",
                "first_name": "Timoth\u00e9"
            },
            {
                "last_name": "Frey",
                "first_name": "Davide"
            },
            {
                "last_name": "Gelles",
                "first_name": "Ran"
            },
            {
                "last_name": "Hazay",
                "first_name": "Carmit"
            },
            {
                "last_name": "Raynal",
                "first_name": "Michel"
            },
            {
                "last_name": "Schiller",
                "first_name": "Elad Michael"
            },
            {
                "last_name": "Taiani",
                "first_name": "Francois"
            },
            {
                "last_name": "Zikas",
                "first_name": "Vassilis"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We address the problem of Reliable Broadcast in asynchronous message-passing systems with $n$ nodes, of which up to $t$ are malicious (faulty), in addition to a message adversary that can drop some of the messages sent by correct (non-faulty) nodes.   We present a Message-Adversary-Tolerant Byzantine Reliable Broadcast (MBRB) algorithm that communicates an almost optimal amount of $O(|m|+n^2\\kappa)$ bits per node, where $|m|$ represents the length of the application message and $\\kappa=\\Omega(\\log n)$ is a security parameter. This improves upon the state-of-the-art MBRB solution (Albouy, Frey, Raynal, and Ta\\\"iani, SSS 2021), which incurs communication of $O(n|m|+n^2\\kappa )$ bits per node.   Our solution sends at most $4n^2$ messages overall, which is asymptotically optimal. Reduced communication is achieved by employing coding techniques that replace the need for all nodes to (re-)broadcast the entire message~$m$. Instead, nodes forward authenticated fragments of the encoding of $m$ using an erasure-correcting code. Under the cryptographic assumptions of PKI and collision-resistant hash, and assuming $n > 3t + 2d$, where the adversary drops at most~$d$ messages per broadcast, our algorithm allows most of the correct nodes to reconstruct~$m$, despite missing fragments caused by the malicious nodes and the message adversary. ",
        "title": "Towards Optimal Communication Byzantine Reliable Broadcast under a  Message Adversary",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01738",
        "abstract_url": "http://arxiv.org/abs/2401.01738",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Ruoyu"
            },
            {
                "last_name": "Cheng",
                "first_name": "Lei"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Lou",
                "first_name": "Yi"
            },
            {
                "last_name": "Gao",
                "first_name": "Yulong"
            },
            {
                "last_name": "Wu",
                "first_name": "Wen"
            },
            {
                "last_name": "Ng",
                "first_name": "Derrick Wing Kwan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Benefitting from the vast spatial degrees of freedom, the amalgamation of integrated sensing and communication (ISAC) and massive multiple-input multiple-output (MIMO) is expected to simultaneously improve spectral and energy efficiencies as well as the sensing capability. However, a large number of antennas deployed in massive MIMO-ISAC raises critical challenges in acquiring both accurate channel state information and target parameter information. To overcome these two challenges with a unified framework, we first analyze their underlying system models and then propose a novel tensor-based approach that addresses both the channel estimation and target sensing problems. Specifically, by parameterizing the high-dimensional communication channel exploiting a small number of physical parameters, we associate the channel state information with the sensing parameters of targets in terms of angular, delay, and Doppler dimensions. Then, we propose a shared training pattern adopting the same time-frequency resources such that both the channel estimation and target parameter estimation can be formulated as a canonical polyadic decomposition problem with a similar mathematical expression. On this basis, we first investigate the uniqueness condition of the tensor factorization and the maximum number of resolvable targets by utilizing the specific Vandermonde ",
        "title": "Integrated Sensing and Communication with Massive MIMO: A Unified Tensor  Approach for Channel and Target Parameter Estimation",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01739",
        "abstract_url": "http://arxiv.org/abs/2401.01739",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinran"
            },
            {
                "last_name": "Lu",
                "first_name": "Qiujie"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongmyoung"
            },
            {
                "last_name": "Gan",
                "first_name": "Zhongxue"
            },
            {
                "last_name": "Rojas",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper introduces a new type of soft continuum robot, called SCoReS, which is capable of self-controlling continuously its curvature at the segment level; in contrast to previous designs which either require external forces or machine elements, or whose variable curvature capabilities are discrete -- depending on the number of locking mechanisms and segments. The ability to have a variable curvature, whose control is continuous and independent from external factors, makes a soft continuum robot more adaptive in constrained environments, similar to what is observed in nature in the elephant's trunk or ostrich's neck for instance which exhibit multiple curvatures. To this end, our soft continuum robot enables reconfigurable variable curvatures utilizing a variable stiffness growing spine based on micro-particle granular jamming for the first time. We detail the design of the proposed robot, presenting its modeling through beam theory and FEA simulation -- which is validated through experiments. The robot's versatile bending profiles are then explored in experiments and an application to grasp fruits at different configurations is demonstrated. ",
        "title": "A Soft Continuum Robot with Self-Controllable Variable Curvature",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01753",
        "abstract_url": "http://arxiv.org/abs/2401.01753",
        "authors": [
            {
                "last_name": "Vaidya",
                "first_name": "Amal"
            },
            {
                "last_name": "Vankayalapati",
                "first_name": "Mohan Krishna"
            },
            {
                "last_name": "Chan",
                "first_name": "Jacky"
            },
            {
                "last_name": "Ibraimoski",
                "first_name": "Senad"
            },
            {
                "last_name": "Moran",
                "first_name": "Sean"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a tool that leverages generative AI to accelerate the migration of on-premises applications to the cloud. The Cloud Migration LLM accepts input from the user specifying the parameters of their migration, and outputs a migration strategy with an architecture diagram. A user study suggests that the migration LLM can assist inexperienced users in finding the right cloud migration profile, while avoiding complexities of a manual approach. ",
        "title": "A Generative AI Assistant to Accelerate Cloud Migration",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.02899",
        "abstract_url": "http://arxiv.org/abs/2401.02899",
        "authors": [
            {
                "last_name": "Lan\u010da",
                "first_name": "Luka"
            },
            {
                "last_name": "Jakac",
                "first_name": "Karlo"
            },
            {
                "last_name": "Ivi\u0107",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA"
        ],
        "abstract": "  This research addresses the challenge of executing multi-UAV survey missions over diverse terrains characterized by varying elevations. The approach integrates advanced two-dimensional ergodic search technique with model predictive control of UAV altitude and velocity. Optimization of altitude and velocity is performed along anticipated UAV ground routes, considering multiple objectives and constraints. This yields a flight regimen tailored to the terrain, as well as the motion and sensing characteristics of the UAVs. The proposed UAV motion control strategy is assessed through simulations of realistic search missions and actual terrain models. Results demonstrate the successful integration of model predictive altitude and velocity control with a two-dimensional potential field-guided ergodic search. Adjusting UAV altitudes to near-ideal levels facilitates the utilization of sensing ranges, thereby enhancing the effectiveness of the search. Furthermore, the control algorithm is capable of real-time computation, encouraging its practical application in real-world scenarios. ",
        "title": "Model predictive altitude and velocity control in ergodic potential  field directed multi-UAV search",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03231",
        "abstract_url": "http://arxiv.org/abs/2401.03231",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Seongbeom"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  Many countries around the world, including Korea, use the school choice lottery system. However, this method has a problem in that many students are assigned to less-preferred schools based on the lottery results. In addition, the task of finding a good assignment with ties often has a time complexity of NP, making it a very difficult problem to improve the quality of the assignment.   In this paper, we prove that the problem of finding a stable matching that maximizes the student-oriented preference utility in a two-sided market with one-sided preference can be solved in polynomial time, and we verify through experiments that the quality of assignment is improved. The main contributions of this paper are as follows. We found that stable student-oriented allocation in a two-sided market with one-sided preferences is the same as stable allocation in a two-sided market with symmetric preferences. In addition, we defined a method to quantify the quality of allocation from a preference utilitarian perspective. Based on the above two, it was proven that the problem of finding a stable match that maximizes the preference utility in a two-sided market with homogeneous preferences can be reduced to an allocation problem. In this paper, through an experiment, we quantitatively verified that optimal student assignment assigns more students to schools of higher preference, even in situations where many students are assigned to schools of low preference using the existing assignment method. ",
        "title": "Stable Marriage with One-Sided Preference",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.03904",
        "abstract_url": "http://arxiv.org/abs/2401.03904",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Yongjie"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Yang",
                "first_name": "Liying"
            },
            {
                "last_name": "Nie",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Chaoxiong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yiwen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Time-optimal control of a multi-rotor remains an open problem due to the under-actuation and nonlinearity of its dynamics, which make it difficult to solve this problem directly. In this paper, the time-optimal control problem of the multi-rotor is studied. Firstly, a thrust limit optimal decomposition method is proposed, which can reasonably decompose the limited thrust into three directions according to the current state and the target state. As a result, the thrust limit constraint is decomposed as a linear constraint. With the linear constraint and decoupled dynamics, a time-optimal guidance trajectory can be obtained. Then, a cost function is defined based on the time-optimal guidance trajectory, which has a quadratic form and can be used to evaluate the time-optimal performance of the system outputs. Finally, based on the cost function, the time-optimal control problem is reformulated as an MPC (Model Predictive Control) problem. The experimental results demonstrate the feasibility and validity of the proposed methods. ",
        "title": "Guided Time-optimal Model Predictive Control of a Multi-rotor",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04316",
        "abstract_url": "http://arxiv.org/abs/2401.04316",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Guangyu"
            },
            {
                "last_name": "He",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Dai",
                "first_name": "Bo"
            },
            {
                "last_name": "Gu",
                "first_name": "Feng"
            },
            {
                "last_name": "Han",
                "first_name": "Jianda"
            },
            {
                "last_name": "Liu",
                "first_name": "Guangjun"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Aerial manipulator, which is composed of an UAV (Unmanned Aerial Vehicle) and a multi-link manipulator and can perform aerial manipulation, has shown great potential of applications. However, dynamic coupling between the UAV and the manipulator makes it difficult to control the aerial manipulator with high performance. In this paper, system modeling and control problem of the aerial manipulator are studied. Firstly, an UAV dynamic model is proposed with consideration of the dynamic coupling from an attached manipulator, which is treated as disturbance for the UAV. In the dynamic model, the disturbance is affected by the variable inertia parameters of the aerial manipulator system. Then, based on the proposed dynamic model, a disturbance compensation robust $H_{\\infty}$ controller is designed to stabilize flight of the UAV while the manipulator is in operation. Finally, experiments are conducted and the experimental results demonstrate the feasibility and validity of the proposed control scheme. ",
        "title": "Robust Control of An Aerial Manipulator Based on A Variable Inertia  Parameters Model",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.04448",
        "abstract_url": "http://arxiv.org/abs/2401.04448",
        "authors": [
            {
                "last_name": "Breci",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Guarnera",
                "first_name": "Luca"
            },
            {
                "last_name": "Battiato",
                "first_name": "Sebastiano"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author. These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features. If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual. The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations. This is made possible by algorithmic solutions based on purely mathematical concepts. Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand. In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper\" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline. Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data. The datasets are available at https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/. ",
        "title": "A Novel Dataset for Non-Destructive Inspection of Handwritten Documents",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05337",
        "abstract_url": "http://arxiv.org/abs/2401.05337",
        "authors": [
            {
                "last_name": "Renucci",
                "first_name": "Pierre"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms. ",
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to  Optimize PnL with Linear Signals",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05338",
        "abstract_url": "http://arxiv.org/abs/2401.05338",
        "authors": [
            {
                "last_name": "Shao",
                "first_name": "Daqian"
            },
            {
                "last_name": "Fesser",
                "first_name": "Lukas"
            },
            {
                "last_name": "Kwiatkowska",
                "first_name": "Marta"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Robustness certification, which aims to formally certify the predictions of neural networks against adversarial inputs, has become an integral part of important tool for safety-critical applications. Despite considerable progress, existing certification methods are limited to elementary architectures, such as convolutional networks, recurrent networks and recently Transformers, on benchmark datasets such as MNIST. In this paper, we focus on the robustness certification of scene text recognition (STR), which is a complex and extensively deployed image-based sequence prediction problem. We tackle three types of STR model architectures, including the standard STR pipelines and the Vision Transformer. We propose STR-Cert, the first certification method for STR models, by significantly extending the DeepPoly polyhedral verification framework via deriving novel polyhedral bounds and algorithms for key STR model components. Finally, we certify and compare STR models on six datasets, demonstrating the efficiency and scalability of robustness certification, particularly for the Vision Transformer. ",
        "title": "STR-Cert: Robustness Certification for Deep Text Recognition on Deep  Learning Pipelines and Vision Transformers",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05339",
        "abstract_url": "http://arxiv.org/abs/2401.05339",
        "authors": [
            {
                "last_name": "Chong",
                "first_name": "Toby"
            },
            {
                "last_name": "Chadwick",
                "first_name": "Alina"
            },
            {
                "last_name": "Shen",
                "first_name": "I-chao"
            },
            {
                "last_name": "Xie",
                "first_name": "Haoran"
            },
            {
                "last_name": "Igarashi",
                "first_name": "Takeo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  In this paper, we present a cosmetic-specific skin image dataset. It consists of skin images from $45$ patches ($5$ skin patches each from $9$ participants) of size $8mm^*8mm$ under three cosmetic products (i.e., foundation, blusher, and highlighter). We designed a novel capturing device inspired by Light Stage. Using the device, we captured over $600$ images of each skin patch under diverse lighting conditions in $30$ seconds. We repeated the process for the same skin patch under three cosmetic products. Finally, we demonstrate the viability of the dataset with an image-to-image translation-based pipeline for cosmetic rendering and compared our data-driven approach to an existing cosmetic rendering method. ",
        "title": "MicroGlam: Microscopic Skin Image Dataset with Cosmetics",
        "date": "2023-11-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05341",
        "abstract_url": "http://arxiv.org/abs/2401.05341",
        "authors": [
            {
                "last_name": "Vogt",
                "first_name": "Yannick"
            },
            {
                "last_name": "Naouar",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Kalweit",
                "first_name": "Maria"
            },
            {
                "last_name": "Miething",
                "first_name": "Christoph Cornelius"
            },
            {
                "last_name": "Duyster",
                "first_name": "Justus"
            },
            {
                "last_name": "Mertelsmann",
                "first_name": "Roland"
            },
            {
                "last_name": "Kalweit",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Boedecker",
                "first_name": "Joschka"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The field of antibody-based therapeutics has grown significantly in recent years, with targeted antibodies emerging as a potentially effective approach to personalized therapies. Such therapies could be particularly beneficial for complex, highly individual diseases such as cancer. However, progress in this field is often constrained by the extensive search space of amino acid sequences that form the foundation of antibody design. In this study, we introduce a novel reinforcement learning method specifically tailored to address the unique challenges of this domain. We demonstrate that our method can learn the design of high-affinity antibodies against multiple targets in silico, utilizing either online interaction or offline datasets. To the best of our knowledge, our approach is the first of its kind and outperforms existing methods on all tested antigens in the Absolut! database. ",
        "title": "Stable Online and Offline Reinforcement Learning for Antibody CDRH3  Design",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05342",
        "abstract_url": "http://arxiv.org/abs/2401.05342",
        "authors": [
            {
                "last_name": "Burg",
                "first_name": "Max F."
            },
            {
                "last_name": "Zenkel",
                "first_name": "Thomas"
            },
            {
                "last_name": "Vystr\u010dilov\u00e1",
                "first_name": "Michaela"
            },
            {
                "last_name": "Oesterle",
                "first_name": "Jonathan"
            },
            {
                "last_name": "H\u00f6fling",
                "first_name": "Larissa"
            },
            {
                "last_name": "Willeke",
                "first_name": "Konstantin F."
            },
            {
                "last_name": "Lause",
                "first_name": "Jan"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Sarah"
            },
            {
                "last_name": "Fahey",
                "first_name": "Paul G."
            },
            {
                "last_name": "Ding",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Restivo",
                "first_name": "Kelli"
            },
            {
                "last_name": "Sridhar",
                "first_name": "Shashwat"
            },
            {
                "last_name": "Gollisch",
                "first_name": "Tim"
            },
            {
                "last_name": "Berens",
                "first_name": "Philipp"
            },
            {
                "last_name": "Tolias",
                "first_name": "Andreas S."
            },
            {
                "last_name": "Euler",
                "first_name": "Thomas"
            },
            {
                "last_name": "Bethge",
                "first_name": "Matthias"
            },
            {
                "last_name": "Ecker",
                "first_name": "Alexander S."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully find discriminative stimuli across species, stages of the visual system and recording techniques. The resulting most discriminative stimuli can be used to assign functional cell types fast and on the fly, without the need to train complex predictive models or show a large natural scene dataset, paving the way for experiments that were previously limited by experimental time. Crucially, MDS are interpretable: they visualize the distinctive stimulus patterns that most unambiguously identify a specific type of neuron. We will make our code available online upon publication. ",
        "title": "Most discriminative stimuli for functional cell type identification",
        "date": "2023-11-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05345",
        "abstract_url": "http://arxiv.org/abs/2401.05345",
        "authors": [
            {
                "last_name": "Durvasula",
                "first_name": "Sankeerth"
            },
            {
                "last_name": "Zhao",
                "first_name": "Adrian"
            },
            {
                "last_name": "Chen",
                "first_name": "Fan"
            },
            {
                "last_name": "Liang",
                "first_name": "Ruofan"
            },
            {
                "last_name": "Sanjaya",
                "first_name": "Pawan Kumar"
            },
            {
                "last_name": "Vijaykumar",
                "first_name": "Nandita"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR",
            "PF"
        ],
        "abstract": "  Differentiable rendering is a technique used in an important emerging class of visual computing applications that involves representing a 3D scene as a model that is trained from 2D images using gradient descent. Recent works (e.g. 3D Gaussian Splatting) use a rasterization pipeline to enable rendering high quality photo-realistic imagery at high speeds from these learned 3D models. These methods have been demonstrated to be very promising, providing state-of-art quality for many important tasks. However, training a model to represent a scene is still a time-consuming task even when using powerful GPUs. In this work, we observe that the gradient computation phase during training is a significant bottleneck on GPUs due to the large number of atomic operations that need to be processed. These atomic operations overwhelm atomic units in the L2 partitions causing stalls. To address this challenge, we leverage the observations that during the gradient computation: (1) for most warps, all threads atomically update the same memory locations; and (2) warps generate varying amounts of atomic traffic (since some threads may be inactive). We propose DISTWAR, a software-approach to accelerate atomic operations based on two key ideas: First, we enable warp-level reduction of threads at the SM sub-cores using registers to leverage the locality in intra-warp atomic updates. Second, we distribute the atomic computation between the warp-level reduction at the SM and the L2 atomic units to increase the throughput of atomic computation. Warps with many threads performing atomic updates to the same memory locations are scheduled at the SM, and the rest using L2 atomic units. We implement DISTWAR using existing warp-level primitives. We evaluate DISTWAR on widely used raster-based differentiable rendering workloads. We demonstrate significant speedups of 2.44x on average (up to 5.7x). ",
        "title": "DISTWAR: Fast Differentiable Rendering on Raster-based Rendering  Pipelines",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05346",
        "abstract_url": "http://arxiv.org/abs/2401.05346",
        "authors": [
            {
                "last_name": "Attapu",
                "first_name": "Amitha"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robots can be very useful to automate tasks and reduce the human effort required. But for the robot to know, how to perform tasks, we need to give it a clear set of steps to follow. It is nearly impossible to provide a robot with instructions for every possible task. Therefore we have a Universal Functional object-oriented network (FOON) which was created and expanded and has a lot of existing recipe information [1]. But certain tasks are complicated for robots to perform and similarly, some tasks are complicated for humans to perform. Therefore weights have been added to functional units to represent the chance of successful execution of the motion by the robot [2]. Given a set of kitchen items and a goal node, using Universal FOON, a robot must be able to determine if the required items are present in the kitchen, and if yes, get the steps to convert the required kitchen items to the goal node. Now through this paper, we use two algorithms (IDS and GBFS) to retrieve a task tree (if possible) for a goal node and a given set of kitchen items. The following would be the different parts of the paper: Section II FOON creation, where we will discuss the different terminologies related to FOON and visualization of FOON. In Section III Methodology we discuss the IDS and GBFS search algorithms and the two different heuristics implemented and used in GBFS. In Section IV Experiment/Discussion, we compare the performance of different algorithms. In the final section V, we specify the references of the papers that have been cited. ",
        "title": "Task tree retrieval from FOON using search algorithms",
        "date": "2023-12-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05350",
        "abstract_url": "http://arxiv.org/abs/2401.05350",
        "authors": [
            {
                "last_name": "Aydin",
                "first_name": "Mehmet Emin"
            },
            {
                "last_name": "Durgut",
                "first_name": "Rafet"
            },
            {
                "last_name": "Rakib",
                "first_name": "Abdur"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Optimisation problems, particularly combinatorial optimisation problems, are difficult to solve due to their complexity and hardness. Such problems have been successfully solved by evolutionary and swarm intelligence algorithms, especially in binary format. However, the approximation may suffer due to the the issues in balance between exploration and exploitation activities (EvE), which remain as the major challenge in this context. Although the complementary usage of multiple operators is becoming more popular for managing EvE with adaptive operator selection schemes, a bespoke adaptive selection system is still an important topic in research. Reinforcement Learning (RL) has recently been proposed as a way to customise and shape up a highly effective adaptive selection system. However, it is still challenging to handle the problem in terms of scalability. This paper proposes and assesses a RL-based novel approach to help develop a generalised framework for gaining, processing, and utilising the experiences for both the immediate and future use. The experimental results support the proposed approach with a certain level of success. ",
        "title": "Adaptive operator selection utilising generalised experience",
        "date": "2023-12-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05351",
        "abstract_url": "http://arxiv.org/abs/2401.05351",
        "authors": [
            {
                "last_name": "Runge",
                "first_name": "Frederic"
            },
            {
                "last_name": "Franke",
                "first_name": "J\u00f6rg K. H."
            },
            {
                "last_name": "Fertmann",
                "first_name": "Daniel"
            },
            {
                "last_name": "Hutter",
                "first_name": "Frank"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Accurate RNA secondary structure prediction is vital for understanding cellular regulation and disease mechanisms. Deep learning (DL) methods have surpassed traditional algorithms by predicting complex features like pseudoknots and multi-interacting base pairs. However, traditional distance measures can hardly deal with such tertiary interactions and the currently used evaluation measures (F1 score, MCC) have limitations. We propose the Weisfeiler-Lehman graph kernel (WL) as an alternative metric. Embracing graph-based metrics like WL enables fair and accurate evaluation of RNA structure prediction algorithms. Further, WL provides informative guidance, as demonstrated in an RNA design experiment. ",
        "title": "Rethinking Performance Measures of RNA Secondary Structure Problems",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05352",
        "abstract_url": "http://arxiv.org/abs/2401.05352",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyun"
            },
            {
                "last_name": "Meinel",
                "first_name": "Christoph"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Generalized Class Discovery (GCD) plays a pivotal role in discerning both known and unknown categories from unlabeled datasets by harnessing the insights derived from a labeled set comprising recognized classes. A significant limitation in prevailing GCD methods is their presumption of an equitably distributed category occurrence in unlabeled data. Contrary to this assumption, visual classes in natural environments typically exhibit a long-tailed distribution, with known or prevalent categories surfacing more frequently than their rarer counterparts. Our research endeavors to bridge this disconnect by focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD) paradigm, which echoes the innate imbalances of real-world unlabeled datasets. In response to the unique challenges posed by Long-tailed GCD, we present a robust methodology anchored in two strategic regularizations: (i) a reweighting mechanism that bolsters the prominence of less-represented, tail-end categories, and (ii) a class prior constraint that aligns with the anticipated class distribution. Comprehensive experiments reveal that our proposed method surpasses previous state-of-the-art GCD methods by achieving an improvement of approximately 6 - 9% on ImageNet100 and competitive performance on CIFAR100. ",
        "title": "Generalized Categories Discovery for Long-tailed Recognition",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05353",
        "abstract_url": "http://arxiv.org/abs/2401.05353",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyun"
            },
            {
                "last_name": "Dai",
                "first_name": "Ben"
            },
            {
                "last_name": "Simsek",
                "first_name": "Furkan"
            },
            {
                "last_name": "Meinel",
                "first_name": "Christoph"
            },
            {
                "last_name": "Yang",
                "first_name": "Haojin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Generalized class discovery (GCD) aims to infer known and unknown categories in an unlabeled dataset leveraging prior knowledge of a labeled set comprising known classes. Existing research implicitly/explicitly assumes that the frequency of occurrence for each category, whether known or unknown, is approximately the same in the unlabeled data. However, in nature, we are more likely to encounter known/common classes than unknown/uncommon ones, according to the long-tailed property of visual classes. Therefore, we present a challenging and practical problem, Imbalanced Generalized Category Discovery (ImbaGCD), where the distribution of unlabeled data is imbalanced, with known classes being more frequent than unknown ones. To address these issues, we propose ImbaGCD, A novel optimal transport-based expectation maximization framework that accomplishes generalized category discovery by aligning the marginal class prior distribution. ImbaGCD also incorporates a systematic mechanism for estimating the imbalanced class prior distribution under the GCD setup. Our comprehensive experiments reveal that ImbaGCD surpasses previous state-of-the-art GCD methods by achieving an improvement of approximately 2 - 4% on CIFAR-100 and 15 - 19% on ImageNet-100, indicating its superior effectiveness in solving the Imbalanced GCD problem. ",
        "title": "ImbaGCD: Imbalanced Generalized Category Discovery",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05354",
        "abstract_url": "http://arxiv.org/abs/2401.05354",
        "authors": [
            {
                "last_name": "Namura",
                "first_name": "Norihisa"
            },
            {
                "last_name": "Nakao",
                "first_name": "Hiroya"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a method for optimizing mutual coupling functions to achieve fast and global synchronization between a pair of weakly coupled limit-cycle oscillators. Our method is based on phase reduction that provides a concise low-dimensional representation of the synchronization dynamics of mutually coupled oscillators, including the case where the coupling depends on past time series of the oscillators. We first describe a method for a pair of identical oscillators and then generalize it to the case of slightly nonidentical oscillators. The coupling function is designed in two optimization steps for the functional form and amplitude, where the amplitude is numerically optimized to minimize the average convergence time under a constraint on the total power. We perform numerical simulations of the synchronization dynamics with the optimized coupling functions using the FitzHugh-Nagumo and R\\\"{o}ssler oscillators as examples. We show that the coupling function optimized by the proposed method can achieve global synchronization more efficiently than the previous methods. ",
        "title": "Optimal coupling functions for fast and global synchronization of weakly  coupled limit-cycle oscillators",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05355",
        "abstract_url": "http://arxiv.org/abs/2401.05355",
        "authors": [
            {
                "last_name": "Mih",
                "first_name": "Atah Nuh"
            },
            {
                "last_name": "Cao",
                "first_name": "Hung"
            },
            {
                "last_name": "Kawnine",
                "first_name": "Asfia"
            },
            {
                "last_name": "Wachowicz",
                "first_name": "Monica"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Resource constraints have restricted several EdgeAI applications to machine learning inference approaches, where models are trained on the cloud and deployed to the edge device. This poses challenges such as bandwidth, latency, and privacy associated with storing data off-site for model building. Training on the edge device can overcome these challenges by eliminating the need to transfer data to another device for storage and model development. On-device training also provides robustness to data variations as models can be retrained on newly acquired data to improve performance. We, therefore, propose a lightweight EdgeAI architecture modified from Xception, for on-device training in a resource-constraint edge environment. We evaluate our model on a PCB defect detection task and compare its performance against existing lightweight models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our experiment show that our model has a remarkable performance with a test accuracy of 73.45% without pre-training. This is comparable to the test accuracy of non-pre-trained MobileViT-XXS (75.40%) and much better than other non-pre-trained models (MobileNetV2 - 50.05%, EfficientNetV2B0 - 54.30%). The test accuracy of our model without pre-training is comparable to pre-trained MobileNetV2 model - 75.45% and better than pre-trained EfficientNetV2B0 model - 58.10%. In terms of memory efficiency, our model performs better than EfficientNetV2B0 and MobileViT-XXS. We find that the resource efficiency of machine learning models does not solely depend on the number of parameters but also depends on architectural considerations. Our method can be applied to other resource-constraint applications while maintaining significant performance. ",
        "title": "Developing a Resource-Constraint EdgeAI model for Surface Defect  Detection",
        "date": "2023-12-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05357",
        "abstract_url": "http://arxiv.org/abs/2401.05357",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Zheyu"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaobo Sharon"
            },
            {
                "last_name": "Shi",
                "first_name": "Yiyu"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  Architectures that incorporate Computing-in-Memory (CiM) using emerging non-volatile memory (NVM) devices have become strong contenders for deep neural network (DNN) acceleration due to their impressive energy efficiency. Yet, a significant challenge arises when using these emerging devices: they can show substantial variations during the weight-mapping process. This can severely impact DNN accuracy if not mitigated. A widely accepted remedy for imperfect weight mapping is the iterative write-verify approach, which involves verifying conductance values and adjusting devices if needed. In all existing publications, this procedure is applied to every individual device, resulting in a significant programming time overhead. In our research, we illustrate that only a small fraction of weights need this write-verify treatment for the corresponding devices and the DNN accuracy can be preserved, yielding a notable programming acceleration. Building on this, we introduce USWIM, a novel method based on the second derivative. It leverages a single iteration of forward and backpropagation to pinpoint the weights demanding write-verify. Through extensive tests on diverse DNN designs and datasets, USWIM manifests up to a 10x programming acceleration against the traditional exhaustive write-verify method, all while maintaining a similar accuracy level. Furthermore, compared to our earlier SWIM technique, USWIM excels, showing a 7x speedup when dealing with devices exhibiting non-uniform variations. ",
        "title": "U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural  Accelerators",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05358",
        "abstract_url": "http://arxiv.org/abs/2401.05358",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhong",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Zheng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The development of U.S. Army and NATO data link systems is introduced first, and then the development trend of future intelligent data link is summarized into integration, generalization, multifunctionality and high security. A unit-level combat system architecture based on the global combat cloud, which is capable of realizing the flexible scheduling of global combat resources and maximizing the overall combat effectiveness, is proposed. Intelligent data link is an important part of this solution, providing strong information support for future urban unit-level warfare. ",
        "title": "Future Intelligent Data link and Unit-Level Combat System Based on  Global Combat Cloud",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05362",
        "abstract_url": "http://arxiv.org/abs/2401.05362",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Ziqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Liyuan"
            },
            {
                "last_name": "Ding",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xingxing"
            },
            {
                "last_name": "Zhong",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Ai",
                "first_name": "Jianyong"
            },
            {
                "last_name": "Li",
                "first_name": "Jianmin"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In real-world applications, an object detector often encounters object instances from new classes and needs to accommodate them effectively. Previous work formulated this critical problem as incremental object detection (IOD), which assumes the object instances of new classes to be fully annotated in incremental data. However, as supervisory signals are usually rare and expensive, the supervised IOD may not be practical for implementation. In this work, we consider a more realistic setting named semi-supervised IOD (SSIOD), where the object detector needs to learn new classes incrementally from a few labelled data and massive unlabelled data without catastrophic forgetting of old classes. A commonly-used strategy for supervised IOD is to encourage the current model (as a student) to mimic the behavior of the old model (as a teacher), but it generally fails in SSIOD because a dominant number of object instances from old and new classes are coexisting and unlabelled, with the teacher only recognizing a fraction of them. Observing that learning only the classes of interest tends to preclude detection of other classes, we propose to bridge the coexistence of unlabelled classes by constructing two teacher models respectively for old and new classes, and using the concatenation of their predictions to instruct the student. This approach is referred to as DualTeacher, which can serve as a strong baseline for SSIOD with limited resource overhead and no extra hyperparameters. We build various benchmarks for SSIOD and perform extensive experiments to demonstrate the superiority of our approach (e.g., the performance lead is up to 18.28 AP on MS-COCO). Our code is available at \\url{https://github.com/chuxiuhong/DualTeacher}. ",
        "title": "DualTeacher: Bridging Coexistence of Unlabelled Classes for  Semi-supervised Incremental Object Detection",
        "date": "2023-12-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05365",
        "abstract_url": "http://arxiv.org/abs/2401.05365",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Rapetti",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Grieco",
                "first_name": "Riccardo"
            },
            {
                "last_name": "Draicchio",
                "first_name": "Francesco"
            },
            {
                "last_name": "Pucci",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper proposes a framework that combines online human state estimation, action recognition and motion prediction to enable early assessment and prevention of worker biomechanical risk during lifting tasks. The framework leverages the NIOSH index to perform online risk assessment, thus fitting real-time applications. In particular, the human state is retrieved via inverse kinematics/dynamics algorithms from wearable sensor data. Human action recognition and motion prediction are achieved by implementing an LSTM-based Guided Mixture of Experts architecture, which is trained offline and inferred online. With the recognized actions, a single lifting activity is divided into a series of continuous movements and the Revised NIOSH Lifting Equation can be applied for risk assessment. Moreover, the predicted motions enable anticipation of future risks. A haptic actuator, embedded in the wearable system, can alert the subject of potential risk, acting as an active prevention device. The performance of the proposed framework is validated by executing real lifting tasks, while the subject is equipped with the iFeel wearable system. ",
        "title": "Online Action Recognition for Human Risk Prediction with Anticipated  Haptic Alert via Wearables",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05367",
        "abstract_url": "http://arxiv.org/abs/2401.05367",
        "authors": [
            {
                "last_name": "Aqajari",
                "first_name": "Seyed Amir Hossein"
            },
            {
                "last_name": "Labbaf",
                "first_name": "Sina"
            },
            {
                "last_name": "Tran",
                "first_name": "Phuc Hoang"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Brenda"
            },
            {
                "last_name": "Mehrabadi",
                "first_name": "Milad Asgari"
            },
            {
                "last_name": "Levorato",
                "first_name": "Marco"
            },
            {
                "last_name": "Dutt",
                "first_name": "Nikil"
            },
            {
                "last_name": "Rahmani",
                "first_name": "Amir M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Daily monitoring of stress is a critical component of maintaining optimal physical and mental health. Physiological signals and contextual information have recently emerged as promising indicators for detecting instances of heightened stress. Nonetheless, developing a real-time monitoring system that utilizes both physiological and contextual data to anticipate stress levels in everyday settings while also gathering stress labels from participants represents a significant challenge. We present a monitoring system that objectively tracks daily stress levels by utilizing both physiological and contextual data in a daily-life environment. Additionally, we have integrated a smart labeling approach to optimize the ecological momentary assessment (EMA) collection, which is required for building machine learning models for stress detection. We propose a three-tier Internet-of-Things-based system architecture to address the challenges. We utilized a cross-validation technique to accurately estimate the performance of our stress models. We achieved the F1-score of 70\\% with a Random Forest classifier using both PPG and contextual data, which is considered an acceptable score in models built for everyday settings. Whereas using PPG data alone, the highest F1-score achieved is approximately 56\\%, emphasizing the significance of incorporating both PPG and contextual data in stress detection tasks. ",
        "title": "Context-Aware Stress Monitoring using Wearable and Mobile Technologies  in Everyday Settings",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05369",
        "abstract_url": "http://arxiv.org/abs/2401.05369",
        "authors": [
            {
                "last_name": "Gandhi",
                "first_name": "Govind"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Growing interest in modelling complex systems from brains to societies to cities using networks has led to increased efforts to describe generative processes that explain those networks. Recent successes in machine learning have prompted the usage of evolutionary computation, especially genetic programming to evolve computer programs that effectively forage a multidimensional search space to iteratively find better solutions that explain network structure. Symbolic regression contributes to these approaches by replicating network morphologies using both structure and processes, all while not relying on the scientists intuition or expertise. It distinguishes itself by introducing a novel formulation of a network generator and a parameter-free fitness function to evaluate the generated network and is found to consistently retrieve synthetically generated growth processes as well as simple, interpretable rules for a range of empirical networks. We extend this approach by modifying generator semantics to create and retrieve rules for time-varying networks. Lexicon to study networks created dynamically in multiple stages is introduced. The framework was improved using methods from the genetic programming toolkit (recombination) and computational improvements (using heuristic distance measures) and used to test the consistency and robustness of the upgrades to the semantics using synthetically generated networks. Using recombination was found to improve retrieval rate and fitness of the solutions. The framework was then used on three empirical datasets - subway networks of major cities, regions of street networks and semantic co-occurrence networks of literature in Artificial Intelligence to illustrate the possibility of obtaining interpretable, decentralised growth processes from complex networks. ",
        "title": "Symbolic Regression of Dynamic Network Models",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05370",
        "abstract_url": "http://arxiv.org/abs/2401.05370",
        "authors": [
            {
                "last_name": "Ghorbani",
                "first_name": "Mahdi"
            },
            {
                "last_name": "Gendelev",
                "first_name": "Leo"
            },
            {
                "last_name": "Beroza",
                "first_name": "Paul"
            },
            {
                "last_name": "Keiser",
                "first_name": "Michael J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we introduce AutoFragDiff, a fragment-based autoregressive diffusion model for generating 3D molecular structures conditioned on target protein structures. We employ geometric vector perceptrons to predict atom types and spatial coordinates of new molecular fragments conditioned on molecular scaffolds and protein pockets. Our approach improves the local geometry of the resulting 3D molecules while maintaining high predicted binding affinity to protein targets. The model can also perform scaffold extension from user-provided starting molecular scaffold. ",
        "title": "Autoregressive fragment-based diffusion for pocket-aware ligand design",
        "date": "2023-12-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05373",
        "abstract_url": "http://arxiv.org/abs/2401.05373",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Nan"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengzhu"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhenghan"
            },
            {
                "last_name": "De Masi",
                "first_name": "Giulia"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            },
            {
                "last_name": "Xiong",
                "first_name": "Huan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph \\underline{N}eural Networks (\\method{}). To mitigate the information loss problem, \\method{} propagates early-layer information directly to the last layer for information compensation. To accommodate the memory requirements, we apply the implicit differentiation on the equilibrium state, which does not rely on the exact reverse of the forward computation. While traditional implicit differentiation methods are usually used for static situations, \\method{} extends it to the dynamic graph setting. Extensive experiments on three large-scale real-world dynamic graph datasets validate the effectiveness of \\method{} on dynamic node classification tasks with lower computational costs. ",
        "title": "Dynamic Spiking Graph Neural Networks",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05375",
        "abstract_url": "http://arxiv.org/abs/2401.05375",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Taining"
            },
            {
                "last_name": "Goldstein",
                "first_name": "Adam"
            },
            {
                "last_name": "Levin",
                "first_name": "Michael"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "DS",
            "MA"
        ],
        "abstract": "  The emerging field of Diverse Intelligence seeks to identify, formalize, and understand commonalities in behavioral competencies across a wide range of implementations. Especially interesting are simple systems that provide unexpected examples of memory, decision-making, or problem-solving in substrates that at first glance do not appear to be complex enough to implement such capabilities. We seek to develop tools to help understand the minimal requirements for such capabilities, and to learn to recognize and predict basal forms of intelligence in unconventional substrates. Here, we apply novel analyses to the behavior of classical sorting algorithms, short pieces of code which have been studied for many decades. To study these sorting algorithms as a model of biological morphogenesis and its competencies, we break two formerly-ubiquitous assumptions: top-down control (instead, showing how each element within a array of numbers can exert minimal agency and implement sorting policies from the bottom up), and fully reliable hardware (instead, allowing some of the elements to be \"damaged\" and fail to execute the algorithm). We quantitatively characterize sorting activity as the traversal of a problem space, showing that arrays of autonomous elements sort themselves more reliably and robustly than traditional implementations in the presence of errors. Moreover, we find the ability to temporarily reduce progress in order to navigate around a defect, and unexpected clustering behavior among the elements in chimeric arrays whose elements follow one of two different algorithms. The discovery of emergent problem-solving capacities in simple, familiar algorithms contributes a new perspective to the field of Diverse Intelligence, showing how basal forms of intelligence can emerge in simple systems without being explicitly encoded in their underlying mechanics. ",
        "title": "Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting  arrays reveal unexpected competencies in a minimal model of basal  intelligence",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05376",
        "abstract_url": "http://arxiv.org/abs/2401.05376",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chunzhuo"
            },
            {
                "last_name": "Kumar",
                "first_name": "T. Sunil"
            },
            {
                "last_name": "De Raedt",
                "first_name": "Walter"
            },
            {
                "last_name": "Camps",
                "first_name": "Guido"
            },
            {
                "last_name": "Hallez",
                "first_name": "Hans"
            },
            {
                "last_name": "Vanrumste",
                "first_name": "Bart"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Eating speed is an important indicator that has been widely scrutinized in nutritional studies. The relationship between eating speed and several intake-related problems such as obesity, diabetes, and oral health has received increased attention from researchers. However, existing studies mainly use self-reported questionnaires to obtain participants' eating speed, where they choose options from slow, medium, and fast. Such a non-quantitative method is highly subjective and coarse in individual level. In this study, we propose a novel approach to measure eating speed in free-living environments automatically and objectively using wrist-worn inertial measurement unit (IMU) sensors. Specifically, a temporal convolutional network combined with a multi-head attention module (TCN-MHA) is developed to detect bites (including eating and drinking gestures) from free-living IMU data. The predicted bite sequences are then clustered to eating episodes. Eating speed is calculated by using the time taken to finish the eating episode to divide the number of bites. To validate the proposed approach on eating speed measurement, a 7-fold cross validation is applied to the self-collected fine-annotated full-day-I (FD-I) dataset, and a hold-out experiment is conducted on the full-day-II (FD-II) dataset. The two datasets are collected from 61 participants in free-living environments with a total duration of 513 h, which are publicly available. Experimental results shows that the proposed approach achieves a mean absolute percentage error (MAPE) of 0.110 and 0.146 in the FD-I and FD-II datasets, respectively, showcasing the feasibility of automated eating speed measurement. To the best of our knowledge, this is the first study investigating automated eating speed measurement. ",
        "title": "Eating Speed Measurement Using Wrist-Worn IMU Sensors in Free-Living  Environments",
        "date": "2023-12-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05377",
        "abstract_url": "http://arxiv.org/abs/2401.05377",
        "authors": [
            {
                "last_name": "Capraro",
                "first_name": "Valerio"
            },
            {
                "last_name": "Lentsch",
                "first_name": "Austin"
            },
            {
                "last_name": "Acemoglu",
                "first_name": "Daron"
            },
            {
                "last_name": "Akgun",
                "first_name": "Selin"
            },
            {
                "last_name": "Akhmedova",
                "first_name": "Aisel"
            },
            {
                "last_name": "Bilancini",
                "first_name": "Ennio"
            },
            {
                "last_name": "Bonnefon",
                "first_name": "Jean-Fran\u00e7ois"
            },
            {
                "last_name": "Bra\u00f1as-Garza",
                "first_name": "Pablo"
            },
            {
                "last_name": "Butera",
                "first_name": "Luigi"
            },
            {
                "last_name": "Douglas",
                "first_name": "Karen M."
            },
            {
                "last_name": "Everett",
                "first_name": "Jim A. C."
            },
            {
                "last_name": "Gigerenzer",
                "first_name": "Gerd"
            },
            {
                "last_name": "Greenhow",
                "first_name": "Christine"
            },
            {
                "last_name": "Hashimoto",
                "first_name": "Daniel A."
            },
            {
                "last_name": "Holt-Lunstad",
                "first_name": "Julianne"
            },
            {
                "last_name": "Jetten",
                "first_name": "Jolanda"
            },
            {
                "last_name": "Johnson",
                "first_name": "Simon"
            },
            {
                "last_name": "Longoni",
                "first_name": "Chiara"
            },
            {
                "last_name": "Lunn",
                "first_name": "Pete"
            },
            {
                "last_name": "Natale",
                "first_name": "Simone"
            },
            {
                "last_name": "Rahwan",
                "first_name": "Iyad"
            },
            {
                "last_name": "Selwyn",
                "first_name": "Neil"
            },
            {
                "last_name": "Singh",
                "first_name": "Vivek"
            },
            {
                "last_name": "Suri",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Sutcliffe",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Tomlinson",
                "first_name": "Joe"
            },
            {
                "last_name": "van der Linden",
                "first_name": "Sander"
            },
            {
                "last_name": "Van Lange",
                "first_name": "Paul A. M."
            },
            {
                "last_name": "Wall",
                "first_name": "Friederike"
            },
            {
                "last_name": "Van Bavel",
                "first_name": "Jay J."
            },
            {
                "last_name": "Viale",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Generative artificial intelligence, including chatbots like ChatGPT, has the potential to both exacerbate and ameliorate existing socioeconomic inequalities. In this article, we provide a state-of-the-art interdisciplinary overview of the probable impacts of generative AI on four critical domains: work, education, health, and information. Our goal is to warn about how generative AI could worsen existing inequalities while illuminating directions for using AI to resolve pervasive social problems. Generative AI in the workplace can boost productivity and create new jobs, but the benefits will likely be distributed unevenly. In education, it offers personalized learning but may widen the digital divide. In healthcare, it improves diagnostics and accessibility but could deepen pre-existing inequalities. For information, it democratizes content creation and access but also dramatically expands the production and proliferation of misinformation. Each section covers a specific topic, evaluates existing research, identifies critical gaps, and recommends research directions. We conclude with a section highlighting the role of policymaking to maximize generative AI's potential to reduce inequalities while mitigating its harmful effects. We discuss strengths and weaknesses of existing policy frameworks in the European Union, the United States, and the United Kingdom, observing that each fails to fully confront the socioeconomic challenges we have identified. We contend that these policies should promote shared prosperity through the advancement of generative AI. We suggest several concrete policies to encourage further research and debate. This article emphasizes the need for interdisciplinary collaborations to understand and address the complex challenges of generative AI. ",
        "title": "The impact of generative artificial intelligence on socioeconomic  inequalities and policy making",
        "date": "2023-12-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05378",
        "abstract_url": "http://arxiv.org/abs/2401.05378",
        "authors": [
            {
                "last_name": "Alam",
                "first_name": "Ridwan"
            },
            {
                "last_name": "Aguirre",
                "first_name": "Aaron"
            },
            {
                "last_name": "Stultz",
                "first_name": "Collin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  For a number of antiarrhythmics, drug loading requires a 3 day hospitalization with monitoring for QT prolongation. Automated QT monitoring with wearable ECG monitors would facilitate out-of-hospital care. We develop a deep learning model that infers QT intervals from ECG lead-I - the lead most often acquired from ambulatory ECG monitors - and to use this model to detect clinically meaningful QT-prolongation episodes during Dofetilide drug loading. Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the Massachusetts General Hospital, we develop a deep learning model, QTNet, that infers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients are used to train the model and an internal-test set containing 633 thousand ECGs from 135 thousand patients was used for testing. QTNet is further evaluated on an external-validation set containing 3.1 million ECGs from 667 thousand patients at another institution. QTNet was used to detect Dofetilide-induced QT prolongation in a publicly available database (ECGRDVQ-dataset) containing ECGs from subjects enrolled in a clinical trial evaluating the effects of antiarrhythmic drugs. QTNet achieves mean absolute errors of 12.63ms (internal-test) and 12.30ms (external-validation) for estimating absolute QT intervals. The associated Pearson correlation coefficients are 0.91 (internal-test) and 0.92 (external-validation). For the ECGRDVQ-dataset, QTNet detects Dofetilide-induced QTc prolongation with 87% sensitivity and 77% specificity. The negative predictive value of the model is greater than 95% when the pre-test probability of drug-induced QTc prolongation is below 25%. Drug-induced QT prolongation risk can be tracked from ECG lead-I using deep learning. ",
        "title": "Detecting QT prolongation From a Single-lead ECG With Deep Learning",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05380",
        "abstract_url": "http://arxiv.org/abs/2401.05380",
        "authors": [
            {
                "last_name": "Dyoub",
                "first_name": "Abeer"
            },
            {
                "last_name": "Letteri",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  In this study, we investigated the application of bio-inspired optimization algorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale Optimization Algorithm, for feature selection in chronic disease prediction. The primary goal was to enhance the predictive accuracy of models streamline data dimensionality, and make predictions more interpretable and actionable.   The research encompassed a comparative analysis of the three bio-inspired feature selection approaches across diverse chronic diseases, including diabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such as accuracy, precision, recall, and f1 score are used to assess the effectiveness of the algorithms in reducing the number of features needed for accurate classification.   The results in general demonstrate that the bio-inspired optimization algorithms are effective in reducing the number of features required for accurate classification. However, there have been variations in the performance of the algorithms on different datasets.   The study highlights the importance of data pre-processing and cleaning in ensuring the reliability and effectiveness of the analysis.   This study contributes to the advancement of predictive analytics in the realm of chronic diseases. The potential impact of this work extends to early intervention, precision medicine, and improved patient outcomes, providing new avenues for the delivery of healthcare services tailored to individual needs. The findings underscore the potential benefits of using bio-inspired optimization algorithms for feature selection in chronic disease prediction, offering valuable insights for improving healthcare outcomes. ",
        "title": "Dataset Optimization for Chronic Disease Prediction with Bio-Inspired  Feature Selection",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05381",
        "abstract_url": "http://arxiv.org/abs/2401.05381",
        "authors": [
            {
                "last_name": "Petralia",
                "first_name": "Adrien"
            },
            {
                "last_name": "Charpentier",
                "first_name": "Philippe"
            },
            {
                "last_name": "Palpanas",
                "first_name": "Themis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Over the past decade, millions of smart meters have been installed by electricity suppliers worldwide, allowing them to collect a large amount of electricity consumption data, albeit sampled at a low frequency (one point every 30min). One of the important challenges these suppliers face is how to utilize these data to detect the presence/absence of different appliances in the customers' households. This valuable information can help them provide personalized offers and recommendations to help customers towards the energy transition. Appliance detection can be cast as a time series classification problem. However, the large amount of data combined with the long and variable length of the consumption series pose challenges when training a classifier. In this paper, we propose ADF, a framework that uses subsequences of a client consumption series to detect the presence/absence of appliances. We also introduce TransApp, a Transformer-based time series classifier that is first pretrained in a self-supervised way to enhance its performance on appliance detection tasks. We test our approach on two real datasets, including a publicly available one. The experimental results with two large real datasets show that the proposed approach outperforms current solutions, including state-of-the-art time series classifiers applied to appliance detection. This paper appeared in VLDB 2024. ",
        "title": "ADF & TransApp: A Transformer-Based Framework for Appliance Detection  Using Smart Meter Consumption Series",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05382",
        "abstract_url": "http://arxiv.org/abs/2401.05382",
        "authors": [
            {
                "last_name": "Ghasemi",
                "first_name": "Zahra"
            },
            {
                "last_name": "Neumann",
                "first_name": "Frank"
            },
            {
                "last_name": "Zanin",
                "first_name": "Max"
            },
            {
                "last_name": "Karageorgos",
                "first_name": "John"
            },
            {
                "last_name": "Chen",
                "first_name": "Lei"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding circuit of mineral processing plants. Accurate prediction of SAG mill throughput as a crucial performance metric is of utmost importance. While empirical models have been developed in previous studies for SAG mill throughput prediction, the potential of applying machine learning (ML) techniques for this purpose remains underexplored. Unlike empirical modelling, which relies on expensive and time-consuming experimental data, ML techniques can utilize data collected during regular operations. Genetic programming (GP) is one of ML techniques that offers the advantage of providing a transparent equation for precise mill throughput prediction. This study explores the application of GP to predict SAG mill throughput and introduces five new GP variants to enhance prediction performance. These variants extract multiple equations, each accurately predicting mill throughput for specific clusters of training data. These equations are then employed to predict mill throughput for test data using various approaches. To assess the effect of distance measures on the new GP variants, four different distance measures are employed. Comparative analysis reveals that the new GP variants achieve an average improvement of 12.49% in prediction accuracy. Further investigation of distance measures indicates that the Euclidean distance measure yields the most accurate results for the majority of data splits. Additionally, the most precise new GP variant considers all equations and incorporates both the number of data points in each data cluster and the distance to clusters when calculating the final prediction. The developed GP variants in this study present a precise, transparent, and cost-effective approach for modelling SAG mill throughput in mineral processing plants. ",
        "title": "An improved genetic programming for predicting semi autogenous grinding  mill throughput",
        "date": "2023-12-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05384",
        "abstract_url": "http://arxiv.org/abs/2401.05384",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Nuo"
            },
            {
                "last_name": "Li",
                "first_name": "Hongguang"
            },
            {
                "last_name": "Wang",
                "first_name": "Baoyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper investigates the performance of Large Language Models (LLMs) and Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We introduce IMP-TIP: Improving Math Reasoning with Tool-augmented Interleaf Prompting, a framework that combines the strengths of both LLMs and Tool-augmented LLMs. IMP-TIP follows the ``From Good to Great\" concept, collecting multiple potential solutions from both LLMs and their Tool-Augmented counterparts for the same math problem, and then selecting or re-generating the most accurate answer after cross-checking these solutions via tool-augmented interleaf prompting. The framework incorporates two key aspects: self-prompt and tool-augmented interleaf prompting (TIP). The former allows LLMs to autonomously refine and improve an initial prompt related to tool usage, while the latter enables LLMs to derive the final answer by dynamically analyzing the problem, cross-checking potential solutions, and revising previous reasoning hints in an interleaved manner. Experimental analysis shows that IMP-TIP achieves enhanced mathematical capabilities and outperforms traditional LLMs and tool-augmented LLMs in accuracy and reasoning diversity on math reasoning tasks. For instance, IMP-TIP can improve Tool-augmented ChatGPT on GSM8K-Hard from 56.0% to 65.2%. ",
        "title": "From Good to Great: Improving Math Reasoning with Tool-Augmented  Interleaf Prompting",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05385",
        "abstract_url": "http://arxiv.org/abs/2401.05385",
        "authors": [
            {
                "last_name": "Oswald",
                "first_name": "Christian"
            },
            {
                "last_name": "Toth",
                "first_name": "Mate"
            },
            {
                "last_name": "Meissner",
                "first_name": "Paul"
            },
            {
                "last_name": "Pernkopf",
                "first_name": "Franz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In automotive applications, frequency modulated continuous wave (FMCW) radar is an established technology to determine the distance, velocity and angle of objects in the vicinity of the vehicle. The quality of predictions might be seriously impaired if mutual interference between radar sensors occurs. Previous work processes data from the entire receiver array in parallel to increase interference mitigation quality using neural networks (NNs). However, these architectures do not generalize well across different angles of arrival (AoAs) of interferences and objects. In this paper we introduce fully convolutional neural network (CNN) with rank-three convolutions which is able to transfer learned patterns between different AoAs. Our proposed architecture outperforms previous work while having higher robustness and a lower number of trainable parameters. We evaluate our network on a diverse data set and demonstrate its angle equivariance. ",
        "title": "Angle-Equivariant Convolutional Neural Networks for Interference  Mitigation in Automotive Radar",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05386",
        "abstract_url": "http://arxiv.org/abs/2401.05386",
        "authors": [
            {
                "last_name": "Colot",
                "first_name": "Martin"
            },
            {
                "last_name": "Simar",
                "first_name": "C\u00e9dric"
            },
            {
                "last_name": "Petieau",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Ana Maria Cebolla"
            },
            {
                "last_name": "Cheron",
                "first_name": "Guy"
            },
            {
                "last_name": "Bontempi",
                "first_name": "Gianluca"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  Electromyograms (EMG)-based hand gesture recognition systems are a promising technology for human/machine interfaces. However, one of their main limitations is the long calibration time that is typically required to handle new users. The paper discusses and analyses the challenge of cross-subject generalization thanks to an original dataset containing the EMG signals of 14 human subjects during hand gestures. The experimental results show that, though an accurate generalization based on pooling multiple subjects is hardly achievable, it is possible to improve the cross-subject estimation by identifying a robust low-dimensional subspace for multiple subjects and aligning it to a target subject. A visualization of the subspace enables us to provide insights for the improvement of cross-subject generalization with EMG signals. ",
        "title": "EMG subspace alignment and visualization for cross-subject hand gesture  classification",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05388",
        "abstract_url": "http://arxiv.org/abs/2401.05388",
        "authors": [
            {
                "last_name": "Cardoso",
                "first_name": "Gabriel V."
            },
            {
                "last_name": "Bedin",
                "first_name": "Lisa"
            },
            {
                "last_name": "Duchateau",
                "first_name": "Josselin"
            },
            {
                "last_name": "Dubois",
                "first_name": "R\u00e9mi"
            },
            {
                "last_name": "Moulines",
                "first_name": "Eric"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we propose a denoising diffusion generative model (DDGM) trained with healthy electrocardiogram (ECG) data that focuses on ECG morphology and inter-lead dependence. Our results show that this innovative generative model can successfully generate realistic ECG signals. Furthermore, we explore the application of recent breakthroughs in solving linear inverse Bayesian problems using DDGM. This approach enables the development of several important clinical tools. These include the calculation of corrected QT intervals (QTc), effective noise suppression of ECG signals, recovery of missing ECG leads, and identification of anomalous readings, enabling significant advances in cardiac health monitoring and diagnosis. ",
        "title": "Bayesian ECG reconstruction using denoising diffusion generative models",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05390",
        "abstract_url": "http://arxiv.org/abs/2401.05390",
        "authors": [
            {
                "last_name": "Troncoso-Pastoriza",
                "first_name": "Francisco"
            },
            {
                "last_name": "Egu\u00eda-Oller",
                "first_name": "Pablo"
            },
            {
                "last_name": "D\u00edaz-Redondo",
                "first_name": "Rebeca P."
            },
            {
                "last_name": "Granada-\u00c1lvarez",
                "first_name": "Enrique"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper we introduce a method that supports the detection, identification and localization of lamps in a building, with the main goal of automatically feeding its energy model by means of Building Information Modeling (BIM) methods. The proposed method, thus, provides useful information to apply energy-saving strategies to reduce energy consumption in the building sector through the correct management of the lighting infrastructure. Based on the unique geometry and brightness of lamps and the use of only greyscale images, our methodology is able to obtain accurate results despite its low computational needs, resulting in near-real-time processing. The main novelty is that the focus of the candidate search is not over the entire image but instead only on a limited region that summarizes the specific characteristics of the lamp. The information obtained from our approach was used on the Green Building XML Schema to illustrate the automatic generation of BIM data from the results of the algorithm. ",
        "title": "Generation of BIM data based on the automatic detection, identification  and localization of lamps in buildings",
        "date": "2023-12-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05391",
        "abstract_url": "http://arxiv.org/abs/2401.05391",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hui"
            },
            {
                "last_name": "Gan",
                "first_name": "Yi"
            },
            {
                "last_name": "Yuan",
                "first_name": "Feng"
            },
            {
                "last_name": "Ma",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Yutao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoli"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinghui"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Transformer based Large Language Models (LLMs) have been widely used in many fields, and the efficiency of LLM inference becomes hot topic in real applications. However, LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.   In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly, we simplify the LLM decoder layer by fusing data movement and element-wise operations to reduce the memory access frequency and lower system latency. We also propose a segment KV cache policy to keep key/value of the request and response tokens in separate physical memory for effective device memory management, helping enlarge the runtime batch size and improve system throughput. A customized Scaled-Dot-Product-Attention kernel is designed to match our fusion policy based on the segment KV cache solution. We implement our LLM inference solution on Intel GPU and publish it publicly. Compared with the standard HuggingFace implementation, the proposed solution achieves up to 7x lower token latency and 27x higher throughput for some popular LLMs on Intel GPU. ",
        "title": "Efficient LLM inference solution on Intel GPU",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05392",
        "abstract_url": "http://arxiv.org/abs/2401.05392",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Vikas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Noise is inevitably common in digital images, leading to visual image deterioration. Therefore, a suitable filtering method is required to lessen the noise while preserving the image features (edges, corners, etc.). This paper presents the efficient type-2 fuzzy weighted mean filter with an adaptive threshold to remove the SAP noise. The present filter has two primary steps: The first stage categorizes images as lightly, medium, and heavily corrupted based on an adaptive threshold by comparing the M-ALD of processed pixels with the upper and lower MF of the type-2 fuzzy identifier. The second stage eliminates corrupted pixels by computing the appropriate weight using GMF with the mean and variance of the uncorrupted pixels in the filter window. Simulation results vividly show that the obtained denoised images preserve image features, i.e., edges, corners, and other sharp structures, compared with different filtering methods. ",
        "title": "AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted  with Salt-and-Pepper",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05395",
        "abstract_url": "http://arxiv.org/abs/2401.05395",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Ruixin"
            },
            {
                "last_name": "Chen",
                "first_name": "Bowei"
            },
            {
                "last_name": "Wilson",
                "first_name": "James M."
            },
            {
                "last_name": "Yan",
                "first_name": "Zhi"
            },
            {
                "last_name": "Huang",
                "first_name": "Yufei"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two application examples. Our developed dataset enhances our understanding of the Chinese automotive market and offers a valuable tool for researchers, policymakers, and industry stakeholders worldwide. ",
        "title": "SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive  market",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05396",
        "abstract_url": "http://arxiv.org/abs/2401.05396",
        "authors": [
            {
                "last_name": "\u00c1lvarez-Tu\u00f1\u00f3n",
                "first_name": "Olaya"
            },
            {
                "last_name": "Brodskiy",
                "first_name": "Yury"
            },
            {
                "last_name": "Kayacan",
                "first_name": "Erdal"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper overviews different pose representations and metric functions in visual odometry (VO) networks. The performance of VO networks heavily relies on how their architecture encodes the information. The choice of pose representation and loss function significantly impacts network convergence and generalization. We investigate these factors in the VO network DeepVO by implementing loss functions based on Euler, quaternion, and chordal distance and analyzing their influence on performance. The results of this study provide insights into how loss functions affect the designing of efficient and accurate VO networks for camera motion estimation. The experiments illustrate that a distance that complies with the mathematical requirements of a metric, such as the chordal distance, provides better generalization and faster convergence. The code for the experiments can be found at https://github.com/remaro-network/Loss_VO_right ",
        "title": "Loss it right: Euclidean and Riemannian Metrics in Learning-based Visual  Odometry",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05397",
        "abstract_url": "http://arxiv.org/abs/2401.05397",
        "authors": [
            {
                "last_name": "Marto",
                "first_name": "Sim\u00e3o da Gra\u00e7a"
            },
            {
                "last_name": "Vasile",
                "first_name": "Massimiliano"
            },
            {
                "last_name": "Campbell",
                "first_name": "Andrew"
            },
            {
                "last_name": "Murray",
                "first_name": "Paul"
            },
            {
                "last_name": "Marshall",
                "first_name": "Stephen"
            },
            {
                "last_name": "Savitski",
                "first_name": "Vasili"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Spectral lightcurves consisting of time series single-pixel spectral measurements of spacecraft are used to infer the spacecraft's attitude and rotation. Two methods are used. One based on numerical optimisation of a regularised least squares cost function, and another based on machine learning with a neural network model. The aim is to work with minimal information, thus no prior is available on the attitude nor on the inertia tensor. The theoretical and practical aspects of this task are investigated, and the methodology is tested on synthetic data. Results are shown based on synthetic data. ",
        "title": "Hyperspectral Lightcurve Inversion for Attitude Determination",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05398",
        "abstract_url": "http://arxiv.org/abs/2401.05398",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenwen"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  GeoAI, or geospatial artificial intelligence, is an exciting new area that leverages artificial intelligence (AI), geospatial big data, and massive computing power to solve problems with high automation and intelligence. This paper reviews the progress of AI in social science research, highlighting important advancements in using GeoAI to fill critical data and knowledge gaps. It also discusses the importance of breaking down data silos, accelerating convergence among GeoAI research methods, as well as moving GeoAI beyond geospatial benefits. ",
        "title": "GeoAI in Social Science",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05399",
        "abstract_url": "http://arxiv.org/abs/2401.05399",
        "authors": [
            {
                "last_name": "Oli",
                "first_name": "Priti"
            },
            {
                "last_name": "Banjade",
                "first_name": "Rabin"
            },
            {
                "last_name": "Chapagain",
                "first_name": "Jeevan"
            },
            {
                "last_name": "Rus",
                "first_name": "Vasile"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CL"
        ],
        "abstract": "  Assessing student's answers and in particular natural language answers is a crucial challenge in the field of education. Advances in machine learning, including transformer-based models such as Large Language Models(LLMs), have led to significant progress in various natural language tasks. Nevertheless, amidst the growing trend of evaluating LLMs across diverse tasks, evaluating LLMs in the realm of automated answer assesment has not received much attention. To address this gap, we explore the potential of using LLMs for automated assessment of student's short and open-ended answer. Particularly, we use LLMs to compare students' explanations with expert explanations in the context of line-by-line explanations of computer programs.   For comparison purposes, we assess both Large Language Models (LLMs) and encoder-based Semantic Textual Similarity (STS) models in the context of assessing the correctness of students' explanation of computer code. Our findings indicate that LLMs, when prompted in few-shot and chain-of-thought setting perform comparable to fine-tuned encoder-based models in evaluating students' short answers in programming domain. ",
        "title": "Automated Assessment of Students' Code Comprehension using LLMs",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05400",
        "abstract_url": "http://arxiv.org/abs/2401.05400",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Gyeong-Geon"
            },
            {
                "last_name": "Mun",
                "first_name": "Seonyeong"
            },
            {
                "last_name": "Shin",
                "first_name": "Myeong-Kyeong"
            },
            {
                "last_name": "Zhai",
                "first_name": "Xiaoming"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This research aims to demonstrate that AI can function not only as a tool for learning, but also as an intelligent agent with which humans can engage in collaborative learning (CL) to change epistemic practices in science classrooms. We adopted a design and development research approach, following the Analysis, Design, Development, Implementation and Evaluation (ADDIE) model, to prototype a tangible instructional system called Collaborative Learning with AI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners join an AI speaker to form a small group, where humans and AI are considered as peers participating in the Jigsaw learning process. The development was carried out using the NUGU AI speaker platform. The CLAIS system was successfully implemented in a Science Education course session with 15 pre-service elementary science teachers. The participants evaluated the CLAIS system through mixed methods surveys as teachers, learners, peers, and users. Quantitative data showed that the participants' Intelligent-Technological, Pedagogical, And Content Knowledge was significantly increased after the CLAIS session, the perception of the CLAIS learning experience was positive, the peer assessment on AI speakers and human peers was different, and the user experience was ambivalent. Qualitative data showed that the participants anticipated future changes in the epistemic process in science classrooms, while acknowledging technical issues such as speech recognition performance and response latency. This study highlights the potential of Human-AI Collaboration for knowledge co-construction in authentic classroom settings and exemplify how AI could shape the future landscape of epistemic practices in the classroom. ",
        "title": "Collaborative Learning with Artificial Intelligence Speakers (CLAIS):  Pre-Service Elementary Science Teachers' Responses to the Prototype",
        "date": "2023-12-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05401",
        "abstract_url": "http://arxiv.org/abs/2401.05401",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xisheng"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Song",
                "first_name": "Pinhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingjun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The inherent characteristics and light fluctuations of water bodies give rise to the huge difference between different layers and regions in underwater environments. When the test set is collected in a different marine area from the training set, the issue of domain shift emerges, significantly compromising the model's ability to generalize. The Domain Adversarial Learning (DAL) training strategy has been previously utilized to tackle such challenges. However, DAL heavily depends on manually one-hot domain labels, which implies no difference among the samples in the same domain. Such an assumption results in the instability of DAL. This paper introduces the concept of Domain Similarity-Perceived Label Assignment (DSP). The domain label for each image is regarded as its similarity to the specified domains. Through domain-specific data augmentation techniques, we achieved state-of-the-art results on the underwater cross-domain object detection benchmark S-UODAC2020. Furthermore, we validated the effectiveness of our method in the Cityscapes dataset. ",
        "title": "Domain Similarity-Perceived Label Assignment for Domain Generalized  Underwater Object Detection",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05402",
        "abstract_url": "http://arxiv.org/abs/2401.05402",
        "authors": [
            {
                "last_name": "Klipfel",
                "first_name": "Astrid"
            },
            {
                "last_name": "Fregier",
                "first_name": "Ya\u00ebl"
            },
            {
                "last_name": "Sayede",
                "first_name": "Adlane"
            },
            {
                "last_name": "Bouraoui",
                "first_name": "Zied"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Discovering crystal structures with specific chemical properties has become an increasingly important focus in material science. However, current models are limited in their ability to generate new crystal lattices, as they only consider atomic positions or chemical composition. To address this issue, we propose a probabilistic diffusion model that utilizes a geometrically equivariant GNN to consider atomic positions and crystal lattices jointly. To evaluate the effectiveness of our model, we introduce a new generation metric inspired by Frechet Inception Distance, but based on GNN energy prediction rather than InceptionV3 used in computer vision. In addition to commonly used metrics like validity, which assesses the plausibility of a structure, this new metric offers a more comprehensive evaluation of our model's capabilities. Our experiments on existing benchmarks show the significance of our diffusion model. We also show that our method can effectively learn meaningful representations. ",
        "title": "Vector Field Oriented Diffusion Model for Crystal Material Generation",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05403",
        "abstract_url": "http://arxiv.org/abs/2401.05403",
        "authors": [
            {
                "last_name": "Honghu",
                "first_name": "Yi"
            },
            {
                "last_name": "Ting",
                "first_name": "Liu"
            },
            {
                "last_name": "Gongjin",
                "first_name": "Lan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Artificial Intelligence (AI) technologies have been applied in various domains, including early childhood education (ECE). Integration of AI educational technology is a recent significant trend in ECE. Currently, there are more and more studies of AI in ECE. To date, there is a lack of survey articles that discuss the studies of AI in ECE. In this paper, we provide an up-to-date and in-depth overview of the key AI technologies in ECE that provides a historical perspective, summarizes the representative works, outlines open questions, discusses the trends and challenges through a detailed bibliometric analysis, and provides insightful recommendations for future research. We mainly discuss the studies that apply AI-based robots and AI technologies to ECE, including improving the social interaction of children with an autism spectrum disorder. This paper significantly contributes to provide an up-to-date and in-depth survey that is suitable as introductory material for beginners to AI in ECE, as well as supplementary material for advanced users. ",
        "title": "The Key Artificial Intelligence Technologies in Early Childhood  Education: A Review",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05405",
        "abstract_url": "http://arxiv.org/abs/2401.05405",
        "authors": [
            {
                "last_name": "Del Pup",
                "first_name": "Federico"
            },
            {
                "last_name": "Zanola",
                "first_name": "Andrea"
            },
            {
                "last_name": "Tshimanga",
                "first_name": "Louis Fabrice"
            },
            {
                "last_name": "Mazzon",
                "first_name": "Paolo Emilio"
            },
            {
                "last_name": "Atzori",
                "first_name": "Manfredo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  SelfEEG is an open-source Python library developed to assist researchers in conducting Self-Supervised Learning (SSL) experiments on electroencephalography (EEG) data. Its primary objective is to offer a user-friendly but highly customizable environment, enabling users to efficiently design and execute self-supervised learning tasks on EEG data.   SelfEEG covers all the stages of a typical SSL pipeline, ranging from data import to model design and training. It includes modules specifically designed to: split data at various granularity levels (e.g., session-, subject-, or dataset-based splits); effectively manage data stored with different configurations (e.g., file extensions, data types) during mini-batch construction; provide a wide range of standard deep learning models, data augmentations and SSL baseline methods applied to EEG data.   Most of the functionalities offered by selfEEG can be executed both on GPUs and CPUs, expanding its usability beyond the self-supervised learning area. Additionally, these functionalities can be employed for the analysis of other biomedical signals often coupled with EEGs, such as electromyography or electrocardiography data.   These features make selfEEG a versatile deep learning tool for biomedical applications and a useful resource in SSL, one of the currently most active fields of Artificial Intelligence. ",
        "title": "SelfEEG: A Python library for Self-Supervised Learning in  Electroencephalography",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05406",
        "abstract_url": "http://arxiv.org/abs/2401.05406",
        "authors": [
            {
                "last_name": "Rosen",
                "first_name": "Daniel"
            },
            {
                "last_name": "Rochez",
                "first_name": "Illa"
            },
            {
                "last_name": "McIrvin",
                "first_name": "Caleb"
            },
            {
                "last_name": "Lee",
                "first_name": "Joshua"
            },
            {
                "last_name": "D'Alessandro",
                "first_name": "Kevin"
            },
            {
                "last_name": "Wiecek",
                "first_name": "Max"
            },
            {
                "last_name": "Hoang",
                "first_name": "Nhan"
            },
            {
                "last_name": "Saffarini",
                "first_name": "Ramzy"
            },
            {
                "last_name": "Philips",
                "first_name": "Sam"
            },
            {
                "last_name": "Jones",
                "first_name": "Vanessa"
            },
            {
                "last_name": "Ivey",
                "first_name": "Will"
            },
            {
                "last_name": "Harris-Smart",
                "first_name": "Zavier"
            },
            {
                "last_name": "Harris-Smart",
                "first_name": "Zavion"
            },
            {
                "last_name": "Chin",
                "first_name": "Zayden"
            },
            {
                "last_name": "Johnson",
                "first_name": "Amos"
            },
            {
                "last_name": "Jones",
                "first_name": "Alyse M."
            },
            {
                "last_name": "Headley",
                "first_name": "William C."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI"
        ],
        "abstract": "  Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely applicable technology in the next generation of wireless communication systems, particularly 6G and next-gen military communications. Given this, our research is focused on developing a tool to promote the development of RFRL techniques that leverage spectrum sensing. In particular, the tool was designed to address two cognitive radio applications, specifically dynamic spectrum access and jamming. In order to train and test reinforcement learning (RL) algorithms for these applications, a simulation environment is necessary to simulate the conditions that an agent will encounter within the Radio Frequency (RF) spectrum. In this paper, such an environment has been developed, herein referred to as the RFRL Gym. Through the RFRL Gym, users can design their own scenarios to model what an RL agent may encounter within the RF spectrum as well as experiment with different spectrum sensing techniques. Additionally, the RFRL Gym is a subclass of OpenAI gym, enabling the use of third-party ML/RL Libraries. We plan to open-source this codebase to enable other researchers to utilize the RFRL Gym to test their own scenarios and RL algorithms, ultimately leading to the advancement of RL research in the wireless communications domain. This paper describes in further detail the components of the Gym, results from example scenarios, and plans for future additions.   Index Terms-machine learning, reinforcement learning, wireless communications, dynamic spectrum access, OpenAI gym ",
        "title": "RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio  Applications",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05407",
        "abstract_url": "http://arxiv.org/abs/2401.05407",
        "authors": [
            {
                "last_name": "Koffi",
                "first_name": "Tresor Y."
            },
            {
                "last_name": "Mourchid",
                "first_name": "Youssef"
            },
            {
                "last_name": "Hindawi",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Dupuis",
                "first_name": "Yohan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Falls among individuals, especially the elderly population, can lead to serious injuries and complications. Detecting impact moments within a fall event is crucial for providing timely assistance and minimizing the negative consequences. In this work, we aim to address this challenge by applying thorough preprocessing techniques to the multisensor dataset, the goal is to eliminate noise and improve data quality. Furthermore, we employ a feature selection process to identify the most relevant features derived from the multisensor UP-FALL dataset, which in turn will enhance the performance and efficiency of machine learning models. We then evaluate the efficiency of various machine learning models in detecting the impact moment using the resulting data information from multiple sensors. Through extensive experimentation, we assess the accuracy of our approach using various evaluation metrics. Our results achieve high accuracy rates in impact detection, showcasing the power of leveraging multisensor data for fall detection tasks. This highlights the potential of our approach to enhance fall detection systems and improve the overall safety and well-being of individuals at risk of falls. ",
        "title": "Machine Learning and Feature Ranking for Impact Fall Detection Event  Using Multisensor Data",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05408",
        "abstract_url": "http://arxiv.org/abs/2401.05408",
        "authors": [
            {
                "last_name": "Grzeszczyk",
                "first_name": "Michal K."
            },
            {
                "last_name": "Lisowska",
                "first_name": "Anna"
            },
            {
                "last_name": "Sitek",
                "first_name": "Arkadiusz"
            },
            {
                "last_name": "Lisowska",
                "first_name": "Aneta"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Automatic detection and tracking of emotional states has the potential for helping individuals with various mental health conditions. While previous studies have captured physiological signals using wearable devices in laboratory settings, providing valuable insights into the relationship between physiological responses and mental states, the transfer of these findings to real-life scenarios is still in its nascent stages. Our research aims to bridge the gap between laboratory-based studies and real-life settings by leveraging consumer-grade wearables and self-report measures. We conducted a preliminary study involving 15 healthy participants to assess the efficacy of wearables in capturing user valence in real-world settings. In this paper, we present the initial analysis of the collected data, focusing primarily on the results of valence classification. Our findings demonstrate promising results in distinguishing between high and low positive valence, achieving an F1 score of 0.65. This research opens up avenues for future research in the field of mobile mental health interventions. ",
        "title": "Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True  Feelings?",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05409",
        "abstract_url": "http://arxiv.org/abs/2401.05409",
        "authors": [
            {
                "last_name": "Maiwald",
                "first_name": "Aaron"
            },
            {
                "last_name": "Ackermann",
                "first_name": "Leon"
            },
            {
                "last_name": "Kalcher",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Wu",
                "first_name": "Daniel J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Alternative data representations are powerful tools that augment the performance of downstream models. However, there is an abundance of such representations within the machine learning toolbox, and the field lacks a comparative understanding of the suitability of each representation method.   In this paper, we propose artifact detection and classification within EEG data as a testbed for profiling image-based data representations of time series data. We then evaluate eleven popular deep learning architectures on each of six commonly-used representation methods.   We find that, while the choice of representation entails a choice within the tradeoff between bias and variance, certain representations are practically more effective in highlighting features which increase the signal-to-noise ratio of the data. We present our results on EEG data, and open-source our testing framework to enable future comparative analyses in this vein. ",
        "title": "Image-based Data Representations of Time Series: A Comparative Analysis  in EEG Artifact Detection",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05410",
        "abstract_url": "http://arxiv.org/abs/2401.05410",
        "authors": [
            {
                "last_name": "Laham",
                "first_name": "Saria Al"
            },
            {
                "last_name": "Baghi",
                "first_name": "Bobak H."
            },
            {
                "last_name": "Lajoie",
                "first_name": "Pierre-Yves"
            },
            {
                "last_name": "Feriani",
                "first_name": "Amal"
            },
            {
                "last_name": "Herath",
                "first_name": "Sachini"
            },
            {
                "last_name": "Liu",
                "first_name": "Steve"
            },
            {
                "last_name": "Dudek",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG",
            "NI"
        ],
        "abstract": "  We present a human state estimation framework that allows us to estimate the location, and even the activities, of people in an indoor environment without the requirement that they carry a specific devices with them. To achieve this \"device free\" localization we use a small number of low-cost Ultra-Wide Band (UWB) sensors distributed across the environment of interest. To achieve high quality estimation from the UWB signals merely reflected of people in the environment, we exploit a deep network that can learn to make inferences. The hardware setup consists of commercial off-the-shelf (COTS) single antenna UWB modules for sensing, paired with Raspberry PI units for computational processing and data transfer. We make use of the channel impulse response (CIR) measurements from the UWB sensors to estimate the human state - comprised of location and activity - in a given area. Additionally, we can also estimate the number of humans that occupy this region of interest. In our approach, first, we pre-process the CIR data which involves meticulous aggregation of measurements and extraction of key statistics. Afterwards, we leverage a convolutional deep neural network to map the CIRs into precise location estimates with sub-30 cm accuracy. Similarly, we achieve accurate human activity recognition and occupancy counting results. We show that we can quickly fine-tune our model for new out-of-distribution users, a process that requires only a few minutes of data and a few epochs of training. Our results show that UWB is a promising solution for adaptable smart-home localization and activity recognition problems. ",
        "title": "Device-Free Human State Estimation using UWB Multi-Static Radios",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05411",
        "abstract_url": "http://arxiv.org/abs/2401.05411",
        "authors": [
            {
                "last_name": "Ben-Moshe",
                "first_name": "Noam"
            },
            {
                "last_name": "Tsutsui",
                "first_name": "Kenta"
            },
            {
                "last_name": "Biton",
                "first_name": "Shany"
            },
            {
                "last_name": "S\u00f6rnmo",
                "first_name": "Leif"
            },
            {
                "last_name": "Behar",
                "first_name": "Joachim A."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Introduction: Deep learning models for detecting episodes of atrial fibrillation (AF) using rhythm information in long-term, ambulatory ECG recordings have shown high performance. However, the rhythm-based approach does not take advantage of the morphological information conveyed by the different ECG waveforms, particularly the f-waves. As a result, the performance of such models may be inherently limited. Methods: To address this limitation, we have developed a deep learning model, named RawECGNet, to detect episodes of AF and atrial flutter (AFl) using the raw, single-lead ECG. We compare the generalization performance of RawECGNet on two external data sets that account for distribution shifts in geography, ethnicity, and lead position. RawECGNet is further benchmarked against a state-of-the-art deep learning model, named ArNet2, which utilizes rhythm information as input. Results: Using RawECGNet, the results for the different leads in the external test sets in terms of the F1 score were 0.91--0.94 in RBDB and 0.93 in SHDB, compared to 0.89--0.91 in RBDB and 0.91 in SHDB for ArNet2. The results highlight RawECGNet as a high-performance, generalizable algorithm for detection of AF and AFl episodes, exploiting information on both rhythm and morphology. ",
        "title": "RawECGNet: Deep Learning Generalization for Atrial Fibrillation  Detection from the Raw ECG",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05412",
        "abstract_url": "http://arxiv.org/abs/2401.05412",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xueyuan"
            },
            {
                "last_name": "Yao",
                "first_name": "Chao"
            },
            {
                "last_name": "Ban",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMUs data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of multiple sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our method not only differentiates between ambiguous actions such as sitting and standing but also produces more precise and natural motion. ",
        "title": "Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted  with Textual Semantics",
        "date": "2023-12-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05415",
        "abstract_url": "http://arxiv.org/abs/2401.05415",
        "authors": [
            {
                "last_name": "Bano",
                "first_name": "Muneera"
            },
            {
                "last_name": "Chaudhri",
                "first_name": "Zahid"
            },
            {
                "last_name": "Zowghi",
                "first_name": "Didar"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  As Artificial Intelligence (AI) transforms the domain of diplomacy in the 21st century, this research addresses the pressing need to evaluate the dualistic nature of these advancements, unpacking both the challenges they pose and the opportunities they offer. It has been almost a year since the launch of ChatGPT by OpenAI that revolutionised various work domains with its capabilities. The scope of application of these capabilities to diplomacy is yet to be fully explored or understood. Our research objective is to systematically examine the current discourse on Digital and AI Diplomacy, thus informing the development of a comprehensive framework for the role of Generative AI in modern diplomatic practices. Through the systematic analysis of 230 scholarly articles, we identified a spectrum of opportunities and challenges, culminating in a strategic framework that captures the multifaceted concepts for integration of Generative AI, setting a course for future research and innovation in diplomacy. ",
        "title": "The Role of Generative AI in Global Diplomatic Practices: A Strategic  Framework",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05416",
        "abstract_url": "http://arxiv.org/abs/2401.05416",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yifeng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanism (CRM), which enables the network to extract and represent category features without increasing trainable parameters. Furthermore, CRM transforms the common fully connected network into category representations, which provide closer supervision to the feature extractor than the far and trivial one-hot classification labels. We call this process of imposing interpretability on a network and using it to supervise the feature extractor the feature supervision mechanism, and its effectiveness is demonstrated experimentally and theoretically in this paper. The enhanced inertial signal can perform impracticable tasks with regard to the original signal, such as trajectory reconstruction. Both quantitative and visual results show that WDSNet outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised method, achieves the state-of-the-art performance of all the compared fully-supervised methods. ",
        "title": "Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05418",
        "abstract_url": "http://arxiv.org/abs/2401.05418",
        "authors": [
            {
                "last_name": "Haidri",
                "first_name": "Salman"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG"
        ],
        "abstract": "  The advent of compact, handheld devices has given us a pool of tracked movement data that could be used to infer trends and patterns that can be made to use. With this flooding of various trajectory data of animals, humans, vehicles, etc., the idea of ANALYTiC originated, using active learning to infer semantic annotations from the trajectories by learning from sets of labeled data. This study explores the application of dimensionality reduction and decision boundaries in combination with the already present active learning, highlighting patterns and clusters in data. We test these features with three different trajectory datasets with objective of exploiting the the already labeled data and enhance their interpretability. Our experimental analysis exemplifies the potential of these combined methodologies in improving the efficiency and accuracy of trajectory labeling. This study serves as a stepping-stone towards the broader integration of machine learning and visual methods in context of movement data analysis. ",
        "title": "ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction  in Machine Learning",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05420",
        "abstract_url": "http://arxiv.org/abs/2401.05420",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Debamita"
            },
            {
                "last_name": "Hanawal",
                "first_name": "Manjesh Kumar"
            },
            {
                "last_name": "Zlatanova",
                "first_name": "Nikola"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Holographic Metasurface Transceivers (HMTs) are emerging as cost-effective substitutes to large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. However, to achieve desired channel gains through beamforming in HMT, phase-shifts of a large number of elements need to be appropriately set, which is challenging. Also, these optimal phase-shifts depend on the location of the receivers, which could be unknown. In this work, we develop a learning algorithm using a {\\it fixed-budget multi-armed bandit framework} to beamform and maximize received signal strength at the receiver for far-field regions. Our algorithm, named \\Algo exploits the parametric form of channel gains of the beams, which can be expressed in terms of two {\\it phase-shifting parameters}. Even after parameterization, the problem is still challenging as phase-shifting parameters take continuous values. To overcome this, {\\it\\HB} works with the discrete values of phase-shifting parameters and exploits their unimodal relations with channel gains to learn the optimal values faster. We upper bound the probability of {\\it\\HB} incorrectly identifying the (discrete) optimal phase-shift parameters in terms of the number of pilots used in learning. We show that this probability decays exponentially with the number of pilot signals. We demonstrate that {\\it\\HB} outperforms state-of-the-art algorithms through extensive simulations. ",
        "title": "HoloBeam: Learning Optimal Beamforming in Far-Field Holographic  Metasurface Transceivers",
        "date": "2023-12-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05421",
        "abstract_url": "http://arxiv.org/abs/2401.05421",
        "authors": [
            {
                "last_name": "Al-Lawati",
                "first_name": "Ali"
            },
            {
                "last_name": "Eshra",
                "first_name": "Elsayed"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  Trajectory generation is an important concern in pedestrian, vehicle, and wildlife movement studies. Generated trajectories help enrich the training corpus in relation to deep learning applications, and may be used to facilitate simulation tasks. This is especially significant in the wildlife domain, where the cost of obtaining additional real data can be prohibitively expensive, time-consuming, and bear ethical considerations. In this paper, we introduce WildGEN: a conceptual framework that addresses this challenge by employing a Variational Auto-encoders (VAEs) based method for the acquisition of movement characteristics exhibited by wild geese over a long horizon using a sparse set of truth samples. A subsequent post-processing step of the generated trajectories is performed based on smoothing filters to reduce excessive wandering. Our evaluation is conducted through visual inspection and the computation of the Hausdorff distance between the generated and real trajectories. In addition, we utilize the Pearson Correlation Coefficient as a way to measure how realistic the trajectories are based on the similarity of clusters evaluated on the generated and real trajectories. ",
        "title": "WildGEN: Long-horizon Trajectory Generation for Wildlife",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05422",
        "abstract_url": "http://arxiv.org/abs/2401.05422",
        "authors": [
            {
                "last_name": "M",
                "first_name": "Karthik R"
            },
            {
                "last_name": "Hegde",
                "first_name": "Dhiraj Nagaraja"
            },
            {
                "last_name": "Sarajlic",
                "first_name": "Muris"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Beam management (BM) protocols are critical for establishing and maintaining connectivity between network radio nodes and User Equipments (UEs). In Distributed Multiple Input Multiple Output systems (D-MIMO), a number of access points (APs), coordinated by a central processing unit (CPU), serves a number of UEs. At mmWave frequencies, the problem of finding the best AP and beam to serve the UEs is challenging due to a large number of beams that need to be sounded with Downlink (DL) reference signals. The objective of this paper is to investigate whether the best AP/beam can be reliably inferred from sounding only a small subset of beams and leveraging AI/ML for inference of best beam/AP. We use Random Forest (RF), MissForest (MF) and conditional Generative Adversarial Networks (c-GAN) for demonstrating the performance benefits of inference. ",
        "title": "Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave  Distributed Multiple Input Multiple Output (D-MIMO) systems",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05424",
        "abstract_url": "http://arxiv.org/abs/2401.05424",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Yuxiang"
            },
            {
                "last_name": "Djemili",
                "first_name": "Karim"
            },
            {
                "last_name": "Elezi",
                "first_name": "Denis"
            },
            {
                "last_name": "Shalman",
                "first_name": "Aaneel"
            },
            {
                "last_name": "P\u00e9rez-Ortiz",
                "first_name": "Mar\u00eda"
            },
            {
                "last_name": "Yilmaz",
                "first_name": "Emine"
            },
            {
                "last_name": "Shawe-Taylor",
                "first_name": "John"
            },
            {
                "last_name": "Bulathwela",
                "first_name": "Sahan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "IR",
            "LG"
        ],
        "abstract": "  With the advancement and utility of Artificial Intelligence (AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models was designed following the \"open learner\" concept, using humanly-intuitive user representations. This family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytics practitioners. The experiments show the utility of both the dataset and the library with predictive performance significantly exceeding comparative baseline models. The dataset contains a large amount of AI-related educational videos, which are of interest for building and validating AI-specific educational recommenders. ",
        "title": "A Toolbox for Modelling Engagement with Educational Videos",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05425",
        "abstract_url": "http://arxiv.org/abs/2401.05425",
        "authors": [
            {
                "last_name": "Aziz",
                "first_name": "Abdul"
            },
            {
                "last_name": "Pham",
                "first_name": "Nhat"
            },
            {
                "last_name": "Vora",
                "first_name": "Neel"
            },
            {
                "last_name": "Reynolds",
                "first_name": "Cody"
            },
            {
                "last_name": "Lehnen",
                "first_name": "Jaime"
            },
            {
                "last_name": "Venkatesh",
                "first_name": "Pooja"
            },
            {
                "last_name": "Yao",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Harvey",
                "first_name": "Jay"
            },
            {
                "last_name": "Vu",
                "first_name": "Tam"
            },
            {
                "last_name": "Ding",
                "first_name": "Kan"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Phuc"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Epilepsy is one of the most common neurological diseases globally, affecting around 50 million people worldwide. Fortunately, up to 70 percent of people with epilepsy could live seizure-free if properly diagnosed and treated, and a reliable technique to monitor the onset of seizures could improve the quality of life of patients who are constantly facing the fear of random seizure attacks. The scalp-based EEG test, despite being the gold standard for diagnosing epilepsy, is costly, necessitates hospitalization, demands skilled professionals for operation, and is discomforting for users. In this paper, we propose EarSD, a novel lightweight, unobtrusive, and socially acceptable ear-worn system to detect epileptic seizure onsets by measuring the physiological signals from behind the user's ears. EarSD includes an integrated custom-built sensing, computing, and communication PCB to collect and amplify the signals of interest, remove the noises caused by motion artifacts and environmental impacts, and stream the data wirelessly to the computer or mobile phone nearby, where data are uploaded to the host computer for further processing. We conducted both in-lab and in-hospital experiments with epileptic seizure patients who were hospitalized for seizure studies. The preliminary results confirm that EarSD can detect seizures with up to 95.3 percent accuracy by just using classical machine learning algorithms. ",
        "title": "An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic  Seizure Detection",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05426",
        "abstract_url": "http://arxiv.org/abs/2401.05426",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Zimin"
            },
            {
                "last_name": "Gei\u00dfler",
                "first_name": "Daniel"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Suh",
                "first_name": "Sungho"
            },
            {
                "last_name": "Lukowicz",
                "first_name": "Paul"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according to the weight score ranking. We tested our framework's effectiveness in optimizing sensor modality and sampling rate selection using three public HAR benchmark datasets. The results show that the sensor and sampling rate combination selected via CoSS achieves similar classification performance to configurations using the highest sampling rate with all sensors but at a reduced hardware cost. ",
        "title": "CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in  Human Activity Recognition",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05427",
        "abstract_url": "http://arxiv.org/abs/2401.05427",
        "authors": [
            {
                "last_name": "van Putten",
                "first_name": "Maurice H. P. M."
            },
            {
                "last_name": "Wilson",
                "first_name": "Leighton"
            },
            {
                "last_name": "Lavely",
                "first_name": "Adam W."
            },
            {
                "last_name": "Hair",
                "first_name": "Mark"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "DS"
        ],
        "abstract": "  Searches for signals at low signal-to-noise ratios frequently involve the Fast Fourier Transform (FFT). For high-throughput searches, we here consider FFT on the homogeneous mesh of Processing Elements (PEs) of a wafer-scale engine (WSE). To minimize memory overhead in the inherently non-local FFT algorithm, we introduce a new synchronous slide operation ({\\em Slide}) exploiting the fast interconnect between adjacent PEs. Feasibility of compute-limited performance is demonstrated in linear scaling of Slide execution times with varying array size in preliminary benchmarks on the CS-2 WSE. The proposed implementation appears opportune to accelerate and open the full discovery potential of FFT-based signal processing in multi-messenger astronomy. ",
        "title": "Slide FFT on a homogeneous mesh in wafer-scale computing",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05430",
        "abstract_url": "http://arxiv.org/abs/2401.05430",
        "authors": [
            {
                "last_name": "You",
                "first_name": "Zinuo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Pengju"
            },
            {
                "last_name": "Zheng",
                "first_name": "Jin"
            },
            {
                "last_name": "Cartlidge",
                "first_name": "John"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also capturing the overall structure of the stock graph. Comprehensive experiments conducted on real-world datasets from two US markets (NASDAQ and NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the effectiveness of our method. Our approach consistently outperforms state-of-the-art baselines in forecasting next trading day stock trends across three test periods spanning seven years. Datasets and code have been released (https://github.com/pixelhero98/MGDPR). ",
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention  for Stock Trends Classification",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05431",
        "abstract_url": "http://arxiv.org/abs/2401.05431",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Luyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Cong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhai",
                "first_name": "Shengfang"
            },
            {
                "last_name": "Fang",
                "first_name": "Yuejian"
            },
            {
                "last_name": "Shen",
                "first_name": "Qingni"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhonghai"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Representation learning frameworks in unlabeled time series have been proposed for medical signal processing. Despite the numerous excellent progresses have been made in previous works, we observe the representation extracted for the time series still does not generalize well. In this paper, we present a Time series (medical signal) Representation Learning framework via Spectrogram (TRLS) to get more informative representations. We transform the input time-domain medical signals into spectrograms and design a time-frequency encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale representations from the augmented spectrograms. Our TRLS takes spectrogram as input with two types of different data augmentations and maximizes the similarity between positive ones, which effectively circumvents the problem of designing negative samples. Our evaluation of four real-world medical signal datasets focusing on medical signal classification shows that TRLS is superior to the existing frameworks. ",
        "title": "TRLS: A Time Series Representation Learning Framework via Spectrogram  for Medical Signal Processing",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05432",
        "abstract_url": "http://arxiv.org/abs/2401.05432",
        "authors": [
            {
                "last_name": "Hossain",
                "first_name": "Khondoker Murad"
            },
            {
                "last_name": "Oates",
                "first_name": "Tim"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on models trained on the MNIST digit dataset, CIFAR-10 dataset, and two difficult datasets from NIST's TrojAI competition. These results show that our method detects backdoored networks more accurately and efficiently than current state-of-the-art methods. ",
        "title": "TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep  Neural Networks",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05433",
        "abstract_url": "http://arxiv.org/abs/2401.05433",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Che",
                "first_name": "Chang"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Liu",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs' writing proficiency within AES.   To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adversarial Weights Perturbation (AWP) and Metric-specific AttentionPooling (6 kinds of AP) for each label in the competition.   The primary focus of this research is to investigate the impact of hyperparameters, particularly the adversarial learning rate, on the performance of the model. By fine-tuning the hyperparameter tuning process, including the influence of 6AP and AWP, the resulting models can provide more accurate evaluations of language proficiency and support tailored learning tasks for ELLs. This work has the potential to significantly benefit ELLs by improving their English language proficiency and facilitating their educational journey. ",
        "title": "Enhancing Essay Scoring with Adversarial Weights Perturbation and  Metric-specific AttentionPooling",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05434",
        "abstract_url": "http://arxiv.org/abs/2401.05434",
        "authors": [
            {
                "last_name": "Akan",
                "first_name": "Taymaz"
            },
            {
                "last_name": "Alp",
                "first_name": "Sait"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Mohammad Alfrad Nobel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat. There are various types of arrhythmias that can originate from different areas of the heart, resulting in either a rapid, slow, or irregular heartbeat. An electrocardiogram (ECG) is a vital diagnostic tool used to detect heart irregularities and abnormalities, allowing experts to analyze the heart's electrical signals to identify intricate patterns and deviations from the norm. Over the past few decades, numerous studies have been conducted to develop automated methods for classifying heartbeats based on ECG data. In recent years, deep learning has demonstrated exceptional capabilities in tackling various medical challenges, particularly with transformers as a model architecture for sequence processing. By leveraging the transformers, we developed the ECGformer model for the classification of various arrhythmias present in electrocardiogram data. We assessed the suggested approach using the MIT-BIH and PTB datasets. ECG heartbeat arrhythmia classification results show that the proposed method is highly effective. ",
        "title": "ECGformer: Leveraging transformer for ECG heartbeat arrhythmia  classification",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05436",
        "abstract_url": "http://arxiv.org/abs/2401.05436",
        "authors": [
            {
                "last_name": "Jameel",
                "first_name": "Abu Shafin Mohammad Mahdee"
            },
            {
                "last_name": "Malhotra",
                "first_name": "Akshay"
            },
            {
                "last_name": "Gamal",
                "first_name": "Aly El"
            },
            {
                "last_name": "Hamidi-Rad",
                "first_name": "Shahab"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI"
        ],
        "abstract": "  In this paper, we propose a deep-learning-based channel estimation scheme in an orthogonal frequency division multiplexing (OFDM) system. Our proposed method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is based on a novel study of recurrent models for exploiting sequential behavior of channels across frequencies. Utilizing the fact that wireless channels have a high degree of correlation across frequencies, we employ recurrent neural network techniques within a single OFDM slot, thus overcoming the latency and memory constraints typically associated with recurrence based methods. The proposed SisRafNet delivers superior estimation performance compared to existing deep-learning-based channel estimation techniques and the performance has been validated on a wide range of 3rd Generation Partnership Project (3GPP) compliant channel scenarios at multiple signal-to-noise ratios. ",
        "title": "Deep OFDM Channel Estimation: Capturing Frequency Recurrence",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05439",
        "abstract_url": "http://arxiv.org/abs/2401.05439",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Biao"
            },
            {
                "last_name": "Heitor",
                "first_name": "Ana"
            },
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaohui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  The emergence of neural networks constrained by physical governing equations has sparked a new trend in deep learning research, which is known as Physics-Informed Neural Networks (PINNs). However, solving high-dimensional problems with PINNs is still a substantial challenge, the space complexity brings difficulty to solving large multidirectional problems. In this paper, a novel PINN framework to quickly predict several three-dimensional Terzaghi consolidation cases under different conditions is proposed. Meanwhile, the loss functions for different cases are introduced, and their differences in three-dimensional consolidation problems are highlighted. The tuning strategies for the PINNs framework for three-dimensional consolidation problems are introduced. Then, the performance of PINNs is tested and compared with traditional numerical methods adopted in forward problems, and the coefficients of consolidation and the impact of noisy data in inverse problems are identified. Finally, the results are summarized and presented from three-dimensional simulations of PINNs, which show an accuracy rate of over 99% compared with ground truth for both forward and inverse problems. These results are desirable with good accuracy and can be used for soil settlement prediction, which demonstrates that the proposed PINNs framework can learn the three-dimensional consolidation PDE well.   Keywords: Three-dimensional Terzaghi consolidation; Physics-informed neural networks (PINNs); Forward problems; Inverse problems; soil settlement ",
        "title": "Physics-informed Deep Learning to Solve Three-dimensional Terzaghi  Consolidation Equation: Forward and Inverse Problems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05440",
        "abstract_url": "http://arxiv.org/abs/2401.05440",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Qian"
            },
            {
                "last_name": "Hao",
                "first_name": "Yanling"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanwei"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  WiFi human sensing is highly regarded for its low-cost and privacy advantages in recognizing human activities. However, its effectiveness is largely confined to controlled, single-user, line-of-sight settings, limited by data collection complexities and the scarcity of labeled datasets. Traditional cross-modal methods, aimed at mitigating these limitations by enabling self-supervised learning without labeled data, struggle to extract meaningful features from amplitude-phase combinations. In response, we introduce AutoSen, an innovative automatic WiFi sensing solution that departs from conventional approaches. AutoSen establishes a direct link between amplitude and phase through automated cross-modal autoencoder learning. This autoencoder efficiently extracts valuable features from unlabeled CSI data, encompassing amplitude and phase information while eliminating their respective unique noises. These features are then leveraged for specific tasks using few-shot learning techniques. AutoSen's performance is rigorously evaluated on a publicly accessible benchmark dataset, demonstrating its exceptional capabilities in automatic WiFi sensing through the extraction of comprehensive cross-modal features. ",
        "title": "Autosen: improving automatic wifi human sensing through cross-modal  autoencoder",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05441",
        "abstract_url": "http://arxiv.org/abs/2401.05441",
        "authors": [
            {
                "last_name": "Mehrban",
                "first_name": "Ali"
            },
            {
                "last_name": "Ahadian",
                "first_name": "Pegah"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "CR",
            "LG"
        ],
        "abstract": "  This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time. ",
        "title": "An adaptive network-based approach for advanced forecasting of  cryptocurrency values",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05443",
        "abstract_url": "http://arxiv.org/abs/2401.05443",
        "authors": [
            {
                "last_name": "Fakih",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Dharmaji",
                "first_name": "Rahul"
            },
            {
                "last_name": "Moghaddas",
                "first_name": "Yasamin"
            },
            {
                "last_name": "Araya",
                "first_name": "Gustavo Quiros"
            },
            {
                "last_name": "Ogundare",
                "first_name": "Oluwatosin"
            },
            {
                "last_name": "Faruque",
                "first_name": "Mohammad Abdullah Al"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "PL"
        ],
        "abstract": "  Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools including grammar checkers, compilers and SMV verifiers to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally flawed code to producing verifiably correct programs for industrial applications. We run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The proposed pipeline improved the generation success rate from 47% to 72%, and the Survey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open research, we share the complete experimental setup, the LLM Fine-Tuning Weights, and the video demonstrations of the different programs on our dedicated webpage. ",
        "title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of  PLCs in Industrial Control Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05444",
        "abstract_url": "http://arxiv.org/abs/2401.05444",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Ding"
            },
            {
                "last_name": "Peng",
                "first_name": "Peixi"
            },
            {
                "last_name": "Huang",
                "first_name": "Tiejun"
            },
            {
                "last_name": "Tian",
                "first_name": "Yonghong"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (DRL). In this paper, we focus on the task where the agent needs to learn multi-dimensional deterministic policies to control, which is very common in real scenarios. Recently, the surrogate gradient method has been utilized for training multi-layer SNNs, which allows SNNs to achieve comparable performance with the corresponding deep networks in this task. Most existing spike-based RL methods take the firing rate as the output of SNNs, and convert it to represent continuous action space (i.e., the deterministic policy) through a fully-connected (FC) layer. However, the decimal characteristic of the firing rate brings the floating-point matrix operations to the FC layer, making the whole SNN unable to deploy on the neuromorphic hardware directly. To develop a fully spiking actor network without any floating-point matrix operations, we draw inspiration from the non-spiking interneurons found in insects and employ the membrane voltage of the non-spiking neurons to represent the action. Before the non-spiking neurons, multiple population neurons are introduced to decode different dimensions of actions. Since each population is used to decode a dimension of action, we argue that the neurons in each population should be connected in time domain and space domain. Hence, the intra-layer connections are used in output populations to enhance the representation capacity. Finally, we propose a fully spiking actor network with intra-layer connections (ILC-SAN). ",
        "title": "Fully Spiking Actor Network with Intra-layer Connections for  Reinforcement Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05446",
        "abstract_url": "http://arxiv.org/abs/2401.05446",
        "authors": [
            {
                "last_name": "Weng",
                "first_name": "Weining"
            },
            {
                "last_name": "Gu",
                "first_name": "Yang"
            },
            {
                "last_name": "Guo",
                "first_name": "Shuai"
            },
            {
                "last_name": "Ma",
                "first_name": "Yuan"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhaohua"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Yiqiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Electroencephalogram (EEG) is a non-invasive technique to record bioelectrical signals. Integrating supervised deep learning techniques with EEG signals has recently facilitated automatic analysis across diverse EEG-based tasks. However, the label issues of EEG signals have constrained the development of EEG-based deep models. Obtaining EEG annotations is difficult that requires domain experts to guide collection and labeling, and the variability of EEG signals among different subjects causes significant label shifts. To solve the above challenges, self-supervised learning (SSL) has been proposed to extract representations from unlabeled samples through well-designed pretext tasks. This paper concentrates on integrating SSL frameworks with temporal EEG signals to achieve efficient representation and proposes a systematic review of the SSL for EEG signals. In this paper, 1) we introduce the concept and theory of self-supervised learning and typical SSL frameworks. 2) We provide a comprehensive review of SSL for EEG analysis, including taxonomy, methodology, and technique details of the existing EEG-based SSL frameworks, and discuss the difference between these methods. 3) We investigate the adaptation of the SSL approach to various downstream tasks, including the task description and related benchmark datasets. 4) Finally, we discuss the potential directions for future SSL-EEG research. ",
        "title": "Self-supervised Learning for Electroencephalogram: A Systematic Survey",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05447",
        "abstract_url": "http://arxiv.org/abs/2401.05447",
        "authors": [
            {
                "last_name": "Lefort",
                "first_name": "Baptiste"
            },
            {
                "last_name": "Benhamou",
                "first_name": "Eric"
            },
            {
                "last_name": "Ohana",
                "first_name": "Jean-Jacques"
            },
            {
                "last_name": "Saltiel",
                "first_name": "David"
            },
            {
                "last_name": "Guez",
                "first_name": "Beatrice"
            },
            {
                "last_name": "Challet",
                "first_name": "Damien"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation. ",
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market  Wraps?",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05450",
        "abstract_url": "http://arxiv.org/abs/2401.05450",
        "authors": [
            {
                "last_name": "Mandran",
                "first_name": "Nadine"
            },
            {
                "last_name": "Prior",
                "first_name": "Estelle"
            },
            {
                "last_name": "Sanchez",
                "first_name": "Eric"
            },
            {
                "last_name": "Vermeulen",
                "first_name": "Mathieu"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  One of the main difficulties remains the collaboration between the various experts involved in designing the Learning Games (LG). Our literature review focuses on the pitfalls and principles that have been identified by various authors in learning games design. Based on this review, a prototype was designed to support the LG design process and to study more precisely the collaboration between actors (teachers, researchers, game designers, data analyst and computer scientist). Indeed, according to the state of the art, the skills and knowledge involved in design are difficult to integrate. It has been tested in a real-world scenario for designing learning games to teach algorithmic. Through participant observation in thirty-three workshops involving nine experts, we were able to identify recurring pitfalls as we applied the recommendations in the literature. The analysis of these workshops led to propose eight principles aimed at facilitating the collaboration between the learning games design process and re-evaluating research on its. ",
        "title": "Reorienting Learning Game Design in Design-Based Research: a Case Study",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05451",
        "abstract_url": "http://arxiv.org/abs/2401.05451",
        "authors": [
            {
                "last_name": "Joshy",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Assessing and comparing player skill in online multiplayer gaming environments is essential for fair matchmaking and player engagement. Traditional ranking models like Elo and Glicko-2, designed for two-player games, are insufficient for the complexity of multi-player, asymmetric team-based matches. To address this gap, the OpenSkill library offers a suite of sophisticated, fast, and adaptable models tailored for such dynamics. Drawing from Bayesian inference methods, OpenSkill provides a more accurate representation of individual player contributions and speeds up the computation of ranks. This paper introduces the OpenSkill library, featuring a Python implementation of the Plackett-Luce model among others, highlighting its performance advantages and predictive accuracy against proprietary systems like TrueSkill. OpenSkill is a valuable tool for game developers and researchers, ensuring a responsive and fair gaming experience by efficiently adjusting player rankings based on game outcomes. The library's support for time decay and diligent documentation further aid in its practical application, making it a robust solution for the nuanced world of multiplayer ranking systems. This paper also acknowledges areas for future enhancement, such as partial play and contribution weighting, emphasizing the library's ongoing development to meet the evolving needs of online gaming communities. ",
        "title": "OpenSkill: A faster asymmetric multi-team, multiplayer rating system",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05452",
        "abstract_url": "http://arxiv.org/abs/2401.05452",
        "authors": [
            {
                "last_name": "Tahir",
                "first_name": "Muhammad Ahmad"
            },
            {
                "last_name": "Mehmood",
                "first_name": "Ahsan"
            },
            {
                "last_name": "Rahman",
                "first_name": "Muhammad Mahboob Ur"
            },
            {
                "last_name": "Nawaz",
                "first_name": "Muhammad Wasim"
            },
            {
                "last_name": "Riaz",
                "first_name": "Kashif"
            },
            {
                "last_name": "Abbasi",
                "first_name": "Qammer H."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "LG"
        ],
        "abstract": "  We propose two novel purpose-built deep learning (DL) models for synthesis of the arterial blood pressure (ABP) waveform in a cuff-less manner, using a single-site photoplethysmography (PPG) signal. We utilize the public UCI dataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our DL models. Firstly, we implement a transformer model that incorporates positional encoding, multi-head attention, layer normalization, and dropout techniques, and synthesizes the ABP waveform with a mean absolute error (MAE) of 14. Secondly, we implement a frequency-domain (FD) learning approach where we first obtain the discrete cosine transform (DCT) coefficients of the PPG and ABP signals corresponding to two cardiac cycles, and then learn a linear/non-linear (L/NL) regression between them. We learn that the FD L/NL regression model outperforms the transformer model by achieving an MAE of 11.87 and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP), respectively. Our FD L/NL regression model also fulfills the AAMI criterion of utilizing data from more than 85 subjects, and achieves grade B by the BHS criterion. ",
        "title": "Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site  PPG using Transformer & Frequency-domain Learning",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05453",
        "abstract_url": "http://arxiv.org/abs/2401.05453",
        "authors": [
            {
                "last_name": "Anderberg",
                "first_name": "Alastair"
            },
            {
                "last_name": "Bailey",
                "first_name": "James"
            },
            {
                "last_name": "Campello",
                "first_name": "Ricardo J. G. B."
            },
            {
                "last_name": "Houle",
                "first_name": "Michael E."
            },
            {
                "last_name": "Marques",
                "first_name": "Henrique O."
            },
            {
                "last_name": "Radovanovi\u0107",
                "first_name": "Milo\u0161"
            },
            {
                "last_name": "Zimek",
                "first_name": "Arthur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN. ",
        "title": "Dimensionality-Aware Outlier Detection: Theoretical and Experimental  Analysis",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05458",
        "abstract_url": "http://arxiv.org/abs/2401.05458",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dongyu"
            },
            {
                "last_name": "Hu",
                "first_name": "Ruofan"
            },
            {
                "last_name": "Rundensteiner",
                "first_name": "Elke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Deep neural networks (DNNs) have advanced many machine learning tasks, but their performance is often harmed by noisy labels in real-world data. Addressing this, we introduce CoLafier, a novel approach that uses Local Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a sample's features and its label to predict the label - which allows it to produce an enhanced internal representation. We observe that LID scores computed from this representation effectively distinguish between correct and incorrect labels across various noise scenarios. In contrast to LID-dis, LID-gen, functioning as a regular classifier, operates solely on the sample's features. During training, CoLafier utilizes two augmented views per instance to feed both subnets. CoLafier considers the LID scores from the two views as produced by LID-dis to assign weights in an adapted loss function for both subnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels. LID-dis then processes these pseudo-labels along with two views to derive LID scores. Finally, these LID scores along with the differences in predictions from the two subnets guide the label update decisions. This dual-view and dual-subnet approach enhances the overall reliability of the framework. Upon completion of the training, we deploy the LID-gen subnet of CoLafier as the final classification model. CoLafier demonstrates improved prediction accuracy, surpassing existing methods, particularly under severe label noise. For more details, see the code at https://github.com/zdy93/CoLafier. ",
        "title": "CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic  Dimensionality Guidance",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05459",
        "abstract_url": "http://arxiv.org/abs/2401.05459",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanchun"
            },
            {
                "last_name": "Wen",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Weijun"
            },
            {
                "last_name": "Li",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yizhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Guohong"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Wenxing"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiang"
            },
            {
                "last_name": "Sun",
                "first_name": "Yi"
            },
            {
                "last_name": "Kong",
                "first_name": "Rui"
            },
            {
                "last_name": "Wang",
                "first_name": "Yile"
            },
            {
                "last_name": "Geng",
                "first_name": "Hanfei"
            },
            {
                "last_name": "Luan",
                "first_name": "Jian"
            },
            {
                "last_name": "Jin",
                "first_name": "Xuefeng"
            },
            {
                "last_name": "Ye",
                "first_name": "Zilong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Guanjing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Li",
                "first_name": "Xiang"
            },
            {
                "last_name": "Xu",
                "first_name": "Mengwei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhijun"
            },
            {
                "last_name": "Li",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ya-Qin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunxin"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "SE"
        ],
        "abstract": "  Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges. ",
        "title": "Personal LLM Agents: Insights and Survey about the Capability,  Efficiency and Security",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05461",
        "abstract_url": "http://arxiv.org/abs/2401.05461",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Zhanliang"
            },
            {
                "last_name": "Xiong",
                "first_name": "Nuoye"
            },
            {
                "last_name": "Li",
                "first_name": "Hongsheng"
            },
            {
                "last_name": "Shen",
                "first_name": "Peiyi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  Despite neural networks (NN) have been widely applied in various fields and generally outperforms humans, they still lack interpretability to a certain extent, and humans are unable to intuitively understand the decision logic of NN. This also hinders the knowledge interaction between humans and NN, preventing humans from getting involved to give direct guidance when NN's decisions go wrong. While recent research in explainable AI has achieved interpretability of NN from various perspectives, it has not yet provided effective methods for knowledge exchange between humans and NN. To address this problem, we constructed a two-way interaction interface that uses structured representations of visual concepts and their relationships as the \"language\" for knowledge exchange between humans and NN. Specifically, NN provide intuitive reasoning explanations to humans based on the class-specific structural concepts graph (C-SCG). On the other hand, humans can modify the biases present in the C-SCG through their prior knowledge and reasoning ability, and thus provide direct knowledge guidance to NN through this interface. Through experimental validation, based on this interaction interface, NN can provide humans with easily understandable explanations of the reasoning process. Furthermore, human involvement and prior knowledge can directly and effectively contribute to enhancing the performance of NN. ",
        "title": "The two-way knowledge interaction interface between humans and neural  networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05462",
        "abstract_url": "http://arxiv.org/abs/2401.05462",
        "authors": [
            {
                "last_name": "Ge",
                "first_name": "Mouzhi"
            },
            {
                "last_name": "Rossi",
                "first_name": "Bruno"
            },
            {
                "last_name": "Chren",
                "first_name": "Stanislav"
            },
            {
                "last_name": "Blanco",
                "first_name": "Jos\u00e9 Miguel"
            }
        ],
        "primary_category": "OH",
        "categories": [
            "OH"
        ],
        "abstract": "  Since the energy domain is in a transformative shift towards sustainability, the integration of new technologies and smart systems into traditional power grids has emerged. As an effective approach, Petri Nets (PN) have been applied to model and analyze the complex dynamics in Smart Grid (SG) environments. However, we are currently missing an overview of types of PNs applied to different areas and problems related to SGs. Therefore, this paper proposes four fundamental research questions related to the application areas of PNs in SGs, PNs types, aspects modelled by PNs in the identified areas, and the validation methods in the evaluation. The answers to the research questions are derived from a comprehensive and interdisciplinary literature analysis. The results capture a valuable overview of PNs applications in the global energy landscape and can offer indications for future research directions. ",
        "title": "Petri Nets for Smart Grids: The Story So Far",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05465",
        "abstract_url": "http://arxiv.org/abs/2401.05465",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lin"
            },
            {
                "last_name": "Xu",
                "first_name": "Linghan"
            },
            {
                "last_name": "Motamed",
                "first_name": "Saman"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Shayok"
            },
            {
                "last_name": "De la Torre",
                "first_name": "Fernando"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unsupervised domain adaptation (UDA) for image classification has made remarkable progress in transferring classification knowledge from a labeled source domain to an unlabeled target domain, thanks to effective domain alignment techniques. Recently, in order to further improve performance on a target domain, many Single-Target Active Domain Adaptation (ST-ADA) methods have been proposed to identify and annotate the salient and exemplar target samples. However, it requires one model to be trained and deployed for each target domain and the domain label associated with each test sample. This largely restricts its application in the ubiquitous scenarios with multiple target domains. Therefore, we propose a Multi-Target Active Domain Adaptation (MT-ADA) framework for image classification, named D3GU, to simultaneously align different domains and actively select samples from them for annotation. This is the first research effort in this field to our best knowledge. D3GU applies Decomposed Domain Discrimination (D3) during training to achieve both source-target and target-target domain alignments. Then during active sampling, a Gradient Utility (GU) score is designed to weight every unlabeled target image by its contribution towards classification and domain alignment tasks, and is further combined with KMeans clustering to form GU-KMeans for diverse image sampling. Extensive experiments on three benchmark datasets, Office31, OfficeHome, and DomainNet, have been conducted to validate consistently superior performance of D3GU for MT-ADA. ",
        "title": "D3GU: Multi-Target Active Domain Adaptation via Enhancing Domain  Alignment",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05467",
        "abstract_url": "http://arxiv.org/abs/2401.05467",
        "authors": [
            {
                "last_name": "Taneja",
                "first_name": "Karan"
            },
            {
                "last_name": "Goel",
                "first_name": "Ashok"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be achieved with supervision on 20-70% of the dataset depending upon the complexity of the task and performance of zero-shot learners. ",
        "title": "Machine Teaching for Building Modular AI Agents based on Zero-shot  Learners",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05468",
        "abstract_url": "http://arxiv.org/abs/2401.05468",
        "authors": [
            {
                "last_name": "Zanardini",
                "first_name": "Damiano"
            },
            {
                "last_name": "Serrano",
                "first_name": "Emilio"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  This paper introduces a new problem in the field of graph mining and social network analysis called new node prediction. More technically, the task can be categorized as zero-shot out-of-graph all-links prediction. This challenging problem aims to predict all links from a new, isolated, and unobserved node that was previously disconnected from the graph. Unlike classic approaches to link prediction (including few-shot out-of-graph link prediction), this problem presents two key differences: (1) the new node has no existing links from which to extract patterns for new predictions; and (2) the goal is to predict not just one, but all the links of this new node, or at least a significant part of them. Experiments demonstrate that an architecture based on Deep Graph Neural Networks can learn to solve this challenging problem in a bibliographic citation network. ",
        "title": "Introducing New Node Prediction in Graph Mining: Predicting All Links  from Isolated Nodes with Graph Neural Networks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05469",
        "abstract_url": "http://arxiv.org/abs/2401.05469",
        "authors": [
            {
                "last_name": "Kazemi",
                "first_name": "Kianoosh"
            },
            {
                "last_name": "Azimi",
                "first_name": "Iman"
            },
            {
                "last_name": "Liljeberg",
                "first_name": "Pasi"
            },
            {
                "last_name": "Rahmani",
                "first_name": "Amir M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Respiratory rate (RR) serves as an indicator of various medical conditions, such as cardiovascular diseases and sleep disorders. These RR estimation methods were mostly designed for finger-based PPG collected from subjects in stationary situations (e.g., in hospitals). In contrast to finger-based PPG signals, wrist-based PPG are more susceptible to noise, particularly in their low frequency range, which includes respiratory information. Therefore, the existing methods struggle to accurately extract RR when PPG data are collected from wrist area under free-living conditions. The increasing popularity of smartwatches, equipped with various sensors including PPG, has prompted the need for a robust RR estimation method. In this paper, we propose a convolutional neural network-based approach to extract RR from PPG, accelerometer, and gyroscope signals captured via smartwatches. Our method, including a dilated residual inception module and 1D convolutions, extract the temporal information from the signals, enabling RR estimation. Our method is trained and tested using data collected from 36 subjects under free-living conditions for one day using Samsung Gear Sport watches. For evaluation, we compare the proposed method with four state-of-the-art RR estimation methods. The RR estimates are compared with RR references obtained from a chest-band device. The results show that our method outperforms the existing methods with the Mean-Absolute-Error and Root-Mean-Square-Error of 1.85 and 2.34, while the best results obtained by the other methods are 2.41 and 3.29, respectively. Moreover, compared to the other methods, the absolute error distribution of our method was narrow (with the lowest median), indicating a higher level of agreement between the estimated and reference RR values. ",
        "title": "Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05470",
        "abstract_url": "http://arxiv.org/abs/2401.05470",
        "authors": [
            {
                "last_name": "Estopinan",
                "first_name": "Joaquim"
            },
            {
                "last_name": "Bonnet",
                "first_name": "Pierre"
            },
            {
                "last_name": "Servajean",
                "first_name": "Maximilien"
            },
            {
                "last_name": "Munoz",
                "first_name": "Fran\u00e7ois"
            },
            {
                "last_name": "Joly",
                "first_name": "Alexis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The post-2020 global biodiversity framework needs ambitious, research-based targets. Estimating the accelerated extinction risk due to climate change is critical. The International Union for Conservation of Nature (IUCN) measures the extinction risk of species. Automatic methods have been developed to provide information on the IUCN status of under-assessed taxa. However, these compensatory methods are based on current species characteristics, mainly geographical, which precludes their use in future projections. Here, we evaluate a novel method for classifying the IUCN status of species benefiting from the generalisation power of species distribution models based on deep learning. Our method matches state-of-the-art classification performance while relying on flexible SDM-based features that capture species' environmental preferences. Cross-validation yields average accuracies of 0.61 for status classification and 0.78 for binary classification. Climate change will reshape future species distributions. Under the species-environment equilibrium hypothesis, SDM projections approximate plausible future outcomes. Two extremes of species dispersal capacity are considered: unlimited or null. The projected species distributions are translated into features feeding our IUCN classification method. Finally, trends in threatened species are analysed over time and i) by continent and as a function of average ii) latitude or iii) altitude. The proportion of threatened species is increasing globally, with critical rates in Africa, Asia and South America. Furthermore, the proportion of threatened species is predicted to peak around the two Tropics, at the Equator, in the lowlands and at altitudes of 800-1,500 m. ",
        "title": "Modelling Species Distributions with Deep Learning to Predict Plant  Extinction Risk and Assess Climate Change Impacts",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05476",
        "abstract_url": "http://arxiv.org/abs/2401.05476",
        "authors": [
            {
                "last_name": "Kapsalis",
                "first_name": "Timo"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "GR"
        ],
        "abstract": "  This paper introduces CADgpt, an innovative plugin integrating Natural Language Processing (NLP) with Rhino3D for enhancing 3D modelling in computer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgpt simplifies the CAD interface, enabling users, particularly beginners, to perform complex 3D modelling tasks through intuitive natural language commands. This approach significantly reduces the learning curve associated with traditional CAD software, fostering a more inclusive and engaging educational environment. The paper discusses CADgpt's technical architecture, including its integration within Rhino3D and the adaptation of GPT-4 capabilities for CAD tasks. It presents case studies demonstrating CADgpt's efficacy in various design scenarios, highlighting its potential to democratise design education by making sophisticated design tools accessible to a broader range of students. The discussion further explores CADgpt's implications for pedagogy and curriculum development, emphasising its role in enhancing creative exploration and conceptual thinking in design education.   Keywords: Natural Language Processing, Computer-Aided Design, 3D Modelling, Design Automation, Design Education, Architectural Education ",
        "title": "CADgpt: Harnessing Natural Language Processing for 3D Modelling to  Enhance Computer-Aided Design Workflows",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05477",
        "abstract_url": "http://arxiv.org/abs/2401.05477",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yiran"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haibin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yexu"
            },
            {
                "last_name": "Riedel",
                "first_name": "Till"
            },
            {
                "last_name": "Beigl",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, deep learning has emerged as a potent tool across a multitude of domains, leading to a surge in research pertaining to its application in the wearable human activity recognition (WHAR) domain. Despite the rapid development, concerns have been raised about the lack of standardization and consistency in the procedures used for experimental model training, which may affect the reproducibility and reliability of research results. In this paper, we provide an exhaustive review of contemporary deep learning research in the field of WHAR and collate information pertaining to the training procedure employed in various studies. Our findings suggest that a major trend is the lack of detail provided by model training protocols. Besides, to gain a clearer understanding of the impact of missing descriptions, we utilize a control variables approach to assess the impact of key tunable components (e.g., optimization techniques and early stopping criteria) on the inter-subject generalization capabilities of HAR models. With insights from the analyses, we define a novel integrated training procedure tailored to the WHAR model. Empirical results derived using five well-known \\ac{whar} benchmark datasets and three classical HAR model architectures demonstrate the effectiveness of our proposed methodology: in particular, there is a significant improvement in macro F1 leave one subject out cross-validation performance. ",
        "title": "Standardizing Your Training Process for Human Activity Recognition  Models: A Comprehensive Review in the Tunable Factors",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05478",
        "abstract_url": "http://arxiv.org/abs/2401.05478",
        "authors": [
            {
                "last_name": "Stephens",
                "first_name": "Anna"
            },
            {
                "last_name": "Santos",
                "first_name": "Francisco"
            },
            {
                "last_name": "Tan",
                "first_name": "Pang-Ning"
            },
            {
                "last_name": "Esfahanian",
                "first_name": "Abdol-Hossein"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks. Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network. In this paper we present OTGCN, a powerful, novel approach to cross-network node classification. This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites. This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment. We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data. ",
        "title": "Population Graph Cross-Network Node Classification for Autism Detection  Across Sample Groups",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05479",
        "abstract_url": "http://arxiv.org/abs/2401.05479",
        "authors": [
            {
                "last_name": "Miniak-G\u00f3recka",
                "first_name": "Alicja"
            },
            {
                "last_name": "Podlaski",
                "first_name": "Krzysztof"
            },
            {
                "last_name": "Gwizda\u0142\u0142a",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The problem of data clustering is one of the most important in data analysis. It can be problematic when dealing with experimental data characterized by measurement uncertainties and errors. Our paper proposes a recursive scheme for clustering data obtained in geographical (climatological) experiments. The discussion of results obtained by k-means and SOM methods with the developed recursive procedure is presented. We show that the clustering using the new approach gives more acceptable results when compared to experts assessments. ",
        "title": "The recursive scheme of clustering",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05480",
        "abstract_url": "http://arxiv.org/abs/2401.05480",
        "authors": [
            {
                "last_name": "Zavanelli",
                "first_name": "Nathan"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This paper summarizes and presents PulsatioMech: an open-source MATLAB toolbox for seismocardiography (SCG) signal processing. The toolbox may be found here: https://github.com/nzavanelli/SCG_master_toolbox PulsatioMech is currently under development as a common tool to promote new studies and discoveries in the use of cardiac mechanical signal for wearable health monitoring. This toolbox is designed to assist users in analyzing SCG signals without the need to devote significant effort into signal processing and coding tasks. Simultaneously, it provides a uniform basis to assess the reproducibility of works based on this toolbox, including those cited here [1-6]. The referenced works contain a great deal more detail regarding the specific algorithms implemented here, whereas this paper will present a short overview of the PulsatioMech Toolbox. ",
        "title": "PulsatioMech: An Open-Source MATLAB Toolbox for Seismocardiography  Signal Processing",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05481",
        "abstract_url": "http://arxiv.org/abs/2401.05481",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Siddharth"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The segmentation of medical images is important for the improvement and creation of healthcare systems, particularly for early disease detection and treatment planning. In recent years, the use of convolutional neural networks (CNNs) and other state-of-the-art methods has greatly advanced medical image segmentation. However, CNNs have been found to struggle with learning long-range dependencies and capturing global context due to the limitations of convolution operations. In this paper, we explore the use of transformers and CNNs for medical image segmentation and propose a hybrid architecture that combines the ability of transformers to capture global dependencies with the ability of CNNs to capture low-level spatial details. We compare various architectures and configurations and conduct multiple experiments to evaluate their effectiveness. ",
        "title": "Transformer-CNN Fused Architecture for Enhanced Skin Lesion Segmentation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05501",
        "abstract_url": "http://arxiv.org/abs/2401.05501",
        "authors": [
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Carley",
                "first_name": "Kathleen M."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  As digitalization increases, countries employ digital diplomacy, harnessing digital resources to project their desired image. Digital diplomacy also encompasses the interactivity of digital platforms, providing a trove of public opinion that diplomatic agents can collect. Social media bots actively participate in political events through influencing political communication and purporting coordinated narratives to influence human behavior. This article provides a methodology towards identifying three types of bots: General Bots, News Bots and Bridging Bots, then further identify these classes of bots on Twitter during a diplomatic incident involving the United States and China. Using a series of computational methods, this article examines the impact of bots on the topics disseminated, the influence and the use of information maneuvers of bots within the social communication network. Among others, our results observe that all three types of bots are present across the two countries; bots geotagged to the US are generally concerned with the balloon location while those geotagged to China discussed topics related to escalating tensions; and perform different extent of positive narrative and network information maneuvers. ",
        "title": "Deflating the Chinese Balloon: Types of Twitter Bots in US-China balloon  incident",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05502",
        "abstract_url": "http://arxiv.org/abs/2401.05502",
        "authors": [
            {
                "last_name": "Thejaswi",
                "first_name": "Suhas"
            },
            {
                "last_name": "Gadekar",
                "first_name": "Ameet"
            },
            {
                "last_name": "Ordozgoiti",
                "first_name": "Bruno"
            },
            {
                "last_name": "Gionis",
                "first_name": "Aristides"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC",
            "LG"
        ],
        "abstract": "  In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \\frac{2}{e}$, $1+\\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\\frac{2}{e}$ and $1+\\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improving the previous best known approximation ratio of factor $5$. ",
        "title": "Diversity-aware clustering: Computational Complexity and Approximation  Algorithms",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05507",
        "abstract_url": "http://arxiv.org/abs/2401.05507",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xueyu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Wei",
                "first_name": "Shuang"
            },
            {
                "last_name": "Chai",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Wang",
                "first_name": "Guoyin"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuwu"
            },
            {
                "last_name": "Su",
                "first_name": "Jing"
            },
            {
                "last_name": "Xu",
                "first_name": "Jingjing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jianbo"
            },
            {
                "last_name": "Kuang",
                "first_name": "Kun"
            },
            {
                "last_name": "Yang",
                "first_name": "Yang"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Wu",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we introduce \"InfiAgent-DABench\", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent. ",
        "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05509",
        "abstract_url": "http://arxiv.org/abs/2401.05509",
        "authors": [
            {
                "last_name": "Injadat",
                "first_name": "MohammadNoor"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The continued growth in the deployment of Internet-of-Things (IoT) devices has been fueled by the increased connectivity demand, particularly in industrial environments. However, this has led to an increase in the number of network related attacks due to the increased number of potential attack surfaces. Industrial IoT (IIoT) devices are prone to various network related attacks that can have severe consequences on the manufacturing process as well as on the safety of the workers in the manufacturing plant. One promising solution that has emerged in recent years for attack detection is Machine learning (ML). More specifically, ensemble learning models have shown great promise in improving the performance of the underlying ML models. Accordingly, this paper proposes a framework based on the combined use of Bayesian Optimization-Gaussian Process (BO-GP) with an ensemble tree-based learning model to improve the performance of intrusion and attack detection in IIoT environments. The proposed framework's performance is evaluated using the Windows 10 dataset collected by the Cyber Range and IoT labs at University of New South Wales. Experimental results illustrate the improvement in detection accuracy, precision, and F-score when compared to standard tree and ensemble tree models. ",
        "title": "Optimized Ensemble Model Towards Secured Industrial IoT Devices",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05511",
        "abstract_url": "http://arxiv.org/abs/2401.05511",
        "authors": [
            {
                "last_name": "Rogha",
                "first_name": "Milad"
            },
            {
                "last_name": "Sah",
                "first_name": "Subham"
            },
            {
                "last_name": "Karduni",
                "first_name": "Alireza"
            },
            {
                "last_name": "Markant",
                "first_name": "Douglas"
            },
            {
                "last_name": "Dou",
                "first_name": "Wenwen"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  News articles containing data visualizations play an important role in informing the public on issues ranging from public health to politics. Recent research on the persuasive appeal of data visualizations suggests that prior attitudes can be notoriously difficult to change. Inspired by an NYT article, we designed two experiments to evaluate the impact of elicitation and contrasting narratives on attitude change, recall, and engagement. We hypothesized that eliciting prior beliefs leads to more elaborative thinking that ultimately results in higher attitude change, better recall, and engagement. Our findings revealed that visual elicitation leads to higher engagement in terms of feelings of surprise. While there is an overall attitude change across all experiment conditions, we did not observe a significant effect of belief elicitation on attitude change. With regard to recall error, while participants in the draw trend elicitation exhibited significantly lower recall error than participants in the categorize trend condition, we found no significant difference in recall error when comparing elicitation conditions to no elicitation. In a follow-up study, we added contrasting narratives with the purpose of making the main visualization (communicating data on the focal issue) appear strikingly different. Compared to the results of study 1, we found that contrasting narratives improved engagement in terms of surprise and interest but interestingly resulted in higher recall error and no significant change in attitude. We discuss the effects of elicitation and contrasting narratives in the context of topic involvement and the strengths of temporal trends encoded in the data visualization. ",
        "title": "The Impact of Elicitation and Contrasting Narratives on Engagement,  Recall and Attitude Change with News Articles Containing Data Visualization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05516",
        "abstract_url": "http://arxiv.org/abs/2401.05516",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "GeonU"
            },
            {
                "last_name": "Youwang",
                "first_name": "Kim"
            },
            {
                "last_name": "Oh",
                "first_name": "Tae-Hyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We present FPRF, a feed-forward photorealistic style transfer method for large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view appearance consistency. Prior arts required tedious per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D neural radiance field, which inherits AdaIN's feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPRF supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPRF achieves favorable photorealistic quality 3D scene stylization for large-scale scenes with diverse reference images. Project page: https://kim-geonu.github.io/FPRF/ ",
        "title": "FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D  Neural Radiance Fields",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05518",
        "abstract_url": "http://arxiv.org/abs/2401.05518",
        "authors": [
            {
                "last_name": "Panferov",
                "first_name": "Andrei"
            },
            {
                "last_name": "Demidovich",
                "first_name": "Yury"
            },
            {
                "last_name": "Rammal",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Richt\u00e1rik",
                "first_name": "Peter"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Quantization (Alistarh et al., 2017) is an important (stochastic) compression technique that reduces the volume of transmitted bits during each communication round in distributed model training. Suresh et al. (2022) introduce correlated quantizers and show their advantages over independent counterparts by analyzing distributed SGD communication complexity. We analyze the forefront distributed non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the proposed correlated quantizers and show that it outperforms the original MARINA and distributed SGD of Suresh et al. (2022) with regard to the communication complexity. We significantly refine the original analysis of MARINA without any additional assumptions using the weighted Hessian variance (Tyurin et al., 2022), and then we expand the theoretical framework of MARINA to accommodate a substantially broader range of potentially correlated and biased compressors, thus dilating the applicability of the method beyond the conventional independent unbiased compressor setup. Extensive experimental results corroborate our theoretical findings. ",
        "title": "Correlated Quantization for Faster Nonconvex Distributed Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05520",
        "abstract_url": "http://arxiv.org/abs/2401.05520",
        "authors": [
            {
                "last_name": "Amadeus",
                "first_name": "Marcellus"
            },
            {
                "last_name": "Casta\u00f1eda",
                "first_name": "William Alberto Cruz"
            },
            {
                "last_name": "Zanella",
                "first_name": "Andr\u00e9 Felipe"
            },
            {
                "last_name": "Mahlow",
                "first_name": "Felipe Rodrigues Perche"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Generative AI has become pervasive in society, witnessing significant advancements in various domains. Particularly in the realm of Text-to-Image (TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities in generating visual content based on textual prompts. This paper addresses the potential of LDMs in representing local cultural concepts, historical figures, and endangered species. In this study, we use the cultural heritage of Rio Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to contribute to the broader understanding of how generative models can help to capture and preserve the cultural and historical identity of regions. The paper outlines the methodology, including subject selection, dataset creation, and the fine-tuning process. The results showcase the images generated, alongside the challenges and feasibility of each concept. In conclusion, this work shows the power of these models to represent and preserve unique aspects of diverse regions and communities. ",
        "title": "From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\\'ucho  Heritage",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05521",
        "abstract_url": "http://arxiv.org/abs/2401.05521",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Danjie"
            },
            {
                "last_name": "Yang",
                "first_name": "Simon X."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The paper presents an innovative approach (CBNNTAP) that addresses the complexities and challenges introduced by ocean currents when optimizing target assignment and motion planning for a multi-unmanned underwater vehicle (UUV) system. The core of the proposed algorithm involves the integration of several key components. Firstly, it incorporates a bio-inspired neural network-based (BINN) approach which predicts the most efficient paths for individual UUVs while simultaneously ensuring collision avoidance among the vehicles. Secondly, an efficient target assignment component is integrated by considering the path distances determined by the BINN algorithm. In addition, a critical innovation within the CBNNTAP algorithm is its capacity to address the disruptive effects of ocean currents, where an adjustment component is seamlessly integrated to counteract the deviations caused by these currents, which enhances the accuracy of both motion planning and target assignment for the UUVs. The effectiveness of the CBNNTAP algorithm is demonstrated through comprehensive simulation results and the outcomes underscore the superiority of the developed algorithm in nullifying the effects of static and dynamic ocean currents in 2D and 3D scenarios. ",
        "title": "Current Effect-eliminated Optimal Target Assignment and Motion Planning  for a Multi-UUV System",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05523",
        "abstract_url": "http://arxiv.org/abs/2401.05523",
        "authors": [
            {
                "last_name": "Levit",
                "first_name": "Vadim E."
            },
            {
                "last_name": "Mandrescu",
                "first_name": "Eugen"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  The graph G=(V,E) is called Konig-Egervary if the sum of its independence number and its matching number equals its order. Let RV(G) denote the number of vertices v such that G-v is Konig-Egervary, and let RE(G) denote the number of edges e such that G-e is Konig-Egervary. Clearly, RV(G) = |V| and RE(G) = |E| for bipartite graphs. Unlike the bipartiteness, the property of being a Konig-Egervary graph is not hereditary. In this paper, we present an equality expressing RV(G) in terms of some graph parameters, and a tight inequality bounding RE(G) in terms of the same parameters, when G is Konig-Egervary. ",
        "title": "On the Number of Vertices/Edges whose Deletion Preserves the  Konig-Egervary Property",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05525",
        "abstract_url": "http://arxiv.org/abs/2401.05525",
        "authors": [
            {
                "last_name": "Dinh",
                "first_name": "Lam"
            },
            {
                "last_name": "Quang",
                "first_name": "Pham Tran Anh"
            },
            {
                "last_name": "Leguay",
                "first_name": "J\u00e9r\u00e9mie"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG"
        ],
        "abstract": "  Deep Reinforcement Learning (DRL) algorithms have recently made significant strides in improving network performance. Nonetheless, their practical use is still limited in the absence of safe exploration and safe decision-making. In the context of commercial solutions, reliable and safe-to-operate systems are of paramount importance. Taking this problem into account, we propose a safe learning-based load balancing algorithm for Software Defined-Wide Area Network (SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with a Control Barrier Function (CBF). It safely projects unsafe actions into feasible ones during both training and testing, and it guides learning towards safe policies. We successfully implemented the solution on GPU to accelerate training by approximately 110x times and achieve model updates for on-policy methods within a few seconds, making the solution practical. We show that our approach delivers near-optimal Quality-of-Service (QoS performance in terms of end-to-end delay while respecting safety requirements related to link capacity constraints. We also demonstrated that on-policy learning based on Proximal Policy Optimization (PPO) performs better than off-policy learning with Deep Deterministic Policy Gradient (DDPG) when both are combined with a CBF for safe load balancing. ",
        "title": "Towards Safe Load Balancing based on Control Barrier Functions and Deep  Reinforcement Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05529",
        "abstract_url": "http://arxiv.org/abs/2401.05529",
        "authors": [
            {
                "last_name": "Di",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Bingchang"
            },
            {
                "last_name": "Gao",
                "first_name": "Yiyi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  This paper presents a novel fuzzing framework, called MicroFuzz, specifically designed for Microservices. Mocking-Assisted Seed Execution, Distributed Tracing, Seed Refresh and Pipeline Parallelism approaches are adopted to address the environmental complexities and dynamics of Microservices and improve the efficiency of fuzzing. MicroFuzz has been successfully implemented and deployed in Ant Group, a prominent FinTech company. Its performance has been evaluated in three distinct industrial scenarios: normalized fuzzing, iteration testing, and taint verification.Throughout five months of operation, MicroFuzz has diligently analyzed a substantial codebase, consisting of 261 Apps with over 74.6 million lines of code (LOC). The framework's effectiveness is evident in its detection of 5,718 potential quality or security risks, with 1,764 of them confirmed and fixed as actual security threats by software specialists. Moreover, MicroFuzz significantly increased program coverage by 12.24% and detected program behavior by 38.42% in the iteration testing. ",
        "title": "MicroFuzz: An Efficient Fuzzing Framework for Microservices",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05530",
        "abstract_url": "http://arxiv.org/abs/2401.05530",
        "authors": [
            {
                "last_name": "Salgado",
                "first_name": "Erik Isai Valle"
            },
            {
                "last_name": "Li",
                "first_name": "Chen"
            },
            {
                "last_name": "Han",
                "first_name": "Yaqi"
            },
            {
                "last_name": "Shi",
                "first_name": "Linchao"
            },
            {
                "last_name": "Li",
                "first_name": "Xinghui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Ensemble methods exploit the availability of a given number of classifiers or detectors trained in single or multiple source domains and tasks to address machine learning problems such as domain adaptation or multi-source transfer learning. Existing research measures the domain distance between the sources and the target dataset, trains multiple networks on the same data with different samples per class, or combines predictions from models trained under varied hyperparameters and settings. Their solutions enhanced the performance on small or tail categories but hurt the rest. To this end, we propose a modified consensus focus for semi-supervised and long-tailed object detection. We introduce a voting system based on source confidence that spots the contribution of each model in a consensus, lets the user choose the relevance of each class in the target label space so that it relaxes minority bounding boxes suppression, and combines multiple models' results without discarding the poisonous networks. Our tests on synthetic driving datasets retrieved higher confidence and more accurate bounding boxes than the NMS, soft-NMS, and WBF. ",
        "title": "Consensus Focus for Object Detection and minority classes",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05531",
        "abstract_url": "http://arxiv.org/abs/2401.05531",
        "authors": [
            {
                "last_name": "Fischer",
                "first_name": "John"
            },
            {
                "last_name": "Orescanin",
                "first_name": "Marko"
            },
            {
                "last_name": "Eckstrand",
                "first_name": "Eric"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD"
        ],
        "abstract": "  Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available. The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance. In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs). VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection dataset. We evaluate the quality of the resulting uncertainty when transferring knowledge from VI-PANNs to other downstream acoustic classification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets. We demonstrate, for the first time, that it is possible to transfer calibrated uncertainty information along with knowledge from upstream tasks to enhance a model's capability to perform downstream tasks. ",
        "title": "VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational  Inference for Improved Generalization in Audio Pattern Recognition",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05535",
        "abstract_url": "http://arxiv.org/abs/2401.05535",
        "authors": [
            {
                "last_name": "Dorador",
                "first_name": "Albert"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods proposed is more accurate than the original random forest, while just using a small fraction of the trees, aiding result interpretability. Compared to current state-of-the-art forestpruning methods, namely sequential forward selection and (a variation of) sequential backward selection, our methods tend to outperform both of them, whether in terms of accuracy, number of trees employed, or both. ",
        "title": "Improving the Accuracy and Interpretability of Random Forests via Forest  Pruning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05538",
        "abstract_url": "http://arxiv.org/abs/2401.05538",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Le Ngu"
            },
            {
                "last_name": "Casado",
                "first_name": "Constantino \u00c1lvarez"
            },
            {
                "last_name": "Ca\u00f1ellas",
                "first_name": "Manuel Lage"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Anirban"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Nhi"
            },
            {
                "last_name": "Jayagopi",
                "first_name": "Dinesh Babu"
            },
            {
                "last_name": "L\u00f3pez",
                "first_name": "Miguel Bordallo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Radio frequency (RF) signals have facilitated the development of non-contact human monitoring tasks, such as vital signs measurement, activity recognition, and user identification. In some specific scenarios, an RF signal analysis framework may prioritize the performance of one task over that of others. In response to this requirement, we employ a multi-objective optimization approach inspired by biological principles to select discriminative features that enhance the accuracy of breathing patterns recognition while simultaneously impeding the identification of individual users. This approach is validated using a novel vital signs dataset consisting of 50 subjects engaged in four distinct breathing patterns. Our findings indicate a remarkable result: a substantial divergence in accuracy between breathing recognition and user identification. As a complementary viewpoint, we present a contrariwise result to maximize user identification accuracy and minimize the system's capacity for breathing activity recognition. ",
        "title": "Multi-objective Feature Selection in Remote Health Monitoring  Applications",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05544",
        "abstract_url": "http://arxiv.org/abs/2401.05544",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yong"
            },
            {
                "last_name": "Luo",
                "first_name": "Senlin"
            },
            {
                "last_name": "Shang",
                "first_name": "Yu-Ming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yifei"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengjun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative features, thus avoiding the need for additional neural network layers and reducing computational costs. Subsequently, we employ an attention mechanism to aggregate multiple layers of related knowledge for each task as final features to boost their accuracy. We conducted extensive experiments on four downstream source code-related tasks to evaluate our approach and our results demonstrate that CodePrompt achieves new state-of-the-art performance on the accuracy metric while also exhibiting computation cost-saving capabilities. ",
        "title": "CodePrompt: Improving Source Code-Related Classification with Knowledge  Features through Prompt Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05548",
        "abstract_url": "http://arxiv.org/abs/2401.05548",
        "authors": [
            {
                "last_name": "Machetti",
                "first_name": "Simone"
            },
            {
                "last_name": "Schiavone",
                "first_name": "Pasquale Davide"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Thomas Christoph"
            },
            {
                "last_name": "Pe\u00f3n-Quir\u00f3s",
                "first_name": "Miguel"
            },
            {
                "last_name": "Atienza",
                "first_name": "David"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The field of edge computing has witnessed remarkable growth owing to the increasing demand for real-time processing of data in applications. However, challenges persist due to limitations in performance and power consumption. To overcome these challenges, heterogeneous architectures have emerged that combine host processors with specialized accelerators tailored to specific applications, leading to improved performance and reduced power consumption. However, most of the existing platforms lack the necessary configurability and extendability options for integrating custom accelerators. To overcome these limitations, we introduce in this paper the eXtendible Heterogeneous Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed to natively support the integration of ultra-low-power edge accelerators. It provides customization options to match specific application requirements by exploring various core types, bus topologies, addressing modes, memory sizes, and peripherals. Moreover, the platform prioritizes energy efficiency by implementing low-power strategies, such as clock-gating and power-gating. We demonstrate the real-world applicability of X-HEEP by providing an integration example tailored for healthcare applications that includes a coarse-grained reconfigurable array (CGRA) and in-memory computing (IMC) accelerators. The resulting design, called HEEPocrates, has been implemented both in field programmable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with TSMC 65 nm low-power CMOS technology. We run a set of healthcare applications and measure their energy consumption to demonstrate the alignment of our chip with other state-of-the-art microcontrollers commonly adopted in this domain. Moreover, we showcase the energy benefit of 4.9 x gained by exploiting the integrated CGRA accelerator, compared to running on the host CPU. ",
        "title": "X-HEEP: An Open-Source, Configurable and Extendible RISC-V  Microcontroller for the Exploration of Ultra-Low-Power Edge Accelerators",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05551",
        "abstract_url": "http://arxiv.org/abs/2401.05551",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Changye"
            },
            {
                "last_name": "Xu",
                "first_name": "Weizhe"
            },
            {
                "last_name": "Cohen",
                "first_name": "Trevor"
            },
            {
                "last_name": "Pakhomov",
                "first_name": "Serguei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  \\textbf{Objectives}: We aimed to investigate how errors from automatic speech recognition (ASR) systems affect dementia classification accuracy, specifically in the ``Cookie Theft'' picture description task. We aimed to assess whether imperfect ASR-generated transcripts could provide valuable information for distinguishing between language samples from cognitively healthy individuals and those with Alzheimer's disease (AD).   \\textbf{Methods}: We conducted experiments using various ASR models, refining their transcripts with post-editing techniques. Both these imperfect ASR transcripts and manually transcribed ones were used as inputs for the downstream dementia classification. We conducted comprehensive error analysis to compare model performance and assess ASR-generated transcript effectiveness in dementia classification.   \\textbf{Results}: Imperfect ASR-generated transcripts surprisingly outperformed manual transcription for distinguishing between individuals with AD and those without in the ``Cookie Theft'' task. These ASR-based models surpassed the previous state-of-the-art approach, indicating that ASR errors may contain valuable cues related to dementia. The synergy between ASR and classification models improved overall accuracy in dementia classification.   \\textbf{Conclusion}: Imperfect ASR transcripts effectively capture linguistic anomalies linked to dementia, improving accuracy in classification tasks. This synergy between ASR and classification models underscores ASR's potential as a valuable tool in assessing cognitive impairment and related clinical applications. ",
        "title": "Useful Blunders: Can Automated Speech Recognition Errors Improve  Downstream Dementia Classification?",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05553",
        "abstract_url": "http://arxiv.org/abs/2401.05553",
        "authors": [
            {
                "last_name": "Pareschi",
                "first_name": "Lorenzo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Probably one of the most striking examples of the close connections between global optimization processes and statistical physics is the simulated annealing method, inspired by the famous Monte Carlo algorithm devised by Metropolis et al. in the middle of the last century. In this paper we show how the tools of linear kinetic theory allow to describe this gradient-free algorithm from the perspective of statistical physics and how convergence to the global minimum can be related to classical entropy inequalities. This analysis highlight the strong link between linear Boltzmann equations and stochastic optimization methods governed by Markov processes. Thanks to this formalism we can establish the connections between the simulated annealing process and the corresponding mean-field Langevin dynamics characterized by a stochastic gradient descent approach. Generalizations to other selection strategies in simulated annealing that avoid the acceptance-rejection dynamic are also provided. ",
        "title": "Optimization by linear kinetic equations and mean-field Langevin  dynamics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05554",
        "abstract_url": "http://arxiv.org/abs/2401.05554",
        "authors": [
            {
                "last_name": "Lo",
                "first_name": "John"
            },
            {
                "last_name": "Parslew",
                "first_name": "Ben"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Previous design methodologies for spring-driven jumping robots focused on jump height optimization for specific tasks. In doing so, numerous designs have been proposed including using nonlinear spring-linkages to increase the elastic energy storage and jump height. However, these systems can never achieve their theoretical maximum jump height due to taking off before the spring energy is fully released, resulting in an incomplete transfer of stored elastic energy to gravitational potential energy. This paper presents low-order models aimed at characterising the energy conversion during the acceleration phase of jumping. It also proposes practical solutions for increasing the energy efficiency of jumping robots. A dynamic analysis is conducted on a multibody system comprised of rotational links, which is experimentally validated using a physical demonstrator. The analysis reveals that inefficient energy conversion is attributed to inertial effects caused by rotational and unsprung masses. Since these masses cannot be entirely eliminated from a physical linkage, a practical approach to improving energy efficiency involves structural redesign to reduce structural mass and moments of inertia while maintaining compliance with structural strength and stiffness requirements. ",
        "title": "Characterising the take-off dynamics and energy efficiency in  spring-driven jumping robots",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05558",
        "abstract_url": "http://arxiv.org/abs/2401.05558",
        "authors": [
            {
                "last_name": "Asinowski",
                "first_name": "Andrei"
            },
            {
                "last_name": "Banderier",
                "first_name": "Cyril"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "CG",
            "FL"
        ],
        "abstract": "  We enumerate several classes of pattern-avoiding rectangulations. We establish bijective links with pattern-avoiding permutations, prove that their generating functions are algebraic, and confirm several conjectures by Merino and M\\\"utze. We also analyze a new class of rectangulations, called whirls, using a generating tree. ",
        "title": "From geometry to generating functions: rectangulations and permutations",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05562",
        "abstract_url": "http://arxiv.org/abs/2401.05562",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhangchen"
            },
            {
                "last_name": "Jiang",
                "first_name": "Fengqing"
            },
            {
                "last_name": "Niu",
                "first_name": "Luyao"
            },
            {
                "last_name": "Jia",
                "first_name": "Jinyuan"
            },
            {
                "last_name": "Poovendran",
                "first_name": "Radha"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "DC"
        ],
        "abstract": "  Federated learning (FL) enables multiple participants to train a global machine learning model without sharing their private training data. Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating the server that aggregates local models from participants and then updates the global model. However, P2P FL is vulnerable to (i) honest-but-curious participants whose objective is to infer private training data of other participants, and (ii) Byzantine participants who can transmit arbitrarily manipulated local models to corrupt the learning process. P2P FL schemes that simultaneously guarantee Byzantine resilience and preserve privacy have been less studied. In this paper, we develop Brave, a protocol that ensures Byzantine Resilience And privacy-preserving property for P2P FL in the presence of both types of adversaries. We show that Brave preserves privacy by establishing that any honest-but-curious adversary cannot infer other participants' private data by observing their models. We further prove that Brave is Byzantine-resilient, which guarantees that all benign participants converge to an identical model that deviates from a global model trained without Byzantine adversaries by a bounded distance. We evaluate Brave against three state-of-the-art adversaries on a P2P FL for image classification tasks on benchmark datasets CIFAR10 and MNIST. Our results show that the global model learned with Brave in the presence of adversaries achieves comparable classification accuracy to a global model trained in the absence of any adversary. ",
        "title": "Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated  Learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05563",
        "abstract_url": "http://arxiv.org/abs/2401.05563",
        "authors": [
            {
                "last_name": "Dwarakanath",
                "first_name": "Kshama"
            },
            {
                "last_name": "Vyetrenko",
                "first_name": "Svitlana"
            },
            {
                "last_name": "Oyebode",
                "first_name": "Toks"
            },
            {
                "last_name": "Balch",
                "first_name": "Tucker"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Is transparency always beneficial in complex systems such as traffic networks and stock markets? How is transparency defined in multi-agent systems, and what is its optimal degree at which social welfare is highest? We take an agent-based view to define transparency (or its lacking) as delay in agent observability of environment states, and utilize simulations to analyze the impact of delay on social welfare. To model the adaptation of agent strategies with varying delays, we model agents as learners maximizing the same objectives under different delays in a simulated environment. Focusing on two agent types - constrained and unconstrained, we use multi-agent reinforcement learning to evaluate the impact of delay on agent outcomes and social welfare. Empirical demonstration of our framework in simulated financial markets shows opposing trends in outcomes of the constrained and unconstrained agents with delay, with an optimal partial transparency regime at which social welfare is maximal. ",
        "title": "Transparency as Delayed Observability in Multi-Agent Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05568",
        "abstract_url": "http://arxiv.org/abs/2401.05568",
        "authors": [
            {
                "last_name": "Vandermause",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Johansson",
                "first_name": "Anders"
            },
            {
                "last_name": "Miao",
                "first_name": "Yucong"
            },
            {
                "last_name": "Vlassak",
                "first_name": "Joost J."
            },
            {
                "last_name": "Kozinsky",
                "first_name": "Boris"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range of biomedical and engineering devices, but direct molecular dynamics simulations of the martensitic B19' -> B2 phase transition driving its shape-memory behavior are rare and have relied on classical force fields with limited accuracy. Here, we train four machine-learned force fields for equiatomic NiTi based on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trained on the fly during NPT molecular dynamics, with DFT calculations and model updates performed automatically whenever the uncertainty of a local energy prediction exceeds a chosen threshold. The models achieve accuracies of 1-2 meV/atom during training and are shown to closely track DFT predictions of B2 and B19' elastic constants and phonon frequencies. Surprisingly, in large-scale molecular dynamics simulations, only the SCAN model predicts a reversible B19' -> B2 phase transition, with the LDA, PBE, and PBEsol models predicting a reversible transition to a previously uncharacterized low-volume phase, which we hypothesize to be a new stable high-pressure phase. We examine the structure of the new phase and estimate its stability on the temperature-pressure phase diagram. This work establishes an automated active learning protocol for studying displacive transformations, reveals important differences between DFT functionals that can only be detected in large-scale simulations, provides an accurate force field for NiTi, and identifies a new phase. ",
        "title": "Phase discovery with active learning: Application to structural phase  transitions in equiatomic NiTi",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05569",
        "abstract_url": "http://arxiv.org/abs/2401.05569",
        "authors": [
            {
                "last_name": "Ozen",
                "first_name": "Irfan"
            },
            {
                "last_name": "Subramani",
                "first_name": "Karthika"
            },
            {
                "last_name": "Vadrevu",
                "first_name": "Phani"
            },
            {
                "last_name": "Perdisci",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Social engineering (SE) aims at deceiving users into performing actions that may compromise their security and privacy. These threats exploit weaknesses in human's decision making processes by using tactics such as pretext, baiting, impersonation, etc. On the web, SE attacks include attack classes such as scareware, tech support scams, survey scams, sweepstakes, etc., which can result in sensitive data leaks, malware infections, and monetary loss. For instance, US consumers lose billions of dollars annually due to various SE attacks. Unfortunately, generic social engineering attacks remain understudied, compared to other important threats, such as software vulnerabilities and exploitation, network intrusions, malicious software, and phishing. The few existing technical studies that focus on social engineering are limited in scope and mostly focus on measurements rather than developing a generic defense. To fill this gap, we present SEShield, a framework for in-browser detection of social engineering attacks. SEShield consists of three main components: (i) a custom security crawler, called SECrawler, that is dedicated to scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet, a deep learning-based image classifier trained on data collected by SECrawler that aims to detect the often glaring visual traits of SE attack pages; and (iii) SEGuard, a proof-of-concept extension that embeds SENet into the web browser and enables real-time SE attack detection. We perform an extensive evaluation of our system and show that SENet is able to detect new instances of SE attacks with a detection rate of up to 99.6% at 1% false positive, thus providing an effective first defense against SE attacks on the web. ",
        "title": "SENet: Visual Detection of Online Social Engineering Attack Campaigns",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05570",
        "abstract_url": "http://arxiv.org/abs/2401.05570",
        "authors": [
            {
                "last_name": "Van Vorst",
                "first_name": "Kevin"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks. However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts. It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues. In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images. Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner. Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean distances between the embeddings of the image pairs into the Siamese network loss. Our method demonstrates superior performance in mammogram patch classification compared to existing self-supervised learning methods. This approach not only leverages a vast amount of image data effectively but also minimizes reliance on costly labels, a significant advantage particularly in the field of medical imaging. ",
        "title": "Siamese Networks with Soft Labels for Unsupervised Lesion Detection and  Patch Pretraining on Screening Mammograms",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05571",
        "abstract_url": "http://arxiv.org/abs/2401.05571",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hanrui"
            },
            {
                "last_name": "Gu",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Zirui"
            },
            {
                "last_name": "Pan",
                "first_name": "David Z."
            },
            {
                "last_name": "Chong",
                "first_name": "Frederic T."
            },
            {
                "last_name": "Han",
                "first_name": "Song"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhangyang"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  Parameterized Quantum Circuits (PQC) have obtained increasing popularity thanks to their great potential for near-term Noisy Intermediate-Scale Quantum (NISQ) computers. Achieving quantum advantages usually requires a large number of qubits and quantum circuits with enough capacity. However, limited coherence time and massive quantum noises severely constrain the size of quantum circuits that can be executed reliably on real machines. To address these two pain points, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive quantum circuits, aiming to achieve two key objectives: (1) implicit circuits capacity during training - by dynamically exploring the circuit's sparse connectivity and sticking a fixed small number of quantum gates throughout the training which satisfies the coherence time and enjoy light noises, enabling feasible executions on real quantum devices; (2) noise robustness - by jointly optimizing the topology and parameters of quantum circuits under real device noise models. In each update step of sparsity, we leverage the moving average of historical gradients to grow necessary gates and utilize salience-based pruning to eliminate insignificant gates. Extensive experiments are conducted with 7 Quantum Machine Learning (QML) and Variational Quantum Eigensolver (VQE) benchmarks on 6 simulated or real quantum computers, where QuantumSEA consistently surpasses noise-aware search, human-designed, and randomly generated quantum circuit baselines by a clear performance margin. For example, even in the most challenging on-chip training regime, our method establishes state-of-the-art results with only half the number of quantum gates and ~2x time saving of circuit executions. Codes are available at https://github.com/VITA-Group/QuantumSEA. ",
        "title": "QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum  Circuits",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05572",
        "abstract_url": "http://arxiv.org/abs/2401.05572",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Qin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "MA",
            "RO"
        ],
        "abstract": "  Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -- innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment and compare the cooperative performance within three characteristics of innate value agents (Coward, Neutral, and Reckless) through three benchmark multi-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate that by organizing individual various needs rationally, the group can achieve better performance with lower costs effectively. ",
        "title": "Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent  Systems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05578",
        "abstract_url": "http://arxiv.org/abs/2401.05578",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Zang",
                "first_name": "Zhenya"
            },
            {
                "last_name": "Li",
                "first_name": "Xingda"
            },
            {
                "last_name": "Li",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training capabilities. ",
        "title": "Fast Cerebral Blood Flow Analysis via Extreme Learning Machine",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05579",
        "abstract_url": "http://arxiv.org/abs/2401.05579",
        "authors": [
            {
                "last_name": "Raihan",
                "first_name": "Ahmed Shoyeb"
            },
            {
                "last_name": "Khosravi",
                "first_name": "Hamed"
            },
            {
                "last_name": "Bhuiyan",
                "first_name": "Tanveer Hossain"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Imtiaz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry, offering benefits like intricate design, minimal waste, rapid prototyping, material versatility, and customized solutions. However, its full industry adoption faces hurdles, particularly in achieving consistent product quality. A crucial aspect for MAM's success is understanding the relationship between process parameters and melt pool characteristics. Integrating Artificial Intelligence (AI) into MAM is essential. Traditional machine learning (ML) methods, while effective, depend on large datasets to capture complex relationships, a significant challenge in MAM due to the extensive time and resources required for dataset creation. Our study introduces a novel surprise-guided sequential learning framework, SurpriseAF-BO, signaling a significant shift in MAM. This framework uses an iterative, adaptive learning process, modeling the dynamics between process parameters and melt pool characteristics with limited data, a key benefit in MAM's cyber manufacturing context. Compared to traditional ML models, our sequential learning method shows enhanced predictive accuracy for melt pool dimensions. Further improving our approach, we integrated a Conditional Tabular Generative Adversarial Network (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This produces synthetic data resembling real experimental data, improving learning effectiveness. This enhancement boosts predictive precision without requiring additional physical experiments. Our study demonstrates the power of advanced data-driven techniques in cyber manufacturing and the substantial impact of sequential AI and ML, particularly in overcoming MAM's traditional challenges. ",
        "title": "An Augmented Surprise-guided Sequential Learning Framework for  Predicting the Melt Pool Geometry",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05580",
        "abstract_url": "http://arxiv.org/abs/2401.05580",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Li",
                "first_name": "Xingda"
            },
            {
                "last_name": "Li",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performance across different SNRs, exhibiting enhanced fitting accuracy, particularly for low SNR datasets when compared with other fitting methods. This highlights its potential for clinical diagnosis and treatment across various scenarios under different clinical setups. ",
        "title": "Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A  Transfer Learning Approach with Noise Robustness Analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05583",
        "abstract_url": "http://arxiv.org/abs/2401.05583",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chaoyang"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Peiye"
            },
            {
                "last_name": "Siarohin",
                "first_name": "Aliaksandr"
            },
            {
                "last_name": "Cao",
                "first_name": "Junli"
            },
            {
                "last_name": "Qian",
                "first_name": "Guocheng"
            },
            {
                "last_name": "Lee",
                "first_name": "Hsin-Ying"
            },
            {
                "last_name": "Tulyakov",
                "first_name": "Sergey"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Dynamic novel view synthesis aims to capture the temporal evolution of visual content within videos. Existing methods struggle to distinguishing between motion and structure, particularly in scenarios where camera poses are either unknown or constrained compared to object motion. Furthermore, with information solely from reference images, it is extremely challenging to hallucinate unseen regions that are occluded or partially observed in the given videos. To address these issues, we first finetune a pretrained RGB-D diffusion model on the video frames using a customization technique. Subsequently, we distill the knowledge from the finetuned model to a 4D representations encompassing both dynamic and static Neural Radiance Fields (NeRF) components. The proposed pipeline achieves geometric consistency while preserving the scene identity. We perform thorough experiments to evaluate the efficacy of the proposed method qualitatively and quantitatively. Our results demonstrate the robustness and utility of our approach in challenging cases, further advancing dynamic novel view synthesis. ",
        "title": "Diffusion Priors for Dynamic View Synthesis from Monocular Videos",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05584",
        "abstract_url": "http://arxiv.org/abs/2401.05584",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Edison"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Maruf"
            },
            {
                "last_name": "Sun",
                "first_name": "Yue"
            },
            {
                "last_name": "Mahendru",
                "first_name": "Rahul"
            },
            {
                "last_name": "Yang",
                "first_name": "Rui"
            },
            {
                "last_name": "Cook",
                "first_name": "Harrison"
            },
            {
                "last_name": "Leeuwenburg",
                "first_name": "Tennessee"
            },
            {
                "last_name": "Evans",
                "first_name": "Ben"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, the FourCastNet Neural Earth System Model (NESM) has shown impressive results on predicting various atmospheric variables, trained on the ERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory complexity in sequence length compared to quadratic complexity in vanilla transformers, training FourCastNet on ERA5 from scratch still requires large amount of compute resources, which is expensive or even inaccessible to most researchers. In this work, we will show improved methods that can train FourCastNet using only 1% of the compute required by the baseline, while maintaining model performance or par or even better than the baseline. ",
        "title": "FourCastNeXt: Improving FourCastNet Training with Limited Compute",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05585",
        "abstract_url": "http://arxiv.org/abs/2401.05585",
        "authors": [
            {
                "last_name": "Kirigin",
                "first_name": "Tajana Ban"
            },
            {
                "last_name": "Comer",
                "first_name": "Jesse"
            },
            {
                "last_name": "Kanovich",
                "first_name": "Max"
            },
            {
                "last_name": "Scedrov",
                "first_name": "Andre"
            },
            {
                "last_name": "Talcott",
                "first_name": "Carolyn"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Most research on formal system design has focused on optimizing various measures of efficiency. However, insufficient attention has been given to the design of systems optimizing resilience, the ability of systems to adapt to unexpected changes or adversarial disruptions. In our prior work, we formalized the intuitive notion of resilience as a property of cyber-physical systems by using a multiset rewriting language with explicit time. In the present work, we study the computational complexity of a formalization of time-bounded resilience problems for the class of progressing timed systems (PTS), where, intuitively, only a finite number of actions can be carried out in a bounded time period. We show that, in the time-bounded model with n (potentially adversarially chosen) updates, the corresponding time-bounded resilience problem is complete for the $\\Sigma^P_{2n+1}$ class of the polynomial hierarchy, PH. To support the formal models and complexity results, we perform automated experiments for time-bounded verification using the rewriting logic tool Maude. ",
        "title": "Technical Report: Time-Bounded Resilience",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05587",
        "abstract_url": "http://arxiv.org/abs/2401.05587",
        "authors": [
            {
                "last_name": "Sullivan",
                "first_name": "Dakota"
            },
            {
                "last_name": "White",
                "first_name": "Nathan Thomas"
            },
            {
                "last_name": "Schoen",
                "first_name": "Andrew"
            },
            {
                "last_name": "Mutlu",
                "first_name": "Bilge"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Robots are ubiquitous in small-to-large-scale manufacturers. While collaborative robots (cobots) have significant potential in these settings due to their flexibility and ease of use, proper integration is critical to realize their full potential. Specifically, cobots need to be integrated in ways that utilize their strengths, improve manufacturing performance, and facilitate use in concert with human workers. Effective integration requires careful consideration and the knowledge of roboticists, manufacturing engineers, and business administrators. We propose an approach involving the stages of planning, analysis, development, and presentation, to inform manufacturers about cobot integration within their facilities prior to the integration process. We contextualize our approach in a case study with an SME collaborator and discuss insights learned. ",
        "title": "Making Informed Decisions: Supporting Cobot Integration Considering  Business and Worker Preferences",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05593",
        "abstract_url": "http://arxiv.org/abs/2401.05593",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Adrian Xuan Wei"
            },
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Griffin",
                "first_name": "Conor"
            },
            {
                "last_name": "Kyger",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Baghernezhad",
                "first_name": "Faraz"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present Reverse Projection, a novel projective texture mapping technique for painting a decal directly to the texture of a 3D object. Designed to be used in games, this technique works in real-time. By using projection techniques that are computed in local space textures and outward-looking, users using low-end android devices to high-end gaming desktops are able to enjoy the personalization of their assets. We believe our proposed pipeline is a step in improving the speed and versatility of model painting. ",
        "title": "Reverse Projection: Real-Time Local Space Texture Mapping",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05602",
        "abstract_url": "http://arxiv.org/abs/2401.05602",
        "authors": [
            {
                "last_name": "Remedios",
                "first_name": "Lucas W."
            },
            {
                "last_name": "Bao",
                "first_name": "Shunxing"
            },
            {
                "last_name": "Remedios",
                "first_name": "Samuel W."
            },
            {
                "last_name": "Lee",
                "first_name": "Ho Hin"
            },
            {
                "last_name": "Cai",
                "first_name": "Leon Y."
            },
            {
                "last_name": "Li",
                "first_name": "Thomas"
            },
            {
                "last_name": "Deng",
                "first_name": "Ruining"
            },
            {
                "last_name": "Cui",
                "first_name": "Can"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Liu",
                "first_name": "Qi"
            },
            {
                "last_name": "Lau",
                "first_name": "Ken S."
            },
            {
                "last_name": "Roland",
                "first_name": "Joseph T."
            },
            {
                "last_name": "Washington",
                "first_name": "Mary K."
            },
            {
                "last_name": "Coburn",
                "first_name": "Lori A."
            },
            {
                "last_name": "Wilson",
                "first_name": "Keith T."
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            },
            {
                "last_name": "Landman",
                "first_name": "Bennett A."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Understanding the way cells communicate, co-locate, and interrelate is essential to understanding human physiology. Hematoxylin and eosin (H&E) staining is ubiquitously available both for clinical studies and research. The Colon Nucleus Identification and Classification (CoNIC) Challenge has recently innovated on robust artificial intelligence labeling of six cell types on H&E stains of the colon. However, this is a very small fraction of the number of potential cell classification types. Specifically, the CoNIC Challenge is unable to classify epithelial subtypes (progenitor, endocrine, goblet), lymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes (fibroblasts, stromal). In this paper, we propose to use inter-modality learning to label previously un-labelable cell types on virtual H&E. We leveraged multiplexed immunofluorescence (MxIF) histology imaging to identify 14 subclasses of cell types. We performed style transfer to synthesize virtual H&E from MxIF and transferred the higher density labels from MxIF to these virtual H&E images. We then evaluated the efficacy of learning in this approach. We identified helper T and progenitor nuclei with positive predictive values of $0.34 \\pm 0.15$ (prevalence $0.03 \\pm 0.01$) and $0.47 \\pm 0.1$ (prevalence $0.07 \\pm 0.02$) respectively on virtual H&E. This approach represents a promising step towards automating annotation in digital pathology. ",
        "title": "Nucleus subtype classification using inter-modality learning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05603",
        "abstract_url": "http://arxiv.org/abs/2401.05603",
        "authors": [
            {
                "last_name": "Jhaver",
                "first_name": "Shagun"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  Personal moderation tools on social media platforms allow users to control their feeds by configuring the acceptable toxicity thresholds for their feed content or muting inappropriate accounts. This research examines how the end-user configuration of these tools is shaped by four critical psychosocial factors - fear of missing out (FoMO), social media addiction, subjective norms, and trust in moderation systems. Findings from a nationally representative sample of 1,061 participants show that FoMO and social media addiction make Facebook users more vulnerable to content-based harms by reducing their likelihood of adopting personal moderation tools to hide inappropriate posts. In contrast, descriptive and injunctive norms positively influence the use of these tools. Further, trust in Facebook's moderation systems also significantly affects users' engagement with personal moderation. This analysis highlights qualitatively different pathways through which FoMO and social media addiction make affected users disproportionately unsafe and offers design and policy solutions to address this challenge. ",
        "title": "Exploring How FoMO, Social Media Addiction, and Subjective Norms  Influence Personal Moderation Configurations",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05604",
        "abstract_url": "http://arxiv.org/abs/2401.05604",
        "authors": [
            {
                "last_name": "Gritsevskiy",
                "first_name": "Andrew"
            },
            {
                "last_name": "Panickssery",
                "first_name": "Arjun"
            },
            {
                "last_name": "Kirtland",
                "first_name": "Aaron"
            },
            {
                "last_name": "Kauffman",
                "first_name": "Derik"
            },
            {
                "last_name": "Gundlach",
                "first_name": "Hans"
            },
            {
                "last_name": "Gritsevskaya",
                "first_name": "Irina"
            },
            {
                "last_name": "Cavanagh",
                "first_name": "Joe"
            },
            {
                "last_name": "Chiang",
                "first_name": "Jonathan"
            },
            {
                "last_name": "La Roux",
                "first_name": "Lydia"
            },
            {
                "last_name": "Hung",
                "first_name": "Michelle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CV",
            "CY"
        ],
        "abstract": "  We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowledge and reasoning of multimodal large language models. ",
        "title": "REBUS: A Robust Evaluation Benchmark of Understanding Symbols",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05605",
        "abstract_url": "http://arxiv.org/abs/2401.05605",
        "authors": [
            {
                "last_name": "Kalajdzievski",
                "first_name": "Damjan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting ",
        "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05609",
        "abstract_url": "http://arxiv.org/abs/2401.05609",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wenxiong"
            },
            {
                "last_name": "Huang",
                "first_name": "Qikun"
            },
            {
                "last_name": "Chen",
                "first_name": "Suiyin"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  This paper introduces a cable finite element model based on an accurate description of the tension field for the static nonlinear analysis of cable structures. The proposed cable element is developed using the geometrically exact beam model that adequately considers the effects of large displacements. By neglecting flexural stiffness and shear deformation, the formulation of the cable finite element for scenarios involving given unstrained length and undetermined unstrained length is respectively presented. Additionally, the implementations of solutions based on complete tangent matrix and element internal iteration are introduced. Numerical examples are conducted to validate the accuracy of the presented formulation for cable analysis under various conditions and to demonstrate the computational efficiency of the proposed element and solution method. The results indicate that the proposed cable finite element not only exhibits extremely high accuracy but also effectively addresses the problem of determining the cable state with an unknown unstrained length, demonstrating the wide applicability of the proposed element. Through the utilization of an iteration algorithm with arc-length control and the introduction of additional control conditions, the proposed cable finite element can be further utilized to solve complex practical engineering problems. ",
        "title": "A cable finite element formulation based on exact tension field for  static nonlinear analysis of cable structures",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05610",
        "abstract_url": "http://arxiv.org/abs/2401.05610",
        "authors": [
            {
                "last_name": "Dax",
                "first_name": "Victoria M."
            },
            {
                "last_name": "Li",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Leahy",
                "first_name": "Kevin"
            },
            {
                "last_name": "Kochenderfer",
                "first_name": "Mykel J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data. In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems. CO concerns optimizing a function over a discrete solution space that is often intractably large. To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality. We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions. We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time. ",
        "title": "Graph Q-Learning for Combinatorial Optimization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05614",
        "abstract_url": "http://arxiv.org/abs/2401.05614",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Lian"
            },
            {
                "last_name": "Pun",
                "first_name": "Chi-Man"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "MM"
        ],
        "abstract": "  Due to the successful application of deep learning, audio spoofing detection has made significant progress. Spoofed audio with speech synthesis or voice conversion can be well detected by many countermeasures. However, an automatic speaker verification system is still vulnerable to spoofing attacks such as replay or Deep-Fake audio. Deep-Fake audio means that the spoofed utterances are generated using text-to-speech (TTS) and voice conversion (VC) algorithms. Here, we propose a novel framework based on hybrid features with the self-attention mechanism. It is expected that hybrid features can be used to get more discrimination capacity. Firstly, instead of only one type of conventional feature, deep learning features and Mel-spectrogram features will be extracted by two parallel paths: convolution neural networks and a short-time Fourier transform (STFT) followed by Mel-frequency. Secondly, features will be concatenated by a max-pooling layer. Thirdly, there is a Self-attention mechanism for focusing on essential elements. Finally, ResNet and a linear layer are built to get the results. Experimental results reveal that the hybrid features, compared with conventional features, can cover more details of an utterance. We achieve the best Equal Error Rate (EER) of 9.67\\% in the physical access (PA) scenario and 8.94\\% in the Deep fake task on the ASVspoof 2021 dataset. Compared with the best baseline system, the proposed approach improves by 74.60\\% and 60.05\\%, respectively. ",
        "title": "Self-Attention and Hybrid Features for Replay and Deep-Fake Audio  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05618",
        "abstract_url": "http://arxiv.org/abs/2401.05618",
        "authors": [
            {
                "last_name": "Renze",
                "first_name": "Matthew"
            },
            {
                "last_name": "Guven",
                "first_name": "Erhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs. ",
        "title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large  Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05624",
        "abstract_url": "http://arxiv.org/abs/2401.05624",
        "authors": [
            {
                "last_name": "Tissaoui",
                "first_name": "Yassine"
            },
            {
                "last_name": "Kelly",
                "first_name": "James F."
            },
            {
                "last_name": "Marras",
                "first_name": "Simone"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Mitigating the impact of waves leaving a numerical domain has been a persistent challenge in numerical modeling. Reducing wave reflection at the domain boundary is crucial for accurate simulations. Absorbing layers, while common, often incur significant computational costs. This paper introduces an efficient application of a Legendre-Laguerre basis for absorbing layers for two-dimensional non-linear compressible Euler equations. The method couples a spectral-element bounded domain with a semi-infinite region, employing a tensor product of Lagrange and scaled Laguerre basis functions. The semi-infinite region serves as an absorbing layer for our simulations. In comparison to existing methods with similar absorbing layer extensions, our approach, a pioneering application to the Euler equations, demonstrates substantial computational savings. The study marks the first application of semi-infinite elements to mitigate wave reflection in the solution of the Euler equations, particularly in nonhydrostatic atmospheric modeling. A comprehensive set of tests demonstrates the method's versatility for general systems of conservation laws, with a focus on its effectiveness in damping vertically propagating gravity waves in a linear hydrostatic mountain simulation a benchmark for atmospheric models. Across all tests, our model consistently exhibits notable performance improvements compared to a traditional Rayleigh damping approach. ",
        "title": "Efficient Spectral Element Method for the Euler Equations on Unbounded  Domains in Multiple Dimensions",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05625",
        "abstract_url": "http://arxiv.org/abs/2401.05625",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Juni"
            },
            {
                "last_name": "Dong",
                "first_name": "Zhikang"
            },
            {
                "last_name": "Polak",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a novel method that combines differential geometry, kernels smoothing, and spectral analysis to quantify facial muscle activity from widely accessible video recordings, such as those captured on personal smartphones. Our approach emphasizes practicality and accessibility. It has significant potential for applications in national security and plastic surgery. Additionally, it offers remote diagnosis and monitoring for medical conditions such as stroke, Bell's palsy, and acoustic neuroma. Moreover, it is adept at detecting and classifying emotions, from the overt to the subtle. The proposed face muscle analysis technique is an explainable alternative to deep learning methods and a non-invasive substitute to facial electromyography (fEMG). ",
        "title": "Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle  Dynamics in Videos",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05626",
        "abstract_url": "http://arxiv.org/abs/2401.05626",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Yizhao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baoheng"
            },
            {
                "last_name": "Ding",
                "first_name": "Yuhao"
            },
            {
                "last_name": "So",
                "first_name": "Hayden Kwok-Hay"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  Event-based vision represents a paradigm shift in how vision information is captured and processed. By only responding to dynamic intensity changes in the scene, event-based sensing produces far less data than conventional frame-based cameras, promising to springboard a new generation of high-speed, low-power machines for edge intelligence. However, processing such dynamically sparse input originated from event cameras efficiently in real time, particularly with complex deep neural networks (DNN), remains a formidable challenge. Existing solutions that employ GPUs and other frame-based DNN accelerators often struggle to efficiently process the dynamically sparse event data, missing the opportunities to improve processing efficiency with sparse data. To address this, we propose ESDA, a composable dynamic sparse dataflow architecture that allows customized DNN accelerators to be constructed rapidly on FPGAs for event-based vision tasks. ESDA is a modular system that is composed of a set of parametrizable modules for each network layer type. These modules share a uniform sparse token-feature interface and can be connected easily to compose an all-on-chip dataflow accelerator on FPGA for each network model. To fully exploit the intrinsic sparsity in event data, ESDA incorporates the use of submanifold sparse convolutions that largely enhance the activation sparsity throughout the layers while simplifying hardware implementation. Finally, a network architecture and hardware implementation co-optimizing framework that allows tradeoffs between accuracy and performance is also presented. Experimental results demonstrate that when compared with existing GPU and hardware-accelerated solutions, ESDA achieves substantial speedup and improvement in energy efficiency across different applications, and it allows much wider design space for real-world deployments. ",
        "title": "A Composable Dynamic Sparse Dataflow Architecture for Efficient  Event-based Vision Processing on FPGA",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05627",
        "abstract_url": "http://arxiv.org/abs/2401.05627",
        "authors": [
            {
                "last_name": "Henzinger",
                "first_name": "Monika"
            },
            {
                "last_name": "Li",
                "first_name": "Jason"
            },
            {
                "last_name": "Rao",
                "first_name": "Satish"
            },
            {
                "last_name": "Wang",
                "first_name": "Di"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In 1996, Karger [Kar96] gave a startling randomized algorithm that finds a minimum-cut in a (weighted) graph in time $O(m\\log^3n)$ which he termed near-linear time meaning linear (in the size of the input) times a polylogarthmic factor. In this paper, we give the first deterministic algorithm which runs in near-linear time for weighted graphs.   Previously, the breakthrough results of Kawarabayashi and Thorup [KT19] gave a near-linear time algorithm for simple graphs. The main technique here is a clustering procedure that perfectly preserves minimum cuts. Recently, Li [Li21] gave an $m^{1+o(1)}$ deterministic minimum-cut algorithm for weighted graphs; this form of running time has been termed \"almost-linear''. Li uses almost-linear time deterministic expander decompositions which do not perfectly preserve minimum cuts, but he can use these clusterings to, in a sense, \"derandomize'' the methods of Karger.   In terms of techniques, we provide a structural theorem that says there exists a sparse clustering that preserves minimum cuts in a weighted graph with $o(1)$ error. In addition, we construct it deterministically in near linear time. This was done exactly for simple graphs in [KT19, HRW20] and with polylogarithmic error for weighted graphs in [Li21]. Extending the techniques in [KT19, HRW20] to weighted graphs presents significant challenges, and moreover, the algorithm can only polylogarithmically approximately preserve minimum cuts. A remaining challenge is to reduce the polylogarithmic-approximate clusterings to $1+o(1/\\log n)$-approximate so that they can be applied recursively as in [Li21] over $O(\\log n)$ many levels. This is an additional challenge that requires building on properties of tree-packings in the presence of a wide range of edge weights to, for example, find sources for local flow computations which identify minimum cuts that cross clusters. ",
        "title": "Deterministic Near-Linear Time Minimum Cut in Weighted Graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05629",
        "abstract_url": "http://arxiv.org/abs/2401.05629",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shaoru"
            },
            {
                "last_name": "Fazlyab",
                "first_name": "Mahyar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Control Barrier Functions (CBFs) provide an elegant framework for designing safety filters for nonlinear control systems by constraining their trajectories to an invariant subset of a prespecified safe set. However, the task of finding a CBF that concurrently maximizes the volume of the resulting control invariant set while accommodating complex safety constraints, particularly in high relative degree systems with actuation constraints, continues to pose a substantial challenge. In this work, we propose a novel self-supervised learning framework that holistically addresses these hurdles. Given a Boolean composition of multiple state constraints that define the safe set, our approach starts with building a single continuously differentiable function whose 0-superlevel set provides an inner approximation of the safe set. We then use this function together with a smooth neural network to parameterize the CBF candidate. Finally, we design a training loss function based on a Hamilton-Jacobi partial differential equation to train the CBF while enlarging the volume of the induced control invariant set. We demonstrate the effectiveness of our approach via numerical experiments. ",
        "title": "Learning Performance-Oriented Control Barrier Functions Under Complex  Safety Constraints and Limited Actuation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05631",
        "abstract_url": "http://arxiv.org/abs/2401.05631",
        "authors": [
            {
                "last_name": "Rosenberg",
                "first_name": "Karl Toby"
            },
            {
                "last_name": "Kazi",
                "first_name": "Rubaiat Habib"
            },
            {
                "last_name": "Wei",
                "first_name": "Li-Yi"
            },
            {
                "last_name": "Xia",
                "first_name": "Haijun"
            },
            {
                "last_name": "Perlin",
                "first_name": "Ken"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CL",
            "GR"
        ],
        "abstract": "  We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces. ",
        "title": "DrawTalking: Building Interactive Worlds by Sketching and Speaking",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05632",
        "abstract_url": "http://arxiv.org/abs/2401.05632",
        "authors": [
            {
                "last_name": "Joshi",
                "first_name": "Aditya"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            },
            {
                "last_name": "Kanojia",
                "first_name": "Diptesh"
            },
            {
                "last_name": "Li",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Haolan"
            },
            {
                "last_name": "Haffari",
                "first_name": "Gholamreza"
            },
            {
                "last_name": "Dippold",
                "first_name": "Doris"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  State-of-the-art natural language processing (NLP) models are trained on massive training corpora, and report a superlative performance on evaluation datasets. This survey delves into an important attribute of these datasets: the dialect of a language. Motivated by the performance degradation of NLP models for dialectic datasets and its implications for the equity of language technologies, we survey past research in NLP for dialects in terms of datasets, and approaches. We describe a wide range of NLP tasks in terms of two categories: natural language understanding (NLU) (for tasks such as dialect classification, sentiment analysis, parsing, and NLU benchmarks) and natural language generation (NLG) (for summarisation, machine translation, and dialogue systems). The survey is also broad in its coverage of languages which include English, Arabic, German among others. We observe that past work in NLP concerning dialects goes deeper than mere dialect classification, and . This includes early approaches that used sentence transduction that lead to the recent approaches that integrate hypernetworks into LoRA. We expect that this survey will be useful to NLP researchers interested in building equitable language technologies by rethinking LLM benchmarks and model architectures. ",
        "title": "Natural Language Processing for Dialects of a Language: A Survey",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05633",
        "abstract_url": "http://arxiv.org/abs/2401.05633",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Gang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junpeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent progress in single-image super-resolution (SISR) has achieved remarkable performance, yet the computational costs of these methods remain a challenge for deployment on resource-constrained devices. Especially for transformer-based methods, the self-attention mechanism in such models brings great breakthroughs while incurring substantial computational costs. To tackle this issue, we introduce the Convolutional Transformer layer (ConvFormer) and the ConvFormer-based Super-Resolution network (CFSR), which offer an effective and efficient solution for lightweight image super-resolution tasks. In detail, CFSR leverages the large kernel convolution as the feature mixer to replace the self-attention module, efficiently modeling long-range dependencies and extensive receptive fields with a slight computational cost. Furthermore, we propose an edge-preserving feed-forward network, simplified as EFN, to obtain local feature aggregation and simultaneously preserve more high-frequency information. Extensive experiments demonstrate that CFSR can achieve an advanced trade-off between computational cost and performance when compared to existing lightweight SR methods. Compared to state-of-the-art methods, e.g. ShuffleMixer, the proposed CFSR achieves 0.39 dB gains on Urban100 dataset for x2 SR task while containing 26% and 31% fewer parameters and FLOPs, respectively. Code and pre-trained models are available at https://github.com/Aitical/CFSR. ",
        "title": "Transforming Image Super-Resolution: A ConvFormer-based Efficient  Approach",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05638",
        "abstract_url": "http://arxiv.org/abs/2401.05638",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Changtai"
            },
            {
                "last_name": "Han",
                "first_name": "Xu"
            },
            {
                "last_name": "Yao",
                "first_name": "Chao"
            },
            {
                "last_name": "Ban",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate and efficient extraction of microstructures in microscopic images of materials plays a critical role in the exploration of structure-property relationships and the optimization of process parameters. Deep learning-based image segmentation techniques that rely on manual annotation are time-consuming and labor-intensive and hardly meet the demand for model transferability and generalization. Segment Anything Model (SAM), a large visual model with powerful deep feature representation and zero-shot generalization capabilities, has provided new solutions for image segmentation. However, directly applying SAM to segmenting microstructures in microscopic images of materials without human annotation cannot achieve the expected results, as the difficulty of adapting its native prompt engineering to the dense and dispersed characteristics of key microstructures in materials microscopy images. In this paper, we propose MatSAM, a general and efficient microstructure extraction solution based on SAM. A new point-based prompts generation strategy is designed, grounded on the distribution and shape of materials microstructures. It generates prompts for different microscopic images, fuses the prompts of the region of interest (ROI) key points and grid key points, and integrates post-processing methods for quantitative characterization of materials microstructures. For common microstructures including grain boundary and phase, MatSAM achieves superior segmentation performance to conventional methods and is even preferable to supervised learning methods evaluated on 18 materials microstructures imaged by the optical microscope (OM) and scanning electron microscope (SEM). We believe that MatSAM can significantly reduce the cost of quantitative characterization of materials microstructures and accelerate the design of new materials. ",
        "title": "MatSAM: Efficient Materials Microstructure Extraction via Visual Large  Model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05639",
        "abstract_url": "http://arxiv.org/abs/2401.05639",
        "authors": [
            {
                "last_name": "Hou",
                "first_name": "Yahui"
            },
            {
                "last_name": "Cheng",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper addresses the full-state prescribed performance-based consensus problem for double-integrator multi-agent systems with jointly connected topologies. To improve the transient performance, a distributed prescribed performance control protocol consisting of the transformed relative position and the transformed relative velocity is proposed, where the communication topology satisfies the jointly connected assumption. Different from the existing literatures, two independent transient performance specifications imposed on relative positions and relative velocities can be guaranteed simultaneously. A numerical example is ultimately used to validate the effectiveness of proposed protocol. ",
        "title": "Full-State Prescribed Performance-Based Consensus of Double-Integrator  Multi-Agent Systems with Jointly Connected Topologies",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05641",
        "abstract_url": "http://arxiv.org/abs/2401.05641",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Tiejin"
            },
            {
                "last_name": "Dai",
                "first_name": "Qinrun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yueqi"
            },
            {
                "last_name": "Wei",
                "first_name": "Hua"
            },
            {
                "last_name": "Zeng",
                "first_name": "Qingkai"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS",
            "CR",
            "LG"
        ],
        "abstract": "  Compartmentalization effectively prevents initial corruption from turning into a successful attack. This paper presents O2C, a pioneering system designed to enforce OS kernel compartmentalization on the fly. It not only provides immediate remediation for sudden threats but also maintains consistent system availability through the enforcement process.   O2C is empowered by the newest advancements of the eBPF ecosystem which allows to instrument eBPF programs that perform enforcement actions into the kernel at runtime. O2C takes the lead in embedding a machine learning model into eBPF programs, addressing unique challenges in on-the-fly compartmentalization. Our comprehensive evaluation shows that O2C effectively confines damage within the compartment. Further, we validate that decision tree is optimally suited for O2C owing to its advantages in processing tabular data, its explainable nature, and its compliance with the eBPF ecosystem. Last but not least, O2C is lightweight, showing negligible overhead and excellent sacalability system-wide. ",
        "title": "When eBPF Meets Machine Learning: On-the-fly OS Kernel  Compartmentalization",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05642",
        "abstract_url": "http://arxiv.org/abs/2401.05642",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Zheng"
            },
            {
                "last_name": "Mathur",
                "first_name": "Umang"
            },
            {
                "last_name": "Pavlogiannis",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Dynamic data race detection has emerged as a key technique for ensuring reliability of concurrent software in practice. However, dynamic approaches can often miss data races owing to nondeterminism in the thread scheduler. Predictive race detection techniques cater to this shortcoming by inferring alternate executions that may expose data races without re-executing the underlying program. More formally, the dynamic data race prediction problem asks, given a trace \\sigma of an execution of a concurrent program, can \\sigma be correctly reordered to expose a data race? Existing state-of-the art techniques for data race prediction either do not scale to executions arising from real world concurrent software, or only expose a limited class of data races, such as those that can be exposed without reversing the order of synchronization operations.   In general, exposing data races by reasoning about synchronization reversals is an intractable problem. In this work, we identify a class of data races, called Optimistic Sync(hronization)-Reversal races that can be detected in a tractable manner and often include non-trivial data races that cannot be exposed by prior tractable techniques. We also propose a sound algorithm OSR for detecting all optimistic sync-reversal data races in overall quadratic time, and show that the algorithm is optimal by establishing a matching lower bound. Our experiments demonstrate the effectiveness of OSR on our extensive suite of benchmarks, OSR reports the largest number of data races, and scales well to large execution traces. ",
        "title": "Optimistic Prediction of Synchronization-Reversal Data Races",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05648",
        "abstract_url": "http://arxiv.org/abs/2401.05648",
        "authors": [
            {
                "last_name": "Curbelo",
                "first_name": "Israel R."
            },
            {
                "last_name": "Malko",
                "first_name": "Hannah R."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We consider the on-line coloring problem restricted to proper interval graphs with known interval representation. Chrobak and \\'{S}lusarek (1981) showed that the greedy $\\textrm{First-Fit}$ algorithm has a strict competitive ratio of $2$. It remains open whether there is an on-line algorithm that performs better than $\\textrm{First-Fit}$. Piotr (2008) showed that if the representation is not known, there is no better on-line algorithm. Epstein and Levy (2005) showed that no on-line algorithm has a strict competitive ratio less than $1.5$ when a unit-interval representation is known, which was later improved to $1.\\overline{3}$. In this paper, we show that there is no on-line algorithm with strict competitive ratio less than $1.75$ by presenting a strategy that can force any on-line algorithm to use $7$ colors on a proper interval graph $G$ with chromatic number $\\chi(G)\\leq 4$ and known interval representation. ",
        "title": "On the on-line coloring of proper interval graphs",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05650",
        "abstract_url": "http://arxiv.org/abs/2401.05650",
        "authors": [
            {
                "last_name": "Jaradat",
                "first_name": "Israa"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haiqi"
            },
            {
                "last_name": "Li",
                "first_name": "Chengkai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Cherry-picking refers to the deliberate selection of evidence or facts that favor a particular viewpoint while ignoring or distorting evidence that supports an opposing perspective. Manually identifying instances of cherry-picked statements in news stories can be challenging, particularly when the opposing viewpoint's story is absent. This study introduces Cherry, an innovative approach for automatically detecting cherry-picked statements in news articles by finding missing important statements in the target news story. Cherry utilizes the analysis of news coverage from multiple sources to identify instances of cherry-picking. Our approach relies on language models that consider contextual information from other news sources to classify statements based on their importance to the event covered in the target news story. Furthermore, this research introduces a novel dataset specifically designed for cherry-picking detection, which was used to train and evaluate the performance of the models. Our best performing model achieves an F-1 score of about %89 in detecting important statements when tested on unseen set of news stories. Moreover, results show the importance incorporating external knowledge from alternative unbiased narratives when assessing a statement's importance. ",
        "title": "On Detecting Cherry-picking in News Coverage Using Large Language Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05653",
        "abstract_url": "http://arxiv.org/abs/2401.05653",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Sean"
            },
            {
                "last_name": "Musunuru",
                "first_name": "Sriya"
            },
            {
                "last_name": "Zong",
                "first_name": "Baoshi"
            },
            {
                "last_name": "Thornton",
                "first_name": "Brooks"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches. ",
        "title": "Quantifying Marketing Performance at Channel-Partner Level by Using  Marketing Mix Modeling (MMM) and Shapley Value Regression",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05654",
        "abstract_url": "http://arxiv.org/abs/2401.05654",
        "authors": [
            {
                "last_name": "Tu",
                "first_name": "Tao"
            },
            {
                "last_name": "Palepu",
                "first_name": "Anil"
            },
            {
                "last_name": "Schaekermann",
                "first_name": "Mike"
            },
            {
                "last_name": "Saab",
                "first_name": "Khaled"
            },
            {
                "last_name": "Freyberg",
                "first_name": "Jan"
            },
            {
                "last_name": "Tanno",
                "first_name": "Ryutaro"
            },
            {
                "last_name": "Wang",
                "first_name": "Amy"
            },
            {
                "last_name": "Li",
                "first_name": "Brenna"
            },
            {
                "last_name": "Amin",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Tomasev",
                "first_name": "Nenad"
            },
            {
                "last_name": "Azizi",
                "first_name": "Shekoofeh"
            },
            {
                "last_name": "Singhal",
                "first_name": "Karan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yong"
            },
            {
                "last_name": "Hou",
                "first_name": "Le"
            },
            {
                "last_name": "Webson",
                "first_name": "Albert"
            },
            {
                "last_name": "Kulkarni",
                "first_name": "Kavita"
            },
            {
                "last_name": "Mahdavi",
                "first_name": "S Sara"
            },
            {
                "last_name": "Semturs",
                "first_name": "Christopher"
            },
            {
                "last_name": "Gottweis",
                "first_name": "Juraj"
            },
            {
                "last_name": "Barral",
                "first_name": "Joelle"
            },
            {
                "last_name": "Chou",
                "first_name": "Katherine"
            },
            {
                "last_name": "Corrado",
                "first_name": "Greg S"
            },
            {
                "last_name": "Matias",
                "first_name": "Yossi"
            },
            {
                "last_name": "Karthikesalingam",
                "first_name": "Alan"
            },
            {
                "last_name": "Natarajan",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.   AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI. ",
        "title": "Towards Conversational Diagnostic AI",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05655",
        "abstract_url": "http://arxiv.org/abs/2401.05655",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Kaixun"
            },
            {
                "last_name": "Rakovi\u0107",
                "first_name": "Mladen"
            },
            {
                "last_name": "Li",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Guan",
                "first_name": "Quanlong"
            },
            {
                "last_name": "Ga\u0161evi\u0107",
                "first_name": "Dragan"
            },
            {
                "last_name": "Chen",
                "first_name": "Guanliang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Automatic Essay Scoring (AES) is a well-established educational pursuit that employs machine learning to evaluate student-authored essays. While much effort has been made in this area, current research primarily focuses on either (i) boosting the predictive accuracy of an AES model for a specific prompt (i.e., developing prompt-specific models), which often heavily relies on the use of the labeled data from the same target prompt; or (ii) assessing the applicability of AES models developed on non-target prompts to the intended target prompt (i.e., developing the AES models in a cross-prompt setting). Given the inherent bias in machine learning and its potential impact on marginalized groups, it is imperative to investigate whether such bias exists in current AES methods and, if identified, how it intervenes with an AES model's accuracy and generalizability. Thus, our study aimed to uncover the intricate relationship between an AES model's accuracy, fairness, and generalizability, contributing practical insights for developing effective AES models in real-world education. To this end, we meticulously selected nine prominent AES methods and evaluated their performance using seven metrics on an open-sourced dataset, which contains over 25,000 essays and various demographic information about students such as gender, English language learner status, and economic status. Through extensive evaluations, we demonstrated that: (1) prompt-specific models tend to outperform their cross-prompt counterparts in terms of predictive accuracy; (2) prompt-specific models frequently exhibit a greater bias towards students of different economic statuses compared to cross-prompt models; (3) in the pursuit of generalizability, traditional machine learning models coupled with carefully engineered features hold greater potential for achieving both high accuracy and fairness than complex neural network models. ",
        "title": "Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive  Investigation of Accuracy, Fairness, and Generalizability",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05657",
        "abstract_url": "http://arxiv.org/abs/2401.05657",
        "authors": [
            {
                "last_name": "Holliday",
                "first_name": "Wesley H."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "MA"
        ],
        "abstract": "  In the context of social choice theory with ordinal preferences, we say that the defensible set is the set of alternatives $x$ such that for any alternative $y$, if $y$ beats $x$ in a head-to-head majority comparison, then there is an alternative $z$ that beats $y$ in a head-to-head majority comparison by a margin at least as large as the margin by which $y$ beat $x$. We show that any ordinal voting method satisfying two well-known axioms from voting theory--positive involvement and the Condorcet winner criterion--refines the defensible set. Using this lemma, we prove an impossibility theorem: there is no such voting method that also satisfies the Condorcet loser criterion, resolvability, and a common invariance property for Condorcet methods, namely that the choice of winners depends only on the relative sizes of majority margins. ",
        "title": "The defensible set and a new impossibility theorem in voting",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05659",
        "abstract_url": "http://arxiv.org/abs/2401.05659",
        "authors": [
            {
                "last_name": "Madugalla",
                "first_name": "Anuradha"
            },
            {
                "last_name": "Huang",
                "first_name": "Yutan"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            },
            {
                "last_name": "Cho",
                "first_name": "Min Hee"
            },
            {
                "last_name": "Gamage",
                "first_name": "Lasith Koswatta"
            },
            {
                "last_name": "Leao",
                "first_name": "Tristan"
            },
            {
                "last_name": "Thiele",
                "first_name": "Sam"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "SE"
        ],
        "abstract": "  Most software applications contain graphics such as charts, diagrams and maps. Currently, these graphics are designed with a ``one size fits all\" approach and do not cater to the needs of people with disabilities. Therefore, when using software with graphics, a colour-impaired user may struggle to interpret graphics with certain colours, and a person with dyslexia may struggle to read the text labels in the graphic. Our research addresses this issue by developing a framework that generates adaptive and accessible information graphics for multiple disabilities. Uniquely, the approach also serves people with multiple simultaneous disabilities. To achieve these, we used a case study of public space floorplans presented via a web tool and worked with four disability groups: people with low vision, colour blindness, dyslexia and mobility impairment. Our research involved gathering requirements from 3 accessibility experts and 80 participants with disabilities, developing a system to generate adaptive graphics that address the identified requirements, and conducting an evaluation with 7 participants with disabilities. The evaluation showed that users found our solution easy to use and suitable for most of their requirements. The study also provides recommendations for front-end developers on engineering accessible graphics for their software and discusses the implications of our work on society from the perspective of public space owners and end users. ",
        "title": "Engineering Adaptive Information Graphics for Disabled Communities: A  Case Study with Public Space Indoor Maps",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05661",
        "abstract_url": "http://arxiv.org/abs/2401.05661",
        "authors": [
            {
                "last_name": "Espinoza",
                "first_name": "Jes\u00fas F."
            },
            {
                "last_name": "Esquer-P\u00e9rez",
                "first_name": "Cynthia G."
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  In this work we study the intersection properties of a finite disk system in the euclidean space. We accomplish this by utilizing subsets of spheres with varying dimensions and analyze specific points within them, referred to as poles. Additionally, we introduce two applications: estimating the common scale factor for the radii that makes the re-scaled disks intersects in a single point, this is the \\v{C}ech scale, and constructing the minimal Axis-Aligned Bounding Box (AABB) that encloses the intersection of all disks in the system. ",
        "title": "Intersection properties of finite disk collections",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05664",
        "abstract_url": "http://arxiv.org/abs/2401.05664",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Jian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Energy efficiency is a big concern in industrial sectors. Finding the root cause of anomaly state of energy efficiency can help to improve energy efficiency of industrial systems and therefore save energy cost. In this research, we propose to use transfer entropy (TE) for root cause analysis on energy efficiency of industrial systems. A method, called TE flow, is proposed in that a TE flow from physical measurements of each subsystem to the energy efficiency indicator along timeline is considered as causal strength for diagnosing root cause of anomaly states of energy efficiency of a system. The copula entropy-based nonparametric TE estimator is used in the proposed method. We conducted experiments on real data collected from a compressing air system to verify the proposed method. Experimental results show that the TE flow method successfully identified the root cause of the energy (in)efficiency of the system. ",
        "title": "Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05665",
        "abstract_url": "http://arxiv.org/abs/2401.05665",
        "authors": [
            {
                "last_name": "Regal",
                "first_name": "Frank"
            },
            {
                "last_name": "Suarez",
                "first_name": "Chris"
            },
            {
                "last_name": "Parra",
                "first_name": "Fabian"
            },
            {
                "last_name": "Pryor",
                "first_name": "Mitch"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Multi-agent human-robot teaming allows for the potential to gather information about various environments more efficiently by exploiting and combining the strengths of humans and robots. In industries like defense, search and rescue, first-response, and others alike, heterogeneous human-robot teams show promise to accelerate data collection and improve team safety by removing humans from unknown and potentially hazardous situations. This work builds upon AugRE, an Augmented Reality (AR) based scalable human-robot teaming framework. It enables users to localize and communicate with 50+ autonomous agents. Through our efforts, users are able to command, control, and supervise agents in large teams, both line-of-sight and non-line-of-sight, without the need to modify the environment prior and without requiring users to use typical hardware (i.e. joysticks, keyboards, laptops, tablets, etc.) in the field. The demonstrated work shows early indications that combining these AR-HMD-based user interaction modalities for command, control, and supervision will help improve human-robot team collaboration, robustness, and trust. ",
        "title": "Augmented Reality User Interface for Command, Control, and Supervision  of Large Multi-Agent Teams",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05667",
        "abstract_url": "http://arxiv.org/abs/2401.05667",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Weijieying"
            },
            {
                "last_name": "Honavar",
                "first_name": "Vasant G"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks. Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks. However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification. To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining. We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters. SDP ensures model with minimal loss of predictive accuracy, accelerating the learning of sparse models at each stage. To accelerate model update, we introduce an intelligent data selection (IDS) strategy that can identify critical instances for estimating loss landscape, yielding substantially improved data efficiency. The results of our experiments show that EsaCL achieves performance that is competitive with the state-of-the-art methods on three continual learning benchmarks, while using substantially reduced memory and computational resources. ",
        "title": "EsaCL: Efficient Continual Learning of Sparse Models",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05668",
        "abstract_url": "http://arxiv.org/abs/2401.05668",
        "authors": [
            {
                "last_name": "Madugalla",
                "first_name": "Anuradha"
            },
            {
                "last_name": "Kanij",
                "first_name": "Tanjila"
            },
            {
                "last_name": "Hoda",
                "first_name": "Rashina"
            },
            {
                "last_name": "Hidellaarachchi",
                "first_name": "Dulaji"
            },
            {
                "last_name": "Pant",
                "first_name": "Aastha"
            },
            {
                "last_name": "Ferdousi",
                "first_name": "Samia"
            },
            {
                "last_name": "Grundy",
                "first_name": "John"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The COVID-19 pandemic changed the way we live, work and the way we conduct research. With the restrictions of lockdowns and social distancing, various impacts were experienced by many software engineering researchers, especially whose studies depend on human participants. We conducted a mixed methods study to understand the extent of this impact. Through a detailed survey with 89 software engineering researchers working with human participants around the world and a further nine follow-up interviews, we identified the key challenges faced, the adaptations made, and the surprising fringe benefits of conducting research involving human participants during the pandemic. Our findings also revealed that in retrospect, many researchers did not wish to revert to the old ways of conducting human-oriented research. Based on our analysis and insights, we share recommendations on how to conduct remote studies with human participants effectively in an increasingly hybrid world when face-to-face engagement is not possible or where remote participation is preferred. ",
        "title": "Challenges, Adaptations, and Fringe Benefits of Conducting Software  Engineering Research with Human Participants during the COVID-19 Pandemic",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05669",
        "abstract_url": "http://arxiv.org/abs/2401.05669",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "Gu",
                "first_name": "Zhouhong"
            },
            {
                "last_name": "Liang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Lu",
                "first_name": "Dakuan"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yanghua"
            },
            {
                "last_name": "Wang",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pre-trained language models (PLMs) have been prevailing in state-of-the-art methods for natural language processing, and knowledge-enhanced PLMs are further proposed to promote model performance in knowledge-intensive tasks. However, conceptual knowledge, one essential kind of knowledge for human cognition, still remains understudied in this line of research. This limits PLMs' performance in scenarios requiring human-like cognition, such as understanding long-tail entities with concepts. In this paper, we propose ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies with entity concept prediction, a novel pre-training objective to predict the concepts of entities mentioned in the pre-training contexts. Unlike previous concept-enhanced methods, ConcEPT can be readily adapted to various downstream applications without entity linking or concept mapping. Results of extensive experiments show the effectiveness of ConcEPT in four tasks such as entity typing, which validates that our model gains improved conceptual knowledge with concept-enhanced pre-training. ",
        "title": "ConcEPT: Concept-Enhanced Pre-Training for Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05673",
        "abstract_url": "http://arxiv.org/abs/2401.05673",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Nick"
            },
            {
                "last_name": "Marsso",
                "first_name": "Lina"
            },
            {
                "last_name": "Yaman",
                "first_name": "Sinem Getir"
            },
            {
                "last_name": "Baatartogtokh",
                "first_name": "Yesugen"
            },
            {
                "last_name": "Ayad",
                "first_name": "Reem"
            },
            {
                "last_name": "de Mello",
                "first_name": "Vict\u00f3ria Oldemburgo"
            },
            {
                "last_name": "Townsend",
                "first_name": "Beverley"
            },
            {
                "last_name": "Standen",
                "first_name": "Isobel"
            },
            {
                "last_name": "Stefanakos",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Imrie",
                "first_name": "Calum"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Gena\u00edna Nunes"
            },
            {
                "last_name": "Cavalcanti",
                "first_name": "Ana"
            },
            {
                "last_name": "Calinescu",
                "first_name": "Radu"
            },
            {
                "last_name": "Chechik",
                "first_name": "Marsha"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  As software systems increasingly interact with humans in application domains such as transportation and healthcare, they raise concerns related to the social, legal, ethical, empathetic, and cultural (SLEEC) norms and values of their stakeholders. Normative non-functional requirements (N-NFRs) are used to capture these concerns by setting SLEEC-relevant boundaries for system behavior. Since N-NFRs need to be specified by multiple stakeholders with widely different, non-technical expertise (ethicists, lawyers, regulators, end users, etc.), N-NFR elicitation is very challenging. To address this challenge, we introduce N-Check, a novel tool-supported formal approach to N-NFR analysis and debugging. N-Check employs satisfiability checking to identify a broad spectrum of N-NFR well-formedness issues (WFI), such as conflicts, redundancy, restrictiveness, insufficiency, yielding diagnostics which pinpoint their causes in a user-friendly way that enables non-technical stakeholders to understand and fix them. We show the effectiveness and usability of our approach through nine case studies in which teams of ethicists, lawyers, philosophers, psychologists, safety analysts, and engineers used N-Check to analyse and debug 233 N-NFRs comprising 62 issues for the software underpinning the operation of systems ranging from assistive-care robots and tree-disease detection drones to manufacturing collaborative robots. ",
        "title": "Analyzing and Debugging Normative Requirements via Satisfiability  Checking",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05674",
        "abstract_url": "http://arxiv.org/abs/2401.05674",
        "authors": [
            {
                "last_name": "Feireisl",
                "first_name": "Eduard"
            },
            {
                "last_name": "Lukacova-Medvidova",
                "first_name": "Maria"
            },
            {
                "last_name": "She",
                "first_name": "Bangwei"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuhuan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider the Navier-Stokes-Fourier system governing the motion of a general compressible, heat conducting, Newtonian fluid driven by random initial/boundary data. Convergence of the stochastic collocation and Monte Carlo numerical methods is shown under the hypothesis that approximate solutions are bounded in probability. Abstract results are illustrated by numerical experiments for the Rayleigh-Benard convection problem. ",
        "title": "Convergence of numerical methods for the Navier-Stokes-Fourier system  driven by uncertain initial/boundary data",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05675",
        "abstract_url": "http://arxiv.org/abs/2401.05675",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Seung Hyun"
            },
            {
                "last_name": "Li",
                "first_name": "Yinxiao"
            },
            {
                "last_name": "Ke",
                "first_name": "Junjie"
            },
            {
                "last_name": "Yoo",
                "first_name": "Innfarn"
            },
            {
                "last_name": "Zhang",
                "first_name": "Han"
            },
            {
                "last_name": "Yu",
                "first_name": "Jiahui"
            },
            {
                "last_name": "Wang",
                "first_name": "Qifei"
            },
            {
                "last_name": "Deng",
                "first_name": "Fei"
            },
            {
                "last_name": "Entis",
                "first_name": "Glenn"
            },
            {
                "last_name": "He",
                "first_name": "Junfeng"
            },
            {
                "last_name": "Li",
                "first_name": "Gang"
            },
            {
                "last_name": "Kim",
                "first_name": "Sangpil"
            },
            {
                "last_name": "Essa",
                "first_name": "Irfan"
            },
            {
                "last_name": "Yang",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent works demonstrate that using reinforcement learning (RL) with quality rewards can enhance the quality of generated images in text-to-image (T2I) generation. However, a simple aggregation of multiple rewards may cause over-optimization in certain metrics and degradation in others, and it is challenging to manually find the optimal weights. An effective strategy to jointly optimize multiple rewards in RL for T2I generation is highly desirable. This paper introduces Parrot, a novel multi-reward RL framework for T2I generation. Through the use of the batch-wise Pareto optimal selection, Parrot automatically identifies the optimal trade-off among different rewards during the RL optimization of the T2I generation. Additionally, Parrot employs a joint optimization approach for the T2I model and the prompt expansion network, facilitating the generation of quality-aware text prompts, thus further enhancing the final image quality. To counteract the potential catastrophic forgetting of the original user prompt due to prompt expansion, we introduce original prompt centered guidance at inference time, ensuring that the generated image remains faithful to the user input. Extensive experiments and a user study demonstrate that Parrot outperforms several baseline methods across various quality criteria, including aesthetics, human preference, image sentiment, and text-image alignment. ",
        "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for  Text-to-Image Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05676",
        "abstract_url": "http://arxiv.org/abs/2401.05676",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Weibo"
            },
            {
                "last_name": "Ren",
                "first_name": "Weihong"
            },
            {
                "last_name": "Tian",
                "first_name": "Jiandong"
            },
            {
                "last_name": "Qu",
                "first_name": "Liangqiong"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Liu",
                "first_name": "Honghai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human-Object Interaction (HOI) detection plays a vital role in scene understanding, which aims to predict the HOI triplet in the form of <human, object, action>. Existing methods mainly extract multi-modal features (e.g., appearance, object semantics, human pose) and then fuse them together to directly predict HOI triplets. However, most of these methods focus on seeking for self-triplet aggregation, but ignore the potential cross-triplet dependencies, resulting in ambiguity of action prediction. In this work, we propose to explore Self- and Cross-Triplet Correlations (SCTC) for HOI detection. Specifically, we regard each triplet proposal as a graph where Human, Object represent nodes and Action indicates edge, to aggregate self-triplet correlation. Also, we try to explore cross-triplet dependencies by jointly considering instance-level, semantic-level, and layout-level relations. Besides, we leverage the CLIP model to assist our SCTC obtain interaction-aware feature by knowledge distillation, which provides useful action clues for HOI detection. Extensive experiments on HICO-DET and V-COCO datasets verify the effectiveness of our proposed SCTC. ",
        "title": "Exploring Self- and Cross-Triplet Correlations for Human-Object  Interaction Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05680",
        "abstract_url": "http://arxiv.org/abs/2401.05680",
        "authors": [
            {
                "last_name": "Mitra",
                "first_name": "Shaswata"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Trisha"
            },
            {
                "last_name": "Neupane",
                "first_name": "Subash"
            },
            {
                "last_name": "Piplai",
                "first_name": "Aritran"
            },
            {
                "last_name": "Mittal",
                "first_name": "Sudip"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            "NE"
        ],
        "abstract": "  In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address each phase of CKC and discuss how GNNs contribute to preparing and preventing an attack from a defensive standpoint. Furthermore, We also discuss open research areas and further improvement scopes. ",
        "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05683",
        "abstract_url": "http://arxiv.org/abs/2401.05683",
        "authors": [
            {
                "last_name": "Sankar",
                "first_name": "V. Udaya"
            },
            {
                "last_name": "Rao",
                "first_name": "Vishisht Srihari"
            },
            {
                "last_name": "Narahari",
                "first_name": "Y."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Mechanism design is essentially reverse engineering of games and involves inducing a game among strategic agents in a way that the induced game satisfies a set of desired properties in an equilibrium of the game. Desirable properties for a mechanism include incentive compatibility, individual rationality, welfare maximisation, revenue maximisation (or cost minimisation), fairness of allocation, etc. It is known from mechanism design theory that only certain strict subsets of these properties can be simultaneously satisfied exactly by any given mechanism. Often, the mechanisms required by real-world applications may need a subset of these properties that are theoretically impossible to be simultaneously satisfied. In such cases, a prominent recent approach is to use a deep learning based approach to learn a mechanism that approximately satisfies the required properties by minimizing a suitably defined loss function. In this paper, we present, from relevant literature, technical details of using a deep learning approach for mechanism design and provide an overview of key results in this topic. We demonstrate the power of this approach for three illustrative case studies: (a) efficient energy management in a vehicular network (b) resource allocation in a mobile network (c) designing a volume discount procurement auction for agricultural inputs. Section 6 concludes the paper. ",
        "title": "Deep Learning Meets Mechanism Design: Key Results and Some Novel  Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05684",
        "abstract_url": "http://arxiv.org/abs/2401.05684",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Sirui"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Li",
                "first_name": "Liang"
            },
            {
                "last_name": "Ding",
                "first_name": "Lingyun"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Multiscale metrics such as negative Sobolev norms are effective for quantifying the degree of mixedness of a passive scalar field advected by an incompressible flow in the absence of diffusion. In this paper we introduce a mix norm that is motivated by Sobolev norm $H^{-1}$ for a general domain with a no-flux boundary. We then derive an explicit expression for the optimal flow that maximizes the instantaneous decay rate of the mix norm under fixed energy and enstrophy constraints. Numerical simulations indicate that the mix norm decays exponentially or faster for various initial conditions and geometries and the rate is closely related to the smallest non-zero eigenvalue of the Laplace operator. These results generalize previous findings restricted for a periodic domain for its analytical and numerical simplicity. Additionally, we observe that periodic boundaries tend to induce a faster decay in mix norm compared to no-flux conditions under the fixed energy constraint, while the comparison is reversed for the fixed enstrophy constraint. In the special case of even initial distributions, two types of boundary conditions yield the same optimal flow and mix norm decay. ",
        "title": "Optimal Stirring Strategies for Passive Scalars in a Domain with a  General Shape and No-Flux Boundary Condition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05686",
        "abstract_url": "http://arxiv.org/abs/2401.05686",
        "authors": [
            {
                "last_name": "Appolinary",
                "first_name": "Blaise"
            },
            {
                "last_name": "Deaconu",
                "first_name": "Alex"
            },
            {
                "last_name": "Yang",
                "first_name": "Sophia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we present a novel method for dynamically expanding Convolutional Neural Networks (CNNs) during training, aimed at meeting the increasing demand for efficient and sustainable deep learning models. Our approach, drawing from the seminal work on Self-Expanding Neural Networks (SENN), employs a natural expansion score as an expansion criteria to address the common issue of over-parameterization in deep convolutional neural networks, thereby ensuring that the model's complexity is finely tuned to the task's specific needs. A significant benefit of this method is its eco-friendly nature, as it obviates the necessity of training multiple models of different sizes. We employ a strategy where a single model is dynamically expanded, facilitating the extraction of checkpoints at various complexity levels, effectively reducing computational resource use and energy consumption while also expediting the development cycle by offering diverse model complexities from a single training session. We evaluate our method on the CIFAR-10 dataset and our experimental results validate this approach, demonstrating that dynamically adding layers not only maintains but also improves CNN performance, underscoring the effectiveness of our expansion criteria. This approach marks a considerable advancement in developing adaptive, scalable, and environmentally considerate neural network architectures, addressing key challenges in the field of deep learning. ",
        "title": "Self Expanding Convolutional Neural Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05689",
        "abstract_url": "http://arxiv.org/abs/2401.05689",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Wang",
                "first_name": "Minghan"
            },
            {
                "last_name": "Qiao",
                "first_name": "Xiaosong"
            },
            {
                "last_name": "Wei",
                "first_name": "Daimeng"
            },
            {
                "last_name": "Shang",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Li",
                "first_name": "Zongyao"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhengzhe"
            },
            {
                "last_name": "Li",
                "first_name": "Yinglu"
            },
            {
                "last_name": "Su",
                "first_name": "Chang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            },
            {
                "last_name": "Tao",
                "first_name": "Shimin"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  Error correction techniques have been used to refine the output sentences from automatic speech recognition (ASR) models and achieve a lower word error rate (WER). Previous works usually adopt end-to-end models and has strong dependency on Pseudo Paired Data and Original Paired Data. But when only pre-training on Pseudo Paired Data, previous models have negative effect on correction. While fine-tuning on Original Paired Data, the source side data must be transcribed by a well-trained ASR model, which takes a lot of time and not universal. In this paper, we propose UCorrect, an unsupervised Detector-Generator-Selector framework for ASR Error Correction. UCorrect has no dependency on the training data mentioned before. The whole procedure is first to detect whether the character is erroneous, then to generate some candidate characters and finally to select the most confident one to replace the error character. Experiments on the public AISHELL-1 dataset and WenetSpeech dataset show the effectiveness of UCorrect for ASR error correction: 1) it achieves significant WER reduction, achieves 6.83\\% even without fine-tuning and 14.29\\% after fine-tuning; 2) it outperforms the popular NAR correction models by a large margin with a competitive low latency; and 3) it is an universal method, as it reduces all WERs of the ASR model with different decoding strategies and reduces all WERs of ASR models trained on different scale datasets. ",
        "title": "UCorrect: An Unsupervised Framework for Automatic Speech Recognition  Error Correction",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05690",
        "abstract_url": "http://arxiv.org/abs/2401.05690",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Cong"
            },
            {
                "last_name": "You",
                "first_name": "Changsheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haodong"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            },
            {
                "last_name": "Shi",
                "first_name": "Shuo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Extremely large-scale array (XL-array) has emerged as a promising technology to enable near-field communications for achieving enhanced spectrum efficiency and spatial resolution, by drastically increasing the number of antennas. However, this also inevitably incurs higher hardware and energy cost, which may not be affordable in future wireless systems. To address this issue, we propose in this paper to exploit two types of sparse arrays (SAs) for enabling near-field communications. Specifically, we first consider the linear sparse array (LSA) and characterize its near-field beam pattern. It is shown that despite the achieved beam-focusing gain, the LSA introduces several undesired grating-lobes, which have comparable beam power with the main-lobe and are focused on specific regions. An efficient hybrid beamforming design is then proposed for the LSA to deal with the potential strong inter-user interference (IUI). Next, we consider another form of SA, called extended coprime array (ECA), which is composed of two LSA subarrays with different (coprime) inter-antenna spacing. By characterizing the ECA near-field beam pattern, we show that compared with the LSA with the same array sparsity, the ECA can greatly suppress the beam power of near-field grating-lobes thanks to the offset effect of the two subarrays, albeit with a larger number of grating-lobes. This thus motivates us to propose a customized two-phase hybrid beamforming design for the ECA. Finally, numerical results are presented to demonstrate the rate performance gain of the proposed two SAs over the conventional uniform linear array (ULA). ",
        "title": "Sparse Array Enabled Near-Field Communications: Beam Pattern Analysis  and Hybrid Beamforming Design",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05695",
        "abstract_url": "http://arxiv.org/abs/2401.05695",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Chengfeng"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Jiao",
                "first_name": "Wenpin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiyan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yongqiang"
            },
            {
                "last_name": "Tao",
                "first_name": "Zhenwei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue tasks, showcasing its potential for improving medical dialogue generation. ",
        "title": "Integrating Physician Diagnostic Logic into Large Language Models:  Preference Learning from Process Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05698",
        "abstract_url": "http://arxiv.org/abs/2401.05698",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Licai"
            },
            {
                "last_name": "Lian",
                "first_name": "Zheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Bin"
            },
            {
                "last_name": "Tao",
                "first_name": "Jianhua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC",
            "MM",
            "SD"
        ],
        "abstract": "  Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-ware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-supervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner. Codes and models will be publicly available at https://github.com/sunlicai/HiCMAE. ",
        "title": "HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised  Audio-Visual Emotion Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05700",
        "abstract_url": "http://arxiv.org/abs/2401.05700",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhanglin"
            },
            {
                "last_name": "Li",
                "first_name": "Zongyao"
            },
            {
                "last_name": "Shang",
                "first_name": "Hengchao"
            },
            {
                "last_name": "Wei",
                "first_name": "Daimeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Rao",
                "first_name": "Zhiqiang"
            },
            {
                "last_name": "Li",
                "first_name": "Shaojun"
            },
            {
                "last_name": "Yang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Incremental Decoding is an effective framework that enables the use of an offline model in a simultaneous setting without modifying the original model, making it suitable for Low-Latency Simultaneous Speech Translation. However, this framework may introduce errors when the system outputs from incomplete input. To reduce these output errors, several strategies such as Hold-$n$, LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be carefully selected for optimal performance. Moreover, these strategies are more suitable for end-to-end systems than cascade systems. In our paper, we propose a new adaptable and efficient policy named \"Regularized Batched Inputs\". Our method stands out by enhancing input diversity to mitigate output errors. We suggest particular regularization techniques for both end-to-end and cascade systems. We conducted experiments on IWSLT Simultaneous Speech Translation (SimulST) tasks, which demonstrate that our approach achieves low latency while maintaining no more than 2 BLEU points loss compared to offline systems. Furthermore, our SimulST systems attained several new state-of-the-art results in various language directions. ",
        "title": "R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework  for Low-Latency Simultaneous Speech Translation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05702",
        "abstract_url": "http://arxiv.org/abs/2401.05702",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Hui"
            },
            {
                "last_name": "Sun",
                "first_name": "Qianru"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%, respectively. More impressively, our approach can provide textual explanations for detected anomalies. ",
        "title": "Video Anomaly Detection and Explanation via Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05705",
        "abstract_url": "http://arxiv.org/abs/2401.05705",
        "authors": [
            {
                "last_name": "Krivorotko",
                "first_name": "Olga"
            },
            {
                "last_name": "Zvonareva",
                "first_name": "Tatiana"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  A numerical algorithm for regularization of the solution of the source problem for the diffusion-logistic model based on information about the process at fixed moments of time of integral type has been developed. The peculiarity of the problem under study is the discrete formulation in space and impossibility to apply classical algorithms for its numerical solution. The regularization of the problem is based on the application of A.N. Tikhonov's approach and a priori information about the source of the process. The problem was formulated in a variational formulation and solved by the global tensor optimization method. It is shown that in the case of noisy data regularization improves the accuracy of the reconstructed source. ",
        "title": "Regularization of the discrete source problem in the nonlinear  diffusive-logistic equation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05707",
        "abstract_url": "http://arxiv.org/abs/2401.05707",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Zhen"
            },
            {
                "last_name": "Xi",
                "first_name": "Dinghao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Liumin"
            },
            {
                "last_name": "Xu",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Text style transfer is increasingly prominent in online entertainment and social media. However, existing research mainly concentrates on style transfer within individual English sentences, while ignoring the complexity of long Chinese texts, which limits the wider applicability of style transfer in digital media realm. To bridge this gap, we propose a Chinese Article-style Transfer framework (CAT-LLM), leveraging the capabilities of Large Language Models (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively analyzing text features in articles, prompting LLMs to efficiently transfer Chinese article-style. The TSD module integrates a series of machine learning algorithms to analyze article-style from both words and sentences levels, thereby aiding LLMs thoroughly grasp the target style without compromising the integrity of the original text. In addition, this module supports dynamic expansion of internal style trees, showcasing robust compatibility and allowing flexible optimization in subsequent research. Moreover, we select five Chinese articles with distinct styles and create five parallel datasets using ChatGPT, enhancing the models' performance evaluation accuracy and establishing a novel paradigm for evaluating subsequent research on article-style transfer. Extensive experimental results affirm that CAT-LLM outperforms current research in terms of transfer accuracy and content preservation, and has remarkable applicability to various types of LLMs. ",
        "title": "CAT-LLM: Prompting Large Language Models with Text Style Definition for  Chinese Article-style Transfer",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05708",
        "abstract_url": "http://arxiv.org/abs/2401.05708",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Che-Kai"
            },
            {
                "last_name": "Li",
                "first_name": "Chao"
            },
            {
                "last_name": "Mao",
                "first_name": "Ruibin"
            },
            {
                "last_name": "Yang",
                "first_name": "Jianyi"
            },
            {
                "last_name": "K\u00e4mpfe",
                "first_name": "Thomas"
            },
            {
                "last_name": "Imani",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Li",
                "first_name": "Can"
            },
            {
                "last_name": "Zhuo",
                "first_name": "Cheng"
            },
            {
                "last_name": "Yin",
                "first_name": "Xunzhao"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Rapid advancements in artificial intelligence have given rise to transformative models, profoundly impacting our lives. These models demand massive volumes of data to operate effectively, exacerbating the data-transfer bottleneck inherent in the conventional von-Neumann architecture. Compute-in-memory (CIM), a novel computing paradigm, tackles these issues by seamlessly embedding in-memory search functions, thereby obviating the need for data transfers. However, existing non-volatile memory (NVM)-based accelerators are application specific. During the similarity based associative search operation, they only support a single, specific distance metric, such as Hamming, Manhattan, or Euclidean distance in measuring the query against the stored data, calling for reconfigurable in-memory solutions adaptable to various applications. To overcome such a limitation, in this paper, we present FeReX, a reconfigurable associative memory (AM) that accommodates various distance metrics including Hamming, Manhattan, and Euclidean distances. Leveraging multi-bit ferroelectric field-effect transistors (FeFETs) as the proxy and a hardware-software co-design approach, we introduce a constrained satisfaction problem (CSP)-based method to automate AM search input voltage and stored voltage configurations for different distance based search functions. Device-circuit co-simulations first validate the effectiveness of the proposed FeReX methodology for reconfigurable search distance functions. Then, we benchmark FeReX in the context of k-nearest neighbor (KNN) and hyperdimensional computing (HDC), which highlights the robustness of FeReX and demonstrates up to 250x speedup and 10^4 energy savings compared with GPU. ",
        "title": "FeReX: A Reconfigurable Design of Multi-bit Ferroelectric  Compute-in-Memory for Nearest Neighbor Search",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05709",
        "abstract_url": "http://arxiv.org/abs/2401.05709",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Penghong"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Wenrui"
            },
            {
                "last_name": "Fan",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Debin"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Localization is one of the pivotal issues in wireless sensor network applications. In 3D localization studies, most algorithms focus on enhancing the location prediction process, lacking theoretical derivation of the detection distance of an anchor node at the varying hops, engenders a localization performance bottleneck. To address this issue, we propose a probability-based average distance estimation (PADE) model that utilizes the probability distribution of node distances detected by an anchor node. The aim is to mathematically derive the average distances of nodes detected by an anchor node at different hops. First, we develop a probability-based maximum distance estimation (PMDE) model to calculate the upper bound of the distance detected by an anchor node. Then, we present the PADE model, which relies on the upper bound obtained of the distance by the PMDE model. Finally, the obtained average distance is used to construct a distance loss function, and it is embedded with the traditional distance loss function into a multi-objective genetic algorithm to predict the locations of unknown nodes. The experimental results demonstrate that the proposed method achieves state-of-the-art performance in random and multimodal distributed sensor networks. The average localization accuracy is improved by 3.49\\%-12.66\\% and 3.99%-22.34%, respectively. ",
        "title": "Probability-based Distance Estimation Model for 3D DV-Hop Localization  in WSNs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05710",
        "abstract_url": "http://arxiv.org/abs/2401.05710",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zhihui"
            },
            {
                "last_name": "Perrault",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed especially for that setting. ",
        "title": "The Distributional Reward Critic Architecture for Perturbed-Reward  Reinforcement Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05711",
        "abstract_url": "http://arxiv.org/abs/2401.05711",
        "authors": [
            {
                "last_name": "Jiao",
                "first_name": "Jiyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Han",
                "first_name": "Chenpei"
            },
            {
                "last_name": "Huang",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yizhuo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  While fingerprinting localization is favored for its effectiveness, it is hindered by high data acquisition costs and the inaccuracy of static database-based estimates. Addressing these issues, this letter presents an innovative indoor localization method using a data-efficient meta-learning algorithm. This approach, grounded in the ``Learning to Learn'' paradigm of meta-learning, utilizes historical localization tasks to improve adaptability and learning efficiency in dynamic indoor environments. We introduce a task-weighted loss to enhance knowledge transfer within this framework. Our comprehensive experiments confirm the method's robustness and superiority over current benchmarks, achieving a notable 23.13\\% average gain in Mean Euclidean Distance, particularly effective in scenarios with limited CSI data. ",
        "title": "Dynamic Indoor Fingerprinting Localization based on Few-Shot  Meta-Learning with CSI Images",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05715",
        "abstract_url": "http://arxiv.org/abs/2401.05715",
        "authors": [
            {
                "last_name": "Jackiewicz",
                "first_name": "Marcel"
            },
            {
                "last_name": "Kasperski",
                "first_name": "Adam"
            },
            {
                "last_name": "Zielinski",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In this paper, the recoverable robust shortest path problem under interval uncertainty representations is discussed. This problem is known to be strongly NP-hard and also hard to approximate in general digraphs. In this paper, the class of acyclic digraphs is considered. It is shown that for the traditional interval uncertainty, the problem can be solved in polynomial time for all natural, known from the literature, neighborhoods. Efficient algorithms for various classes of acyclic digraphs are constructed. Some negative results for general digraphs are strengthened. Finally, some exact and approximate methods of solving the problem under budgeted interval uncertainty are proposed. ",
        "title": "Recoverable robust shortest path problem under interval uncertainty  representations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05716",
        "abstract_url": "http://arxiv.org/abs/2401.05716",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Xu"
            },
            {
                "last_name": "Scarlett",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we study the problem of estimating the normalizing constant $\\int e^{-\\lambda f(x)}dx$ through queries to the black-box function $f$, where $f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\\lambda$ is a problem parameter. We show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of $\\lambda$: When $\\lambda$ approaches zero, the problem is similar to Bayesian quadrature (BQ), while when $\\lambda$ approaches infinity, the problem is similar to Bayesian optimization (BO). More generally, the problem varies between BQ and BO. We find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. Our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions. ",
        "title": "Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature  and Bayesian Optimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05722",
        "abstract_url": "http://arxiv.org/abs/2401.05722",
        "authors": [
            {
                "last_name": "Court\u00e8s",
                "first_name": "Cl\u00e9mentine"
            },
            {
                "last_name": "Boileau",
                "first_name": "Matthieu"
            },
            {
                "last_name": "C\u00f4te",
                "first_name": "Rapha\u00ebl"
            },
            {
                "last_name": "Hervieux",
                "first_name": "Paul-Antoine"
            },
            {
                "last_name": "Manfredi",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We solve the Landau-Lifshitz-Gilbert equation in the finite-temperature regime, where thermal fluctuations are modeled by a random magnetic field whose variance is proportional to the temperature. By rescaling the temperature proportionally to the computational cell size $\\Delta x$ ($T \\to T\\,\\Delta x/a_{\\text{eff}}$, where $a_{\\text{eff}}$ is the lattice constant) [M. B. Hahn, J. Phys. Comm., 3:075009, 2019], we obtain Curie temperatures $T_{\\text{C}}$ that are in line with the experimental values for cobalt, iron and nickel. For finite-sized objects such as nanowires (1D) and nanolayers (2D), the Curie temperature varies with the smallest size $d$ of the system. We show that the difference between the computed finite-size $T_{\\text{C}}$ and the bulk $T_{\\text{C}}$ follows a power-law of the type: $(\\xi_0/d)^\\lambda$, where $\\xi_0$ is the correlation length at zero temperature, and $\\lambda$ is a critical exponent. We obtain values of $\\xi_0$ in the nanometer range, also in accordance with other simulations and experiments. The computed critical exponent is close to $\\lambda=2$ for all considered materials and geometries. This is the expected result for a mean-field approach, but slightly larger than the values observed experimentally. ",
        "title": "Micromagnetic simulations of the size dependence of the Curie  temperature in ferromagnetic nanowires and nanolayers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05725",
        "abstract_url": "http://arxiv.org/abs/2401.05725",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Han"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Mu",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weile"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Wong",
                "first_name": "Kai-Kit"
            },
            {
                "last_name": "Yang",
                "first_name": "Kun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) enhanced unnamed aerial vehicle (UAV)-enabled multi-user mobile edge computing (MEC) scheme is proposed in this paper. Different from the existing MEC works, the proposed scheme allows bi-directional offloading where users can simultaneously offload their computing tasks to the MEC servers situated at the ground base station (BS) and aerial UAV with the assistance of the STARRIS. Specifically, we formulate an optimization problem aiming at maximizing the energy efficiency of the system while ensuring the quality of service (QoS) constraint by jointly optimizing the resource allocation, user scheduling, passive beamforming of the STAR-RIS, and the UAV trajectory. An iterative algorithm designed with the Dinkelbach's algorithm and the successive convex approximation (SCA) is proposed to effectively handle the formulated non-convex optimization problem. Simulation results indicate that the proposed STAR-RIS enhanced UAV-enabled MEC scheme possesses significant advantages in enhancing the system energy efficiency over other baseline schemes including the conventional RIS-aided scheme. ",
        "title": "STAR-RIS Enhanced UAV-Enabled MEC Networks with Bi-Directional Task  Offloading",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05727",
        "abstract_url": "http://arxiv.org/abs/2401.05727",
        "authors": [
            {
                "last_name": "Chopra",
                "first_name": "Sahil"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Part of speech tagging in zero-resource settings can be an effective approach for low-resource languages when no labeled training data is available. Existing systems use two main techniques for POS tagging i.e. pretrained multilingual large language models(LLM) or project the source language labels into the zero resource target language and train a sequence labeling model on it. We explore the latter approach using the off-the-shelf alignment module and train a hidden Markov model(HMM) to predict the POS tags. We evaluate transfer learning setup with English as a source language and French, German, and Spanish as target languages for part-of-speech tagging. Our conclusion is that projected alignment data in zero-resource language can be beneficial to predict POS tags. ",
        "title": "Zero Resource Cross-Lingual Part Of Speech Tagging",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05730",
        "abstract_url": "http://arxiv.org/abs/2401.05730",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Jaeill"
            },
            {
                "last_name": "Hwang",
                "first_name": "Duhun"
            },
            {
                "last_name": "Lee",
                "first_name": "Eunjung"
            },
            {
                "last_name": "Suh",
                "first_name": "Jangwon"
            },
            {
                "last_name": "Kim",
                "first_name": "Jimyeong"
            },
            {
                "last_name": "Rhee",
                "first_name": "Wonjong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the past few years, contrastive learning has played a central role for the success of visual unsupervised representation learning. Around the same time, high-performance non-contrastive learning methods have been developed as well. While most of the works utilize only two views, we carefully review the existing multi-view methods and propose a general multi-view strategy that can improve learning speed and performance of any contrastive or non-contrastive method. We first analyze CMC's full-graph paradigm and empirically show that the learning speed of $K$-views can be increased by $_{K}\\mathrm{C}_{2}$ times for small learning rate and early training. Then, we upgrade CMC's full-graph by mixing views created by a crop-only augmentation, adopting small-size views as in SwAV multi-crop, and modifying the negative sampling. The resulting multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing). We investigate the effectiveness of ECPP by applying it to SimCLR and assessing the linear evaluation performance for CIFAR-10 and ImageNet-100. For each benchmark, we achieve a state-of-the-art performance. In case of ImageNet-100, ECPP boosted SimCLR outperforms supervised learning. ",
        "title": "Enhancing Contrastive Learning with Efficient Combinatorial Positive  Pairing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05731",
        "abstract_url": "http://arxiv.org/abs/2401.05731",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Xiaohui"
            },
            {
                "last_name": "Li",
                "first_name": "Wenxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhongzhi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In order to investigate the relationship between Shannon information measure of random variables, scholars such as Yeung utilized information diagrams to explore the structured representation of information measures, establishing correspondences with sets. However, this method has limitations when studying information measures of five or more random variables. In this paper, we consider employing algebraic methods to study the relationship of information measures of random variables. By introducing a semiring generated by random variables, we establish correspondences between sets and elements of the semiring. Utilizing the Grobner-Shirshov basis, we present the structure of the semiring and its standard form. Furthermore, we delve into the structure of the semiring generated under Markov chain conditions (referred to as Markov semiring), obtaining its standard form. ",
        "title": "On Grobner-Shirshov bases for Markov semirings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05735",
        "abstract_url": "http://arxiv.org/abs/2401.05735",
        "authors": [
            {
                "last_name": "Kahatapitiya",
                "first_name": "Kumara"
            },
            {
                "last_name": "Karjauv",
                "first_name": "Adil"
            },
            {
                "last_name": "Abati",
                "first_name": "Davide"
            },
            {
                "last_name": "Porikli",
                "first_name": "Fatih"
            },
            {
                "last_name": "Asano",
                "first_name": "Yuki M."
            },
            {
                "last_name": "Habibian",
                "first_name": "Amirhossein"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, which reduces cost of cross-frame attention by fusing redundant tokens in unimportant background regions. Both techniques are readily applicable to a given video editing model \\textit{without} retraining, and can drastically reduce its memory and computational cost. We evaluate our proposals on inversion-based and control-signal-based editing pipelines, and show a latency reduction up to 10x for a comparable synthesis quality. ",
        "title": "Object-Centric Diffusion for Efficient Video Editing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05736",
        "abstract_url": "http://arxiv.org/abs/2401.05736",
        "authors": [
            {
                "last_name": "Lerner",
                "first_name": "Paul"
            },
            {
                "last_name": "Ferret",
                "first_name": "Olivier"
            },
            {
                "last_name": "Guinaudeau",
                "first_name": "Camille"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Knowledge-based Visual Question Answering about Named Entities is a challenging task that requires retrieving information from a multimodal Knowledge Base. Named entities have diverse visual representations and are therefore difficult to recognize. We argue that cross-modal retrieval may help bridge the semantic gap between an entity and its depictions, and is foremost complementary with mono-modal retrieval. We provide empirical evidence through experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE, InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different strategies to fine-tune such a model: mono-modal, cross-modal, or joint training. Our method, which combines mono-and cross-modal retrieval, is competitive with billion-parameter models on the three datasets, while being conceptually simpler and computationally cheaper. ",
        "title": "Cross-modal Retrieval for Knowledge-based Visual Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05737",
        "abstract_url": "http://arxiv.org/abs/2401.05737",
        "authors": [
            {
                "last_name": "Manjavacas",
                "first_name": "Antonio"
            },
            {
                "last_name": "Campoy-Nieves",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Jim\u00e9nez-Raboso",
                "first_name": "Javier"
            },
            {
                "last_name": "Molina-Solana",
                "first_name": "Miguel"
            },
            {
                "last_name": "G\u00f3mez-Romero",
                "first_name": "Juan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers' robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning. ",
        "title": "An experimental evaluation of Deep Reinforcement Learning algorithms for  HVAC control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05738",
        "abstract_url": "http://arxiv.org/abs/2401.05738",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Boheng"
            },
            {
                "last_name": "Lu",
                "first_name": "Yi"
            },
            {
                "last_name": "Shi",
                "first_name": "Pengbo"
            },
            {
                "last_name": "Chen",
                "first_name": "Qingzi"
            },
            {
                "last_name": "Liu",
                "first_name": "Jirui"
            },
            {
                "last_name": "Zhu",
                "first_name": "Lingyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We revisit the relationship between attention mechanisms and large kernel ConvNets in visual transformers and propose a new spatial attention named Large Kernel Convolutional Attention (LKCA). It simplifies the attention operation by replacing it with a single large kernel convolution. LKCA combines the advantages of convolutional neural networks and visual transformers, possessing a large receptive field, locality, and parameter sharing. We explained the superiority of LKCA from both convolution and attention perspectives, providing equivalent code implementations for each view. Experiments confirm that LKCA implemented from both the convolutional and attention perspectives exhibit equivalent performance. We extensively experimented with the LKCA variant of ViT in both classification and segmentation tasks. The experiments demonstrated that LKCA exhibits competitive performance in visual tasks. Our code will be made publicly available at https://github.com/CatworldLee/LKCA. ",
        "title": "LKCA: Large Kernel Convolutional Attention",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05739",
        "abstract_url": "http://arxiv.org/abs/2401.05739",
        "authors": [
            {
                "last_name": "Jia",
                "first_name": "Ang"
            },
            {
                "last_name": "Fan",
                "first_name": "Ming"
            },
            {
                "last_name": "Xu",
                "first_name": "Xi"
            },
            {
                "last_name": "Jin",
                "first_name": "Wuxia"
            },
            {
                "last_name": "Wang",
                "first_name": "Haijun"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Binary function similarity detection plays an important role in a wide range of security applications. Existing works usually assume that the query function and target function share equal semantics and compare their full semantics to obtain the similarity. However, we find that the function mapping is more complex, especially when function inlining happens.   In this paper, we will systematically investigate cross-inlining binary function similarity detection. We first construct a cross-inlining dataset by compiling 51 projects using 9 compilers, with 4 optimizations, to 6 architectures, with 2 inlining flags, which results in two datasets both with 216 combinations. Then we construct the cross-inlining function mappings by linking the common source functions in these two datasets. Through analysis of this dataset, we find that three cross-inlining patterns widely exist while existing work suffers when detecting cross-inlining binary function similarity. Next, we propose a pattern-based model named CI-Detector for cross-inlining matching. CI-Detector uses the attributed CFG to represent the semantics of binary functions and GNN to embed binary functions into vectors. CI-Detector respectively trains a model for these three cross-inlining patterns. Finally, the testing pairs are input to these three models and all the produced similarities are aggregated to produce the final similarity. We conduct several experiments to evaluate CI-Detector. Results show that CI-Detector can detect cross-inlining pairs with a precision of 81% and a recall of 97%, which exceeds all state-of-the-art works. ",
        "title": "Cross-Inlining Binary Function Similarity Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05740",
        "abstract_url": "http://arxiv.org/abs/2401.05740",
        "authors": [
            {
                "last_name": "Berger",
                "first_name": "Andre"
            },
            {
                "last_name": "Rouhani",
                "first_name": "Arman"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Marc"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  In this paper, we introduce an improved upper bound for the efficiency of Nash equilibria in utilitarian scheduling games on related machines. The machines have varying speeds and adhere to the Shortest Processing Time (SPT) policy as the global order for job processing. The goal of each job is to minimize its completion time, while the social objective is to minimize the sum of completion times. Our main result provides an upper bound of $2-\\frac{1}{2\\cdot(2m-1)}$ on the price of anarchy for the general case of $m$ machines. We improve this bound to 3/2 for the case of two machines, and to $2-\\frac{1}{2\\cdot m}$ for the general case of $m$ machines when the machines have divisible speeds. ",
        "title": "An improved bound for the price of anarchy for related machine  scheduling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05743",
        "abstract_url": "http://arxiv.org/abs/2401.05743",
        "authors": [
            {
                "last_name": "Marconi",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Rosati",
                "first_name": "Riccardo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We study consistent query answering over knowledge bases expressed by existential rules. Specifically, we establish the data complexity of consistent query answering and repair checking under tuple-deletion semantics for a general class of disjunctive existential rules and for several subclasses thereof (acyclic, linear, full, guarded, and sticky). In particular, we identify several cases in which the above problems are tractable or even first-order rewritable, and present new query rewriting techniques that can be the basis for practical inconsistency-tolerant query answering systems. ",
        "title": "Consistent Query Answering for Existential Rules under Tuple-Deletion  Semantics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05744",
        "abstract_url": "http://arxiv.org/abs/2401.05744",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yicong"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiangguo"
            },
            {
                "last_name": "Chen",
                "first_name": "Hongxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Sixiao"
            },
            {
                "last_name": "Yang",
                "first_name": "Yu"
            },
            {
                "last_name": "Xu",
                "first_name": "Guandong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Compared with only pursuing recommendation accuracy, the explainability of a recommendation model has drawn more attention in recent years. Many graph-based recommendations resort to informative paths with the attention mechanism for the explanation. Unfortunately, these attention weights are intentionally designed for model accuracy but not explainability. Recently, some researchers have started to question attention-based explainability because the attention weights are unstable for different reproductions, and they may not always align with human intuition. Inspired by the counterfactual reasoning from causality learning theory, we propose a novel explainable framework targeting path-based recommendations, wherein the explainable weights of paths are learned to replace attention weights. Specifically, we design two counterfactual reasoning algorithms from both path representation and path topological structure perspectives. Moreover, unlike traditional case studies, we also propose a package of explainability evaluation solutions with both qualitative and quantitative methods. We conduct extensive experiments on three real-world datasets, the results of which further demonstrate the effectiveness and reliability of our method. ",
        "title": "Attention Is Not the Only Choice: Counterfactual Reasoning for  Path-Based Explainable Recommendation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05745",
        "abstract_url": "http://arxiv.org/abs/2401.05745",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Barry Shichen"
            },
            {
                "last_name": "Liang",
                "first_name": "Siyun"
            },
            {
                "last_name": "Paetzold",
                "first_name": "Johannes"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose the use of a Transformer to accurately predict normals from point clouds with noise and density variations. Previous learning-based methods utilize PointNet variants to explicitly extract multi-scale features at different input scales, then focus on a surface fitting method by which local point cloud neighborhoods are fitted to a geometric surface approximated by either a polynomial function or a multi-layer perceptron (MLP). However, fitting surfaces to fixed-order polynomial functions can suffer from overfitting or underfitting, and learning MLP-represented hyper-surfaces requires pre-generated per-point weights. To avoid these limitations, we first unify the design choices in previous works and then propose a simplified Transformer-based model to extract richer and more robust geometric features for the surface normal estimation task. Through extensive experiments, we demonstrate that our Transformer-based method achieves state-of-the-art performance on both the synthetic shape dataset PCPNet, and the real-world indoor scene dataset SceneNN, exhibiting more noise-resilient behavior and significantly faster inference. Most importantly, we demonstrate that the sophisticated hand-designed modules in existing works are not necessary to excel at the task of surface normal estimation. ",
        "title": "Surface Normal Estimation with Transformers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05746",
        "abstract_url": "http://arxiv.org/abs/2401.05746",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Heqing"
            },
            {
                "last_name": "Shen",
                "first_name": "Meng"
            },
            {
                "last_name": "Hu",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Chng",
                "first_name": "Eng Siong"
            },
            {
                "last_name": "Rajan",
                "first_name": "Deepu"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Audio-visual deepfake detection scrutinizes manipulations in public video using complementary multimodal cues. Current methods, which train on fused multimodal data for multimodal targets face challenges due to uncertainties and inconsistencies in learned representations caused by independent modality manipulations in deepfake videos. To address this, we propose cross-modality and within-modality regularization to preserve modality distinctions during multimodal representation learning. Our approach includes an audio-visual transformer module for modality correspondence and a cross-modality regularization module to align paired audio-visual signals, preserving modality distinctions. Simultaneously, a within-modality regularization module refines unimodal representations with modality-specific targets to retain modal-specific details. Experimental results on the public audio-visual dataset, FakeAVCeleb, demonstrate the effectiveness and competitiveness of our approach. ",
        "title": "Cross-Modality and Within-Modality Regularization for Audio-Visual  DeepFake Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05749",
        "abstract_url": "http://arxiv.org/abs/2401.05749",
        "authors": [
            {
                "last_name": "Thompson",
                "first_name": "Brian"
            },
            {
                "last_name": "Dhaliwal",
                "first_name": "Mehak Preet"
            },
            {
                "last_name": "Frisch",
                "first_name": "Peter"
            },
            {
                "last_name": "Domhan",
                "first_name": "Tobias"
            },
            {
                "last_name": "Federico",
                "first_name": "Marcello"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web. ",
        "title": "A Shocking Amount of the Web is Machine Translated: Insights from  Multi-Way Parallelism",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05750",
        "abstract_url": "http://arxiv.org/abs/2401.05750",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Peng"
            },
            {
                "last_name": "Tan",
                "first_name": "Feitong"
            },
            {
                "last_name": "Yu",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yinda"
            },
            {
                "last_name": "Qi",
                "first_name": "Xiaojuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite advances in 3D generation, the direct creation of 3D objects within an existing 3D scene represented as NeRF remains underexplored. This process requires not only high-quality 3D object generation but also seamless composition of the generated 3D content into the existing NeRF. To this end, we propose a new method, GO-NeRF, capable of utilizing scene context for high-quality and harmonious 3D object generation within an existing NeRF. Our method employs a compositional rendering formulation that allows the generated 3D objects to be seamlessly composited into the scene utilizing learned 3D-aware opacity maps without introducing unintended scene modification. Moreover, we also develop tailored optimization objectives and training strategies to enhance the model's ability to exploit scene context and mitigate artifacts, such as floaters, originating from 3D object generation within a scene. Extensive experiments on both feed-forward and $360^o$ scenes show the superior performance of our proposed GO-NeRF in generating objects harmoniously composited with surrounding scenes and synthesizing high-quality novel view images. Project page at {\\url{https://daipengwa.github.io/GO-NeRF/}. ",
        "title": "GO-NeRF: Generating Virtual Objects in Neural Radiance Fields",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05752",
        "abstract_url": "http://arxiv.org/abs/2401.05752",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Na"
            },
            {
                "last_name": "Qi",
                "first_name": "Lei"
            },
            {
                "last_name": "Guo",
                "first_name": "Jintao"
            },
            {
                "last_name": "Shi",
                "first_name": "Yinghuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Domain generalization (DG) intends to train a model on multiple source domains to ensure that it can generalize well to an arbitrary unseen target domain. The acquisition of domain-invariant representations is pivotal for DG as they possess the ability to capture the inherent semantic information of the data, mitigate the influence of domain shift, and enhance the generalization capability of the model. Adopting multiple perspectives, such as the sample and the feature, proves to be effective. The sample perspective facilitates data augmentation through data manipulation techniques, whereas the feature perspective enables the extraction of meaningful generalization features. In this paper, we focus on improving the generalization ability of the model by compelling it to acquire domain-invariant representations from both the sample and feature perspectives by disentangling spurious correlations and enhancing potential correlations. 1) From the sample perspective, we develop a frequency restriction module, guiding the model to focus on the relevant correlations between object features and labels, thereby disentangling spurious correlations. 2) From the feature perspective, the simple Tail Interaction module implicitly enhances potential correlations among all samples from all source domains, facilitating the acquisition of domain-invariant representations across multiple domains for the model. The experimental results show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs) with a strong baseline embedded with these two modules can achieve superior results, e.g., an average accuracy of 92.30% on Digits-DG. ",
        "title": "Learning Generalizable Models via Disentangling Spurious and Enhancing  Potential Correlations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05753",
        "abstract_url": "http://arxiv.org/abs/2401.05753",
        "authors": [
            {
                "last_name": "Ko",
                "first_name": "Yousun"
            },
            {
                "last_name": "Burgstaller",
                "first_name": "Bernd"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Soft errors are a type of transient digital signal corruption that occurs in digital hardware components such as the internal flip-flops of CPU pipelines, the register file, memory cells, and even internal communication buses. Soft errors are caused by environmental radioactivity, magnetic interference, lasers, and temperature fluctuations, either unintentionally, or as part of a deliberate attempt to compromise a system and expose confidential data.   We propose a bit-level error coalescing (BEC) static program analysis and its two use cases to understand and improve program reliability against soft errors. The BEC analysis tracks each bit corruption in the register file and classifies the effect of the corruption by its semantics at compile time. The usefulness of the proposed analysis is demonstrated in two scenarios, fault injection campaign pruning, and reliability-aware program transformation. Experimental results show that bit-level analysis pruned up to 30.04 % of exhaustive fault injection campaigns (13.71 % on average), without loss of accuracy. Program vulnerability was reduced by up to 13.11 % (4.94 % on average) through bit-level vulnerability-aware instruction scheduling. The analysis has been implemented within LLVM and evaluated on the RISC-V architecture.   To the best of our knowledge, the proposed BEC analysis is the first bit-level compiler analysis for program reliability against soft errors. The proposed method is generic and not limited to a specific computer architecture. ",
        "title": "BEC: Bit-Level Static Analysis for Reliability against Soft Errors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05757",
        "abstract_url": "http://arxiv.org/abs/2401.05757",
        "authors": [
            {
                "last_name": "Aramaki",
                "first_name": "Mitsuko"
            },
            {
                "last_name": "Bernard",
                "first_name": "Corentin"
            },
            {
                "last_name": "Kronland-Martinet",
                "first_name": "Richard"
            },
            {
                "last_name": "Poirot",
                "first_name": "Samuel"
            },
            {
                "last_name": "Ystad",
                "first_name": "S\u00f8lvi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Intuitive control of synthesis processes is an ongoing challenge within the domain of auditory perception and cognition. Previous works on sound modelling combined with psychophysical tests have enabled our team to develop a synthesizer that provides intuitive control of actions and objects based on semantic descriptions for sound sources. In this demo we present an augmented version of the synthesizer in which we added tactile stimulations to increase the sensation of true continuous friction interactions (rubbing and scratching) with the simulated objects. This is of interest for several reasons. Firstly, it enables to evaluate the realism of our sound model in presence of stimulations from other modalities. Secondly it enables to compare tactile and auditory signal structures linked to the same evocation, and thirdly it provides a tool to investigate multimodal perception and how stimulations from different modalities should be combined to provide realistic user interfaces. ",
        "title": "Intuitive Control of Scraping and Rubbing Through Audio-tactile  Synthesis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05759",
        "abstract_url": "http://arxiv.org/abs/2401.05759",
        "authors": [
            {
                "last_name": "Vaccon",
                "first_name": "Tristan"
            },
            {
                "last_name": "Verron",
                "first_name": "Thibaut"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC"
        ],
        "abstract": "  A universal analytic Gr{\\\"o}bner basis (UAGB) of an ideal of a Tate algebra is a set containing a local Gr{\\\"o}bner basis for all suitable convergence radii. In a previous article, the authors proved the existence of finite UAGB's for polynomial ideals, leaving open the question of how to compute them. In this paper, we provide an algorithm computing a UAGB for a given polynomial ideal, by traversing the Gr{\\\"o}bner fan of the ideal. As an application, it offers a new point of view on algorithms for computing tropical varieties of homogeneous polynomial ideals, which typically rely on lifting the computations to an algebra of power series. Motivated by effective computations in tropical analytic geometry, we also examine local bases for more general convergence conditions, constraining the radii to a convex polyhedron. In this setting, we provide an algorithm to compute local Gr{\\\"o}bner bases and discuss obstacles towards proving the existence of finite UAGBs. CCS CONCEPTS $\\bullet$ Computing methodologies $\\rightarrow$ Algebraic algorithms. ",
        "title": "Universal Analytic Gr{\\\"o}bner Bases and Tropical Geometry",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05761",
        "abstract_url": "http://arxiv.org/abs/2401.05761",
        "authors": [
            {
                "last_name": "Caramancion",
                "first_name": "Kevin Matthe"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  This study embarked on a comprehensive exploration of user preferences between Search Engines and Large Language Models (LLMs) in the context of various information retrieval scenarios. Conducted with a sample size of 100 internet users (N=100) from across the United States, the research delved into 20 distinct use cases ranging from factual searches, such as looking up COVID-19 guidelines, to more subjective tasks, like seeking interpretations of complex concepts in layman's terms. Participants were asked to state their preference between using a traditional search engine or an LLM for each scenario. This approach allowed for a nuanced understanding of how users perceive and utilize these two predominant digital tools in differing contexts. The use cases were carefully selected to cover a broad spectrum of typical online queries, thus ensuring a comprehensive analysis of user preferences. The findings reveal intriguing patterns in user choices, highlighting a clear tendency for participants to favor search engines for direct, fact-based queries, while LLMs were more often preferred for tasks requiring nuanced understanding and language processing. These results offer valuable insights into the current state of digital information retrieval and pave the way for future innovations in this field. This study not only sheds light on the specific contexts in which each tool is favored but also hints at the potential for developing hybrid models that leverage the strengths of both search engines and LLMs. The insights gained from this research are pivotal for developers, researchers, and policymakers in understanding the evolving landscape of digital information retrieval and user interaction with these technologies. ",
        "title": "Large Language Models vs. Search Engines: Evaluating User Preferences  Across Varied Information Retrieval Scenarios",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05765",
        "abstract_url": "http://arxiv.org/abs/2401.05765",
        "authors": [
            {
                "last_name": "Boschi",
                "first_name": "Tobia"
            },
            {
                "last_name": "Bonin",
                "first_name": "Francesca"
            },
            {
                "last_name": "Epperlein",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Ordonez-Hurtado",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Pascale",
                "first_name": "Alessandra"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Functional data analysis has emerged as a crucial tool in many contemporary scientific domains that require the integration and interpretation of complex data. Moreover, the advent of new technologies has facilitated the collection of a large number of longitudinal variables, making feature selection pivotal for avoiding overfitting and improving prediction performance. This paper introduces a novel methodology called FSFC (Feature Selection for Functional Classification), that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and longitudinal features. Our approach tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial features for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm that leverages the sparsity structure of the problem for dimensionality reduction. The computational efficiency of FSFC enables handling high-dimensional scenarios where the number of features may considerably exceed the number of statistical units. Simulation experiments demonstrate that FSFC outperforms other machine learning and deep learning methods in computational time and classification accuracy. Furthermore, the FSFC feature selection capability can be leveraged to significantly reduce the problem's dimensionality and enhance the performances of other classification algorithms. The efficacy of FSFC is also demonstrated through a real data application, analyzing relationships between four chronic diseases and other health and socio-demographic factors. ",
        "title": "Feature Selection for Functional Data Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05767",
        "abstract_url": "http://arxiv.org/abs/2401.05767",
        "authors": [
            {
                "last_name": "Tran",
                "first_name": "Ly-Duyen"
            },
            {
                "last_name": "Gurrin",
                "first_name": "Cathal"
            },
            {
                "last_name": "Smeaton",
                "first_name": "Alan F."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "HC"
        ],
        "abstract": "  Personal data includes the digital footprints that we leave behind as part of our everyday activities, both online and offline in the real world. It includes data we collect ourselves, such as from wearables, as well as the data collected by others about our online behaviour and activities. Sometimes we are able to use the personal data we ourselves collect, in order to examine some parts of our lives but for the most part, our personal data is leveraged by third parties including internet companies, for services like targeted advertising and recommendations. Lifelogging is a form of extreme personal data gathering and in this article we present an overview of the tools used to manage access to lifelogs as demonstrated at the most recent of the annual Lifelog Search Challenge benchmarking workshops. Here, experimental systems are showcased in live, real time information seeking tasks by real users. This overview of these systems' capabilities show the range of possibilities for accessing our own personal data which may, in time, become more easily available as consumer-level services. ",
        "title": "Lifelogging As An Extreme Form of Personal Information Management --  What Lessons To Learn",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05768",
        "abstract_url": "http://arxiv.org/abs/2401.05768",
        "authors": [
            {
                "last_name": "Gheorghiu",
                "first_name": "Adrian"
            },
            {
                "last_name": "T\u0103iatu",
                "first_name": "Iulian-Marius"
            },
            {
                "last_name": "Cercel",
                "first_name": "Dumitru-Clementin"
            },
            {
                "last_name": "Marin",
                "first_name": "Iuliana"
            },
            {
                "last_name": "Pop",
                "first_name": "Florin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The detection and classification of diseases in Robusta coffee leaves are essential to ensure that plants are healthy and the crop yield is kept high. However, this job requires extensive botanical knowledge and much wasted time. Therefore, this task and others similar to it have been extensively researched subjects in image classification. Regarding leaf disease classification, most approaches have used the more popular PlantVillage dataset while completely disregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. As the RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of pre-trained models and multiple augmentation techniques need to be used. The current paper uses the RoCoLe dataset and approaches based on deep learning for classifying coffee leaf diseases from images, incorporating the pix2pix model for segmentation and cycle-generative adversarial network (CycleGAN) for augmentation. Our study demonstrates the effectiveness of Transformer-based models, online augmentations, and CycleGAN augmentation in improving leaf disease classification. While synthetic data has limitations, it complements real data, enhancing model performance. These findings contribute to developing robust techniques for plant disease detection and classification. ",
        "title": "Evaluating Data Augmentation Techniques for Coffee Leaf Disease  Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05771",
        "abstract_url": "http://arxiv.org/abs/2401.05771",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhiying"
            },
            {
                "last_name": "Guo",
                "first_name": "Yongxin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate lesion classification in Wireless Capsule Endoscopy (WCE) images is vital for early diagnosis and treatment of gastrointestinal (GI) cancers. However, this task is confronted with challenges like tiny lesions and background interference. Additionally, WCE images exhibit higher intra-class variance and inter-class similarities, adding complexity. To tackle these challenges, we propose Decoupled Supervised Contrastive Learning for WCE image classification, learning robust representations from zoomed-in WCE images generated by Saliency Augmentor. Specifically, We use uniformly down-sampled WCE images as anchors and WCE images from the same class, especially their zoomed-in images, as positives. This approach empowers the Feature Extractor to capture rich representations from various views of the same image, facilitated by Decoupled Supervised Contrastive Learning. Training a linear Classifier on these representations within 10 epochs yields an impressive 92.01% overall accuracy, surpassing the prior state-of-the-art (SOTA) by 0.72% on a blend of two publicly accessible WCE datasets. Code is available at: https://github.com/Qiukunpeng/DSCL. ",
        "title": "Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image  Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05772",
        "abstract_url": "http://arxiv.org/abs/2401.05772",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Wujie"
            },
            {
                "last_name": "Chen",
                "first_name": "Defang"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Feng",
                "first_name": "Yan"
            },
            {
                "last_name": "Chen",
                "first_name": "Chun"
            },
            {
                "last_name": "Wang",
                "first_name": "Can"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategies to enhance model performance despite restricted training data, and successfully demonstrate the feasibility of KT on the MNIST dataset. Code is available at \\url{https://github.com/zju-SWJ/KT}. ",
        "title": "Knowledge Translation: A New Pathway for Model Compression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05777",
        "abstract_url": "http://arxiv.org/abs/2401.05777",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jinxin"
            },
            {
                "last_name": "Cao",
                "first_name": "Shulin"
            },
            {
                "last_name": "Shi",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tingjian"
            },
            {
                "last_name": "Hou",
                "first_name": "Lei"
            },
            {
                "last_name": "Li",
                "first_name": "Juanzi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancement in the capabilities of large language models (LLMs) has triggered a new surge in LLMs' evaluation. Most recent evaluation works tends to evaluate the comprehensive ability of LLMs over series of tasks. However, the deep structure understanding of natural language is rarely explored. In this work, we examine the ability of LLMs to deal with structured semantics on the tasks of question answering with the help of the human-constructed formal language. Specifically, we implement the inter-conversion of natural and formal language through in-context learning of LLMs to verify their ability to understand and generate the structured logical forms. Extensive experiments with models of different sizes and in different formal languages show that today's state-of-the-art LLMs' understanding of the logical forms can approach human level overall, but there still are plenty of room in generating correct logical forms, which suggest that it is more effective to use LLMs to generate more natural language training data to reinforce a small model than directly answering questions with LLMs. Moreover, our results also indicate that models exhibit considerable sensitivity to different formal languages. In general, the formal language with the lower the formalization level, i.e. the more similar it is to natural language, is more LLMs-friendly. ",
        "title": "Probing Structured Semantics Understanding and Generation of Language  Models via Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05778",
        "abstract_url": "http://arxiv.org/abs/2401.05778",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanling"
            },
            {
                "last_name": "Fu",
                "first_name": "Chuanpu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yong"
            },
            {
                "last_name": "Li",
                "first_name": "Sijia"
            },
            {
                "last_name": "Deng",
                "first_name": "Xinhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yunpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinglin"
            },
            {
                "last_name": "Qiu",
                "first_name": "Ziyi"
            },
            {
                "last_name": "Li",
                "first_name": "Peiyang"
            },
            {
                "last_name": "Tan",
                "first_name": "Zhixing"
            },
            {
                "last_name": "Xiong",
                "first_name": "Junwu"
            },
            {
                "last_name": "Kong",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Wen",
                "first_name": "Zujie"
            },
            {
                "last_name": "Xu",
                "first_name": "Ke"
            },
            {
                "last_name": "Li",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems. ",
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language  Model Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05779",
        "abstract_url": "http://arxiv.org/abs/2401.05779",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jing"
            },
            {
                "last_name": "Le",
                "first_name": "Trung"
            },
            {
                "last_name": "Hayat",
                "first_name": "Munawar"
            },
            {
                "last_name": "Harandi",
                "first_name": "Mehrtash"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In response to data protection regulations and the ``right to be forgotten'', in this work, we introduce an unlearning algorithm for diffusion models. Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization. To achieve this, we formulate the unlearning problem as a bi-level optimization problem, wherein the outer objective is to preserve the utility of the diffusion model on the remaining data. The inner objective aims to scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure. To solve the resulting bi-level problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process and solving a bi-level problem therein. Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across two widely-used diffusion models and in both conditional and unconditional image generation scenarios. In our experiments, we demonstrate the unlearning of classes, attributes, and even a race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and CIFAR10. ",
        "title": "EraseDiff: Erasing Data Influence in Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05782",
        "abstract_url": "http://arxiv.org/abs/2401.05782",
        "authors": [
            {
                "last_name": "Noom",
                "first_name": "Jacques"
            },
            {
                "last_name": "Soloviev",
                "first_name": "Oleg"
            },
            {
                "last_name": "Smith",
                "first_name": "Carlas"
            },
            {
                "last_name": "Verhaegen",
                "first_name": "Michel"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Stochastic Closed-Loop Active Fault Diagnosis (CLAFD) aims to select the input sequentially in order to improve the discrimination of different models by minimizing the predicted error probability. As computation of these error probabilities encompasses the evaluation of multidimensional probability integrals, relaxation methods are of interest. This manuscript presents a new method that allows to make an improved trade-off between three factors -- namely maximized accuracy of diagnosis, minimized number of consecutive measurements to achieve that accuracy, and minimized computational effort per time step -- with respect to the state-of-the-art. It relies on minimizing an upper bound on the error probability, which is in the case of linear models with Gaussian noise proven to be concave in the most challenging discrimination conditions. A simulation study is conducted both for open-loop and feedback controlled candidate models. The results demonstrate the favorable trade-off using the new contributions in this manuscript. ",
        "title": "Online input design for discrimination of linear models using concave  minimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05783",
        "abstract_url": "http://arxiv.org/abs/2401.05783",
        "authors": [
            {
                "last_name": "Vlachou",
                "first_name": "Maria"
            },
            {
                "last_name": "Macdonald",
                "first_name": "Craig"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In Conversational Recommendation Systems (CRS), a user can provide feedback on recommended items at each interaction turn, leading the CRS towards more desirable recommendations. Currently, different types of CRS offer various possibilities for feedback, i.e., natural language feedback, or answering clarifying questions. In most cases, a user simulator is employed for training as well as evaluating the CRS. Such user simulators typically critique the current retrieved items based on knowledge of a single target item. Still, evaluating systems in offline settings with simulators suffers from problems, such as focusing entirely on a single target item (not addressing the exploratory nature of a recommender system), and exhibiting extreme patience (consistent feedback over a large number of turns). To overcome these limitations, we obtain extra judgements for a selection of alternative items in common CRS datasets, namely Shoes and Fashion IQ Dresses. Going further, we propose improved user simulators that allow simulated users not only to express their preferences about alternative items to their original target, but also to change their mind and level of patience. In our experiments using the relative image captioning CRS setting and different CRS models, we find that using the knowledge of alternatives by the simulator can have a considerable impact on the evaluation of existing CRS models, specifically that the existing single-target evaluation underestimates their effectiveness, and when simulated users are allowed to instead consider alternatives, the system can rapidly respond to more quickly satisfy the user. ",
        "title": "What Else Would I Like? A User Simulator using Alternatives for Improved  Evaluation of Fashion Conversational Recommendation Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05787",
        "abstract_url": "http://arxiv.org/abs/2401.05787",
        "authors": [
            {
                "last_name": "Parvez",
                "first_name": "Md Rizwan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce Evidence to Generate (E2G), a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of \"evidence for decision making\" by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLMs. \\tool achieves remarkable results robustly across a wide range of knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For example, (i) on LogiQA benchmark using GPT-4 as backbone model, \\tool achieves a new state-of-the Accuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant of E2G with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, reaching an F1 score of 83.3 on a subset of DROP. ",
        "title": "Evidence to Generate (E2G): A Single-agent Two-step Prompting for  Context Grounded and Retrieval Augmented Reasoning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05790",
        "abstract_url": "http://arxiv.org/abs/2401.05790",
        "authors": [
            {
                "last_name": "Dolata",
                "first_name": "Mateusz"
            },
            {
                "last_name": "Lange",
                "first_name": "Norbert"
            },
            {
                "last_name": "Schwabe",
                "first_name": "Gerhard"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The rise of generative AI has led many companies to hire freelancers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with aspects they perceive as unique to generative AI such as unpredictability of its output, the occurrence of hallucinations, and the inconsistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token limits and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies. ",
        "title": "Development in times of hype: How freelancers explore Generative AI?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05792",
        "abstract_url": "http://arxiv.org/abs/2401.05792",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhihui"
            },
            {
                "last_name": "Zhao",
                "first_name": "Handong"
            },
            {
                "last_name": "Yu",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Shuai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the subspace is found, we can directly project the original embeddings into the null space to boost language agnosticism without finetuning. We systematically evaluate our method on various tasks including the challenging language-agnostic QA retrieval task. Empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs. ",
        "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual  Representations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05794",
        "abstract_url": "http://arxiv.org/abs/2401.05794",
        "authors": [
            {
                "last_name": "Geneson",
                "first_name": "Jesse"
            },
            {
                "last_name": "Tang",
                "first_name": "Linus"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DM"
        ],
        "abstract": "  We improve several worst-case bounds for various online learning scenarios from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an upper bound for delayed ambiguous reinforcement learning by a factor of 2, an upper bound for learning compositions of families of functions by a factor of 2.41, and an upper bound for agnostic learning by a factor of 1.09. We also improve a lower bound from the same paper for learning compositions of $k$ families of functions by a factor of $\\Theta(\\ln{k})$, matching the upper bound up to a constant factor. In addition, we solve a problem from (Long, Theoretical Computer Science, 2020) on the price of bandit feedback with respect to standard feedback for multiclass learning, and we improve an upper bound from (Feng et al., Theoretical Computer Science, 2023) on the price of $r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching a lower bound from the same paper up to the leading term. ",
        "title": "Bounds on the price of feedback for mistake-bounded online learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05797",
        "abstract_url": "http://arxiv.org/abs/2401.05797",
        "authors": [
            {
                "last_name": "Deb",
                "first_name": "Soubhik"
            },
            {
                "last_name": "Raynor",
                "first_name": "Robert"
            },
            {
                "last_name": "Kannan",
                "first_name": "Sreeram"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  As of July 15, 2023, Ethererum, which is a Proof-of-Stake (PoS) blockchain [1] has around 410 Billion USD in total assets on chain (popularly referred to as total-value-locked, TVL) but has only 33 Billion USD worth of ETH staked in securing the underlying consensus of the chain [2]. A preliminary analysis might suggest that as the amount staked is far less (11x less) than the value secured, the Ethereum blockchain is insecure and \"over-leveraged\" in a purely cryptoeconomic sense. In this work, we investigate how Ethereum, or, more generally, any PoS blockchain can be made secure despite this apparent imbalance. Towards that end, we attempt to formalize a model for analyzing the cryptoeconomic safety of PoS blockchain, which separately analyzes the cost-of-corruption, the cost incurred by an attacker, and the profit-from-corruption, the profit gained by an attacker. We derive sharper bounds on profit-from-corruption, as well as new confirmation rules that significantly decrease this upper-bound. We evaluate cost-of-corruption and profit-from-corruption only from the perspective of attacking safety. Finally, we present a new \"insurance\" mechanism, STAKESURE, for allocating the slashed funds in a PoS system, that has several highly desirable properties: solving common information problem in existing blockchains, creating a mechanism for provably safe bridging, and providing the first sharp solution for automatically adjusting how much economic security is sufficient in a PoS system. Finally, we show that the system satisfies a notion of strong cryptoeconomic safety, which guarantees that no honest transactor ever loses money, and creates a closed system of Karma, which not only ensures that the attacker suffers a loss of funds but also that the harmed parties are sufficiently compensated. ",
        "title": "STAKESURE: Proof of Stake Mechanisms with Strong Cryptoeconomic Safety",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05799",
        "abstract_url": "http://arxiv.org/abs/2401.05799",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Frank"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "MA"
        ],
        "abstract": "  Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed. ",
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05800",
        "abstract_url": "http://arxiv.org/abs/2401.05800",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Yu"
            },
            {
                "last_name": "Koh",
                "first_name": "Huan Yee"
            },
            {
                "last_name": "Jin",
                "first_name": "Ming"
            },
            {
                "last_name": "Chi",
                "first_name": "Lianhua"
            },
            {
                "last_name": "Wang",
                "first_name": "Haishuai"
            },
            {
                "last_name": "Phan",
                "first_name": "Khoa T."
            },
            {
                "last_name": "Chen",
                "first_name": "Yi-Ping Phoebe"
            },
            {
                "last_name": "Pan",
                "first_name": "Shirui"
            },
            {
                "last_name": "Xiang",
                "first_name": "Wei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control. However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values. In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series. Our approach comprises two main components. First, we propose a graph spatiotemporal process based on neural controlled differential equations. This process enables effective modeling of multivariate time series from both spatial and temporal perspectives, even when the data contains missing values. Second, we present a novel distribution-based anomaly scoring mechanism that alleviates the reliance on complete uniform observations. By analyzing the predictions of the graph spatiotemporal process, our approach allows anomalies to be easily detected. Our experimental results show that the GST-Pro method can effectively detect anomalies in time series data and outperforms state-of-the-art methods, regardless of whether there are missing values present in the data. Our code is available: https://github.com/huankoh/GST-Pro. ",
        "title": "Graph Spatiotemporal Process for Multivariate Time Series Anomaly  Detection with Missing Values",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05802",
        "abstract_url": "http://arxiv.org/abs/2401.05802",
        "authors": [
            {
                "last_name": "Johal",
                "first_name": "Wafa"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  With advancement of robotics and artificial intelligence, applications for robotics are flourishing. Human-robot interaction (HRI) is an important area of robotics as it allows robots to work closer to humans (with them or for them). One crucial factor for the success of HRI research is transferability, which refers to the ability of research outputs to be adopted by industry and provide benefits to society. In this paper, we explore the potentials and challenges of transferability in HRI research. Firstly, we examine the current state of HRI research and identify various types of contributions that could lead to successful outcomes. Secondly, we discuss the potential benefits for each type of contribution and identify factors that could facilitate industry adoption of HRI research. However, we also recognize that there are several challenges associated with transferability, such as the diversity of well-defined job/skill-sets required from HRI practitioners, the lack of industry-led research, and the lack of standardization in HRI research methods. We discuss these challenges and propose potential solutions to bridge the gap between industry expectations and academic research in HRI. ",
        "title": "Transferability of HRI Research: Potential and Challenges",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05807",
        "abstract_url": "http://arxiv.org/abs/2401.05807",
        "authors": [
            {
                "last_name": "Cobo",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Valle",
                "first_name": "Roberto"
            },
            {
                "last_name": "Buenaposada",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Baumela",
                "first_name": "Luis"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360{\\deg} rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles' gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set. ",
        "title": "On the representation and methodology for wide and short range head pose  estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05808",
        "abstract_url": "http://arxiv.org/abs/2401.05808",
        "authors": [
            {
                "last_name": "Azarbahram",
                "first_name": "Ali"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The paper proposes an intermittent communication mechanism for the tracking consensus of high-order nonlinear multi-agent systems (MASs) surrounded by random disturbances. Each collaborating agent is described by a class of high-order nonlinear uncertain strict-feedback dynamics which is disturbed by a wide stationary process representing the external noise. The resiliency level of this networked control system (NCS) to the failures of physical devices or unreliability of communication channels is analyzed by introducing a linear auxiliary trajectory of the system. More precisely, the unreliability of communication channels sometimes makes an agent incapable of sensing the local information or receiving it from neighboring nodes. Therefore, an intermittent communication scheme is proposed among the follower agents as a consequence of employing the linear auxiliary dynamics. The closed-loop networked system signals are proved to be noise-to-state practically stable in probability (NSpS-P). It has been justified that each agent follows the trajectory of the corresponding local auxiliary virtual system practically in probability. The simulation experiments finally quantify the effectiveness of our proposed approach in terms of providing a resilient performance against unreliability of communication channels and reaching the tracking consensus. ",
        "title": "Tracking Consensus of Networked Random Nonlinear Multi-agent Systems  with Intermittent Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05809",
        "abstract_url": "http://arxiv.org/abs/2401.05809",
        "authors": [
            {
                "last_name": "Tomita",
                "first_name": "Yoshihide"
            },
            {
                "last_name": "Koyama",
                "first_name": "Shoichi"
            },
            {
                "last_name": "Saruwatari",
                "first_name": "Hiroshi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  A method for synthesizing the desired sound field while suppressing the exterior radiation power with directional weighting is proposed. The exterior radiation from the loudspeakers in sound field synthesis systems can be problematic in practical situations. Although several methods to suppress the exterior radiation have been proposed, suppression in all outward directions is generally difficult, especially when the number of loudspeakers is not sufficiently large. We propose the directionally weighted exterior radiation representation to prioritize the suppression directions by incorporating it into the optimization problem of sound field synthesis. By using the proposed representation, the exterior radiation in the prioritized directions can be significantly reduced while maintaining high interior synthesis accuracy, owing to the relaxed constraint on the exterior radiation. Its performance is evaluated with the application of the proposed representation to amplitude matching in numerical experiments. ",
        "title": "Localizing Acoustic Energy in Sound Field Synthesis by Directionally  Weighted Exterior Radiation Suppression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05811",
        "abstract_url": "http://arxiv.org/abs/2401.05811",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Zhuoyuan"
            },
            {
                "last_name": "Yu",
                "first_name": "Yen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions; (4) AlignInstruct improved performance in 30 zero-shot directions. ",
        "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine  Translation in Unseen, Low-resource Languages",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05815",
        "abstract_url": "http://arxiv.org/abs/2401.05815",
        "authors": [
            {
                "last_name": "Kaiser",
                "first_name": "Jan"
            },
            {
                "last_name": "Xu",
                "first_name": "Chenran"
            },
            {
                "last_name": "Eichler",
                "first_name": "Annika"
            },
            {
                "last_name": "Garcia",
                "first_name": "Andrea Santamaria"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Machine learning has emerged as a powerful solution to the modern challenges in accelerator physics. However, the limited availability of beam time, the computational cost of simulations, and the high-dimensionality of optimisation problems pose significant challenges in generating the required data for training state-of-the-art machine learning models. In this work, we introduce Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code. Cheetah enables the fast collection of large data sets by reducing computation times by multiple orders of magnitude and facilitates efficient gradient-based optimisation for accelerator tuning and system identification. This positions Cheetah as a user-friendly, readily extensible tool that integrates seamlessly with widely adopted machine learning tools. We showcase the utility of Cheetah through five examples, including reinforcement learning training, gradient-based beamline tuning, gradient-based system identification, physics-informed Bayesian optimisation priors, and modular neural network surrogate modelling of space charge effects. The use of such a high-speed differentiable simulation code will simplify the development of machine learning-based methods for particle accelerators and fast-track their integration into everyday operations of accelerator facilities. ",
        "title": "Cheetah: Bridging the Gap Between Machine Learning and Particle  Accelerator Physics with High-Speed, Differentiable Simulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05818",
        "abstract_url": "http://arxiv.org/abs/2401.05818",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Raquel"
            },
            {
                "last_name": "Alvarez",
                "first_name": "Alberto"
            },
            {
                "last_name": "Mekler",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Writing and genre conventions are extant to any scientific community, and CHI is no different. In this paper, we present the early phases of an AI tool we created called KITSUNE, which supports authors in placing their work into the format of a CHI paper, taking into account many conventions that are ever-present in CHI papers. We describe the development of the tool with the intent to promote discussion around how writing conventions are upheld and unquestioned by the CHI community, and how this translates to the work produced. In addition, we bring up questions surrounding how the introduction of LLMs into academic writing fundamentally change how conventions will be upheld now and in the future ",
        "title": "How to write a CHI paper (asking for a friend)",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05819",
        "abstract_url": "http://arxiv.org/abs/2401.05819",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yuting"
            },
            {
                "last_name": "Chen",
                "first_name": "Fei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Auditory spatial attention detection (ASAD) is used to determine the direction of a listener's attention to a speaker by analyzing her/his electroencephalographic (EEG) signals. This study aimed to further improve the performance of ASAD with a short decision window (i.e., <1 s) rather than with long decision windows in previous studies. An end-to-end temporal attention network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head attention (MHA) mechanism, which can more effectively capture the interactions among time steps in collected EEG signals and efficiently assign corresponding weights to those EEG time steps. Experiments demonstrated that, compared with the CNN-based method and recent ASAD methods, TAnet provided improved decoding performance in the KUL dataset, with decoding accuracies of 92.4% (decision window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s) with short decision windows (i.e., <1 s). As a new ASAD model with a short decision window, TAnet can potentially facilitate the design of EEG-controlled intelligent hearing aids and sound recognition systems. ",
        "title": "TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial  Attention Decoding with a Short Decision Window",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05820",
        "abstract_url": "http://arxiv.org/abs/2401.05820",
        "authors": [
            {
                "last_name": "Emonds",
                "first_name": "Yannick"
            },
            {
                "last_name": "Xi",
                "first_name": "Kai"
            },
            {
                "last_name": "Fr\u00f6ning",
                "first_name": "Holger"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "ET",
            "PF"
        ],
        "abstract": "  Resistive memory is a promising alternative to SRAM, but is also an inherently unstable device that requires substantial effort to ensure correct read and write operations. To avoid the associated costs in terms of area, time and energy, the present work is concerned with exploring how much noise in memory operations can be tolerated by image classification tasks based on neural networks. We introduce a special noisy operator that mimics the noise in an exemplary resistive memory unit, explore the resilience of convolutional neural networks on the CIFAR-10 classification task, and discuss a couple of countermeasures to improve this resilience. ",
        "title": "Implications of Noise in Resistive Memory on Deep Neural Networks for  Image Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05821",
        "abstract_url": "http://arxiv.org/abs/2401.05821",
        "authors": [
            {
                "last_name": "Delfosse",
                "first_name": "Quentin"
            },
            {
                "last_name": "Sztwiertnia",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Stammer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Rothermel",
                "first_name": "Mark"
            },
            {
                "last_name": "Kersting",
                "first_name": "Kristian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SC"
        ],
        "abstract": "  Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it. ",
        "title": "Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05822",
        "abstract_url": "http://arxiv.org/abs/2401.05822",
        "authors": [
            {
                "last_name": "Free",
                "first_name": "Michael"
            },
            {
                "last_name": "Langworthy",
                "first_name": "Andrew"
            },
            {
                "last_name": "Dimitropoulaki",
                "first_name": "Mary"
            },
            {
                "last_name": "Thompson",
                "first_name": "Simon"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The objective of this work is to train a chatbot capable of solving evolving problems through conversing with a user about a problem the chatbot cannot directly observe. The system consists of a virtual problem (in this case a simple game), a simulated user capable of answering natural language questions that can observe and perform actions on the problem, and a Deep Q-Network (DQN)-based chatbot architecture. The chatbot is trained with the goal of solving the problem through dialogue with the simulated user using reinforcement learning. The contributions of this paper are as follows: a proposed architecture to apply a conversational DQN-based agent to evolving problems, an exploration of training methods such as curriculum learning on model performance and the effect of modified reward functions in the case of increasing environment complexity. ",
        "title": "Towards Goal-Oriented Agents for Evolving Problems Observed via  Conversation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05824",
        "abstract_url": "http://arxiv.org/abs/2401.05824",
        "authors": [
            {
                "last_name": "Phang",
                "first_name": "Kenji"
            },
            {
                "last_name": "Pradhan",
                "first_name": "Siddharth Saarathi"
            },
            {
                "last_name": "Ikwuegbu",
                "first_name": "Chino"
            },
            {
                "last_name": "Ramos",
                "first_name": "Gonzalo"
            },
            {
                "last_name": "Ford",
                "first_name": "Denae"
            },
            {
                "last_name": "Okoli",
                "first_name": "Ebele"
            },
            {
                "last_name": "Chishti",
                "first_name": "Salman Muin Kayser"
            },
            {
                "last_name": "Suh",
                "first_name": "Jina"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Mental health is a pressing concern in today's digital age, particularly among youth who are deeply intertwined with technology. Despite the influx of technology solutions addressing mental health issues, youth often remain sidelined during the design process. While co-design methods have been employed to improve participation by youth, many such initiatives are limited to design activities and lack training for youth to research and develop solutions for themselves. In this case study, we detail our 8-week remote, collaborative research initiative called Youth WellTech, designed to facilitate remote co-design sprints aimed at equipping youth with the tools and knowledge to envision and design tech futures for their own communities. We pilot this initiative with 12 student technology evangelists across 8 countries globally to foster the sharing of mental health challenges and diverse perspectives. We highlight insights from our experiences running this global program remotely, its structure, and recommendations for co-research. ",
        "title": "Youth WellTech: A Global Remote Co-Design Sprint for Youth Mental Health  Technology",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05826",
        "abstract_url": "http://arxiv.org/abs/2401.05826",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Nivedita"
            },
            {
                "last_name": "Do",
                "first_name": "Yejin"
            },
            {
                "last_name": "Fouad",
                "first_name": "Yongsang Yu. Imane"
            },
            {
                "last_name": "Kim",
                "first_name": "Jungrae"
            },
            {
                "last_name": "Kim",
                "first_name": "Hyoungshick"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Despite stringent data protection regulations such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and other country-specific regulations, many websites continue to use cookies to track user activities. Recent studies have revealed several data protection violations, resulting in significant penalties, especially for multinational corporations. Motivated by the question of why these data protection violations continue to occur despite strong data protection regulations, we examined 360 popular e-commerce websites in multiple countries to analyze whether they comply with regulations to protect user privacy from a cookie perspective. ",
        "title": "Crumbled Cookie Exploring E-commerce Websites Cookie Policies with Data  Protection Regulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05827",
        "abstract_url": "http://arxiv.org/abs/2401.05827",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jinge"
            },
            {
                "last_name": "Kim",
                "first_name": "Yunsoo"
            },
            {
                "last_name": "Wu",
                "first_name": "Honghan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CV"
        ],
        "abstract": "  The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies. ",
        "title": "Hallucination Benchmark in Medical Visual Question Answering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05833",
        "abstract_url": "http://arxiv.org/abs/2401.05833",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Attaining ultra-reliable communication (URC) in fifth-generation (5G) and beyond networks requires deriving statistics of channel in ultra-reliable region by modeling the extreme events. Extreme value theory (EVT) has been previously adopted in channel modeling to characterize the lower tail of received powers in URC systems. In this paper, we propose a multivariate EVT (MEVT)-based channel modeling methodology for tail of the joint distribution of multi-channel by characterizing the multivariate extremes of multiple-input multiple-output (MIMO) system. The proposed approach derives lower tail statistics of received power of each channel by using the generalized Pareto distribution (GPD). Then, tail of the joint distribution is modeled as a function of estimated GPD parameters based on two approaches: logistic distribution, which utilizes logistic distribution to determine dependency factors among the Frechet transformed tail sequence and obtain a bi-variate extreme value model, and Poisson point process, which estimates probability measure function of the Pickands angular component to model bi-variate extreme values. Finally, validity of the proposed models is assessed by incorporating the mean constraint on probability measure function of Pichanks coordinates. Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the superiority of proposed methodology compared to the conventional extrapolation-based methods in providing the best fit to the multivariate extremes. ",
        "title": "Multivariate Extreme Value Theory Based Channel Modeling for  Ultra-Reliable Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05835",
        "abstract_url": "http://arxiv.org/abs/2401.05835",
        "authors": [
            {
                "last_name": "Hosseinalizadeh",
                "first_name": "Teimour"
            },
            {
                "last_name": "Schl\u00fcter",
                "first_name": "Nils"
            },
            {
                "last_name": "Darup",
                "first_name": "Moritz Schulze"
            },
            {
                "last_name": "Monshizadeh",
                "first_name": "Nima"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Search for the optimizer in computationally demanding model predictive control (MPC) setups can be facilitated by Cloud as a service provider in cyber-physical systems. This advantage introduces the risk that Cloud can obtain unauthorized access to the privacy-sensitive parameters of the system and cost function. To solve this issue, i.e., preventing Cloud from accessing the parameters while benefiting from Cloud computation, random affine transformations provide an exact yet light weight in computation solution. This research deals with analyzing privacy preserving properties of these transformations when they are adopted for MPC problems. We consider two common strategies for outsourcing the optimization required in MPC problems, namely separate and dense forms, and establish that random affine transformations utilized in these forms are vulnerable to side-knowledge from Cloud. Specifically, we prove that the privacy guarantees of these methods and their extensions for separate form are undermined when a mild side-knowledge about the problem in terms of structure of MPC cost function is available. In addition, while we prove that outsourcing the MPC problem in the dense form inherently leads to some degree of privacy for the system and cost function parameters, we also establish that affine transformations applied to this form are nevertheless prone to be undermined by a Cloud with mild side-knowledge. Numerical simulations confirm our results. ",
        "title": "Privacy Analysis of Affine Transformations in Cloud-based MPC:  Vulnerability to Side-knowledge",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05836",
        "abstract_url": "http://arxiv.org/abs/2401.05836",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Feng"
            },
            {
                "last_name": "Xu",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xveqing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuantai"
            },
            {
                "last_name": "Chen",
                "first_name": "Weijie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaohong"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The essential of navigation, perception, and decision-making which are basic tasks for intelligent robots, is to estimate necessary system states. Among them, navigation is fundamental for other upper applications, providing precise position and orientation, by integrating measurements from multiple sensors. With observations of each sensor appropriately modelled, multi-sensor fusion tasks for navigation are reduced to the state estimation problem which can be solved by two approaches: optimization and filtering. Recent research has shown that optimization-based frameworks outperform filtering-based ones in terms of accuracy. However, both methods are based on maximum likelihood estimation (MLE) and should be theoretically equivalent with the same linearization points, observation model, measurements, and Gaussian noise assumption. In this paper, we deeply dig into the theories and existing strategies utilized in both optimization-based and filtering-based approaches. It is demonstrated that the two methods are equal theoretically, but this equivalence corrupts due to different strategies applied in real-time operation. By adjusting existing strategies of the filtering-based approaches, the Monte-Carlo simulation and vehicular ablation experiments based on visual odometry (VO) indicate that the strategy adjusted filtering strictly equals to optimization. Therefore, future research on sensor-fusion problems should concentrate on their own algorithms and strategies rather than state estimation approaches. ",
        "title": "On State Estimation in Multi-Sensor Fusion Navigation: Optimization and  Filtering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05840",
        "abstract_url": "http://arxiv.org/abs/2401.05840",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhuoyan"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Yin",
                "first_name": "Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult. In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making. By conceptualizing AI assistance as the ``{\\em nudge}'' in human decision making processes, our approach centers around modelling how different forms of AI assistance modify humans' strategy in weighing different information in making their decisions. Evaluations on behavior data collected from real human decision makers show that the proposed framework outperforms various baselines in accurately predicting human behavior in AI-assisted decision making. Based on the proposed framework, we further provide insights into how individuals with different cognitive styles are nudged by AI assistance differently. ",
        "title": "Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in  AI-assisted Decision Making",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05841",
        "abstract_url": "http://arxiv.org/abs/2401.05841",
        "authors": [
            {
                "last_name": "Br\u00fcning",
                "first_name": "Frederik"
            },
            {
                "last_name": "Driemel",
                "first_name": "Anne"
            },
            {
                "last_name": "Erg\u00fcr",
                "first_name": "Alperen"
            },
            {
                "last_name": "R\u00f6glin",
                "first_name": "Heiko"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  The DTW Barycenter Averaging (DBA) algorithm is a widely used algorithm for estimating the mean of a given set of point sequences. In this context, the mean is defined as a point sequence that minimises the sum of dynamic time warping distances (DTW). The algorithm is similar to the $k$-means algorithm in the sense that it alternately repeats two steps: (1) computing an optimal assignment to the points of the current mean, and (2) computing an optimal mean under the current assignment. The popularity of DBA can be attributed to the fact that it works well in practice, despite any theoretical guarantees to be known. In our paper, we aim to initiate a theoretical study of the number of iterations that DBA performs until convergence. We assume the algorithm is given $n$ sequences of $m$ points in $\\mathbb{R}^d$ and a parameter $k$ that specifies the length of the mean sequence to be computed. We show that, in contrast to its fast running time in practice, the number of iterations can be exponential in $k$ in the worst case - even if the number of input sequences is $n=2$. We complement these findings with experiments on real-world data that suggest this worst-case behaviour is likely degenerate. To better understand the performance of the algorithm on non-degenerate input, we study DBA in the model of smoothed analysis, upper-bounding the expected number of iterations in the worst case under random perturbations of the input. Our smoothed upper bound is polynomial in $k$, $n$ and $d$, and for constant $n$, it is also polynomial in $m$. For our analysis, we adapt the set of techniques that were developed for analysing $k$-means and observe that this set of techniques is not sufficient to obtain tight bounds for general $n$. ",
        "title": "On the number of iterations of the DBA algorithm",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05842",
        "abstract_url": "http://arxiv.org/abs/2401.05842",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Tao"
            },
            {
                "last_name": "Bao",
                "first_name": "Jialu"
            },
            {
                "last_name": "Hsu",
                "first_name": "Justin"
            },
            {
                "last_name": "Silva",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Zanasi",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  The logic of Dependence and Independence Bunched Implications (DIBI) is a logic to reason about conditional independence (CI); for instance, DIBI formulas can characterise CI in probability distributions and relational databases, using the probabilistic and relational DIBI models, respectively. Despite the similarity of the probabilistic and relational models, a uniform, more abstract account remains unsolved. The laborious case-by-case verification of the frame conditions required for constructing new models also calls for such a treatment. In this paper, we develop an abstract framework for systematically constructing DIBI models, using category theory as the unifying mathematical language. In particular, we use string diagrams -- a graphical presentation of monoidal categories -- to give a uniform definition of the parallel composition and subkernel relation in DIBI models. Our approach not only generalises known models, but also yields new models of interest and reduces properties of DIBI models to structures in the underlying categories. Furthermore, our categorical framework enables a logical notion of CI, in terms of the satisfaction of specific DIBI formulas. We compare it with string diagrammatic approaches to CI and show that it is an extension of string diagrammatic CI under reasonable conditions. ",
        "title": "A Categorical Approach to DIBI Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05845",
        "abstract_url": "http://arxiv.org/abs/2401.05845",
        "authors": [
            {
                "last_name": "Konrad",
                "first_name": "Christian"
            },
            {
                "last_name": "O'Sullivan",
                "first_name": "Conor"
            },
            {
                "last_name": "Traistaru",
                "first_name": "Victor"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We consider the Graph Reconstruction problem given only query access to the input graph via a Maximal Independent Set oracle. In this setting, in each round, the player submits a query consisting of a subset of vertices to the oracle, and the oracle returns any maximal independent set in the subgraph induced by the queried vertices. The goal for the player is to learn all the edges of the input graph.   In this paper, we give tight (up to a logarithmic factor) upper and lower bounds for this problem:   1. We give a randomized query algorithm that uses $O(\\Delta^2 \\log n)$ non-adaptive queries and succeeds with high probability to reconstruct an $n$-vertex graph with maximum degree $\\Delta$. Using the probabilistic method, we also show that a non-adaptive deterministic algorithm that executes $O(\\Delta^3 \\log n)$ queries exists.   2. We give two lower bounds that apply to arbitrary adaptive randomized algorithms that succeed with probability greater than $\\frac{1}{2}$. We show that, for such algorithms, $\\Omega(\\Delta^2)$ rounds are necessary in graphs of maximum degree $\\Delta$, and that $\\Omega(\\log n)$ rounds are necessary even when the input graph is an $n$-vertex cycle. ",
        "title": "Graph Reconstruction via MIS Queries",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05848",
        "abstract_url": "http://arxiv.org/abs/2401.05848",
        "authors": [
            {
                "last_name": "Riebesell",
                "first_name": "Janosh"
            },
            {
                "last_name": "Surta",
                "first_name": "T. Wesley"
            },
            {
                "last_name": "Goodall",
                "first_name": "Rhys"
            },
            {
                "last_name": "Gaultois",
                "first_name": "Michael"
            },
            {
                "last_name": "Lee",
                "first_name": "Alpha A"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Materials with high-dielectric constant easily polarize under external electric fields, allowing them to perform essential functions in many modern electronic devices. Their practical utility is determined by two conflicting properties: high dielectric constants tend to occur in materials with narrow band gaps, limiting the operating voltage before dielectric breakdown. We present a high-throughput workflow that combines element substitution, ML pre-screening, ab initio simulation and human expert intuition to efficiently explore the vast space of unknown materials for potential dielectrics, leading to the synthesis and characterization of two novel dielectric materials, CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective optimization setting with concave Pareto front. While usually considered more challenging than single-objective optimization, we argue and show preliminary evidence that the $1/x$-correlation between band gap and permittivity in fact makes the task more amenable to ML methods by allowing separate models for band gap and permittivity to each operate in regions of good training support while still predicting materials of exceptional merit. To our knowledge, this is the first instance of successful ML-guided multi-objective materials optimization achieving experimental synthesis and characterization. CsTaTeO6 is a structure generated via element substitution not present in our reference data sources, thus exemplifying successful de-novo materials design. Meanwhile, we report the first high-purity synthesis and dielectric characterization of Bi2Zr2O7 with a band gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics of our multi-objective search. ",
        "title": "Pushing the Pareto front of band gap and permittivity: ML-guided search  for dielectric materials",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05849",
        "abstract_url": "http://arxiv.org/abs/2401.05849",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Litian"
            },
            {
                "last_name": "Molhoek",
                "first_name": "Jord"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "HC"
        ],
        "abstract": "  Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to speak, but people also often shift posture without having an intention to speak, or have an intention to speak without shifting their posture. More modalities are likely needed to reliably infer intentions to speak. ",
        "title": "Inferring Intentions to Speak Using Accelerometer Data In-the-Wild",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05850",
        "abstract_url": "http://arxiv.org/abs/2401.05850",
        "authors": [
            {
                "last_name": "Guan",
                "first_name": "Yadong"
            },
            {
                "last_name": "Han",
                "first_name": "Jiqing"
            },
            {
                "last_name": "Song",
                "first_name": "Hongwei"
            },
            {
                "last_name": "Song",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Zheng",
                "first_name": "Guibin"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieran"
            },
            {
                "last_name": "He",
                "first_name": "Yongjun"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Overlapping sound events are ubiquitous in real-world environments, but existing end-to-end sound event detection (SED) methods still struggle to detect them effectively. A critical reason is that these methods represent overlapping events using shared and entangled frame-wise features, which degrades the feature discrimination. To solve the problem, we propose a disentangled feature learning framework to learn a category-specific representation. Specifically, we employ different projectors to learn the frame-wise features for each category. To ensure that these feature does not contain information of other categories, we maximize the common information between frame-wise features within the same category and propose a frame-wise contrastive loss. In addition, considering that the labeled data used by the proposed method is limited, we propose a semi-supervised frame-wise contrastive loss that can leverage large amounts of unlabeled data to achieve feature disentanglement. The experimental results demonstrate the effectiveness of our method. ",
        "title": "Contrastive Loss Based Frame-wise Feature disentanglement for Polyphonic  Sound Event Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05856",
        "abstract_url": "http://arxiv.org/abs/2401.05856",
        "authors": [
            {
                "last_name": "Barnett",
                "first_name": "Scott"
            },
            {
                "last_name": "Kurniawan",
                "first_name": "Stefanus"
            },
            {
                "last_name": "Thudumu",
                "first_name": "Srikanth"
            },
            {
                "last_name": "Brannelly",
                "first_name": "Zach"
            },
            {
                "last_name": "Abdelrazek",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community. ",
        "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation  System",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05857",
        "abstract_url": "http://arxiv.org/abs/2401.05857",
        "authors": [
            {
                "last_name": "Azarbahram",
                "first_name": "Ali"
            },
            {
                "last_name": "Amini",
                "first_name": "Amir"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This article proposes a secure implementation for consensus using a dynamic event-triggered (DET) communication scheme in high-order nonlinear multi-agent systems (MAS) under asynchronous (distributed) denial of service (DoS) attacks. By introducing a linear auxiliary trajectory of the system, the DET data transmission scheme among the neighboring agents is employed to reduce the communication for each agent. The asynchronous DoS attacks can block each communication channel among the cooperative agents independently in an unknown pattern. To guarantee state consensus of auxiliary MAS under DoS, a linear matrix inequality (LMI) based optimization approach is proposed which simultaneously designs all the unknown DET communication parameters as well as the state feedback control gain. In addition to asynchronous DoS attacks over the graph topology, the destructive effects of independent DoS attacks over the communication links between actual and auxiliary states are compensated as an additional layer of resiliency for the system. The output of each agent ultimately tracks the auxiliary state of the system and this results in the output consensus. ",
        "title": "Secure Dynamic Event-triggered Consensus Under Asynchronous Denial of  Service",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05859",
        "abstract_url": "http://arxiv.org/abs/2401.05859",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Wentu"
            },
            {
                "last_name": "Cai",
                "first_name": "Kui"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, for any fixed integer $q>2$, we construct $q$-ary codes correcting a burst of at most $t$ deletions with redundancy $\\log n+8\\log\\log n+o(\\log\\log n)+\\gamma_{q,t}$ bits and near-linear encoding/decoding complexity, where $n$ is the message length and $\\gamma_{q,t}$ is a constant that only depends on $q$ and $t$. In previous works there are constructions of such codes with redundancy $\\log n+O(\\log q\\log\\log n)$ bits or $\\log n+O(t^2\\log\\log n)+O(t\\log q)$. The redundancy of our new construction is independent of $q$ and $t$ in the second term. ",
        "title": "New Construction of $q$-ary Codes Correcting a Burst of at most $t$  Deletions",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05860",
        "abstract_url": "http://arxiv.org/abs/2401.05860",
        "authors": [
            {
                "last_name": "Phan",
                "first_name": "Thomy"
            },
            {
                "last_name": "Driscoll",
                "first_name": "Joseph"
            },
            {
                "last_name": "Romberg",
                "first_name": "Justin"
            },
            {
                "last_name": "Koenig",
                "first_name": "Sven"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  A wide range of real-world applications can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with individual start and goal locations. State-of-the-art MAPF solvers are mainly centralized and depend on global information, which limits their scalability and flexibility regarding changes or new maps that would require expensive replanning. Multi-agent reinforcement learning (MARL) offers an alternative way by learning decentralized policies that can generalize over a variety of maps. While there exist some prior works that attempt to connect both areas, the proposed techniques are heavily engineered and very complex due to the integration of many mechanisms that limit generality and are expensive to use. We argue that much simpler and general approaches are needed to bring the areas of MARL and MAPF closer together with significantly lower costs. In this paper, we propose Confidence-based Auto-Curriculum for Team Update Stability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a simple reverse curriculum scheme, where the goal of each agent is randomly placed within an allocation radius around the agent's start location. The allocation radius increases gradually as all agents improve, which is assessed by a confidence-based measure. We evaluate CACTUS in various maps of different sizes, obstacle densities, and numbers of agents. Our experiments demonstrate better performance and generalization capabilities than state-of-the-art MARL approaches with less than 600,000 trainable parameters, which is less than 5% of the neural network size of current MARL approaches to MAPF. ",
        "title": "Confidence-Based Curriculum Learning for Multi-Agent Path Finding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05861",
        "abstract_url": "http://arxiv.org/abs/2401.05861",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Pengzhi"
            },
            {
                "last_name": "He",
                "first_name": "Zhongjun"
            },
            {
                "last_name": "Wu",
                "first_name": "Hua"
            },
            {
                "last_name": "Wang",
                "first_name": "Haifeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on pretrained multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting the many-to-many multilingual translation performance of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during instruction finetuning are crucial to zero-shot translation performance and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for multilingual finetuning on LLMs with translation instructions. Experimental results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves translation performance. Our implementations are available at https://github.com/gpengzhi/CrossConST-LLM. ",
        "title": "Towards Boosting Many-to-Many Multilingual Machine Translation with  Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05868",
        "abstract_url": "http://arxiv.org/abs/2401.05868",
        "authors": [
            {
                "last_name": "Ham",
                "first_name": "David A."
            },
            {
                "last_name": "Hapla",
                "first_name": "Vaclav"
            },
            {
                "last_name": "Knepley",
                "first_name": "Matthew G."
            },
            {
                "last_name": "Mitchell",
                "first_name": "Lawrence"
            },
            {
                "last_name": "Sagiyama",
                "first_name": "Koki"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "MS"
        ],
        "abstract": "  In this work, we introduce a new algorithm for N-to-M checkpointing in finite element simulations. This new algorithm allows efficient saving/loading of functions representing physical quantities associated with the mesh representing the physical domain. Specifically, the algorithm allows for using different numbers of parallel processes for saving and loading, allowing for restarting and post-processing on the process count appropriate to the given phase of the simulation and other conditions. For demonstration, we implemented this algorithm in PETSc, the Portable, Extensible Toolkit for Scientific Computation, and added a convenient high-level interface into Firedrake, a system for solving partial differential equations using finite element methods. We evaluated our new implementation by saving and loading data involving 8.2 billion finite element degrees of freedom using 8,192 parallel processes on ARCHER2, the UK National Supercomputing Service. ",
        "title": "Efficient N-to-M Checkpointing Algorithm for Finite Element Simulations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05870",
        "abstract_url": "http://arxiv.org/abs/2401.05870",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Hanzhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Yang",
                "first_name": "Jinze"
            },
            {
                "last_name": "Yu",
                "first_name": "Zhongrui"
            },
            {
                "last_name": "Xie",
                "first_name": "Zeke"
            },
            {
                "last_name": "Tian",
                "first_name": "Lei"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xinyan"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            },
            {
                "last_name": "Sun",
                "first_name": "Mingming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \\textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for video diffusion model training, which significantly improve cross-frame temporal consistency in the premise of maintaining stylization strength. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our HiCAST outperforms the existing SoTA methods in generating visually plausible stylization results. ",
        "title": "HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced  Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05871",
        "abstract_url": "http://arxiv.org/abs/2401.05871",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Yahui"
            },
            {
                "last_name": "Song",
                "first_name": "Haiyue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions. One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models. Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue. To address the first challenge, we introduce personality trait interpolation for speaker data augmentation. For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits. Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines. ",
        "title": "Enhancing Personality Recognition in Dialogue by Data Augmentation and  Heterogeneous Conversational Graph Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05876",
        "abstract_url": "http://arxiv.org/abs/2401.05876",
        "authors": [
            {
                "last_name": "Baumann",
                "first_name": "Dominik"
            },
            {
                "last_name": "Sch\u00f6n",
                "first_name": "Thomas B."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  When deploying machine learning algorithms in the real world, guaranteeing safety is an essential asset. Existing safe learning approaches typically consider continuous variables, i.e., regression tasks. However, in practice, robotic systems are also subject to discrete, external environmental changes, e.g., having to carry objects of certain weights or operating on frozen, wet, or dry surfaces. Such influences can be modeled as discrete context variables. In the existing literature, such contexts are, if considered, mostly assumed to be known. In this work, we drop this assumption and show how we can perform safe learning when we cannot directly measure the context variables. To achieve this, we derive frequentist guarantees for multi-class classification, allowing us to estimate the current context from measurements. Further, we propose an approach for identifying contexts through experiments. We discuss under which conditions we can retain theoretical guarantees and demonstrate the applicability of our algorithm on a Furuta pendulum with camera measurements of different weights that serve as contexts. ",
        "title": "Safe reinforcement learning in uncertain contexts",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05879",
        "abstract_url": "http://arxiv.org/abs/2401.05879",
        "authors": [
            {
                "last_name": "Jing",
                "first_name": "Yu"
            },
            {
                "last_name": "Yujuan",
                "first_name": "Tan"
            },
            {
                "last_name": "Ao",
                "first_name": "Ren"
            },
            {
                "last_name": "Duo",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Occlusions pose a significant challenge to optical flow algorithms that even rely on global evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work only used the current frame as the only input, which could not guarantee providing correct global reference information for occluded points, and had problems such as long calculation time and poor accuracy in predicting optical flow at occluded points. To enable both high accuracy and efficiency, We fully mine and utilize the spatiotemporal information provided by the frame pair, design a loopback judgment algorithm to ensure that correct global reference information is obtained, mine multiple necessary global information, and design an efficient refinement module that fuses these global information. Specifically, we propose a YOIO framework, which consists of three main components: an initial flow estimator, a multiple global information extraction module, and a unified refinement module. We demonstrate that optical flow estimates in the occluded regions can be significantly improved in only one iteration without damaging the performance in non-occluded regions. Compared with GMA, the optical flow prediction accuracy of this method in the occluded area is improved by more than 10%, and the occ_out area exceeds 15%, while the calculation time is 27% shorter. This approach, running up to 18.9fps with 436*1024 image resolution, obtains new state-of-the-art results on the challenging Sintel dataset among all published and unpublished approaches that can run in real-time, suggesting a new paradigm for accurate and efficient optical flow estimation. ",
        "title": "YOIO: You Only Iterate Once by mining and fusing multiple necessary  global information in the optical flow estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05881",
        "abstract_url": "http://arxiv.org/abs/2401.05881",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Chendong"
            },
            {
                "last_name": "Yang",
                "first_name": "Dapeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiachen"
            },
            {
                "last_name": "Dai",
                "first_name": "Yiming"
            },
            {
                "last_name": "Jiang",
                "first_name": "Li"
            },
            {
                "last_name": "Liu",
                "first_name": "Hong"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The fabric-based pneumatic exosuit is now a hot research topic because it is lighter and softer than traditional exoskeletons. Existing research focused more on the mechanical properties of the exosuit (e.g., torque and speed), but less on its wearability (e.g., appearance and comfort). This work presents a new design concept for fabric-based pneumatic exosuits Volume Transfer, which means transferring the volume of pneumatic actuators beyond the garments profile to the inside. This allows for a concealed appearance and a larger stress area while maintaining adequate torques. In order to verify this concept, we develop a fabric-based pneumatic exosuit for knee extension assistance. Its profile is only 26mm and its stress area wraps around almost half of the leg. We use a mathematical model and simulation to determine the parameters of the exosuit, avoiding multiple iterations of the prototype. Experiment results show that the exosuit can generate a torque of 7.6Nm at a pressure of 90kPa and produce a significant reduction in the electromyography activity of the knee extensor muscles. We believe that Volume Transfer could be utilized prevalently in future fabric-based pneumatic exosuit designs to achieve a significant improvement in wearability. ",
        "title": "Volume Transfer: A New Design Concept for Fabric-Based Pneumatic  Exosuits",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05882",
        "abstract_url": "http://arxiv.org/abs/2401.05882",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Ultra-reliable low latency communication (URLLC) requires the packet error rate to be on the order of $10^{-9}$-$10^{-5}$. Determining the appropriate transmission rate to satisfy this ultra-reliability constraint requires deriving the statistics of the channel in the ultra-reliable region and then incorporating these statistics into the rate selection. In this paper, we propose a framework for determining the rate selection for ultra-reliable communications based on the extreme value theory (EVT). We first model the wireless channel at URLLC by estimating the parameters of the generalized Pareto distribution (GPD) best fitting to the tail distribution of the received powers, i.e., the power values below a certain threshold. Then, we determine the maximum transmission rate by incorporating the Pareto distribution into the rate selection function. Finally, we validate the selected rate by computing the resulting error probability. Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the superior performance of the proposed methodology in determining the maximum transmission rate compared to the traditional extrapolation-based approaches. ",
        "title": "Extreme Value Theory Based Rate Selection for Ultra-Reliable  Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05888",
        "abstract_url": "http://arxiv.org/abs/2401.05888",
        "authors": [
            {
                "last_name": "Mehrnia",
                "first_name": "Niloofar"
            },
            {
                "last_name": "Coleri",
                "first_name": "Sinem"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Proper determination of the transmission rate in ultra-reliable low latency communication (URLLC) needs to incorporate a confidence interval (CI) for the estimated parameters due to the large amount of data required for their accurate estimation. In this paper, we propose a framework based on the extreme value theory (EVT) for determining the transmission rate along with its corresponding CI for an ultra-reliable communication system. This framework consists of characterizing the statistics of extreme events by fitting the generalized Pareto distribution (GPD) to the channel tail, deriving the GPD parameters and their associated CIs, and obtaining the transmission rate within a confidence interval. Based on the data collected within the engine compartment of Fiat Linea, we demonstrate the accuracy of the estimated rate obtained through the EVT-based framework considering the confidence interval for the GPD parameters. Additionally, we show that proper estimation of the transmission rate based on the proposed framework requires a lower number of samples compared to the traditional extrapolation-based approaches. ",
        "title": "Incorporation of Confidence Interval into Rate Selection Based on the  Extreme Value Theory for Ultra-Reliable Communications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05891",
        "abstract_url": "http://arxiv.org/abs/2401.05891",
        "authors": [
            {
                "last_name": "Ciobotari",
                "first_name": "Ion"
            },
            {
                "last_name": "Pr\u00edncipe",
                "first_name": "Adriana"
            },
            {
                "last_name": "Oliveira",
                "first_name": "Maria Alexandra"
            },
            {
                "last_name": "Silva",
                "first_name": "Jo\u00e3o Nuno"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The collection of ecological data in the field is essential to diagnose, monitor and manage ecosystems in a sustainable way. Since acquisition of this information through traditional methods are generally time-consuming, due to the capability of recording large volumes of data in short time periods, automation of data acquisition sees a growing trend. Terrestrial laser scanners (TLS), particularly LiDAR sensors, have been used in ecology, allowing to reconstruct the 3D structure of vegetation, and thus, infer ecosystem characteristics based on the spatial variation of the density of points. However, the low amount of information obtained per beam, lack of data analysis tools and the high cost of the equipment limit their use. This way, a low-cost TLS (<10k$) was developed along with data acquisition and processing mechanisms applicable in two case studies: an urban garden and a target area for ecological restoration. The orientation of LiDAR was modified to make observations in the vertical plane and a motor was integrated for its rotation, enabling the acquisition of 360 degree data with high resolution. Motion and location sensors were also integrated for automatic error correction and georeferencing. From the data generated, histograms of point density variation along the vegetation height were created, where shrub stratum was easily distinguishable from tree stratum, and maximum tree height and shrub cover were calculated. These results agreed with the field data, whereby the developed TLS has proved to be effective in calculating metrics of structural complexity of vegetation. ",
        "title": "LiDAR data acquisition and processing for ecology applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05892",
        "abstract_url": "http://arxiv.org/abs/2401.05892",
        "authors": [
            {
                "last_name": "Goetze",
                "first_name": "Miriam"
            },
            {
                "last_name": "Jungeblut",
                "first_name": "Paul"
            },
            {
                "last_name": "Ueckerdt",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  We study the recognition complexity of subgraphs of 2- and 3-connected planar cubic graphs. Recently, we presented [ESA 2022] a quadratic-time algorithm to recognize subgraphs of planar cubic bridgeless (but not necessarily connected) graphs, both in the variable and fixed embedding setting (the latter only for 2-connected inputs). Here, we extend our results in two directions: First, we present a quartic-time algorithm to recognize subgraphs of 2-connected planar cubic graphs in the fixed embedding setting, even for disconnected inputs. Second, we prove NP-hardness of recognizing subgraphs of 3-connected planar cubic graphs in the variable embedding setting. ",
        "title": "Recognition Complexity of Subgraphs of 2- and 3-Connected Planar Cubic  Graphs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05894",
        "abstract_url": "http://arxiv.org/abs/2401.05894",
        "authors": [
            {
                "last_name": "Banaei",
                "first_name": "Mohsen"
            },
            {
                "last_name": "Ebrahimy",
                "first_name": "Razgar"
            },
            {
                "last_name": "Madsen",
                "first_name": "Henrik"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, a computationally lightweight algorithm is introduced for hybrid PV/Battery/Load systems that is price responsive, responds fast, does not require powerful hardware, and considers the operational limitations of the system. The method is applied to two buildings equipped with PV and battery. Simulation results show that the method can give results that are up to 3.9% more expensive than the Model predictive control (MPC) approach while the runtime of the program is up to 1000 times less than the MPC. Also, while the runtime of the proposed method is in the range of the self-consumption maximization (SCM) approach as the fastest method, its electricity cost is about 3.2% cheaper than the SCM method. Simulation results also show that in case of providing grid services by the battery the difference between electricity cost of the proposed approach and MPC can reduce which makes the method good for such applications. ",
        "title": "A Lightweight Energy Management Method for Hybrid PV/Battery/Load  Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05895",
        "abstract_url": "http://arxiv.org/abs/2401.05895",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Tianxiu"
            },
            {
                "last_name": "Gai",
                "first_name": "Keke"
            },
            {
                "last_name": "Yu",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Liehuang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "DC"
        ],
        "abstract": "  Distributed machine learning enables parallel training of extensive datasets by delegating computing tasks across multiple workers. Despite the cost reduction benefits of distributed machine learning, the dissemination of final model weights often leads to potential conflicts over model ownership as workers struggle to substantiate their involvement in the training computation. To address the above ownership issues and prevent accidental failures and malicious attacks, verifying the computational integrity and effectiveness of workers becomes particularly crucial in distributed machine learning. In this paper, we proposed a novel binary linear tree commitment-based ownership protection model to ensure computational integrity with limited overhead and concise proof. Due to the frequent updates of parameters during training, our commitment scheme introduces a maintainable tree structure to reduce the costs of updating proofs. Distinguished from SNARK-based verifiable computation, our model achieves efficient proof aggregation by leveraging inner product arguments. Furthermore, proofs of model weights are watermarked by worker identity keys to prevent commitments from being forged or duplicated. The performance analysis and comparison with SNARK-based hash commitments validate the efficacy of our model in preserving computational integrity within distributed machine learning. ",
        "title": "Binary Linear Tree Commitment-based Ownership Protection for Distributed  Machine Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05896",
        "abstract_url": "http://arxiv.org/abs/2401.05896",
        "authors": [
            {
                "last_name": "Abdi",
                "first_name": "Nima"
            },
            {
                "last_name": "Albaseer",
                "first_name": "Abdullatif"
            },
            {
                "last_name": "Abdallah",
                "first_name": "Mohamed"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            "NI"
        ],
        "abstract": "  As smart grids (SG) increasingly rely on advanced technologies like sensors and communication systems for efficient energy generation, distribution, and consumption, they become enticing targets for sophisticated cyberattacks. These evolving threats demand robust security measures to maintain the stability and resilience of modern energy systems. While extensive research has been conducted, a comprehensive exploration of proactive cyber defense strategies utilizing Deep Learning (DL) in {SG} remains scarce in the literature. This survey bridges this gap, studying the latest DL techniques for proactive cyber defense. The survey begins with an overview of related works and our distinct contributions, followed by an examination of SG infrastructure. Next, we classify various cyber defense techniques into reactive and proactive categories. A significant focus is placed on DL-enabled proactive defenses, where we provide a comprehensive taxonomy of DL approaches, highlighting their roles and relevance in the proactive security of SG. Subsequently, we analyze the most significant DL-based methods currently in use. Further, we explore Moving Target Defense, a proactive defense strategy, and its interactions with DL methodologies. We then provide an overview of benchmark datasets used in this domain to substantiate the discourse.{ This is followed by a critical discussion on their practical implications and broader impact on cybersecurity in Smart Grids.} The survey finally lists the challenges associated with deploying DL-based security systems within SG, followed by an outlook on future developments in this key field. ",
        "title": "The Role of Deep Learning in Advancing Proactive Cybersecurity Measures  for Smart Grid Networks: A Survey",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05897",
        "abstract_url": "http://arxiv.org/abs/2401.05897",
        "authors": [
            {
                "last_name": "Bartels",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Tscherner",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  It is shown that discretizations based on variational or weak formulations of the plate bending problem with simple support boundary conditions do not lead to failure of convergence when polygonal domain approximations are used and the imposed boundary conditions are compatible with the nodal interpolation of the restriction of certain regular functions to approximating domains. It is further shown that this is optimal in the sense that a full realization of the boundary conditions leads to failure of convergence for conforming methods. The abstract conditions imply that standard nonconforming and discontinuous Galerkin methods converge correctly while conforming methods require a suitable relaxation of the boundary condition. The results are confirmed by numerical experiments. ",
        "title": "Necessary and Sufficient Conditions for Avoiding Babuska's Paradox on  Simplicial Meshes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05898",
        "abstract_url": "http://arxiv.org/abs/2401.05898",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Weihang"
            },
            {
                "last_name": "Shikh-Bahaei",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this work, we propose a novel partial compress-and-forward (PCF) scheme for improving the maximum achievable transmission rate of a diamond relay network with two noisy relays. PCF combines conventional compress-and-forward (CF) and amplify-and-forward (AF) protocols, enabling one relay to operate alternately in the CF or the AF mode, while the other relay works purely in the CF mode. As the direct link from the source to the destination is unavailable, and there is no noiseless relay in the diamond network, messages received from both relays must act as side information for each other and must be decoded jointly. We propose a joint decoder to decode two Luby transform (LT) codes received from both relays corresponding to the same original message. Numerical results show that PCF can achieve significant performance improvements compared to decode-and-forward (DF) and pure CF protocols when at least the channels connected to one of the relays are of high quality. ",
        "title": "A Partial Compress-and-Forward Strategy for Relay-assisted Wireless  Networks Based on Rateless Coding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05899",
        "abstract_url": "http://arxiv.org/abs/2401.05899",
        "authors": [
            {
                "last_name": "Zhai",
                "first_name": "Yuanzhao"
            },
            {
                "last_name": "Li",
                "first_name": "Yiying"
            },
            {
                "last_name": "Gao",
                "first_name": "Zijian"
            },
            {
                "last_name": "Gong",
                "first_name": "Xudong"
            },
            {
                "last_name": "Xu",
                "first_name": "Kele"
            },
            {
                "last_name": "Feng",
                "first_name": "Dawei"
            },
            {
                "last_name": "Bo",
                "first_name": "Ding"
            },
            {
                "last_name": "Wang",
                "first_name": "Huaimin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Model-based offline reinforcement learning (RL) has made remarkable progress, offering a promising avenue for improving generalization with synthetic model rollouts. Existing works primarily focus on incorporating pessimism for policy optimization, usually via constructing a Pessimistic Markov Decision Process (P-MDP). However, the P-MDP discourages the policies from learning in out-of-distribution (OOD) regions beyond the support of offline datasets, which can under-utilize the generalization ability of dynamics models. In contrast, we propose constructing an Optimistic MDP (O-MDP). We initially observed the potential benefits of optimism brought by encouraging more OOD rollouts. Motivated by this observation, we present ORPO, a simple yet effective model-based offline RL framework. ORPO generates Optimistic model Rollouts for Pessimistic offline policy Optimization. Specifically, we train an optimistic rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel the sampled state-action pairs with penalized rewards and optimize the output policy in the P-MDP. Theoretically, we demonstrate that the performance of policies trained with ORPO can be lower-bounded in linear MDPs. Experimental results show that our framework significantly outperforms P-MDP baselines by a margin of 30%, achieving state-of-the-art performance on the widely-used benchmark. Moreover, ORPO exhibits notable advantages in problems that require generalization. ",
        "title": "Optimistic Model Rollouts for Pessimistic Offline Policy Optimization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05901",
        "abstract_url": "http://arxiv.org/abs/2401.05901",
        "authors": [
            {
                "last_name": "Rivas-Villar",
                "first_name": "David"
            },
            {
                "last_name": "Hervella",
                "first_name": "\u00c1lvaro S."
            },
            {
                "last_name": "Rouco",
                "first_name": "Jos\u00e9"
            },
            {
                "last_name": "Novo",
                "first_name": "Jorge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Retinal image registration is of utmost importance due to its wide applications in medical practice. In this context, we propose ConKeD, a novel deep learning approach to learn descriptors for retinal image registration. In contrast to current registration methods, our approach employs a novel multi-positive multi-negative contrastive learning strategy that enables the utilization of additional information from the available training samples. This makes it possible to learn high quality descriptors from limited training data. To train and evaluate ConKeD, we combine these descriptors with domain-specific keypoints, particularly blood vessel bifurcations and crossovers, that are detected using a deep neural network. Our experimental results demonstrate the benefits of the novel multi-positive multi-negative strategy, as it outperforms the widely used triplet loss technique (single-positive and single-negative) as well as the single-positive multi-negative alternative. Additionally, the combination of ConKeD with the domain-specific keypoints produces comparable results to the state-of-the-art methods for retinal image registration, while offering important advantages such as avoiding pre-processing, utilizing fewer training samples, and requiring fewer detected keypoints, among others. Therefore, ConKeD shows a promising potential towards facilitating the development and application of deep learning-based methods for retinal image registration. ",
        "title": "ConKeD: Multiview contrastive descriptor learning for keypoint-based  retinal image registration",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05902",
        "abstract_url": "http://arxiv.org/abs/2401.05902",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Weihang"
            },
            {
                "last_name": "Shikh-Bahaei",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers downlink incremental redundancy Hybrid Automatic Repeat Request (IR-HARQ) over unreliable feedback channels. Since the impact of positive feedback (i.e., ACK) error is smaller than that of negative feedback (i.e., NACK) error, an asymmetric feedback detection scheme is proposed to protect NACK and further reduce the outage probability. We formulate the HARQ process as a Markov Decision Process (MDP) model to adapt to the transmission rate of each transmission attempt without enriched feedback and additional feedback cost. We aim to optimize the performance of HARQ process under certain outage probability requirements by finding optimal asymmetric detection thresholds. Numerical results obtained on the downlink Rayleigh fading channel and 5G new radio (NR) PUCCH feedback channel show that by applying asymmetric feedback detection and adaptive rate allocation, higher throughput can be achieved under outage probability limitations. ",
        "title": "Optimized Asymmetric Feedback Detection for Rate-adaptive HARQ with  Unreliable Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05906",
        "abstract_url": "http://arxiv.org/abs/2401.05906",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Hyunjin"
            },
            {
                "last_name": "Sung",
                "first_name": "Minhyuk"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p improvement in mAP_50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model. ",
        "title": "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05907",
        "abstract_url": "http://arxiv.org/abs/2401.05907",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Kang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanjie"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This article introduces a sliding window model for defocus deblurring that achieves the best performance to date with extremely low memory usage. Named Swintormer, the method utilizes a diffusion model to generate latent prior features that assist in restoring more detailed images. It also extends the sliding window strategy to specialized Transformer blocks for efficient inference. Additionally, we have further optimized Multiply-Accumulate operations (Macs). Compared to the currently top-performing GRL method, our Swintormer model drastically reduces computational complexity from 140.35 GMACs to 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocus deblurring from 27.04 dB to 27.07 dB. This new method allows for the processing of higher resolution images on devices with limited memory, significantly expanding potential application scenarios. The article concludes with an ablation study that provides an in-depth analysis of the impact of each network module on final performance. The source code and model will be available at the following website: https://github.com/bnm6900030/swintormer. ",
        "title": "Efficient Image Deblurring Networks based on Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05908",
        "abstract_url": "http://arxiv.org/abs/2401.05908",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Xuyang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qibin"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Toshihisa"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  With large training datasets and massive amounts of computing sources, large language models (LLMs) achieve remarkable performance in comprehensive and generative ability. Based on those powerful LLMs, the model fine-tuned with domain-specific datasets posseses more specialized knowledge and thus is more practical like medical LLMs. However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English. In this work, we focus on the particular disease of Epilepsy with Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our model is trained from the pre-trained LLM by fine-tuning technique using datasets from the epilepsy domain. The datasets contain knowledge of basic information about disease, common treatment methods and drugs, and important notes in life and work. The experimental results demonstrate that EpilepsyLLM can provide more reliable and specialized medical knowledge responses. ",
        "title": "EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with  Epilepsy Medical Knowledge",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05909",
        "abstract_url": "http://arxiv.org/abs/2401.05909",
        "authors": [
            {
                "last_name": "Pavlichenko",
                "first_name": "Dmytro"
            },
            {
                "last_name": "Ficht",
                "first_name": "Grzegorz"
            },
            {
                "last_name": "Villar-Corrales",
                "first_name": "Angel"
            },
            {
                "last_name": "Denninger",
                "first_name": "Luis"
            },
            {
                "last_name": "Brocker",
                "first_name": "Julia"
            },
            {
                "last_name": "Sinen",
                "first_name": "Tim"
            },
            {
                "last_name": "Schreiber",
                "first_name": "Michael"
            },
            {
                "last_name": "Behnke",
                "first_name": "Sven"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The RoboCup Humanoid League holds annual soccer robot world championships towards the long-term objective of winning against the FIFA world champions by 2050. The participating teams continuously improve their systems. This paper presents the upgrades to our humanoid soccer system, leading our team NimbRo to win the Soccer Tournament in the Humanoid AdultSize League at RoboCup 2023 in Bordeaux, France. The mentioned upgrades consist of: an updated model architecture for visual perception, extended fused angles feedback mechanisms and an additional COM-ZMP controller for walking robustness, and parametric in-walk kicks through waveforms. ",
        "title": "RoboCup 2023 Humanoid AdultSize Winner NimbRo: NimbRoNet3 Visual  Perception and Responsive Gait with Waveform In-walk Kicks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05912",
        "abstract_url": "http://arxiv.org/abs/2401.05912",
        "authors": [
            {
                "last_name": "Santos",
                "first_name": "Wesley Ramos dos"
            },
            {
                "last_name": "Paraboni",
                "first_name": "Ivandre"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This article presents a method for prompt-based mental health screening from a large and noisy dataset of social media text. Our method uses GPT 3.5. prompting to distinguish publications that may be more relevant to the task, and then uses a straightforward bag-of-words text classifier to predict actual user labels. Results are found to be on pair with a BERT mixture of experts classifier, and incurring only a fraction of its computational costs. ",
        "title": "Prompt-based mental health screening from social media text",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05914",
        "abstract_url": "http://arxiv.org/abs/2401.05914",
        "authors": [
            {
                "last_name": "Elkins",
                "first_name": "Sabina"
            },
            {
                "last_name": "Kochmar",
                "first_name": "Ekaterina"
            },
            {
                "last_name": "Cheung",
                "first_name": "Jackie C. K."
            },
            {
                "last_name": "Serban",
                "first_name": "Iulian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the classroom setting. ",
        "title": "How Teachers Can Use Large Language Models and Bloom's Taxonomy to  Create Educational Quizzes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05916",
        "abstract_url": "http://arxiv.org/abs/2401.05916",
        "authors": [
            {
                "last_name": "Heikkinen",
                "first_name": "Mikko"
            },
            {
                "last_name": "Politis",
                "first_name": "Archontis"
            },
            {
                "last_name": "Virtanen",
                "first_name": "Tuomas"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Ambisonics encoding of microphone array signals can enable various spatial audio applications, such as virtual reality or telepresence, but it is typically designed for uniformly-spaced spherical microphone arrays. This paper proposes a method for Ambisonics encoding that uses a deep neural network (DNN) to estimate a signal transform from microphone inputs to Ambisonics signals. The approach uses a DNN consisting of a U-Net structure with a learnable preprocessing as well as a loss function consisting of mean average error, spatial correlation, and energy preservation components. The method is validated on two microphone arrays with regular and irregular shapes having four microphones, on simulated reverberant scenes with multiple sources. The results of the validation show that the proposed method can meet or exceed the performance of a conventional signal-independent Ambisonics encoder on a number of error metrics. ",
        "title": "Neural Ambisonics encoding for compact irregular microphone arrays",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05918",
        "abstract_url": "http://arxiv.org/abs/2401.05918",
        "authors": [
            {
                "last_name": "Boll",
                "first_name": "Bastian"
            },
            {
                "last_name": "Cassel",
                "first_name": "Jonas"
            },
            {
                "last_name": "Albers",
                "first_name": "Peter"
            },
            {
                "last_name": "Petra",
                "first_name": "Stefania"
            },
            {
                "last_name": "Schn\u00f6rr",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  This paper studies a meta-simplex concept and geometric embedding framework for multi-population replicator dynamics. Central results are two embedding theorems which constitute a formal reduction of multi-population replicator dynamics to single-population ones. In conjunction with a robust mathematical formalism, this provides a toolset for analyzing complex multi-population models. Our framework provides a unifying perspective on different population dynamics in the literature which in particular enables to establish a formal link between multi-population and multi-game dynamics. ",
        "title": "A Geometric Embedding Approach to Multiple Games and Multiple  Populations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05925",
        "abstract_url": "http://arxiv.org/abs/2401.05925",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Bin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Ma",
                "first_name": "Yongjia"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaohui"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zejian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method firstly optimizes Gaussian points' position, convariance and color attributes under the supervision of RGB images. After Gaussian Locating, we distill multi-scale DINO features extracted from images through unprojection to each Gaussian, which is then incorporated with spatial features from the fast point features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is applied to the multi-scale fused features to obtain compact segmentation. Experimental results show that our model can perform high-quality zero-shot scene segmentation, as our model outperforms other segmentation methods on both semantic and panoptic segmentation task, meanwhile consumes approximately only 10% segmenting time compared to NeRF-based segmentation. Code and more results will be available at https://David-Dou.github.io/CoSSegGaussians ",
        "title": "CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05928",
        "abstract_url": "http://arxiv.org/abs/2401.05928",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiashuo"
            },
            {
                "last_name": "Xu",
                "first_name": "Chunpu"
            },
            {
                "last_name": "Leong",
                "first_name": "Chak Tou"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specifically, Muffin employs a multifaceted AI feedback module to assess the helpfulness of responses generated by a specific model with consideration of multiple factors. Using contrastive learning, it then reduces the likelihood of the model generating unhelpful responses compared to the helpful ones. Experimental results demonstrate that Muffin effectively mitigates the generation of unhelpful responses while slightly increasing response fluency and relevance. ",
        "title": "Mitigating Unhelpfulness in Emotional Support Conversations with  Multifaceted AI Feedback",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05930",
        "abstract_url": "http://arxiv.org/abs/2401.05930",
        "authors": [
            {
                "last_name": "Kai",
                "first_name": "Jushi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianhang"
            },
            {
                "last_name": "Hu",
                "first_name": "Hai"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhouhan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts. Significant and consistent improvements are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks. ",
        "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05932",
        "abstract_url": "http://arxiv.org/abs/2401.05932",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Langwen"
            },
            {
                "last_name": "Gianinazzi",
                "first_name": "Lukas"
            },
            {
                "last_name": "Yu",
                "first_name": "Yuejiang"
            },
            {
                "last_name": "Dueben",
                "first_name": "Peter D."
            },
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss of lead time of at most 24 hours when compared to initial conditions of state-of-the-art data assimilation suites. This enables to apply the method to real world applications such as the creation of reanalysis datasets with autoregressive data assimilation. ",
        "title": "DiffDA: a diffusion model for weather-scale data assimilation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05933",
        "abstract_url": "http://arxiv.org/abs/2401.05933",
        "authors": [
            {
                "last_name": "Aribe Jr.",
                "first_name": "Sales G."
            },
            {
                "last_name": "Gerardo",
                "first_name": "Bobby D."
            },
            {
                "last_name": "Medina",
                "first_name": "Ruji P."
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific. Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties. Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program. In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines. After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273. Additionally, there is very little difference between observed and anticipated HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well as a greater coefficient of determination. Further research revealed that the Philippines seems far from achieving Sustainable Development Goal 3 of Project 2030 due to an increase in the nations rate of new HIV infections. Despite the detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the Philippine government, under the Marcos administration, must continue to adhere to the United Nations 90-90-90 targets by enhancing its ART program and ensuring that all vital health services are readily accessible and available. ",
        "title": "Time Series Forecasting of HIV/AIDS in the Philippines Using Deep  Learning: Does COVID-19 Epidemic Matter?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05938",
        "abstract_url": "http://arxiv.org/abs/2401.05938",
        "authors": [
            {
                "last_name": "Picasarri-Arrieta",
                "first_name": "Lucas"
            },
            {
                "last_name": "Rambaud",
                "first_name": "Cl\u00e9ment"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  Aboulker et al. proved that a digraph with large enough dichromatic number contains any fixed digraph as a subdivision. The dichromatic number of a digraph is the smallest order of a partition of its vertex set into acyclic induced subdigraphs. A digraph is dicritical if the removal of any arc or vertex decreases its dichromatic number. In this paper we give sufficient conditions on a dicritical digraph of large order or large directed girth to contain a given digraph as a subdivision. In particular, we prove that (i) for every integers $k,\\ell$, large enough dicritical digraphs with dichromatic number $k$ contain an orientation of a cycle with at least $\\ell$ vertices; (ii) there are functions $f,g$ such that for every subdivision $F^*$ of a digraph $F$, digraphs with directed girth at least $f(F^*)$ and dichromatic number at least $g(F)$ contain a subdivision of $F^*$, and if $F$ is a tree, then $g(F)=|V(F)|$; (iii) there is a function $f$ such that for every subdivision $F^*$ of $TT_3$ (the transitive tournament on three vertices), digraphs with directed girth at least $f(F^*)$ and minimum out-degree at least $2$ contain $F^*$ as a subdivision. ",
        "title": "Subdivisions in dicritical digraphs with large order or digirth",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05939",
        "abstract_url": "http://arxiv.org/abs/2401.05939",
        "authors": [
            {
                "last_name": "Chatterjee",
                "first_name": "Shubham"
            },
            {
                "last_name": "Mackie",
                "first_name": "Iain"
            },
            {
                "last_name": "Dalton",
                "first_name": "Jeff"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a \"hybrid\" representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach. ",
        "title": "DREQ: Document Re-Ranking Using Entity-based Query Understanding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05940",
        "abstract_url": "http://arxiv.org/abs/2401.05940",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ziyu"
            },
            {
                "last_name": "Shin",
                "first_name": "Donghwan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.   In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. We apply different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. We then use these pairs to test the ability of LLMs to correctly detect the inconsistencies.   We propose a new LLM testing method, called Mutation-based Consistency Testing (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of six programming languages (Python, C++, Java, Go, JavaScript, and Rust). We compare the performance of the LLMs across different types of code mutations and programming languages and analyze the results. We find that the LLMs show significant variation in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. ",
        "title": "Mutation-based Consistency Testing for Evaluating the Code Understanding  Capability of LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05943",
        "abstract_url": "http://arxiv.org/abs/2401.05943",
        "authors": [
            {
                "last_name": "Harnes",
                "first_name": "H\u00e5kon"
            },
            {
                "last_name": "Morrison",
                "first_name": "Donn"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  WebAssembly is a low-level bytecode language that allows high-level languages like C, C++, and Rust to be executed in the browser at near-native performance. In recent years, WebAssembly has gained widespread adoption is now natively supported by all modern browsers. However, vulnerabilities in memory-unsafe languages, like C and C++, can translate into vulnerabilities in WebAssembly binaries. Unfortunately, most WebAssembly binaries are compiled from such memory-unsafe languages, and these vulnerabilities have been shown to be practical in real-world scenarios. WebAssembly smart contracts have also been found to be vulnerable, causing significant financial loss. Additionally, WebAssembly has been used for malicious purposes like cryptojacking. To address these issues, several analysis techniques for WebAssembly binaries have been proposed. In this paper, we conduct a comprehensive literature review of these techniques and categorize them based on their analysis strategy and objectives. Furthermore, we compare and evaluate the techniques using quantitative data, highlighting their strengths and weaknesses. In addition, one of the main contributions of this paper is the identification of future research directions based on the thorough literature review conducted. ",
        "title": "SoK: Analysis techniques for WebAssembly",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05946",
        "abstract_url": "http://arxiv.org/abs/2401.05946",
        "authors": [
            {
                "last_name": "Dedieu",
                "first_name": "Antoine"
            },
            {
                "last_name": "Lehrach",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Zhou",
                "first_name": "Guangyao"
            },
            {
                "last_name": "George",
                "first_name": "Dileep"
            },
            {
                "last_name": "L\u00e1zaro-Gredilla",
                "first_name": "Miguel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems. ",
        "title": "Learning Cognitive Maps from Transformer Representations for Efficient  Planning in Partially Observed Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05947",
        "abstract_url": "http://arxiv.org/abs/2401.05947",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhuolun"
            },
            {
                "last_name": "Majumdar",
                "first_name": "Srijoni"
            },
            {
                "last_name": "Pournaras",
                "first_name": "Evangelos"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Conditional Information Reveal (CIR) automates the release of information upon meeting specific pre-defined conditions, such as time or location. This paper advances the understanding and implementation of CIR by introducing a new paradigm to highlight the security challenges in CIR design, and proposes a decentralized architecture as a design guideline for secure CIR systems. Furthermore, in the context of time-sensitive data sharing, this paper proposes a practical timed-release cryptography system employing the proposed architecture and a novel verifiable secret sharing scheme. Key achievements of this study include the creation of an open-source prototype for practical deployment and a comprehensive system evaluation that highlights the enhanced security and efficiency of the proposed system. Furthermore, the paper delves into the application of this system in E-voting scenarios, illustrating its capacity to secure and ensure fair electronic voting processes. ",
        "title": "Blockchain-based Decentralized Time Lock Machines: Automated Reveal of  Time-sensitive Information",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05950",
        "abstract_url": "http://arxiv.org/abs/2401.05950",
        "authors": [
            {
                "last_name": "Trombini",
                "first_name": "Sofia"
            },
            {
                "last_name": "Pasta",
                "first_name": "Edoardo"
            },
            {
                "last_name": "Fagiano",
                "first_name": "Lorenzo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This study investigates deep offshore, pumping Airborne Wind Energy systems, focusing on the kite-platform interaction. The considered system includes a 360 m2 soft-wing kite, connected by a tether to a winch installed on a 10-meter-deep spar with four mooring lines. Wind power is converted into electricity with a feedback controlled periodic trajectory of the kite and corresponding reeling motion of the tether. An analysis of the mutual influence between the platform and the kite dynamics, with different wave regimes, reveals a rather small sensitivity of the flight pattern to the platform oscillations; on the other hand, the frequency of tether force oscillations can be close to the platform resonance peaks, resulting in possible increased fatigue loads and damage of the floating and submerged components. A control design procedure is then proposed to avoid this problem, acting on the kite path planner. Simulation results confirm the effectiveness of the approach. ",
        "title": "On the Kite-Platform Interactions in Offshore Airborne Wind Energy  Systems: Frequency Analysis and Control Approach",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05952",
        "abstract_url": "http://arxiv.org/abs/2401.05952",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Chujie"
            },
            {
                "last_name": "Chen",
                "first_name": "Dongping"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qihui"
            },
            {
                "last_name": "Huang",
                "first_name": "Yue"
            },
            {
                "last_name": "Wan",
                "first_name": "Yao"
            },
            {
                "last_name": "Sun",
                "first_name": "Lichao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  With the remarkable development and widespread applications of large language models (LLMs), the use of machine-generated text (MGT) is becoming increasingly common. This trend brings potential risks, particularly to the quality and completeness of information in fields such as news and education. Current research predominantly addresses the detection of pure MGT without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To confront this challenge, we introduce mixcase, a novel concept representing a hybrid text form involving both machine-generated and human-generated content. We collected mixcase instances generated from multiple daily text-editing scenarios and composed MixSet, the first dataset dedicated to studying these mixed modification scenarios. We conduct experiments to evaluate the efficacy of popular MGT detectors, assessing their effectiveness, robustness, and generalization performance. Our findings reveal that existing detectors struggle to identify mixcase as a separate class or MGT, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixcase, offering valuable insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet. ",
        "title": "LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05956",
        "abstract_url": "http://arxiv.org/abs/2401.05956",
        "authors": [
            {
                "last_name": "Rohwedder",
                "first_name": "Lars"
            },
            {
                "last_name": "Safari",
                "first_name": "Ashkan"
            },
            {
                "last_name": "Vredeveld",
                "first_name": "Tjark"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains. In this paper, we address a scheduling problem on two identical parallel machines with the objective of \\emph{makespan minimization}. For this problem, we consider a local search neighborhood, called \\emph{$k$-swap}, which is a more generalized version of the widely-used \\emph{swap} and \\emph{jump} neighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines in our schedule. First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm. Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood. For the case $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound on the number of local search steps, and for the case $k = 3$, we provide an exponential lower bound. Finally, we conduct computational experiments on various families of instances, and we discuss extensions to more than two machines in our schedule. ",
        "title": "A k-swap Local Search for Makespan Scheduling",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05960",
        "abstract_url": "http://arxiv.org/abs/2401.05960",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xijun"
            },
            {
                "last_name": "Zhu",
                "first_name": "Fangzhou"
            },
            {
                "last_name": "Zhen",
                "first_name": "Hui-Ling"
            },
            {
                "last_name": "Luo",
                "first_name": "Weilin"
            },
            {
                "last_name": "Lu",
                "first_name": "Meng"
            },
            {
                "last_name": "Huang",
                "first_name": "Yimin"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhenan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zirui"
            },
            {
                "last_name": "Kuang",
                "first_name": "Yufei"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhihai"
            },
            {
                "last_name": "Geng",
                "first_name": "Zijie"
            },
            {
                "last_name": "Li",
                "first_name": "Yang"
            },
            {
                "last_name": "Liu",
                "first_name": "Haoyang"
            },
            {
                "last_name": "An",
                "first_name": "Zhiwu"
            },
            {
                "last_name": "Yang",
                "first_name": "Muming"
            },
            {
                "last_name": "Li",
                "first_name": "Jianshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Jie"
            },
            {
                "last_name": "Yan",
                "first_name": "Junchi"
            },
            {
                "last_name": "Sun",
                "first_name": "Defeng"
            },
            {
                "last_name": "Zhong",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yong"
            },
            {
                "last_name": "Zeng",
                "first_name": "Jia"
            },
            {
                "last_name": "Yuan",
                "first_name": "Mingxuan"
            },
            {
                "last_name": "Hao",
                "first_name": "Jianye"
            },
            {
                "last_name": "Yao",
                "first_name": "Jun"
            },
            {
                "last_name": "Mao",
                "first_name": "Kun"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries. To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques. We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem. Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments. Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolving and cut selection. Additionally, we detail the incorporation of state-of-the-art parameter tuning algorithms which markedly elevate solver performance. Compared with traditional solvers such as Gurobi and SCIP, our ML-augmented OptVerse AI Solver demonstrates superior speed and precision across both established benchmarks and real-world scenarios, reinforcing the practical imperative and effectiveness of machine learning techniques in mathematical programming solvers. ",
        "title": "Machine Learning Insides OptVerse AI Solver: Design Principles and  Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05961",
        "abstract_url": "http://arxiv.org/abs/2401.05961",
        "authors": [
            {
                "last_name": "Cesarano",
                "first_name": "Carmine"
            },
            {
                "last_name": "Natella",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Application Layer Gateways (ALGs) play a crucial role in securing critical systems, including railways, industrial automation, and defense applications, by segmenting networks at different levels of criticality. However, they require rigorous security testing to prevent software vulnerabilities, not only at the network level but also at the application layer (e.g., deep traffic inspection components). This paper presents a vulnerability-driven methodology for the comprehensive security testing of ALGs. We present the methodology in the context of an industrial case study in the railways domain, and a simulation-based testing environment to support the methodology. ",
        "title": "Securing an Application Layer Gateway: An Industrial Case Study",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05963",
        "abstract_url": "http://arxiv.org/abs/2401.05963",
        "authors": [
            {
                "last_name": "L\u00f3pez-Ure\u00f1a",
                "first_name": "Sergio"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we introduce a novel non-linear uniform subdivision scheme for the generation of curves in $\\mathbb{R}^n$, $n\\geq2$. This scheme is distinguished by its capacity to reproduce second-degree polynomial data on non-uniform grids without necessitating prior knowledge of the grid specificities. Our approach exploits the potential of annihilation operators to infer the underlying grid, thereby obviating the need for end-users to specify such information. We define the scheme in a non-stationary manner, ensuring that it progressively approaches a classical linear scheme as the iteration number increases, all while preserving its polynomial reproduction capability.   The convergence is established through two distinct theoretical methods. Firstly, we propose a new class of schemes, including ours, for which we establish $\\mathcal{C}^1$ convergence by combining results from the analysis of quasilinear schemes and asymptotically equivalent linear non-uniform non-stationary schemes. Secondly, we adapt conventional analytical tools for non-linear schemes to the non-stationary case, allowing us to again conclude the convergence of the proposed class of schemes.   We show its practical usefulness through numerical examples, showing that the generated curves are curvature continuous. ",
        "title": "A uniform non-linear subdivision scheme reproducing polynomials at any  non-uniform grid",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05964",
        "abstract_url": "http://arxiv.org/abs/2401.05964",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hongjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the way to achieve artificial general intelligence in the future. ",
        "title": "An attempt to generate new bridge types from latent space of PixelCNN",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05965",
        "abstract_url": "http://arxiv.org/abs/2401.05965",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shiwei"
            },
            {
                "last_name": "Diao",
                "first_name": "Lansong"
            },
            {
                "last_name": "Wu",
                "first_name": "Chuan"
            },
            {
                "last_name": "Cao",
                "first_name": "Zongyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Siyu"
            },
            {
                "last_name": "Lin",
                "first_name": "Wei"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to train large deep neural networks (DNNs). Few studies have explored its applicability on heterogeneous clusters, to fully exploit available resources for large model learning. This paper presents \\OurSystem, an automated system designed to expedite SPMD DNN training on heterogeneous clusters. \\OurSystem jointly optimizes the tensor sharding strategy, sharding ratios across heterogeneous devices and the communication methods for tensor exchanges for optimized distributed training with SPMD parallelism. We novelly formulate model partitioning as a program synthesis problem, in which we generate a distributed program from scratch on a distributed instruction set that semantically resembles the program designed for a single device, and systematically explore the solution space with an A*-based search algorithm. We derive the optimal tensor sharding ratios by formulating it as a linear programming problem. Additionally, \\OurSystem explores tensor communication optimization in a heterogeneous cluster and integrates it as part of the program synthesis process, for automatically choosing optimal collective communication primitives and applying sufficient factor broadcasting technique. Extensive experiments on representative workloads demonstrate that \\OurSystem achieves up to 2.41x speed-up on heterogeneous clusters. ",
        "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated  Program Synthesis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05967",
        "abstract_url": "http://arxiv.org/abs/2401.05967",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Yihua"
            },
            {
                "last_name": "Shimodaira",
                "first_name": "Hidetoshi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters. ",
        "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph  Embedding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05968",
        "abstract_url": "http://arxiv.org/abs/2401.05968",
        "authors": [
            {
                "last_name": "Chaudhuri",
                "first_name": "Yashwardhan"
            },
            {
                "last_name": "Kumar",
                "first_name": "Ankit"
            },
            {
                "last_name": "Phukan",
                "first_name": "Orchid Chetia"
            },
            {
                "last_name": "Buduru",
                "first_name": "Arun Balaji"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Crowd counting finds direct applications in real-world situations, making computational efficiency and performance crucial. However, most of the previous methods rely on a heavy backbone and a complex downstream architecture that restricts the deployment. To address this challenge and enhance the versatility of crowd-counting models, we introduce two lightweight models. These models maintain the same downstream architecture while incorporating two distinct backbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to extract diverse scale features from a Pre-Trained Model (PTM) and subsequently combine these features seamlessly. This approach empowers our models to achieve improved performance while maintaining a compact and efficient design. With the comparison of our proposed models with previously available state-of-the-art (SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it achieves comparable results while being the most computationally efficient model. Finally, we present a comparative study, an extensive ablation study, along with pruning to show the effectiveness of our models. ",
        "title": "A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd  Counting",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05969",
        "abstract_url": "http://arxiv.org/abs/2401.05969",
        "authors": [
            {
                "last_name": "Strau\u00df",
                "first_name": "Niklas"
            },
            {
                "last_name": "Schubert",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we propose a novel message-passing module for learning future inter-action correlations in the given environment. Thus, the agent can estimate the potential to fine further parking violations after executing an action. We evaluate our method using an environment based on real-world data from Melbourne. Our results show that SATOP consistently outperforms state-of-the-art TOP agents and is able to fine up to 22% more parking offenses. ",
        "title": "Spatial-Aware Deep Reinforcement Learning for the Traveling Officer  Problem",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05971",
        "abstract_url": "http://arxiv.org/abs/2401.05971",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Rouwan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xiaoya"
            },
            {
                "last_name": "Zhu",
                "first_name": "Juelin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xuxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Maojun"
            },
            {
                "last_name": "Yan",
                "first_name": "Shen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets. Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data. To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization. Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space. Experimental results on the new dataset demonstrate the effectiveness of the proposed approach. Code and dataset are available at https://github.com/RingoWRW/UAVD4L ",
        "title": "UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05972",
        "abstract_url": "http://arxiv.org/abs/2401.05972",
        "authors": [
            {
                "last_name": "Gahr",
                "first_name": "Constatin"
            },
            {
                "last_name": "Farcas",
                "first_name": "Ionut-Gabriel"
            },
            {
                "last_name": "Jenko",
                "first_name": "Frank"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "LG"
        ],
        "abstract": "  This paper focuses on the construction of non-intrusive Scientific Machine Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma turbulence simulations. In particular, we propose using Operator Inference (OpInf) to build low-cost physics-based ROMs from data for such simulations. As a representative example, we focus on the Hasegawa-Wakatani (HW) equations used for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a comprehensive perspective of the potential of OpInf to construct accurate ROMs for this model, we consider a setup for the HW equations that leads to the formation of complex, nonlinear, and self-driven dynamics, and perform two sets of experiments. We first use the data obtained via a direct numerical simulation of the HW equations starting from a specific initial condition and train OpInf ROMs for predictions beyond the training time horizon. In the second, more challenging set of experiments, we train ROMs using the same dataset as before but this time perform predictions for six other initial conditions. Our results show that the OpInf ROMs capture the important features of the turbulent dynamics and generalize to new and unseen initial conditions while reducing the evaluation time of the high-fidelity model by up to five orders of magnitude in single-core performance. In the broader context of fusion research, this shows that non-intrusive SciML ROMs have the potential to drastically accelerate numerical studies, which can ultimately enable tasks such as the design and real-time control of optimized fusion devices. ",
        "title": "Learning physics-based reduced models from data for the  Hasegawa-Wakatani equations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05975",
        "abstract_url": "http://arxiv.org/abs/2401.05975",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhu",
                "first_name": "Shihao"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Ma",
                "first_name": "Yingwei"
            },
            {
                "last_name": "Ma",
                "first_name": "Jian"
            },
            {
                "last_name": "Zhong",
                "first_name": "Wenliang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guannan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kejun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xinwang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \\underline{ELCRec}, which integrates representation learning into an \\underline{E}nd-to-end \\underline{L}earnable \\underline{C}lustering framework for \\underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameters. Additionally, we design a clustering loss that guides the networks to differentiate between different cluster centers and pull similar samples towards their respective cluster centers. This allows simultaneous optimization of recommendation and clustering using mini-batch data. Moreover, we leverage the learned cluster centers as self-supervision signals for representation learning, resulting in further enhancement of recommendation performance. Extensive experiments conducted on open benchmarks and industry data validate the superiority, effectiveness, and efficiency of our proposed ELCRec method. Code is available at: https://github.com/yueliu1999/ELCRec. ",
        "title": "End-to-end Learnable Clustering for Intent Learning in Recommendation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05986",
        "abstract_url": "http://arxiv.org/abs/2401.05986",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Yifan"
            },
            {
                "last_name": "Chai",
                "first_name": "Bingxu"
            },
            {
                "last_name": "Yu",
                "first_name": "Siyu"
            },
            {
                "last_name": "Li",
                "first_name": "Ying"
            },
            {
                "last_name": "He",
                "first_name": "Pinjia"
            },
            {
                "last_name": "Jiang",
                "first_name": "Wei"
            },
            {
                "last_name": "Li",
                "first_name": "Jianguo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Due to the sheer size of software logs, developers rely on automated log analysis. Log parsing, which parses semi-structured logs into a structured format, is a prerequisite of automated log analysis. However, existing log parsers are unsatisfactory when applied in practice because: 1) they ignore categories of variables, and 2) have poor generalization ability. To address the limitations of existing approaches, we propose LogPTR, the first end-to-end variable-aware log parser that can extract the static and dynamic parts in logs, and further identify the categories of variables. The key of LogPTR is using pointer network to copy words from the log message. We have performed extensive experiments on 16 public log datasets and the results show that LogPTR outperforms state-of-the-art log parsers both on general log parsing that extracts the log template and variable-aware log parsing that further identifies the category of variables. ",
        "title": "LogPTR: Variable-Aware Log Parsing with Pointer Network",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05987",
        "abstract_url": "http://arxiv.org/abs/2401.05987",
        "authors": [
            {
                "last_name": "von Gladiss",
                "first_name": "Anselm"
            },
            {
                "last_name": "Ahmadian",
                "first_name": "Amir Shayan"
            },
            {
                "last_name": "J\u00fcrjens",
                "first_name": "Jan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Magnetic particle imaging (MPI) is an emerging medical imaging modality which offers a unique combination of high temporal and spatial resolution, sensitivity and biocompatibility. For system-matrix (SM) based image reconstruction in MPI, a huge amount of calibration data needs to be acquired prior to reconstruction in a time-consuming procedure. Conventionally, the data is recorded on-site inside the scanning device, which significantly limits the time that the scanning device is available for patient care in a clinical setting. Due to its size, handling the calibration data can be challenging. To solve these issues of recording and handling the data, data spaces could be used, as it has been shown that the calibration data can be measured in dedicated devices off-site. We propose a data space aimed at improving the efficiency of SM-based image reconstruction in MPI. The data space consists of imaging facilities, calibration data providers and reconstruction experts. Its specifications follow the reference architecture model of international data spaces (IDS). Use-cases of image reconstruction in MPI are formulated. The stakeholders and tasks are listed and mapped to the terminology of IDS. The signal chain in MPI is analysed to identify a minimum information model which is used by the data space. ",
        "title": "Reconstruction as a service: a data space for off-site image  reconstruction in magnetic particle imaging",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05993",
        "abstract_url": "http://arxiv.org/abs/2401.05993",
        "authors": [
            {
                "last_name": "Da R\u00f9",
                "first_name": "Pietro"
            },
            {
                "last_name": "Benoni",
                "first_name": "Arianna"
            },
            {
                "last_name": "Salucci",
                "first_name": "Marco"
            },
            {
                "last_name": "Rocca",
                "first_name": "Paolo"
            },
            {
                "last_name": "Massa",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In the framework of the \"Smart ElectroMagnetic Environment\" (SEME), an innovative strategy leveraging Equivalence Source concepts is introduced for enhancing the performance of large-scale outdoor wireless communication systems. The proposed Opportunistic Sources Synthesis (OSS) approach is aimed at unconventionally synthesizing the primary source (i.e., the base transceiver station (BTS) antenna array), so that the complex scattering phenomena induced in the surrounding scatterers are profitably exploited to enhance the received power within user-defined regions of interest (RoIs). To yield a computationally feasible synthesis process, an innovative \"Embedded-plus-Environment Patterns\" (EPEPs) method is introduced. A set of representative numerical examples, concerned with realistic large-scale outdoor scenarios, is presented to assess the effectiveness and the efficiency of the proposed optimization-driven approach for a realistic SEME implementation. ",
        "title": "An Opportunistic Source Synthesis Method for Smart Electromagnetic  Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05994",
        "abstract_url": "http://arxiv.org/abs/2401.05994",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Qian"
            },
            {
                "last_name": "Chen",
                "first_name": "Jieyang"
            },
            {
                "last_name": "Whitney",
                "first_name": "Ben"
            },
            {
                "last_name": "Liang",
                "first_name": "Xin"
            },
            {
                "last_name": "Reshniak",
                "first_name": "Viktor"
            },
            {
                "last_name": "Banerjee",
                "first_name": "Tania"
            },
            {
                "last_name": "Lee",
                "first_name": "Jaemoon"
            },
            {
                "last_name": "Rangarajan",
                "first_name": "Anand"
            },
            {
                "last_name": "Wan",
                "first_name": "Lipeng"
            },
            {
                "last_name": "Vidal",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Liu",
                "first_name": "Qing"
            },
            {
                "last_name": "Gainaru",
                "first_name": "Ana"
            },
            {
                "last_name": "Podhorszki",
                "first_name": "Norbert"
            },
            {
                "last_name": "Archibald",
                "first_name": "Richard"
            },
            {
                "last_name": "Ranka",
                "first_name": "Sanjay"
            },
            {
                "last_name": "Klasky",
                "first_name": "Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids. With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis. It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures. MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations. ",
        "title": "MGARD: A multigrid framework for high-performance, error-controlled data  compression and refactoring",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05995",
        "abstract_url": "http://arxiv.org/abs/2401.05995",
        "authors": [
            {
                "last_name": "Dasgupta",
                "first_name": "Sankarshan"
            },
            {
                "last_name": "Buckley",
                "first_name": "James"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  In this new digital era, accessibility to real-world events is moving towards web-based modules. This is mostly visible on e-commerce websites where there is limited availability of physical verification. With this unforeseen development, we depend on the verification in the virtual world to influence our decisions. One of the decision making process is deeply based on review reading. Reviews play an important part in this transactional process. And seeking a real review can be very tenuous work for the user. On the other hand, fake review heavily impacts these transaction records of a product. The article presents an implementation of a Siamese network for detecting fake reviews. The fake reviews dataset, consisting of 40K reviews, preprocessed with different techniques. The cleaned data is passed through embeddings generated by MiniLM BERT for contextual relationship and Word2Vec for semantic relationship to form vectors. Further, the embeddings are trained in a Siamese network with LSTM layers connected to fuzzy logic for decision-making. The results show that fake reviews can be detected with high accuracy on a siamese network for prediction and verification. ",
        "title": "A Multi-Embedding Convergence Network on Siamese Architecture for Fake  Reviews",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05998",
        "abstract_url": "http://arxiv.org/abs/2401.05998",
        "authors": [
            {
                "last_name": "Chern",
                "first_name": "Steffi"
            },
            {
                "last_name": "Fan",
                "first_name": "Zhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Andy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics. ",
        "title": "Combating Adversarial Attacks with Multi-Agent Debate",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.05999",
        "abstract_url": "http://arxiv.org/abs/2401.05999",
        "authors": [
            {
                "last_name": "Margarido",
                "first_name": "Solange"
            },
            {
                "last_name": "Roque",
                "first_name": "Lic\u00ednio"
            },
            {
                "last_name": "Machado",
                "first_name": "Penousal"
            },
            {
                "last_name": "Martins",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  In recent years, there has been a growing application of mixed-initiative co-creative approaches in the creation of video games. The rapid advances in the capabilities of artificial intelligence (AI) systems further propel creative collaboration between humans and computational agents. In this tutorial, we present guidelines for researchers and practitioners to develop game design tools with a high degree of mixed-initiative co-creativity (MI-CCy). We begin by reviewing a selection of current works that will serve as case studies and categorize them by the type of game content they address. We introduce the MI-CCy Quantifier, a framework that can be used by researchers and developers to assess co-creative tools on their level of MI-CCy through a visual scheme of quantifiable criteria scales. We demonstrate the usage of the MI-CCy Quantifier by applying it to the selected works. This analysis enabled us to discern prevalent patterns within these tools, as well as features that contribute to a higher level of MI-CCy. We highlight current gaps in MI-CCy approaches within game design, which we propose as pivotal aspects to tackle in the development of forthcoming approaches. ",
        "title": "Boosting Mixed-Initiative Co-Creativity in Game Design: A Tutorial",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06000",
        "abstract_url": "http://arxiv.org/abs/2401.06000",
        "authors": [
            {
                "last_name": "Bian",
                "first_name": "Sizhen"
            },
            {
                "last_name": "Liu",
                "first_name": "Mengxi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Lukowicz",
                "first_name": "Paul"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Due to the fact that roughly sixty percent of the human body is essentially composed of water, the human body is inherently a conductive object, being able to, firstly, form an inherent electric field from the body to the surroundings and secondly, deform the distribution of an existing electric field near the body. Body-area capacitive sensing, also called body-area electric field sensing, is becoming a promising alternative for wearable devices to accomplish certain tasks in human activity recognition and human-computer interaction. Over the last decade, researchers have explored plentiful novel sensing systems backed by the body-area electric field. On the other hand, despite the pervasive exploration of the body-area electric field, a comprehensive survey does not exist for an enlightening guideline. Moreover, the various hardware implementations, applied algorithms, and targeted applications result in a challenging task to achieve a systematic overview of the subject. This paper aims to fill in the gap by comprehensively summarizing the existing works on body-area capacitive sensing so that researchers can have a better view of the current exploration status. To this end, we first sorted the explorations into three domains according to the involved body forms: body-part electric field, whole-body electric field, and body-to-body electric field, and enumerated the state-of-art works in the domains with a detailed survey of the backed sensing tricks and targeted applications. We then summarized the three types of sensing frontends in circuit design, which is the most critical part in body-area capacitive sensing, and analyzed the data processing pipeline categorized into three kinds of approaches. Finally, we described the challenges and outlooks of body-area electric sensing. ",
        "title": "Body-Area Capacitive or Electric Field Sensing for Human Activity  Recognition and Human-Computer Interaction: A Comprehensive Survey",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06003",
        "abstract_url": "http://arxiv.org/abs/2401.06003",
        "authors": [
            {
                "last_name": "Franke",
                "first_name": "Linus"
            },
            {
                "last_name": "R\u00fcckert",
                "first_name": "Darius"
            },
            {
                "last_name": "Fink",
                "first_name": "Laura"
            },
            {
                "last_name": "Stamminger",
                "first_name": "Marc"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage. ",
        "title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06005",
        "abstract_url": "http://arxiv.org/abs/2401.06005",
        "authors": [
            {
                "last_name": "Peters",
                "first_name": "Benjamin"
            },
            {
                "last_name": "DiCarlo",
                "first_name": "James J."
            },
            {
                "last_name": "Gureckis",
                "first_name": "Todd"
            },
            {
                "last_name": "Haefner",
                "first_name": "Ralf"
            },
            {
                "last_name": "Isik",
                "first_name": "Leyla"
            },
            {
                "last_name": "Tenenbaum",
                "first_name": "Joshua"
            },
            {
                "last_name": "Konkle",
                "first_name": "Talia"
            },
            {
                "last_name": "Naselaris",
                "first_name": "Thomas"
            },
            {
                "last_name": "Stachenfeld",
                "first_name": "Kimberly"
            },
            {
                "last_name": "Tavares",
                "first_name": "Zenna"
            },
            {
                "last_name": "Tsao",
                "first_name": "Doris"
            },
            {
                "last_name": "Yildirim",
                "first_name": "Ilker"
            },
            {
                "last_name": "Kriegeskorte",
                "first_name": "Nikolaus"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of \"inference\" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmholtz's sense, where the sensory evidence is evaluated in the context of a generative model of the causal processes giving rise to it. In this conception, vision inverts a generative model through an interrogation of the evidence in a process often thought to involve top-down predictions of sensory data to evaluate the likelihood of alternative hypotheses. The authors include scientists rooted in roughly equal numbers in each of the conceptions and motivated to overcome what might be a false dichotomy between them and engage the other perspective in the realm of theory and experiment. The primate brain employs an unknown algorithm that may combine the advantages of both conceptions. We explain and clarify the terminology, review the key empirical evidence, and propose an empirical research program that transcends the dichotomy and sets the stage for revealing the mysterious hybrid algorithm of primate vision. ",
        "title": "How does the primate brain combine generative and discriminative  computations in vision?",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06009",
        "abstract_url": "http://arxiv.org/abs/2401.06009",
        "authors": [
            {
                "last_name": "Rogers",
                "first_name": "Martin S J"
            },
            {
                "last_name": "Fox",
                "first_name": "Maria"
            },
            {
                "last_name": "Fleming",
                "first_name": "Andrew"
            },
            {
                "last_name": "van Zeeland",
                "first_name": "Louisa"
            },
            {
                "last_name": "Wilkinson",
                "first_name": "Jeremy"
            },
            {
                "last_name": "Hosking",
                "first_name": "J. Scott"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\\_IceD). ViSual\\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery. ViSual\\_IceD outperforms the other networks, with a F1 score 1.60\\% points higher than the next best network, and results indicate that ViSual\\_IceD is selective in the image type it uses during image segmentation. Outputs from ViSual\\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how ViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions. ",
        "title": "Sea ice detection using concurrent multispectral and synthetic aperture  radar imagery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06010",
        "abstract_url": "http://arxiv.org/abs/2401.06010",
        "authors": [
            {
                "last_name": "del Amor",
                "first_name": "Roc\u00edo"
            },
            {
                "last_name": "Silva-Rodr\u00edguez",
                "first_name": "Julio"
            },
            {
                "last_name": "Colomer",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Naranjo",
                "first_name": "Valery"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The development of computer vision solutions for gigapixel images in digital pathology is hampered by significant computational limitations due to the large size of whole slide images. In particular, digitizing biopsies at high resolutions is a time-consuming process, which is necessary due to the worsening results from the decrease in image detail. To alleviate this issue, recent literature has proposed using knowledge distillation to enhance the model performance at reduced image resolutions. In particular, soft labels and features extracted at the highest magnification level are distilled into a model that takes lower-magnification images as input. However, this approach fails to transfer knowledge about the most discriminative image regions in the classification process, which may be lost when the resolution is decreased. In this work, we propose to distill this information by incorporating attention maps during training. In particular, our formulation leverages saliency maps of the target class via grad-CAMs, which guides the lower-resolution Student model to match the Teacher distribution by minimizing the l2 distance between them. Comprehensive experiments on prostate histology image grading demonstrate that the proposed approach substantially improves the model performance across different image resolutions compared to previous literature. ",
        "title": "Attention to detail: inter-resolution knowledge distillation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06019",
        "abstract_url": "http://arxiv.org/abs/2401.06019",
        "authors": [
            {
                "last_name": "Alonso",
                "first_name": "Pablo"
            },
            {
                "last_name": "de Gordoa",
                "first_name": "Jon Ander I\u00f1iguez"
            },
            {
                "last_name": "Ortega",
                "first_name": "Juan Diego"
            },
            {
                "last_name": "Garc\u00eda",
                "first_name": "Sara"
            },
            {
                "last_name": "Iriarte",
                "first_name": "Francisco Javier"
            },
            {
                "last_name": "Nieto",
                "first_name": "Marcos"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time. To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections. UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost. In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs. The proposed method is based on Deep Learning (DL) to segment defects in the image. The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation. To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets. We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios. ",
        "title": "Automatic UAV-based Airport Pavement Inspection Using Mixed Real and  Virtual Scenarios",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06021",
        "abstract_url": "http://arxiv.org/abs/2401.06021",
        "authors": [
            {
                "last_name": "de Groot",
                "first_name": "Oscar"
            },
            {
                "last_name": "Ferranti",
                "first_name": "Laura"
            },
            {
                "last_name": "Gavrila",
                "first_name": "Dariu"
            },
            {
                "last_name": "Alonso-Mora",
                "first_name": "Javier"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Ground robots navigating in complex, dynamic environments must compute collision-free trajectories to avoid obstacles safely and efficiently. Nonconvex optimization is a popular method to compute a trajectory in real-time. However, these methods often converge to locally optimal solutions and frequently switch between different local minima, leading to inefficient and unsafe robot motion. In this work, We propose a novel topology-driven trajectory optimization strategy for dynamic environments that plans multiple distinct evasive trajectories to enhance the robot's behavior and efficiency. A global planner iteratively generates trajectories in distinct homotopy classes. These trajectories are then optimized by local planners working in parallel. While each planner shares the same navigation objectives, they are locally constrained to a specific homotopy class, meaning each local planner attempts a different evasive maneuver. The robot then executes the feasible trajectory with the lowest cost in a receding horizon manner. We demonstrate, on a mobile robot navigating among pedestrians, that our approach leads to faster and safer trajectories than existing planners. ",
        "title": "Topology-Driven Parallel Trajectory Optimization in Dynamic Environments",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06030",
        "abstract_url": "http://arxiv.org/abs/2401.06030",
        "authors": [
            {
                "last_name": "Sheng",
                "first_name": "Lijun"
            },
            {
                "last_name": "Liang",
                "first_name": "Jian"
            },
            {
                "last_name": "He",
                "first_name": "Ran"
            },
            {
                "last_name": "Wang",
                "first_name": "Zilei"
            },
            {
                "last_name": "Tan",
                "first_name": "Tieniu"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Model adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, becoming a popular paradigm due to its great privacy protection. Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples. In this paper, we explore the potential backdoor attacks on model adaptation launched by well-designed poisoning target data. Concretely, we provide two backdoor triggers with two poisoning strategies for different prior knowledge owned by attackers. These attacks achieve a high success rate and keep the normal performance on clean samples in the test stage. To defend against backdoor embedding, we propose a plug-and-play method named MixAdapt, combining it with existing adaptation algorithms. Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of MixAdapt. We hope this work will shed light on the safety of learning with unlabeled data. ",
        "title": "Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and  Defense on Model Adaptation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06031",
        "abstract_url": "http://arxiv.org/abs/2401.06031",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Huaming"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiayu"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhibo"
            },
            {
                "last_name": "Choo",
                "first_name": "Kim-Kwang Raymond"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN ",
        "title": "GE-AdvGAN: Improving the transferability of adversarial samples by  gradient editing-based adversarial generative model",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06034",
        "abstract_url": "http://arxiv.org/abs/2401.06034",
        "authors": [
            {
                "last_name": "Adilazuarda",
                "first_name": "Muhammad Farid"
            },
            {
                "last_name": "Cahyawijaya",
                "first_name": "Samuel"
            },
            {
                "last_name": "Aji",
                "first_name": "Alham Fikri"
            },
            {
                "last_name": "Winata",
                "first_name": "Genta Indra"
            },
            {
                "last_name": "Purwarianti",
                "first_name": "Ayu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. We further introduce AlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the linguistic regularization weights automatically, alleviating the need for hyperparameter search. LinguAlchemy enables better cross-lingual generalization to unseen languages which is vital for better inclusivity and accessibility of PLMs. ",
        "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen  Language Generalization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06035",
        "abstract_url": "http://arxiv.org/abs/2401.06035",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Partha"
            },
            {
                "last_name": "Sanyal",
                "first_name": "Soubhik"
            },
            {
                "last_name": "Schmid",
                "first_name": "Cordelia"
            },
            {
                "last_name": "Sch\u00f6lkopf",
                "first_name": "Bernhard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN) based generator architecture, thereby compensating for the constraints imposed by a smaller generator size. As a result, our model is capable of synthesizing high-fidelity video clips at a resolution of $256\\times256$ pixels, with durations extending to more than $5$ seconds at a frame rate of 30 fps. The efficacy and versatility of our approach are empirically validated through qualitative and quantitative assessments across three different datasets comprising both synthetic and real video clips. ",
        "title": "RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane  Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06040",
        "abstract_url": "http://arxiv.org/abs/2401.06040",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Qipeng"
            },
            {
                "last_name": "Mallick",
                "first_name": "Tanwi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the information from all streams for traffic metrics reconstruction and prediction. Furthermore, road-network-informed graphs and data-driven graph learning are combined to accurately capture spatial correlation. The proposed method can offer well-defined interpretability, powerful learning capability, and competitive forecasting performance on real-world traffic data sets. ",
        "title": "Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for  Traffic Forecasting",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06044",
        "abstract_url": "http://arxiv.org/abs/2401.06044",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Xun"
            },
            {
                "last_name": "Beillahi",
                "first_name": "Sidi Mohamed"
            },
            {
                "last_name": "Minwalla",
                "first_name": "Cyrus"
            },
            {
                "last_name": "Du",
                "first_name": "Han"
            },
            {
                "last_name": "Veneris",
                "first_name": "Andreas"
            },
            {
                "last_name": "Long",
                "first_name": "Fan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  This paper presents OVer, a framework designed to automatically analyze the behavior of decentralized finance (DeFi) protocols when subjected to a \"skewed\" oracle input. OVer firstly performs symbolic analysis on the given contract and constructs a model of constraints. Then, the framework leverages an SMT solver to identify parameters that allow its secure operation. Furthermore, guard statements may be generated for smart contracts that may use the oracle values, thus effectively preventing oracle manipulation attacks. Empirical results show that OVer can successfully analyze all 10 benchmarks collected, which encompass a diverse range of DeFi protocols. Additionally, this paper also illustrates that current parameters utilized in the majority of benchmarks are inadequate to ensure safety when confronted with significant oracle deviations. ",
        "title": "Safeguarding DeFi Smart Contracts against Oracle Deviations",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06047",
        "abstract_url": "http://arxiv.org/abs/2401.06047",
        "authors": [
            {
                "last_name": "Agarwal",
                "first_name": "Pankaj K."
            },
            {
                "last_name": "Raychaudhury",
                "first_name": "Rahul"
            },
            {
                "last_name": "Sintos",
                "first_name": "Stavros"
            },
            {
                "last_name": "Yang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DB"
        ],
        "abstract": "  We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each $R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in [0,1]$ denotes its \\emph{selectivity}. The goal is to compute a small-size \\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots, (q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in [0,1]$ for each $1\\leq j\\leq m$, and $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most \\emph{consistent} with $\\mathcal{Z}$, i.e., $\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\! \\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized. In a database setting, $\\mathcal{Z}$ corresponds to a workload of range queries over some table, together with their observed selectivities (i.e., fraction of tuples returned), and $\\mathcal{D}$ can be used as compact model for approximating the data distribution within the table without accessing the underlying contents.   In this paper, we obtain both upper and lower bounds for this problem. In particular, we show that the problem of finding the best data distribution from selectivity queries is $\\mathsf{NP}$-complete. On the positive side, we describe a Monte Carlo algorithm that constructs, in time $O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discrete distribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that $\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq \\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for $p=1,2,\\infty$) where the minimum is taken over all discrete distributions. We also establish conditional lower bounds, which strongly indicate the infeasibility of relative approximations as well as removal of the exponential dependency on the dimension for additive approximations. This suggests that significant improvements to our algorithm are unlikely. ",
        "title": "Computing Data Distribution from Query Selectivities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06048",
        "abstract_url": "http://arxiv.org/abs/2401.06048",
        "authors": [
            {
                "last_name": "Guettala",
                "first_name": "Walid"
            },
            {
                "last_name": "Guly\u00e1s",
                "first_name": "L\u00e1szl\u00f3"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science. Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes. All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs. The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial features. GNN architectures with higher computational power, like GIN and GATv2, perform well for most augmentation strategies. On the other hand, artificial features with higher information content, like ID or degree, not only consistently outperform other augmentation strategies, but can also help GNN architectures with lower computational power to achieve good performance. ",
        "title": "On the Power of Graph Neural Networks and Feature Augmentation  Strategies to Classify Social Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06052",
        "abstract_url": "http://arxiv.org/abs/2401.06052",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Guanjun"
            },
            {
                "last_name": "Yi",
                "first_name": "Taoran"
            },
            {
                "last_name": "Fang",
                "first_name": "Jiemin"
            },
            {
                "last_name": "Liu",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinggang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code are available at \\url{https://guanjunwu.github.io/HDR-HexPlane/}. ",
        "title": "Fast High Dynamic Range Radiance Fields for Dynamic Scenes",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06056",
        "abstract_url": "http://arxiv.org/abs/2401.06056",
        "authors": [
            {
                "last_name": "Vecchio",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Deschaintre",
                "first_name": "Valentin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth. ",
        "title": "MatSynth: A Modern PBR Materials Dataset",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06059",
        "abstract_url": "http://arxiv.org/abs/2401.06059",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Minhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Ken Ziyu"
            },
            {
                "last_name": "Zhong",
                "first_name": "Ming"
            },
            {
                "last_name": "Schaeffer",
                "first_name": "Rylan"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Siru"
            },
            {
                "last_name": "Han",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Koyejo",
                "first_name": "Sanmi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies. ",
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06066",
        "abstract_url": "http://arxiv.org/abs/2401.06066",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Damai"
            },
            {
                "last_name": "Deng",
                "first_name": "Chengqi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chenggang"
            },
            {
                "last_name": "Xu",
                "first_name": "R. X."
            },
            {
                "last_name": "Gao",
                "first_name": "Huazuo"
            },
            {
                "last_name": "Chen",
                "first_name": "Deli"
            },
            {
                "last_name": "Li",
                "first_name": "Jiashi"
            },
            {
                "last_name": "Zeng",
                "first_name": "Wangding"
            },
            {
                "last_name": "Yu",
                "first_name": "Xingkai"
            },
            {
                "last_name": "Wu",
                "first_name": "Y."
            },
            {
                "last_name": "Xie",
                "first_name": "Zhenda"
            },
            {
                "last_name": "Li",
                "first_name": "Y. K."
            },
            {
                "last_name": "Huang",
                "first_name": "Panpan"
            },
            {
                "last_name": "Luo",
                "first_name": "Fuli"
            },
            {
                "last_name": "Ruan",
                "first_name": "Chong"
            },
            {
                "last_name": "Sui",
                "first_name": "Zhifang"
            },
            {
                "last_name": "Liang",
                "first_name": "Wenfeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations. ",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06070",
        "abstract_url": "http://arxiv.org/abs/2401.06070",
        "authors": [
            {
                "last_name": "Jafarzadeh",
                "first_name": "Siavash"
            },
            {
                "last_name": "Silling",
                "first_name": "Stewart"
            },
            {
                "last_name": "Liu",
                "first_name": "Ning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhongqiang"
            },
            {
                "last_name": "Yu",
                "first_name": "Yue"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural operators, which can act as implicit solution operators of hidden governing equations, have recently become popular tools for learning the responses of complex real-world physical systems. Nevertheless, most neural operator applications have thus far been data-driven and neglect the intrinsic preservation of fundamental physical laws in data. In this work, we introduce a novel integral neural operator architecture called the Peridynamic Neural Operator (PNO) that learns a nonlocal constitutive law from data. This neural operator provides a forward model in the form of state-based peridynamics, with objectivity and momentum balance laws automatically guaranteed. As applications, we demonstrate the expressivity and efficacy of our model in learning complex material behaviors from both synthetic and experimental data sets. We show that, owing to its ability to capture complex responses, our learned neural operator achieves improved accuracy and efficiency compared to baseline models that use predefined constitutive laws. Moreover, by preserving the essential physical laws within the neural network architecture, the PNO is robust in treating noisy data. The method shows generalizability to different domain configurations, external loadings, and discretizations. ",
        "title": "Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model  for Complex Material Responses",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06072",
        "abstract_url": "http://arxiv.org/abs/2401.06072",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Ruilin"
            },
            {
                "last_name": "Gu",
                "first_name": "Tianle"
            },
            {
                "last_name": "Li",
                "first_name": "Haoling"
            },
            {
                "last_name": "Li",
                "first_name": "Junzhe"
            },
            {
                "last_name": "Lin",
                "first_name": "Zicheng"
            },
            {
                "last_name": "Li",
                "first_name": "Jiayi"
            },
            {
                "last_name": "Yang",
                "first_name": "Yujiu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments to explore the key influencing factors when LLMs perform structured temporal knowledge inference tasks. ",
        "title": "Chain of History: Learning and Forecasting with LLMs for Temporal  Knowledge Graph Completion",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06081",
        "abstract_url": "http://arxiv.org/abs/2401.06081",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kun"
            },
            {
                "last_name": "Zhao",
                "first_name": "Wayne Xin"
            },
            {
                "last_name": "Wan",
                "first_name": "Junchen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fuzheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Di"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at \\url{https://github.com/RUCAIBox/RLMEC}. ",
        "title": "Improving Large Language Models via Fine-grained Reinforcement Learning  with Minimum Editing Constraint",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06085",
        "abstract_url": "http://arxiv.org/abs/2401.06085",
        "authors": [
            {
                "last_name": "Smaldore",
                "first_name": "Valentino"
            },
            {
                "last_name": "Zanella",
                "first_name": "Corrado"
            },
            {
                "last_name": "Zullo",
                "first_name": "Ferdinando"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper we will study the action of $\\mathbb{F}_{q^n}^{2 \\times 2}$ on the graph of an $\\mathbb{F}_q$-linear function of $\\mathbb{F}_{q^n}$ into itself. In particular we will see that, under certain combinatorial assumptions, its stabilizer (together with the sum and product of matrices) is a field. We will also see some examples for which this does not happen. Moreover, we will establish a connection between such a stabilizer and the right idealizer of the rank-metric code defined by the linear function and give some structural results in the case in which the polynomials are partially scattered. ",
        "title": "On the stabilizer of the graph of linear functions over finite fields",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06086",
        "abstract_url": "http://arxiv.org/abs/2401.06086",
        "authors": [
            {
                "last_name": "Terawong",
                "first_name": "Chawin"
            },
            {
                "last_name": "Cliff",
                "first_name": "Dave"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE",
            "MA"
        ],
        "abstract": "  We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here show that XGBoost trained in this way can indeed learn profitable betting strategies, and can generalise to learn strategies that outperform each of the set of strategies used for creation of the training data. To foster further research and enhancements, the complete version of our extended BBE, including the XGBoost integration, has been made freely available as an open-source release on GitHub. ",
        "title": "XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an  Agent-Based Model of a Sports Betting Exchange",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06088",
        "abstract_url": "http://arxiv.org/abs/2401.06088",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "K M Sajjadul"
            },
            {
                "last_name": "Nipu",
                "first_name": "Ayesha Siddika"
            },
            {
                "last_name": "Madiraju",
                "first_name": "Praveen"
            },
            {
                "last_name": "Deshpande",
                "first_name": "Priya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on the perplexity score, modified BERTScore, and cosine similarity score. The results show that BioGPT-Large exhibits superior performance compared to the other models. It consistently achieves a remarkably low perplexity score of 1.65 when generating CC, whereas the baseline LSTM model achieves the best perplexity score of 170. Further, we evaluate and assess the proposed models' performance and the outcome of GPT-4.0. Our study demonstrates that utilizing LLMs such as BioGPT, leads to the development of an effective autocompletion tool for generating CC documentation in healthcare settings. ",
        "title": "Autocompletion of Chief Complaints in the Electronic Health Records  using Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06089",
        "abstract_url": "http://arxiv.org/abs/2401.06089",
        "authors": [
            {
                "last_name": "Sao",
                "first_name": "Piyush"
            },
            {
                "last_name": "Prokopenko",
                "first_name": "Andrey"
            },
            {
                "last_name": "Lebrun-Grandi\u00e9",
                "first_name": "Damien"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  This paper presents \\pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \\hdbscan. Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.   \\pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram. This process makes \\pandora asymptotically work-optimal, independent of dendrogram skewness. All steps in \\pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.   Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \\pandora is 2.2$\\times$ faster than the current best-multithreaded implementation, while the GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and 10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora. These advancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over the current best, which only offload MST construction to GPUs and perform multithreaded dendrogram construction. ",
        "title": "PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage  Clustering on GPU",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06091",
        "abstract_url": "http://arxiv.org/abs/2401.06091",
        "authors": [
            {
                "last_name": "McDermott",
                "first_name": "Matthew B. A."
            },
            {
                "last_name": "Hansen",
                "first_name": "Lasse Hyldig"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Angelotti",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Gallifant",
                "first_name": "Jack"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The results expose a significant deficit in empirical backing and a trend of misattributions that have fuelled the widespread acceptance of AUPRC's supposed advantages. Our findings represent a dual contribution: a significant technical advancement in understanding metric behaviors and a stark warning about unchecked assumptions in the ML community. All experiments are accessible at https://github.com/mmcdermott/AUC_is_all_you_need. ",
        "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06098",
        "abstract_url": "http://arxiv.org/abs/2401.06098",
        "authors": [
            {
                "last_name": "Bako",
                "first_name": "Laurent"
            },
            {
                "last_name": "Nadri",
                "first_name": "Madiha"
            },
            {
                "last_name": "Andrieu",
                "first_name": "Vincent"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper discusses a general framework for designing robust state estimators for a class of discrete-time nonlinear systems. We consider systems that may be impacted by impulsive (sparse but otherwise arbitrary) measurement noise sequences. We show that a family of state estimators, robust to this type of undesired signal, can be obtained by minimizing a class of nonsmooth convex functions at each time step. The resulting state observers are defined through proximal operators. We obtain a nonlinear implicit dynamical system in term of estimation error and prove, in the noise-free setting, that it vanishes asymptotically when the minimized loss function and the to-beobserved system enjoy appropriate properties. From a computational perspective, even though the proposed observers can be implemented via efficient numerical procedures, they do not admit closed-form expressions. The paper argues that by adopting appropriate relaxations, simple and fast analytic expressions can be derived. ",
        "title": "Proximal observers for secure state estimation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06104",
        "abstract_url": "http://arxiv.org/abs/2401.06104",
        "authors": [
            {
                "last_name": "Oren",
                "first_name": "Matanel"
            },
            {
                "last_name": "Hassid",
                "first_name": "Michael"
            },
            {
                "last_name": "Adi",
                "first_name": "Yossi"
            },
            {
                "last_name": "Schwartz",
                "first_name": "Roy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that pretrained transformers can be converted into $\\textit{finite}$ multi-state RNNs by fixing the size of their hidden state. We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies. Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\\frac{1}{8}$ of the original cache size. Our results indicate that transformer decoder LLMs often behave in practice as RNNs. They also lay out the option of mitigating one of their most painful computational bottlenecks - the size of their cache memory. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA. ",
        "title": "Transformers are Multi-State RNNs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06105",
        "abstract_url": "http://arxiv.org/abs/2401.06105",
        "authors": [
            {
                "last_name": "Arar",
                "first_name": "Moab"
            },
            {
                "last_name": "Voynov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Hertz",
                "first_name": "Amir"
            },
            {
                "last_name": "Avrahami",
                "first_name": "Omri"
            },
            {
                "last_name": "Fruchter",
                "first_name": "Shlomi"
            },
            {
                "last_name": "Pritch",
                "first_name": "Yael"
            },
            {
                "last_name": "Cohen-Or",
                "first_name": "Daniel"
            },
            {
                "last_name": "Shamir",
                "first_name": "Ariel"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "GR",
            "LG"
        ],
        "abstract": "  Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \\emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our method in multi- and single-shot settings and further show that it can compose multiple subjects or use inspiration from reference images, such as artworks. We compare our approach quantitatively and qualitatively with existing baselines and state-of-the-art techniques. ",
        "title": "PALP: Prompt Aligned Personalization of Text-to-Image Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06109",
        "abstract_url": "http://arxiv.org/abs/2401.06109",
        "authors": [
            {
                "last_name": "Szab\u00f3",
                "first_name": "D\u00e1niel"
            },
            {
                "last_name": "Apers",
                "first_name": "Simon"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DM"
        ],
        "abstract": "  We show that the graph property of having a (very) large $k$-th Betti number $\\beta_k$ for constant $k$ is testable with a constant number of queries in the dense graph model. More specifically, we consider a clique complex defined by an underlying graph and prove that for any $\\varepsilon>0$, there exists $\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta) d_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing $(k+2)$-clique-freeness, which is known to be testable. This complements a result by Elek (2010) showing that Betti numbers are testable in the bounded-degree model. Our result combines the Euler characteristic, matroid theory and the graph removal lemma. ",
        "title": "Holey graphs: very large Betti numbers are testable",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06112",
        "abstract_url": "http://arxiv.org/abs/2401.06112",
        "authors": [
            {
                "last_name": "Yamagiwa",
                "first_name": "Hiroaki"
            },
            {
                "last_name": "Takase",
                "first_name": "Yusuke"
            },
            {
                "last_name": "Shimodaira",
                "first_name": "Hidetoshi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA. ",
        "title": "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed  Embeddings",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06116",
        "abstract_url": "http://arxiv.org/abs/2401.06116",
        "authors": [
            {
                "last_name": "Bolanos",
                "first_name": "Luis"
            },
            {
                "last_name": "Su",
                "first_name": "Shih-Yang"
            },
            {
                "last_name": "Rhodin",
                "first_name": "Helge"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability. ",
        "title": "Gaussian Shadow Casting for Neural Characters",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06118",
        "abstract_url": "http://arxiv.org/abs/2401.06118",
        "authors": [
            {
                "last_name": "Egiazarian",
                "first_name": "Vage"
            },
            {
                "last_name": "Panferov",
                "first_name": "Andrei"
            },
            {
                "last_name": "Kuznedelev",
                "first_name": "Denis"
            },
            {
                "last_name": "Frantar",
                "first_name": "Elias"
            },
            {
                "last_name": "Babenko",
                "first_name": "Artem"
            },
            {
                "last_name": "Alistarh",
                "first_name": "Dan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of \"extreme\" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 perplexity (a .22 improvement) on WikiText2. We release our implementation of Additive Quantization for Language Models AQLM as a baseline to facilitate future research in LLM quantization. ",
        "title": "Extreme Compression of Large Language Models via Additive Quantization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06121",
        "abstract_url": "http://arxiv.org/abs/2401.06121",
        "authors": [
            {
                "last_name": "Maini",
                "first_name": "Pratyush"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhili"
            },
            {
                "last_name": "Schwarzschild",
                "first_name": "Avi"
            },
            {
                "last_name": "Lipton",
                "first_name": "Zachary C."
            },
            {
                "last_name": "Kolter",
                "first_name": "J. Zico"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all. ",
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06122",
        "abstract_url": "http://arxiv.org/abs/2401.06122",
        "authors": [
            {
                "last_name": "Bareeva",
                "first_name": "Dilyara"
            },
            {
                "last_name": "H\u00f6hne",
                "first_name": "Marina M. -C."
            },
            {
                "last_name": "Warnecke",
                "first_name": "Alexander"
            },
            {
                "last_name": "Pirch",
                "first_name": "Lukas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Klaus-Robert"
            },
            {
                "last_name": "Rieck",
                "first_name": "Konrad"
            },
            {
                "last_name": "Bykov",
                "first_name": "Kirill"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which substantiates our findings. ",
        "title": "Manipulating Feature Visualizations with Gradient Slingshots",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06125",
        "abstract_url": "http://arxiv.org/abs/2401.06125",
        "authors": [
            {
                "last_name": "D\u00e6hli",
                "first_name": "Karen M."
            },
            {
                "last_name": "Obead",
                "first_name": "Sarah A"
            },
            {
                "last_name": "Lin",
                "first_name": "Hsuan-Yin"
            },
            {
                "last_name": "Rosnes",
                "first_name": "Eirik"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "IT"
        ],
        "abstract": "  In private computation, a user wishes to retrieve a function evaluation of messages stored on a set of databases without revealing the function's identity to the databases. Obead \\emph{et al.} introduced a capacity outer bound for private nonlinear computation, dependent on the order of the candidate functions. Focusing on private \\emph{quadratic monomial} computation, we propose three methods for ordering candidate functions: a graph edge-coloring method, a graph-distance method, and an entropy-based greedy method. We confirm, via an exhaustive search, that all three methods yield an optimal ordering for $f < 6$ messages. For $6 \\leq f \\leq 12$ messages, we numerically evaluate the performance of the proposed methods compared with a directed random search. For almost all scenarios considered, the entropy-based greedy method gives the smallest gap to the best-found ordering. ",
        "title": "Improved Capacity Outer Bound for Private Quadratic Monomial Computation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06126",
        "abstract_url": "http://arxiv.org/abs/2401.06126",
        "authors": [
            {
                "last_name": "Saunders",
                "first_name": "Jack"
            },
            {
                "last_name": "Namboodiri",
                "first_name": "Vinay"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio. Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption. Existing methods are split into either person-generic or person-specific models. Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets. Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts. Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches. Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures. This method allows for $\\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors. We show that we achieve state-of-the-art in terms of $\\textbf{visual quality}$ and $\\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies. Our prior learning and adaptation method $\\textbf{generalises to limited data}$ better and is more $\\textbf{scalable}$ than existing person-specific models. Our experiments on real-world, limited data scenarios find that our model is preferred over all others. The project page may be found at https://dubbingforeveryone.github.io/ ",
        "title": "Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural  Rendering Priors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06127",
        "abstract_url": "http://arxiv.org/abs/2401.06127",
        "authors": [
            {
                "last_name": "Gong",
                "first_name": "Yifan"
            },
            {
                "last_name": "Zhan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Jin",
                "first_name": "Qing"
            },
            {
                "last_name": "Li",
                "first_name": "Yanyu"
            },
            {
                "last_name": "Idelbayev",
                "first_name": "Yerlan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xian"
            },
            {
                "last_name": "Zharkov",
                "first_name": "Andrey"
            },
            {
                "last_name": "Aberman",
                "first_name": "Kfir"
            },
            {
                "last_name": "Tulyakov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanzhi"
            },
            {
                "last_name": "Ren",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time. Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept. ",
        "title": "E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image  Translation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06129",
        "abstract_url": "http://arxiv.org/abs/2401.06129",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhao",
                "first_name": "Long"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Wu",
                "first_name": "Jialin"
            },
            {
                "last_name": "Chu",
                "first_name": "Chun-Te"
            },
            {
                "last_name": "Miao",
                "first_name": "Hui"
            },
            {
                "last_name": "Schroff",
                "first_name": "Florian"
            },
            {
                "last_name": "Adam",
                "first_name": "Hartwig"
            },
            {
                "last_name": "Liu",
                "first_name": "Ting"
            },
            {
                "last_name": "Gong",
                "first_name": "Boqing"
            },
            {
                "last_name": "Kr\u00e4henb\u00fchl",
                "first_name": "Philipp"
            },
            {
                "last_name": "Yuan",
                "first_name": "Liangzhe"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%. ",
        "title": "Distilling Vision-Language Models on Millions of Videos",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06132",
        "abstract_url": "http://arxiv.org/abs/2401.06132",
        "authors": [
            {
                "last_name": "Akkurt",
                "first_name": "Semih"
            },
            {
                "last_name": "Witherden",
                "first_name": "Freddie"
            },
            {
                "last_name": "Vincent",
                "first_name": "Peter"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF"
        ],
        "abstract": "  In this article, cache blocking is implemented for the Navier Stokes equations with anti-aliasing support on mixed grids in PyFR for CPUs. In particular, cache blocking is used as an alternative to kernel fusion to eliminate unnecessary data movements between kernels at the main memory level. Specifically, kernels that exchange data are grouped together, and these groups are then executed on small sub-regions of the domain that fit in per-core private data cache. Additionally, cache blocking is also used to efficiently implement a tensor product factorisation of the interpolation operators associated with anti-aliasing. By using cache blocking, the intermediate results between application of the sparse factors are stored in per-core private data cache, and a significant amount of data movement from main memory is avoided. In order to assess the performance gains a theoretical model is developed, and the implementation is benchmarked using a compressible 3D Taylor-Green vortex test case on both hexahedral and prismatic grids, with third- and forth-order solution polynomials. The expected performance gains based on the theoretical model range from 1.99 to 2.62, and the speedups obtained in practice range from 1.67 to 3.67 compared to PyFR v1.11.0. ",
        "title": "Cache Blocking for Flux Reconstruction: Extension to Navier-Stokes  Equations and Anti-aliasing",
        "date": "2023-11-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06133",
        "abstract_url": "http://arxiv.org/abs/2401.06133",
        "authors": [
            {
                "last_name": "Kong",
                "first_name": "Chung To"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Every country must dispose of old banknotes. At the Hong Kong Monetary Authority visitor center, visitors can buy a paperweight souvenir full of shredded banknotes. Even though the shredded banknotes are small, by using computer vision, it is possible to reconstruct the whole banknote like a jigsaw puzzle. Each paperweight souvenir costs $\\$100$ HKD, and it is claimed to contain shredded banknotes equivalent to 138 complete $\\$1000$ HKD banknotes. In theory, $\\$138,000$ HKD can be recovered by using computer vision. This paper discusses the technique of collecting shredded banknote pieces and applying a computer vision program. ",
        "title": "The possibility of making $\\$138,000$ from shredded banknote pieces  using computer vision",
        "date": "2023-11-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06135",
        "abstract_url": "http://arxiv.org/abs/2401.06135",
        "authors": [
            {
                "last_name": "Pase",
                "first_name": "Francesco"
            },
            {
                "last_name": "Giordani",
                "first_name": "Marco"
            },
            {
                "last_name": "Cavallero",
                "first_name": "Sara"
            },
            {
                "last_name": "Schellmann",
                "first_name": "Malte"
            },
            {
                "last_name": "Eichinger",
                "first_name": "Josef"
            },
            {
                "last_name": "Verdone",
                "first_name": "Roberto"
            },
            {
                "last_name": "Zorzi",
                "first_name": "Michele"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG",
            "MA"
        ],
        "abstract": "  Industrial Internet of Things (IIoT) networks will provide Ultra-Reliable Low-Latency Communication (URLLC) to support critical processes underlying the production chains. However, standard protocols for allocating wireless resources may not optimize the latency-reliability trade-off, especially for uplink communication. For example, centralized grant-based scheduling can ensure almost zero collisions, but introduces delays in the way resources are requested by the User Equipments (UEs) and granted by the gNB. In turn, distributed scheduling (e.g., based on random access), in which UEs autonomously choose the resources for transmission, may lead to potentially many collisions especially when the traffic increases. In this work we propose DIStributed combinatorial NEural linear Thompson Sampling (DISNETS), a novel scheduling framework that combines the best of the two worlds. By leveraging a feedback signal from the gNB and reinforcement learning, the UEs are trained to autonomously optimize their uplink transmissions by selecting the available resources to minimize the number of collisions, without additional message exchange to/from the gNB. DISNETS is a distributed, multi-agent adaptation of the Neural Linear Thompson Sampling (NLTS) algorithm, which has been further extended to admit multiple parallel actions. We demonstrate the superior performance of DISNETS in addressing URLLC in IIoT scenarios compared to other baselines. ",
        "title": "A Distributed Neural Linear Thompson Sampling Framework to Achieve URLLC  in Industrial IoT",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06137",
        "abstract_url": "http://arxiv.org/abs/2401.06137",
        "authors": [
            {
                "last_name": "Malinovsk\u00e1",
                "first_name": "Krist\u00edna"
            },
            {
                "last_name": "Holenda",
                "first_name": "Slavom\u00edr"
            },
            {
                "last_name": "Malinovsk\u00fd",
                "first_name": "\u013dudov\u00edt"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE",
            "LG"
        ],
        "abstract": "  Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications. ",
        "title": "QuasiNet: a neural network with trainable product layers",
        "date": "2023-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06139",
        "abstract_url": "http://arxiv.org/abs/2401.06139",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Bohan"
            },
            {
                "last_name": "Wang",
                "first_name": "Yiheng"
            },
            {
                "last_name": "Lu",
                "first_name": "Yuchao"
            },
            {
                "last_name": "Hu",
                "first_name": "Tianzixuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Jinling"
            },
            {
                "last_name": "Houlihan",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce \"Stockformer,\" a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized return of 30.80%, significantly surpassing current state-of-the-art models. Stockformer has emerged as a beacon of innovation in these volatile times, offering investors a potent tool for market forecasting. To advance the field and foster community collaboration, we have open-sourced Stockformer, available at https://github.com/Eric991005/Stockformer. ",
        "title": "StockFormer: A Swing Trading Strategy Based on STL Decomposition and  Self-Attention Networks",
        "date": "2023-11-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06143",
        "abstract_url": "http://arxiv.org/abs/2401.06143",
        "authors": [
            {
                "last_name": "Surmann",
                "first_name": "Hartmut"
            },
            {
                "last_name": "Digakis",
                "first_name": "Niklas"
            },
            {
                "last_name": "Kremer",
                "first_name": "Jan-Nicklas"
            },
            {
                "last_name": "Meine",
                "first_name": "Julien"
            },
            {
                "last_name": "Schulte",
                "first_name": "Max"
            },
            {
                "last_name": "Voigt",
                "first_name": "Niklas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the realm of digital situational awareness during disaster situations, accurate digital representations, like 3D models, play an indispensable role. To ensure the safety of rescue teams, robotic platforms are often deployed to generate these models. In this paper, we introduce an innovative approach that synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller than 30 cm, equipped with 360 degree cameras and the advances of Neural Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D representation of any scene using 2D images and then synthesize it from various angles upon request. This method is especially tailored for urban environments which have experienced significant destruction, where the structural integrity of buildings is compromised to the point of barring entry-commonly observed post-earthquakes and after severe fires. We have tested our approach through recent post-fire scenario, underlining the efficacy of NeRFs even in challenging outdoor environments characterized by water, snow, varying light conditions, and reflective surfaces. ",
        "title": "Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and  Neural Radiance Fields",
        "date": "2023-11-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06144",
        "abstract_url": "http://arxiv.org/abs/2401.06144",
        "authors": [
            {
                "last_name": "Havrilla",
                "first_name": "Alex"
            },
            {
                "last_name": "Rojas",
                "first_name": "Kevin"
            },
            {
                "last_name": "Liao",
                "first_name": "Wenjing"
            },
            {
                "last_name": "Tao",
                "first_name": "Molei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Diffusion generative models have achieved remarkable success in generating images with a fixed resolution. However, existing models have limited ability to generalize to different resolutions when training data at those resolutions are not available. Leveraging techniques from operator learning, we present a novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the score operator by combining both spatial and spectral information at multiple resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1) simultaneously training on multiple resolutions improves FID over training at any single fixed resolution; 2) DFU generalizes beyond its training resolutions, allowing for coherent, high-fidelity generation at higher-resolutions with the same model, i.e. zero-shot super-resolution image-generation; 3) we propose a fine-tuning strategy to further enhance the zero-shot super-resolution image-generation capability of our model, leading to a FID of 11.3 at 1.66 times the maximum training resolution on FFHQ, which no other method can come close to achieving. ",
        "title": "DFU: scale-robust diffusion model for zero-shot super-resolution image  generation",
        "date": "2023-11-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06145",
        "abstract_url": "http://arxiv.org/abs/2401.06145",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Giannoula",
                "first_name": "Christina"
            },
            {
                "last_name": "Wu",
                "first_name": "Jun"
            },
            {
                "last_name": "Elhoushi",
                "first_name": "Mostafa"
            },
            {
                "last_name": "Gleeson",
                "first_name": "James"
            },
            {
                "last_name": "Pekhimenko",
                "first_name": "Gennady"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "CV",
            "LG",
            "PF"
        ],
        "abstract": "  Sparse Convolution (SC) is widely used for processing 3D point clouds that are inherently sparse. Different from dense convolution, SC preserves the sparsity of the input point cloud by only allowing outputs to specific locations. To efficiently compute SC, prior SC engines first use hash tables to build a kernel map that stores the necessary General Matrix Multiplication (GEMM) operations to be executed (Map step), and then use a Gather-GEMM-Scatter process to execute these GEMM operations (GMaS step). In this work, we analyze the shortcomings of prior state-of-the-art SC engines, and propose Minuet, a novel memory-efficient SC engine tailored for modern GPUs. Minuet proposes to (i) replace the hash tables used in the Map step with a novel segmented sorting double-traversed binary search algorithm that highly utilizes the on-chip memory hierarchy of GPUs, (ii) use a lightweight scheme to autotune the tile size in the Gather and Scatter operations of the GMaS step, such that to adapt the execution to the particular characteristics of each SC layer, dataset, and GPU architecture, and (iii) employ a padding-efficient GEMM grouping approach that reduces both memory padding and kernel launching overheads. Our evaluations show that Minuet significantly outperforms prior SC engines by on average $1.74\\times$ (up to $2.22\\times$) for end-to-end point cloud network executions. Our novel segmented sorting double-traversed binary search algorithm achieves superior speedups by $15.8\\times$ on average (up to $26.8\\times$) over prior SC engines in the Map step. The source code of Minuet is publicly available at https://github.com/UofT-EcoSystem/Minuet. ",
        "title": "Minuet: Accelerating 3D Sparse Convolutions on GPUs",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06146",
        "abstract_url": "http://arxiv.org/abs/2401.06146",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Qiao",
                "first_name": "Calvin"
            },
            {
                "last_name": "Ren",
                "first_name": "Guanqiao"
            },
            {
                "last_name": "Yin",
                "first_name": "KangKang"
            },
            {
                "last_name": "Ha",
                "first_name": "Sehoon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Interactive motion synthesis is essential in creating immersive experiences in entertainment applications, such as video games and virtual reality. However, generating animations that are both high-quality and contextually responsive remains a challenge. Traditional techniques in the game industry can produce high-fidelity animations but suffer from high computational costs and poor scalability. Trained neural network models alleviate the memory and speed issues, yet fall short on generating diverse motions. Diffusion models offer diverse motion synthesis with low memory usage, but require expensive reverse diffusion processes. This paper introduces the Accelerated Auto-regressive Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to achieve quality, diversity, and efficiency all together. AAMDM integrates Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a lower-dimensional embedded space rather than the full-dimensional pose space, which reduces the training complexity as well as further improves the performance. We show that AAMDM outperforms existing methods in motion quality, diversity, and runtime efficiency, through comprehensive quantitative analyses and visual comparisons. We also demonstrate the effectiveness of each algorithmic component through ablation studies. ",
        "title": "AAMDM: Accelerated Auto-regressive Motion Diffusion Model",
        "date": "2023-12-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06148",
        "abstract_url": "http://arxiv.org/abs/2401.06148",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Andrew H."
            },
            {
                "last_name": "Jaume",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Williamson",
                "first_name": "Drew F. K."
            },
            {
                "last_name": "Lu",
                "first_name": "Ming Y."
            },
            {
                "last_name": "Vaidya",
                "first_name": "Anurag"
            },
            {
                "last_name": "Miller",
                "first_name": "Tiffany R."
            },
            {
                "last_name": "Mahmood",
                "first_name": "Faisal"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Advances in digitizing tissue slides and the fast-paced progress in artificial intelligence, including deep learning, have boosted the field of computational pathology. This field holds tremendous potential to automate clinical diagnosis, predict patient prognosis and response to therapy, and discover new morphological biomarkers from tissue images. Some of these artificial intelligence-based systems are now getting approved to assist clinical diagnosis; however, technical barriers remain for their widespread clinical adoption and integration as a research tool. This Review consolidates recent methodological advances in computational pathology for predicting clinical end points in whole-slide images and highlights how these developments enable the automation of clinical practice and the discovery of new biomarkers. We then provide future perspectives as the field expands into a broader range of clinical and research tasks with increasingly diverse modalities of clinical data. ",
        "title": "Artificial Intelligence for Digital and Computational Pathology",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06149",
        "abstract_url": "http://arxiv.org/abs/2401.06149",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Yang"
            },
            {
                "last_name": "Dou",
                "first_name": "Weiping"
            },
            {
                "last_name": "Cohen",
                "first_name": "Andrew"
            },
            {
                "last_name": "Bisharat",
                "first_name": "Dia'a"
            },
            {
                "last_name": "Tian",
                "first_name": "Yuandong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Qing Huo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  To extend the antenna design on printed circuit boards (PCBs) for more engineers of interest, we propose a simple method that models PCB antennas with a few basic components. By taking two separate steps to decide their geometric dimensions and positions, antenna prototypes can be facilitated with no experience required. Random sampling statistics relate to the quality of dimensions are used in selecting among dimension candidates. A novel image-based classifier using a convolutional neural network (CNN) is introduced to further determine the positions of these fixed-dimension components. Two examples from wearable products have been chosen to examine the entire workflow. Their final designs are realistic and their performance metrics are not inferior to the ones designed by experienced engineers. ",
        "title": "Image Classifier Based Generative Method for Planar Antenna Design",
        "date": "2023-12-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06150",
        "abstract_url": "http://arxiv.org/abs/2401.06150",
        "authors": [
            {
                "last_name": "Mourchid",
                "first_name": "Youssef"
            },
            {
                "last_name": "Slama",
                "first_name": "Rim"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our proposed approach on the KIMORE and UI-PRMD datasets highlighted its potential, surpassing state-of-the-art methods in terms of accuracy and computational time. This resulted in faster and more accurate learning and assessment of rehabilitation exercises. Additionally, our model provides valuable feedback through qualitative illustrations, effectively highlighting the significance of joints in specific exercises. ",
        "title": "D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on  transformer for assessment of patient physical rehabilitation",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06151",
        "abstract_url": "http://arxiv.org/abs/2401.06151",
        "authors": [
            {
                "last_name": "Morehead",
                "first_name": "Alex"
            },
            {
                "last_name": "Ruffolo",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Bhatnagar",
                "first_name": "Aadyot"
            },
            {
                "last_name": "Madani",
                "first_name": "Ali"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Generative models of macromolecules carry abundant and impactful implications for industrial and biomedical efforts in protein engineering. However, existing methods are currently limited to modeling protein structures or sequences, independently or jointly, without regard to the interactions that commonly occur between proteins and other macromolecules. In this work, we introduce MMDiff, a generative model that jointly designs sequences and structures of nucleic acid and protein complexes, independently or in complex, using joint SE(3)-discrete diffusion noise. Such a model has important implications for emerging areas of macromolecular design including structure-based transcription factor design and design of noncoding RNA sequences. We demonstrate the utility of MMDiff through a rigorous new design benchmark for macromolecular complex generation that we introduce in this work. Our results demonstrate that MMDiff is able to successfully generate micro-RNA and single-stranded DNA molecules while being modestly capable of joint modeling DNA and RNA molecules in interaction with multi-chain protein complexes. Source code: https://github.com/Profluent-Internships/MMDiff. ",
        "title": "Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein  Complexes with SE(3)-Discrete Diffusion",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06152",
        "abstract_url": "http://arxiv.org/abs/2401.06152",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Wonseok"
            },
            {
                "last_name": "Chong",
                "first_name": "Sanggyu"
            },
            {
                "last_name": "Kim",
                "first_name": "Jihan"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In this study, a versatile methodology for initiating polymerization from monomers in highly cross-linked materials is investigated. As polymerization progresses, force-field parameters undergo continuous modification due to the formation of new chemical bonds. This dynamic process not only impacts the atoms directly involved in bonding, but also influences the neighboring atomic environment. Monitoring these complex changes in highly cross-linked structures poses a challenge. To address this issue, we introduce a graph-network-based algorithm that offers both rapid and accurate predictions. The algorithm merges polymer construction protocols with LAMMPS, a large-scale molecular dynamics simulation software. The adaptability of this code has been demonstrated by its successful application to various amorphous polymers, including porous polymer networks (PPNs), and epoxy-resins, while the algorithm has been employed for additional tasks, such as implementing pore-piercing deformations and calculating material properties. ",
        "title": "Graph-Network-Based Predictive Modeling for Highly Cross-Linked Polymer  Systems",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06153",
        "abstract_url": "http://arxiv.org/abs/2401.06153",
        "authors": [
            {
                "last_name": "Yenin",
                "first_name": "Kemal Erdem"
            },
            {
                "last_name": "Sayin",
                "first_name": "Reha Oguz"
            },
            {
                "last_name": "Arar",
                "first_name": "Kuzey"
            },
            {
                "last_name": "Atalay",
                "first_name": "Kadir Kaan"
            },
            {
                "last_name": "Stroppa",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Multi-modal optimization is often encountered in engineering problems, especially when different and alternative solutions are sought. Evolutionary algorithms can efficiently tackle multi-modal optimization thanks to their features such as the concept of population, exploration/exploitation, and being suitable for parallel computation.   This paper introduces a multi-modal optimization version of the Big Bang-Big Crunch algorithm based on clustering, namely, k-BBBC. This algorithm guarantees a complete convergence of the entire population, retrieving on average the 99\\% of local optima for a specific problem. Additionally, we introduce two post-processing methods to (i) identify the local optima in a set of retrieved solutions (i.e., a population), and (ii) quantify the number of correctly retrieved optima against the expected ones (i.e., success rate).   Our results show that k-BBBC performs well even with problems having a large number of optima (tested on 379 optima) and high dimensionality (tested on 32 decision variables). When compared to other multi-modal optimization methods, it outperforms them in terms of accuracy (in both search and objective space) and success rate (number of correctly retrieved optima) -- especially when elitism is applied. Lastly, we validated our proposed post-processing methods by comparing their success rate to the actual one. Results suggest that these methods can be used to evaluate the performance of a multi-modal optimization algorithm by correctly identifying optima and providing an indication of success -- without the need to know where the optima are located in the search space. ",
        "title": "Multi-Modal Optimization with k-Cluster Big Bang-Big Crunch Algorithm",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06154",
        "abstract_url": "http://arxiv.org/abs/2401.06154",
        "authors": [
            {
                "last_name": "Verma",
                "first_name": "Rajat"
            },
            {
                "last_name": "Mittal",
                "first_name": "Shagun"
            },
            {
                "last_name": "Lei",
                "first_name": "Zengxiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaowei"
            },
            {
                "last_name": "Ukkusuri",
                "first_name": "Satish V."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "SI"
        ],
        "abstract": "  Estimation of people's home locations using location-based services data from smartphones is a common task in human mobility assessment. However, commonly used home detection algorithms (HDAs) are often arbitrary and unexamined. In this study, we review existing HDAs and examine five HDAs using eight high-quality mobile phone geolocation datasets. These include four commonly used HDAs as well as an HDA proposed in this work. To make quantitative comparisons, we propose three novel metrics to assess the quality of detected home locations and test them on eight datasets across four U.S. cities. We find that all three metrics show a consistent rank of HDAs' performances, with the proposed HDA outperforming the others. We infer that the temporal and spatial continuity of the geolocation data points matters more than the overall size of the data for accurate home detection. We also find that HDAs with high (and similar) performance metrics tend to create results with better consistency and closer to common expectations. Further, the performance deteriorates with decreasing data quality of the devices, though the patterns of relative performance persist. Finally, we show how the differences in home detection can lead to substantial differences in subsequent inferences using two case studies - (i) hurricane evacuation estimation, and (ii) correlation of mobility patterns with socioeconomic status. Our work contributes to improving the transparency of large-scale human mobility assessment applications. ",
        "title": "Comparison of home detection algorithms using smartphone GPS data",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06155",
        "abstract_url": "http://arxiv.org/abs/2401.06155",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiuyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Guoqing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "LG"
        ],
        "abstract": "  De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT. ",
        "title": "De novo Drug Design using Reinforcement Learning with Multiple GPT  Agents",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06156",
        "abstract_url": "http://arxiv.org/abs/2401.06156",
        "authors": [
            {
                "last_name": "Peleska",
                "first_name": "Jan"
            },
            {
                "last_name": "Br\u00fcning",
                "first_name": "Felix"
            },
            {
                "last_name": "Gleirscher",
                "first_name": "Mario"
            },
            {
                "last_name": "Huang",
                "first_name": "Wen-ling"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This technical report presents research results achieved in the field of verification of trained Convolutional Neural Network (CNN) used for image classification in safety-critical applications. As running example, we use the obstacle detection function needed in future autonomous freight trains with Grade of Automation (GoA) 4. It is shown that systems like GoA 4 freight trains are indeed certifiable today with new standards like ANSI/UL 4600 and ISO 21448 used in addition to the long-existing standards EN 50128 and EN 50129. Moreover, we present a quantitative analysis of the system-level hazard rate to be expected from an obstacle detection function. It is shown that using sensor/perceptor fusion, the fused detection system can meet the tolerable hazard rate deemed to be acceptable for the safety integrity level to be applied (SIL-3). A mathematical analysis of CNN models is performed which results in the identification of classification clusters and equivalence classes partitioning the image input space of the CNN. These clusters and classes are used to introduce a novel statistical testing method for determining the residual error probability of a trained CNN and an associated upper confidence limit. We argue that this greybox approach to CNN verification, taking into account the CNN model's internal structure, is essential for justifying that the statistical tests have covered the trained CNN with its neurons and inter-layer mappings in a comprehensive way. ",
        "title": "A Stochastic Approach to Classification Error Estimates in Convolutional  Neural Networks",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06157",
        "abstract_url": "http://arxiv.org/abs/2401.06157",
        "authors": [
            {
                "last_name": "Monari",
                "first_name": "Dennis"
            },
            {
                "last_name": "Larkin",
                "first_name": "Jack"
            },
            {
                "last_name": "Machado",
                "first_name": "Pedro"
            },
            {
                "last_name": "Bird",
                "first_name": "Jordan J."
            },
            {
                "last_name": "Ihianle",
                "first_name": "Isibor Kennedy"
            },
            {
                "last_name": "Yahaya",
                "first_name": "Salisu Wada"
            },
            {
                "last_name": "Tash",
                "first_name": "Farhad Fassihi"
            },
            {
                "last_name": "Hasan",
                "first_name": "Md Mahmudul"
            },
            {
                "last_name": "Lotfi",
                "first_name": "Ahmad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources and leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90% in certain English counties, making them highly susceptible to extinction. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and discarded plastics in the United Kingdom's river ecosystem's. The UDEEP platform can play a crucial role in environmental monitoring by performing on-the-fly classification of Signal crayfish and plastic debris while leveraging the efficacy of AI, IoT devices and the power of edge computing (i.e., NJN). By providing accurate data on the presence, spread and abundance of these species, the UDEEP platform can contribute to monitoring efforts and aid in mitigating the spread of invasive species. ",
        "title": "UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and  Plastic Detection",
        "date": "2023-12-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06159",
        "abstract_url": "http://arxiv.org/abs/2401.06159",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Chanho"
            },
            {
                "last_name": "Son",
                "first_name": "Jinsu"
            },
            {
                "last_name": "Shon",
                "first_name": "Hyounguk"
            },
            {
                "last_name": "Jeon",
                "first_name": "Yunho"
            },
            {
                "last_name": "Kim",
                "first_name": "Junmo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets in the deformable convolution, thereby enhancing the existing advantages of spatial adaptation. Leveraging full rotation-equivariance, our FRED demonstrates higher robustness to image-level rotation compared to existing methods. Furthermore, we show that FRED is one step closer to non-axis aligned learning through our experiments. Compared to state-of-the-art methods, our proposed method delivers comparable performance on DOTA-v1.0 and outperforms by 1.5 mAP on DOTA-v1.5, all while significantly reducing the model parameters to 16%. ",
        "title": "FRED: Towards a Full Rotation-Equivariance in Aerial Image Object  Detection",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06160",
        "abstract_url": "http://arxiv.org/abs/2401.06160",
        "authors": [
            {
                "last_name": "Nitze",
                "first_name": "Andr\u00e9"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This study explores the impact of Large Language Models (LLMs) in higher education, focusing on an automated oral examination simulation using a prototype. The design considerations of the prototype are described, and the system is evaluated with a select group of educators and students. Technical and pedagogical observations are discussed. The prototype proved to be effective in simulating oral exams, providing personalized feedback, and streamlining educators' workloads. The promising results of the prototype show the potential for LLMs in democratizing education, inclusion of diverse student populations, and improvement of teaching quality and efficiency. ",
        "title": "Future-proofing Education: A Prototype for Simulating Oral Examinations  Using Large Language Models",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06161",
        "abstract_url": "http://arxiv.org/abs/2401.06161",
        "authors": [
            {
                "last_name": "Cabrera",
                "first_name": "Marcelino"
            },
            {
                "last_name": "Cruz",
                "first_name": "Carlos"
            },
            {
                "last_name": "Novoa-Hern\u00e1ndez",
                "first_name": "Pavel"
            },
            {
                "last_name": "Pelta",
                "first_name": "David A."
            },
            {
                "last_name": "Verdegay",
                "first_name": "Jos\u00e9 Luis"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Automated Decision-Making Systems (ADS) have become pervasive across various fields, activities, and occupations, to enhance performance. However, this widespread adoption introduces potential risks, including the misuse of ADS. Such misuse may manifest when ADS is employed in situations where it is unnecessary or when essential requirements, conditions, and terms are overlooked, leading to unintended consequences. This research paper presents a thorough examination of the implications, distinctions, and ethical considerations associated with digitalization, digital transformation, and the utilization of ADS in contemporary society and future contexts. Emphasis is placed on the imperative need for regulation, transparency, and ethical conduct in the deployment of ADS. ",
        "title": "Trustworthy human-centric based Automated Decision-Making Systems",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06162",
        "abstract_url": "http://arxiv.org/abs/2401.06162",
        "authors": [
            {
                "last_name": "Einarsson",
                "first_name": "Alexander"
            },
            {
                "last_name": "Oestmo",
                "first_name": "Simen"
            },
            {
                "last_name": "Wollman",
                "first_name": "Lester"
            },
            {
                "last_name": "Purves",
                "first_name": "Duncan"
            },
            {
                "last_name": "Jenkins",
                "first_name": "Ryan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  In recent years, there has been a revolution in data-driven policing. With that has come scrutiny on how bias in historical data affects algorithmic decision making. In this exploratory work, we introduce a debiasing technique for place-based algorithmic patrol management systems. We show that the technique efficiently eliminates racially biased features while retaining high accuracy in the models. Finally, we provide a lengthy list of potential future research in the realm of fairness and data-driven policing which this work uncovered. ",
        "title": "A debiasing technique for place-based algorithmic patrol management",
        "date": "2023-12-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06164",
        "abstract_url": "http://arxiv.org/abs/2401.06164",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Lezhi"
            },
            {
                "last_name": "Chang",
                "first_name": "Ting-Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hai"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: https://github.com/Firenze11/finance_lm. ",
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "date": "2023-12-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06165",
        "abstract_url": "http://arxiv.org/abs/2401.06165",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Yifan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bingjie"
            },
            {
                "last_name": "Wu",
                "first_name": "Ke"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In this work, simulation-based equations to calculate propagation constant in uniform or periodic structures (SES) are deduced and verified through simulations in various types of structures. The modeling of those structures are essentially based on field distributions from a driven-mode solver, and the field distributions are used as the input parameters of the FPPS. It allows the separation of forward and backward waves from a total wave inside such a uniform or periodic structure, and thus it can be used to calculate the propagation constants inside both uniform and periodic structures even with a strong reflection. In order to test the performance and function of the FPPS, it has been applied to a variety of typical structures, including uniform waveguides, lossfree closed structures, lossy closed structures, and open radiation structures, and compared with the results of eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The comparison shows the easy-to-use and adaptable nature of the FPPS. the FPPS. This FPPS could be also applied to open radiating structures, and even multi-dimensional periodic/uniform structures. ",
        "title": "Simulation-Based Equations for Propagation Constant in Uniform or  Periodic Transmission",
        "date": "2023-12-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06166",
        "abstract_url": "http://arxiv.org/abs/2401.06166",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Yan"
            },
            {
                "last_name": "Cheng",
                "first_name": "Hao"
            },
            {
                "last_name": "Ye",
                "first_name": "Zeliang"
            },
            {
                "last_name": "Feng",
                "first_name": "Ruyi"
            },
            {
                "last_name": "Gu",
                "first_name": "Zhongze"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combined with generative molecular canonicalization pre-training tasks, enhances the validity, novelty, and uniqueness in generative tasks. These features of AdaMR demonstrate its strong performance in numerous downstream tasks. We use different molecular properties prediction tasks on six different datasets on MoleculeNet and two generative tasks on ZINC250K dataset to evaluate our proposed molecular encoding and pre-training methods, and obtain state-of-the-art (SOTA) results on five of these tasks. ",
        "title": "Adjustable Molecular Representation for Unified Pre-training Strategy",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06167",
        "abstract_url": "http://arxiv.org/abs/2401.06167",
        "authors": [
            {
                "last_name": "Che",
                "first_name": "Chang"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Yu",
                "first_name": "Liqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The process of transforming input images into corresponding textual explanations stands as a crucial and complex endeavor within the domains of computer vision and natural language processing. In this paper, we propose an innovative ensemble approach that harnesses the capabilities of Contrastive Language-Image Pretraining models. ",
        "title": "Enhancing Multimodal Understanding with CLIP-Based Image-to-Text  Transformation",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06168",
        "abstract_url": "http://arxiv.org/abs/2401.06168",
        "authors": [
            {
                "last_name": "Sonawane",
                "first_name": "Prathamesh"
            },
            {
                "last_name": "Chheda",
                "first_name": "Arav"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Poker is in the family of imperfect information games unlike other games such as chess, connect four, etc which are perfect information game instead. While many perfect information games have been solved, no non-trivial imperfect information game has been solved to date. This makes poker a great test bed for Artificial Intelligence research. In this paper we firstly compare Game theory optimal poker to Exploitative poker. Secondly, we discuss the intricacies of abstraction techniques, betting models, and specific strategies employed by successful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also explore 2-player vs multi-player games and the limitations that come when playing with more players. Finally, this paper discusses the role of machine learning and theoretical approaches in developing winning strategies and suggests future directions for this rapidly evolving field. ",
        "title": "A Survey on Game Theory Optimal Poker",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06169",
        "abstract_url": "http://arxiv.org/abs/2401.06169",
        "authors": [
            {
                "last_name": "Puget",
                "first_name": "Chlo\u00e9"
            },
            {
                "last_name": "Ganz",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Ostermaier",
                "first_name": "Julian"
            },
            {
                "last_name": "Konrad",
                "first_name": "Thomas"
            },
            {
                "last_name": "Parlak",
                "first_name": "Eda"
            },
            {
                "last_name": "Bertram",
                "first_name": "Christof Albert"
            },
            {
                "last_name": "Kiupel",
                "first_name": "Matti"
            },
            {
                "last_name": "Breininger",
                "first_name": "Katharina"
            },
            {
                "last_name": "Aubreville",
                "first_name": "Marc"
            },
            {
                "last_name": "Klopfleisch",
                "first_name": "Robert"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Numerous prognostic factors are currently assessed histopathologically in biopsies of canine mast cell tumors to evaluate clinical behavior. In addition, PCR analysis of the c-Kit exon 11 mutational status is often performed to evaluate the potential success of a tyrosine kinase inhibitor therapy. This project aimed at training deep learning models (DLMs) to identify the c-Kit-11 mutational status of MCTs solely based on morphology without additional molecular analysis. HE slides of 195 mutated and 173 non-mutated tumors were stained consecutively in two different laboratories and scanned with three different slide scanners. This resulted in six different datasets (stain-scanner variations) of whole slide images. DLMs were trained with single and mixed datasets and their performances was assessed under scanner and staining domain shifts. The DLMs correctly classified HE slides according to their c-Kit 11 mutation status in, on average, 87% of cases for the best-suited stain-scanner variant. A relevant performance drop could be observed when the stain-scanner combination of the training and test dataset differed. Multi-variant datasets improved the average accuracy but did not reach the maximum accuracy of algorithms trained and tested on the same stain-scanner variant. In summary, DLM-assisted morphological examination of MCTs can predict c-Kit-exon 11 mutational status of MCTs with high accuracy. However, the recognition performance is impeded by a change of scanner or staining protocol. Larger data sets with higher numbers of scans originating from different laboratories and scanners may lead to more robust DLMs to identify c-Kit mutations in HE slides. ",
        "title": "Deep Learning model predicts the c-Kit-11 mutational status of canine  cutaneous mast cell tumors by HE stained histological slides",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06171",
        "abstract_url": "http://arxiv.org/abs/2401.06171",
        "authors": [
            {
                "last_name": "Gikunda",
                "first_name": "Kinyua"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper explores the transformative potential of artificial intelligence (AI) in the context of sustainable agricultural development across diverse regions in Africa. Delving into opportunities, challenges, and impact, the study navigates through the dynamic landscape of AI applications in agriculture. Opportunities such as precision farming, crop monitoring, and climate-resilient practices are examined, alongside challenges related to technological infrastructure, data accessibility, and skill gaps. The article analyzes the impact of AI on smallholder farmers, supply chains, and inclusive growth. Ethical considerations and policy implications are also discussed, offering insights into responsible AI integration. By providing a nuanced understanding, this paper contributes to the ongoing discourse on leveraging AI for fostering sustainability in African agriculture. ",
        "title": "Harnessing Artificial Intelligence for Sustainable Agricultural  Development in Africa: Opportunities, Challenges, and Impact",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06172",
        "abstract_url": "http://arxiv.org/abs/2401.06172",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yue"
            },
            {
                "last_name": "Andrew",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Supasanya",
                "first_name": "Salintip"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events. ",
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine  Learning Methods",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06173",
        "abstract_url": "http://arxiv.org/abs/2401.06173",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Hui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jinghong"
            },
            {
                "last_name": "Chen",
                "first_name": "Wentao"
            },
            {
                "last_name": "Wang",
                "first_name": "Huazheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Mengdi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  While modern biotechnologies allow synthesizing new proteins and function measurements at scale, efficiently exploring a protein sequence space and engineering it remains a daunting task due to the vast sequence space of any given protein. Protein engineering is typically conducted through an iterative process of adding mutations to the wild-type or lead sequences, recombination of mutations, and running new rounds of screening. To enhance the efficiency of such a process, we propose a tree search-based bandit learning method, which expands a tree starting from the initial sequence with the guidance of a bandit machine learning model. Under simplified assumptions and a Gaussian Process prior, we provide theoretical analysis and a Bayesian regret bound, demonstrating that the combination of local search and bandit learning method can efficiently discover a near-optimal design. The full algorithm is compatible with a suite of randomized tree search heuristics, machine learning models, pre-trained embeddings, and bandit techniques. We test various instances of the algorithm across benchmark protein datasets using simulated screens. Experiment results demonstrate that the algorithm is both sample-efficient and able to find top designs using reasonably small mutation counts. ",
        "title": "Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06174",
        "abstract_url": "http://arxiv.org/abs/2401.06174",
        "authors": [
            {
                "last_name": "Ghezelbash",
                "first_name": "Farshid"
            },
            {
                "last_name": "Eskandari",
                "first_name": "Amir Hossein"
            },
            {
                "last_name": "Robert-Lachaine",
                "first_name": "Xavier"
            },
            {
                "last_name": "Cao",
                "first_name": "Frank"
            },
            {
                "last_name": "Pesteie",
                "first_name": "Mehran"
            },
            {
                "last_name": "Qiao",
                "first_name": "Zhuohua"
            },
            {
                "last_name": "Shirazi-Adl",
                "first_name": "Aboulfazl"
            },
            {
                "last_name": "Larivi\u00e8re",
                "first_name": "Christian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Spine biomechanics is at a transformation with the advent and integration of machine learning and computer vision technologies. These novel techniques facilitate the estimation of 3D body shapes, anthropometrics, and kinematics from as simple as a single-camera image, making them more accessible and practical for a diverse range of applications. This study introduces a framework that merges these methodologies with traditional musculoskeletal modeling, enabling comprehensive analysis of spinal biomechanics during complex activities from a single camera. Additionally, we aim to evaluate their performance and limitations in spine biomechanics applications. The real-world applications explored in this study include assessment in workplace lifting, evaluation of whiplash injuries in car accidents, and biomechanical analysis in professional sports. Our results demonstrate potential and limitations of various algorithms in estimating body shape, kinematics, and conducting in-field biomechanical analyses. In industrial settings, the potential to utilize these new technologies for biomechanical risk assessments offers a pathway for preventive measures against back injuries. In sports activities, the proposed framework provides new opportunities for performance optimization, injury prevention, and rehabilitation. The application in forensic domain further underscores the wide-reaching implications of this technology. While certain limitations were identified, particularly in accuracy of predictions, complex interactions, and external load estimation, this study demonstrates their potential for advancement in spine biomechanics, heralding an optimistic future in both research and practical applications. ",
        "title": "Machine Learning Applications in Spine Biomechanics",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06175",
        "abstract_url": "http://arxiv.org/abs/2401.06175",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jinyang"
            },
            {
                "last_name": "Gu",
                "first_name": "Wenwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuangbin"
            },
            {
                "last_name": "Li",
                "first_name": "Yichen"
            },
            {
                "last_name": "Su",
                "first_name": "Yuxin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Michael R."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Key Performance Indicators (KPIs) are essential time-series metrics for ensuring the reliability and stability of many software systems. They faithfully record runtime states to facilitate the understanding of anomalous system behaviors and provide informative clues for engineers to pinpoint the root causes. The unprecedented scale and complexity of modern software systems, however, make the volume of KPIs explode. Consequently, many traditional methods of KPI anomaly detection become impractical, which serves as a catalyst for the fast development of machine learning-based solutions in both academia and industry. However, there is currently a lack of rigorous comparison among these KPI anomaly detection methods, and re-implementation demands a non-trivial effort. Moreover, we observe that different works adopt independent evaluation processes with different metrics. Some of them may not fully reveal the capability of a model and some are creating an illusion of progress. To better understand the characteristics of different KPI anomaly detectors and address the evaluation issue, in this paper, we provide a comprehensive review and evaluation of twelve state-of-the-art methods, and propose a novel metric called salience. Particularly, the selected methods include five traditional machine learning-based methods and seven deep learning-based methods. These methods are evaluated with five multivariate KPI datasets that are publicly available. A unified toolkit with easy-to-use interfaces is also released. We report the benchmark results in terms of accuracy, salience, efficiency, and delay, which are of practical importance for industrial deployment. We believe our work can contribute as a basis for future academic research and industrial application. ",
        "title": "MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly  Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06176",
        "abstract_url": "http://arxiv.org/abs/2401.06176",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Luzhi"
            },
            {
                "last_name": "He",
                "first_name": "Dongxiao"
            },
            {
                "last_name": "Zhang",
                "first_name": "He"
            },
            {
                "last_name": "Liu",
                "first_name": "Yixin"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Pan",
                "first_name": "Shirui"
            },
            {
                "last_name": "Jin",
                "first_name": "Di"
            },
            {
                "last_name": "Chua",
                "first_name": "Tat-Seng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets. The code is available at Github: https://github.com/Ee1s/GOODAT ",
        "title": "GOODAT: Towards Test-time Graph Out-of-Distribution Detection",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06178",
        "abstract_url": "http://arxiv.org/abs/2401.06178",
        "authors": [
            {
                "last_name": "Goetze",
                "first_name": "Trystan S."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Since the launch of applications such as DALL-E, Midjourney, and Stable Diffusion, generative artificial intelligence has been controversial as a tool for creating artwork. While some have presented longtermist worries about these technologies as harbingers of fully automated futures to come, more pressing is the impact of generative AI on creative labour in the present. Already, business leaders have begun replacing human artistic labour with AI-generated images. In response, the artistic community has launched a protest movement, which argues that AI image generation is a kind of theft. This paper analyzes, substantiates, and critiques these arguments, concluding that AI image generators involve an unethical kind of labour theft. If correct, many other AI applications also rely upon theft. ",
        "title": "AI Art is Theft: Labour, Extraction, and Exploitation, Or, On the  Dangers of Stochastic Pollocks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06179",
        "abstract_url": "http://arxiv.org/abs/2401.06179",
        "authors": [
            {
                "last_name": "Montazeri",
                "first_name": "Sina"
            },
            {
                "last_name": "Mirzaeinia",
                "first_name": "Akram"
            },
            {
                "last_name": "Jumakhan",
                "first_name": "Haseebullah"
            },
            {
                "last_name": "Mirzaeinia",
                "first_name": "Amir"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards. ",
        "title": "CNN-DRL for Scalable Actions in Finance",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06180",
        "abstract_url": "http://arxiv.org/abs/2401.06180",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jingyun"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yading"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  Federated learning (FL) has emerged as a promising strategy for collaboratively training complicated machine learning models from different medical centers without the need of data sharing. However, the traditional FL relies on a central server to orchestrate the global model training among clients. This makes it vulnerable to the failure of the model server. Meanwhile, the model trained based on the global data property may not yield the best performance on the local data of a particular site due to the variations of data characteristics among them. To address these limitations, we proposed Gossip Mutual Learning(GML), a decentralized collaborative learning framework that employs Gossip Protocol for direct peer-to-peer communication and encourages each site to optimize its local model by leveraging useful information from peers through mutual learning. On the task of tumor segmentation on PET/CT images using HECKTOR21 dataset with 223 cases from five clinical sites, we demonstrated GML could improve tumor segmentation performance in terms of Dice Similarity Coefficient (DSC) by 3.2%, 4.6% and 10.4% on site-specific testing cases as compared to three baseline methods: pooled training, FedAvg and individual training, respectively. We also showed GML has comparable generalization performance as pooled training and FedAvg when applying them on 78 cases from two out-of-sample sites where no case was used for model training. In our experimental setup, GML showcased a sixfold decrease in communication overhead compared to FedAvg, requiring only 16.67% of the total communication overhead. ",
        "title": "Decentralized Gossip Mutual Learning (GML) for automatic head and neck  tumor segmentation",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06182",
        "abstract_url": "http://arxiv.org/abs/2401.06182",
        "authors": [
            {
                "last_name": "Dai",
                "first_name": "Baiyang"
            },
            {
                "last_name": "Yang",
                "first_name": "Jiamin"
            },
            {
                "last_name": "Shroff",
                "first_name": "Hari"
            },
            {
                "last_name": "La Riviere",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Determining cell identities in imaging sequences is an important yet challenging task. The conventional method for cell identification is via cell tracking, which is complex and can be time-consuming. In this study, we propose an innovative approach to cell identification during early C. elegans embryogenesis using machine learning. We employed random forest, MLP, and LSTM models, and tested cell classification accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of embryogenesis. By leveraging a small number of spatial-temporal features of individual cells, including cell trajectory and cell fate information, our models achieve an accuracy of over 90%, even with limited data. We also determine the most important feature contributions and can interpret these features in the context of biological knowledge. Our research demonstrates the success of predicting cell identities in 4D imaging sequences directly from simple spatio-temporal features. ",
        "title": "Prediction of Cellular Identities from Trajectory and Cell Fate  Information",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06183",
        "abstract_url": "http://arxiv.org/abs/2401.06183",
        "authors": [
            {
                "last_name": "Tathe",
                "first_name": "Aniket"
            },
            {
                "last_name": "Kamble",
                "first_name": "Anand"
            },
            {
                "last_name": "Kumbharkar",
                "first_name": "Suyash"
            },
            {
                "last_name": "Bhandare",
                "first_name": "Atharva"
            },
            {
                "last_name": "Mitra",
                "first_name": "Anirban C."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio. ",
        "title": "End to end Hindi to English speech conversion using Bark, mBART and a  finetuned XLSR Wav2Vec2",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06184",
        "abstract_url": "http://arxiv.org/abs/2401.06184",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Wu",
                "first_name": "Yanan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we construct a new family of distance-optimal binary cyclic codes with the minimum distance $6$ and a new family of distance-optimal quaternary cyclic codes with the minimum distance $4$. We also construct several families of cyclic and negacyclic codes over ${\\bf F}_2$, ${\\bf F}_3$, ${\\bf F}_4$, ${\\bf F}_5$, ${\\bf F}_7$ and ${\\bf F}_9$ with good parameters $n,\\,k,\\,d$, such that the maximal possible minimum distance $d_{max}$ of a linear $[n, k]_q$ code is at most $d_{max} \\leq d+8$. The first codes in these families have optimal or best known minimum distances. $145$ optimal or best known codes are constructed as cyclic codes, negacyclic codes, their shortening codes and punctured codes. All optimal or best known codes constructed in this paper are not equivalent to the presently best known codes. Several infinite families of negacyclic $[n,\\frac{n+1}{2}, d]_q$ codes or $[n, \\frac{n}{2}, d]_q$ codes, such that their minimum distances satisfy $d\\approx O(\\frac{n}{\\log_q n})$, are also constructed. These are first several families of such negacyclic codes. ",
        "title": "Cyclic and Negacyclic Codes with Optimal or Best Known Minimum Distances",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06187",
        "abstract_url": "http://arxiv.org/abs/2401.06187",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jing"
            },
            {
                "last_name": "Harandi",
                "first_name": "Mehrtash"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve information on the remaining data while discarding information related to the forgetting data. Our experimental results, conducted across five distinct datasets and utilizing both CNN and ViT, demonstrate that Scissorhands, despite utilizing only a limited portion of the training data, showcases competitive performance when compared to existing methods. ",
        "title": "Scissorhands: Scrub Data Influence via Connection Sensitivity in  Networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06188",
        "abstract_url": "http://arxiv.org/abs/2401.06188",
        "authors": [
            {
                "last_name": "Vallero",
                "first_name": "Marzio"
            },
            {
                "last_name": "Vella",
                "first_name": "Flavio"
            },
            {
                "last_name": "Rech",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The frontier of quantum computing (QC) simulation on classical hardware is quickly reaching the hard scalability limits for computational feasibility. Nonetheless, there is still a need to simulate large quantum systems classically, as the Noisy Intermediate Scale Quantum (NISQ) devices are yet to be considered fault tolerant and performant enough in terms of operations per second. Each of the two main exact simulation techniques, state vector and tensor network simulators, boasts specific limitations. The exponential memory requirement of state vector simulation, when compared to the qubit register sizes of currently available quantum computers, quickly saturates the capacity of the top HPC machines currently available. Tensor network contraction approaches, which encode quantum circuits into tensor networks and then contract them over an output bit string to obtain its probability amplitude, still fall short of the inherent complexity of finding an optimal contraction path, which maps to a max-cut problem on a dense mesh, a notably NP-hard problem.   This article aims at investigating the limits of current state-of-the-art simulation techniques on a test bench made of eight widely used quantum subroutines, each in 31 different configurations, with special emphasis on performance. We then correlate the performance measures of the simulators with the metrics that characterise the benchmark circuits, identifying the main reasons behind the observed performance trend. From our observations, given the structure of a quantum circuit and the number of qubits, we highlight how to select the best simulation strategy, obtaining a speedup of up to an order of magnitude. ",
        "title": "State of practice: evaluating GPU performance of state vector and tensor  network methods",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06191",
        "abstract_url": "http://arxiv.org/abs/2401.06191",
        "authors": [
            {
                "last_name": "Khatib",
                "first_name": "Rajaei"
            },
            {
                "last_name": "Giryes",
                "first_name": "Raja"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, the neural radiance field (NeRF) model has gained popularity due to its ability to recover complex 3D scenes. Following its success, many approaches proposed different NeRF representations in order to further improve both runtime and performance. One such example is Triplane, in which NeRF is represented using three 2D feature planes. This enables easily using existing 2D neural networks in this framework, e.g., to generate the three planes. Despite its advantage, the triplane representation lagged behind in its 3D recovery quality compared to NeRF solutions. In this work, we propose TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF, which closes the 3D recovery performance gap and is competitive with current state-of-the-art methods. Building upon the triplane framework, we also propose a novel super-resolution (SR) technique that combines a diffusion model with TriNeRFLet for improving NeRF resolution. ",
        "title": "TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06192",
        "abstract_url": "http://arxiv.org/abs/2401.06192",
        "authors": [
            {
                "last_name": "Christensen",
                "first_name": "Kristoffer"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng Grace"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Electric vehicles are expected to significantly contribute to CO2-eq. emissions reduction, but the increasing number of EVs also introduces chal-lenges to the energy system, and to what extent it contributes to achieving cli-mate goals remains unknown. Static modeling and assumption-based simula-tions have been used for such investigation, but they cannot capture the realistic ecosystem dynamics. To fill the gap, this paper investigates the impacts of two adoption curves of private EVs on the electricity distribution grids and national climate goals. This paper develops a multi-agent based simulation with two adoption curves, the Traditional EV charging strategy, various EV models, driv-ing patterns, and CO2-eq. emission data to capture the full ecosystem dynamics during a long-term period from 2020 to 2032. The Danish 2030 climate goal and a Danish distribution network with 126 residential consumers are chosen as the case study. The results show that both EV adoption curves of 1 million and 775k EVs by 2030 will not satisfy the Danish climate goal of reducing transport sector emissions by 30% by 2030. The results also show that the current resi-dential electricity distribution grids cannot handle the load from increasing EVs. The first grid overload will occur in 2031 (around 16 and 24 months later for the 1 million and 775k EVs adopted by 2030) with a 67% share of EVs in the grid. ",
        "title": "Multi-Agent Based Simulation for Investigating Electric Vehicle Adoption  and Its Impacts on Electricity Distribution Grids and CO2 Emissions",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06194",
        "abstract_url": "http://arxiv.org/abs/2401.06194",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Shubham"
            },
            {
                "last_name": "Saini",
                "first_name": "Nandini"
            },
            {
                "last_name": "Kundu",
                "first_name": "Suman"
            },
            {
                "last_name": "Das",
                "first_name": "Debasis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction algorithm. Along with this, a guided cross-attention module is implemented to fill the semantic gap in integrating visual and textual data. In order to ensure reliability, we employ a model-specific approach called Gradient-weighted Class Activation Mapping (Grad-CAM) that provides a robust explanation of the predictions of the proposed model. The comprehensive experiments conducted on the CrisisMMD dataset yield in-depth analysis across various crisis-specific tasks and settings. As a result, CrisisKAN outperforms existing SOTA methodologies and provides a novel view in the domain of explainable multimodal event classification. ",
        "title": "CrisisKAN: Knowledge-infused and Explainable Multimodal Attention  Network for Crisis Event Classification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06195",
        "abstract_url": "http://arxiv.org/abs/2401.06195",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            },
            {
                "last_name": "Danouchi",
                "first_name": "Kamal"
            },
            {
                "last_name": "Prenat",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Anghel",
                "first_name": "Lorena"
            },
            {
                "last_name": "Tahoori",
                "first_name": "Mehdi B."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "LG",
            "NE"
        ],
        "abstract": "  Internet of Things (IoT) and smart wearable devices for personalized healthcare will require storing and computing ever-increasing amounts of data. The key requirements for these devices are ultra-low-power, high-processing capabilities, autonomy at low cost, as well as reliability and accuracy to enable Green AI at the edge. Artificial Intelligence (AI) models, especially Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges with traditional computing architectures due to the memory wall problem. Computing-in-Memory (CIM) with emerging resistive memories offers a solution by combining memory blocks and computing units for higher efficiency and lower power consumption. However, implementing BayNNs on CIM hardware, particularly with spintronic technologies, presents technical challenges due to variability and manufacturing defects. The NeuSPIN project aims to address these challenges through full-stack hardware and software co-design, developing novel algorithmic and circuit design approaches to enhance the performance, energy-efficiency and robustness of BayNNs on sprintronic-based CIM platforms. ",
        "title": "NeuSpin: Design of a Reliable Edge Neuromorphic System Based on  Spintronics for Green AI",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06196",
        "abstract_url": "http://arxiv.org/abs/2401.06196",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Cao",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Liao",
                "first_name": "Fei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weiwei"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Physics-informed neural networks (PINNs) have shown remarkable prospects in the solving the forward and inverse problems involving partial differential equations (PDEs). The method embeds PDEs into the neural network by calculating PDE loss at a series of collocation points, providing advantages such as meshfree and more convenient adaptive sampling. However, when solving PDEs using nonuniform collocation points, PINNs still face challenge regarding inefficient convergence of PDE residuals or even failure. In this work, we first analyze the ill-conditioning of the PDE loss in PINNs under nonuniform collocation points. To address the issue, we define volume-weighted residual and propose volume-weighted physics-informed neural networks (VW-PINNs). Through weighting the PDE residuals by the volume that the collocation points occupy within the computational domain, we embed explicitly the spatial distribution characteristics of collocation points in the residual evaluation. The fast and sufficient convergence of the PDE residuals for the problems involving nonuniform collocation points is guaranteed. Considering the meshfree characteristics of VW-PINNs, we also develop a volume approximation algorithm based on kernel density estimation to calculate the volume of the collocation points. We verify the universality of VW-PINNs by solving the forward problems involving flow over a circular cylinder and flow over the NACA0012 airfoil under different inflow conditions, where conventional PINNs fail; By solving the Burgers' equation, we verify that VW-PINNs can enhance the efficiency of existing the adaptive sampling method in solving the forward problem by 3 times, and can reduce the relative error of conventional PINNs in solving the inverse problem by more than one order of magnitude. ",
        "title": "VW-PINNs: A volume weighting method for PDE residuals in  physics-informed neural networks",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06197",
        "abstract_url": "http://arxiv.org/abs/2401.06197",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Yuwen"
            },
            {
                "last_name": "Li",
                "first_name": "Zhiqi"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuntao"
            },
            {
                "last_name": "Wang",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xizhou"
            },
            {
                "last_name": "Luo",
                "first_name": "Jiapeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenhai"
            },
            {
                "last_name": "Lu",
                "first_name": "Tong"
            },
            {
                "last_name": "Li",
                "first_name": "Hongsheng"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            },
            {
                "last_name": "Lu",
                "first_name": "Lewei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jie"
            },
            {
                "last_name": "Dai",
                "first_name": "Jifeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models. ",
        "title": "Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator  for Vision Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06199",
        "abstract_url": "http://arxiv.org/abs/2401.06199",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Bo"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xingyi"
            },
            {
                "last_name": "Li",
                "first_name": "Pan"
            },
            {
                "last_name": "Geng",
                "first_name": "Yangli-ao"
            },
            {
                "last_name": "Gong",
                "first_name": "Jing"
            },
            {
                "last_name": "Li",
                "first_name": "Shen"
            },
            {
                "last_name": "Bei",
                "first_name": "Zhilei"
            },
            {
                "last_name": "Tan",
                "first_name": "Xu"
            },
            {
                "last_name": "Wang",
                "first_name": "Boyan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Chiming"
            },
            {
                "last_name": "Zeng",
                "first_name": "Aohan"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuxiao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jie"
            },
            {
                "last_name": "Song",
                "first_name": "Le"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can perform programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contributing to the evolving landscape of foundation models in protein science. ",
        "title": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering  the Language of Protein",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06201",
        "abstract_url": "http://arxiv.org/abs/2401.06201",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Siyu"
            },
            {
                "last_name": "Song",
                "first_name": "Kaitao"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiangjie"
            },
            {
                "last_name": "Tan",
                "first_name": "Xu"
            },
            {
                "last_name": "Shen",
                "first_name": "Yongliang"
            },
            {
                "last_name": "Kan",
                "first_name": "Ren"
            },
            {
                "last_name": "Li",
                "first_name": "Dongsheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Deqing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at \\url{https://github.com/microsoft/JARVIS/} in the future. ",
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06203",
        "abstract_url": "http://arxiv.org/abs/2401.06203",
        "authors": [
            {
                "last_name": "Daly",
                "first_name": "Matthew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD"
        ],
        "abstract": "  This paper introduces our system submission for the Cadenza ICASSP 2024 Grand Challenge, which presents the problem of remixing and enhancing music for hearing aid users. Our system placed first in the challenge, achieving the best average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data set. We describe the system, which uses an ensemble of deep learning music source separators that are fine tuned on the challenge data. We demonstrate the effectiveness of our system through the challenge results and analyze the importance of different system aspects through ablation studies. ",
        "title": "Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source  Separators",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06204",
        "abstract_url": "http://arxiv.org/abs/2401.06204",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Qilei"
            },
            {
                "last_name": "Mott",
                "first_name": "John H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Large Language Models (LLMs) hold transformative potential in aviation, particularly in reconstructing flight trajectories. This paper investigates this potential, grounded in the notion that LLMs excel at processing sequential data and deciphering complex data structures. Utilizing the LLaMA 2 model, a pre-trained open-source LLM, the study focuses on reconstructing flight trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with irregularities inherent in real-world scenarios. The findings demonstrate the model's proficiency in filtering noise and estimating both linear and curved flight trajectories. However, the analysis also reveals challenges in managing longer data sequences, which may be attributed to the token length limitations of LLM models. The study's insights underscore the promise of LLMs in flight trajectory reconstruction and open new avenues for their broader application across the aviation and transportation sectors. ",
        "title": "An Exploratory Assessment of LLM's Potential Toward Flight Trajectory  Reconstruction Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06205",
        "abstract_url": "http://arxiv.org/abs/2401.06205",
        "authors": [
            {
                "last_name": "Smith",
                "first_name": "D. Hudson"
            },
            {
                "last_name": "Ehrett",
                "first_name": "Carl"
            },
            {
                "last_name": "Warren",
                "first_name": "Patrick L."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  This paper introduces and tests an unsupervised method for detecting novel coordinated inauthentic information operations (CIOs) in realistic settings. This method uses Bayesian inference to identify groups of accounts that share similar account-level characteristics and target similar narratives. We solve the inferential problem using amortized variational inference, allowing us to efficiently infer group identities for millions of accounts. We validate this method using a set of five CIOs from three countries discussing four topics on Twitter. Our unsupervised approach increases detection power (area under the precision-recall curve) relative to a naive baseline (by a factor of 76 to 580), relative to the use of simple flags or narratives on their own (by a factor of 1.3 to 4.8), and comes quite close to a supervised benchmark. Our method is robust to observing only a small share of messaging on the topic, having only weak markers of inauthenticity, and to the CIO accounts making up a tiny share of messages and accounts on the topic. Although we evaluate the results on Twitter, the method is general enough to be applied in many social-media settings. ",
        "title": "Unsupervised detection of coordinated information operations in the wild",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06207",
        "abstract_url": "http://arxiv.org/abs/2401.06207",
        "authors": [
            {
                "last_name": "Campos",
                "first_name": "Beatriz"
            },
            {
                "last_name": "Canela",
                "first_name": "Jordi"
            },
            {
                "last_name": "Rodr\u00edguez-Arenas",
                "first_name": "Alberto"
            },
            {
                "last_name": "Vindel",
                "first_name": "Pura"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we present an algorithm to obtain the parameter planes of families of root-finding methods with several free critical points. The parameter planes show the joint behaviour of all critical points. This algorithm avoids the inconsistencies arising from the relationship between the different critical points as well as the indeterminacy caused by the square roots involved in their computation.   We analyse the suitability of this algorithm by drawing the parameter planes of different Newton-like methods with two and three critical points. We also present some results of the expressions of the Newton-like operators and their derivatives in terms of palindromic polynomials, and we show how to obtain the expression of the critical points of a Newton-like method with real coefficients. ",
        "title": "Computing parameter planes of iterative root-finding methods with  several free critical points",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06209",
        "abstract_url": "http://arxiv.org/abs/2401.06209",
        "authors": [
            {
                "last_name": "Tong",
                "first_name": "Shengbang"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Zhai",
                "first_name": "Yuexiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Yi"
            },
            {
                "last_name": "LeCun",
                "first_name": "Yann"
            },
            {
                "last_name": "Xie",
                "first_name": "Saining"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems. ",
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06210",
        "abstract_url": "http://arxiv.org/abs/2401.06210",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Hao-Ming"
            },
            {
                "last_name": "Cheng",
                "first_name": "Pu-Jen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "IR"
        ],
        "abstract": "  Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin. ",
        "title": "Learning Unsupervised Semantic Document Representation for Fine-grained  Aspect-based Sentiment Analysis",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06224",
        "abstract_url": "http://arxiv.org/abs/2401.06224",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinyuan"
            },
            {
                "last_name": "Pan",
                "first_name": "Chengwei"
            },
            {
                "last_name": "Dai",
                "first_name": "Hongming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Gangming"
            },
            {
                "last_name": "Li",
                "first_name": "Jinpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Yu",
                "first_name": "Yizhou"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Coronary microvascular disease constitutes a substantial risk to human health. Employing computer-aided analysis and diagnostic systems, medical professionals can intervene early in disease progression, with 3D vessel segmentation serving as a crucial component. Nevertheless, conventional U-Net architectures tend to yield incoherent and imprecise segmentation outcomes, particularly for small vessel structures. While models with attention mechanisms, such as Transformers and large convolutional kernels, demonstrate superior performance, their extensive computational demands during training and inference lead to increased time complexity. In this study, we leverage Fourier domain learning as a substitute for multi-scale convolutional kernels in 3D hierarchical segmentation models, which can reduce computational expenses while preserving global receptive fields within the network. Furthermore, a zero-parameter frequency domain fusion method is designed to improve the skip connections in U-Net architecture. Experimental results on a public dataset and an in-house dataset indicate that our novel Fourier transformation-based network achieves remarkable dice performance (84.37\\% on ASACA500 and 80.32\\% on ImageCAS) in tubular vessel segmentation tasks and substantially reduces computational requirements without compromising global receptive fields. ",
        "title": "Leveraging Frequency Domain Learning in 3D Vessel Segmentation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06226",
        "abstract_url": "http://arxiv.org/abs/2401.06226",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Yanying"
            },
            {
                "last_name": "Garcke",
                "first_name": "Jochen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios. ",
        "title": "Learning Crowd Behaviors in Navigation with Attention-based  Spatial-Temporal Graphs",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06230",
        "abstract_url": "http://arxiv.org/abs/2401.06230",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Ziyi"
            },
            {
                "last_name": "Orozco",
                "first_name": "Rafael"
            },
            {
                "last_name": "Louboutin",
                "first_name": "Mathias"
            },
            {
                "last_name": "Herrmann",
                "first_name": "Felix J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a probabilistic technique for full-waveform inversion, employing variational inference and conditional normalizing flows to quantify uncertainty in migration-velocity models and its impact on imaging. Our approach integrates generative artificial intelligence with physics-informed common-image gathers, reducing reliance on accurate initial velocity models. Considered case studies demonstrate its efficacy producing realizations of migration-velocity models conditioned by the data. These models are used to quantify amplitude and positioning effects during subsequent imaging. ",
        "title": "WISE: full-Waveform variational Inference via Subsurface Extensions",
        "date": "2023-12-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06233",
        "abstract_url": "http://arxiv.org/abs/2401.06233",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Shruti"
            },
            {
                "last_name": "Alam",
                "first_name": "Shoaib"
            },
            {
                "last_name": "Singh",
                "first_name": "Mayank"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c . ",
        "title": "LEGOBench: Leaderboard Generation Benchmark for Scientific Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06234",
        "abstract_url": "http://arxiv.org/abs/2401.06234",
        "authors": [
            {
                "last_name": "Bertossi",
                "first_name": "Leopoldo"
            },
            {
                "last_name": "Kimelfeld",
                "first_name": "Benny"
            },
            {
                "last_name": "Livshits",
                "first_name": "Ester"
            },
            {
                "last_name": "Monet",
                "first_name": "Mika\u00ebl"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  Attribution scores can be applied in data management to quantify the contribution of individual items to conclusions from the data, as part of the explanation of what led to these conclusions. In Artificial Intelligence, Machine Learning, and Data Management, some of the common scores are deployments of the Shapley value, a formula for profit sharing in cooperative game theory. Since its invention in the 1950s, the Shapley value has been used for contribution measurement in many fields, from economics to law, with its latest researched applications in modern machine learning. Recent studies investigated the application of the Shapley value to database management. This article gives an overview of recent results on the computational complexity of the Shapley value for measuring the contribution of tuples to query answers and to the extent of inconsistency with respect to integrity constraints. More specifically, the article highlights lower and upper bounds on the complexity of calculating the Shapley value, either exactly or approximately, as well as solutions for realizing the calculation in practice. ",
        "title": "The Shapley Value in Database Management",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06238",
        "abstract_url": "http://arxiv.org/abs/2401.06238",
        "authors": [
            {
                "last_name": "Conni",
                "first_name": "Giovanni"
            },
            {
                "last_name": "Piccardo",
                "first_name": "Stefano"
            },
            {
                "last_name": "Perotto",
                "first_name": "Simona"
            },
            {
                "last_name": "Porta",
                "first_name": "Giovanni Michele"
            },
            {
                "last_name": "Icardi",
                "first_name": "Matteo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a new model reduction technique for multiscale scalar transport problems that exhibit dominant axial dynamics. To this aim, we rely on the separation of variables to combine a Hierarchical Model (HiMod) reduction with a two-scale asymptotic expansion. We extend the two-scale asymptotic expansion to an arbitrary order and exploit the high-order correctors to define the HiMod modal basis, which approximates the transverse dynamics of the flow, while we adopt a finite element discretisation to model the leading stream. The resulting method, which is named HiPhom$\\varepsilon$ (HIgh-order Projection-based HOMogEnisation), is successfully assessed both in steady and unsteady advection-diffusion-reaction settings. The numerical results confirm the very good performance of HiPhom$\\varepsilon$, which improves the accuracy and the convergence rate of HiMod and extends the reliability of the standard homogenised solution to transient and pre-asymptotic regimes. ",
        "title": "HiPhom$\\varepsilon$ -: HIgh order Projection-based HOMogenisation for  advection diffusion reaction problems",
        "date": "2023-11-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06240",
        "abstract_url": "http://arxiv.org/abs/2401.06240",
        "authors": [
            {
                "last_name": "Low",
                "first_name": "Guang Hao"
            },
            {
                "last_name": "Su",
                "first_name": "Yuan"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Many problems in linear algebra -- such as those arising from non-Hermitian physics and differential equations -- can be solved on a quantum computer by processing eigenvalues of the non-normal input matrices. However, the existing Quantum Singular Value Transformation (QSVT) framework is ill-suited to this task, as eigenvalues and singular values are different in general. We present a Quantum EigenValue Transformation (QEVT) framework for applying arbitrary polynomial transformations on eigenvalues of block-encoded non-normal operators, and a related Quantum EigenValue Estimation (QEVE) algorithm for operators with real spectra. QEVT has query complexity to the block encoding nearly recovering that of the QSVT for a Hermitian input, and QEVE achieves the Heisenberg-limited scaling for diagonalizable input matrices. As applications, we develop a linear differential equation solver with strictly linear time query complexity for average-case diagonalizable operators, as well as a ground state preparation algorithm that upgrades previous nearly optimal results for Hermitian Hamiltonians to diagonalizable matrices with real spectra. Underpinning our algorithms is an efficient method to prepare a quantum superposition of Faber polynomials, which generalize the nearly-best uniform approximation properties of Chebyshev polynomials to the complex plane. Of independent interest, we also develop techniques to generate $n$ Fourier coefficients with $\\mathbf{O}(\\mathrm{polylog}(n))$ gates compared to prior approaches with linear cost. ",
        "title": "Quantum eigenvalue processing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06243",
        "abstract_url": "http://arxiv.org/abs/2401.06243",
        "authors": [
            {
                "last_name": "Herrin",
                "first_name": "Baker"
            },
            {
                "last_name": "Close",
                "first_name": "Victoria"
            },
            {
                "last_name": "Berner",
                "first_name": "Nathan"
            },
            {
                "last_name": "Herbert",
                "first_name": "Joshua"
            },
            {
                "last_name": "Reussow",
                "first_name": "Ethan"
            },
            {
                "last_name": "James",
                "first_name": "Ryan"
            },
            {
                "last_name": "Woodward",
                "first_name": "Cale"
            },
            {
                "last_name": "Mindlin",
                "first_name": "Jared"
            },
            {
                "last_name": "Paez",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Bretas",
                "first_name": "Nilson"
            },
            {
                "last_name": "Shin",
                "first_name": "Jane"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Autonomous underwater robots typically require higher cost and time for demonstrations compared to other domains due to the complexity of the environment. Due to the limited capacity and payload flexibility, it is challenging to find off-the-shelf underwater robots that are affordable, customizable, and subject to environmental variability. Custom-built underwater robots may be necessary for specialized applications or missions, but the process can be more costly and time-consuming than purchasing an off-the-shelf autonomous underwater vehicle (AUV). To address these challenges, we propose a modular underwater robot, Modularis, that can serve as an open-source testbed system. Our proposed system expedites the testing of perception, planning, and control algorithms. ",
        "title": "Modularis: Modular Underwater Robot for Rapid Development and Validation  of Autonomous Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06244",
        "abstract_url": "http://arxiv.org/abs/2401.06244",
        "authors": [
            {
                "last_name": "Khoramdel",
                "first_name": "Javad"
            },
            {
                "last_name": "Moori",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Borhani",
                "first_name": "Yasamin"
            },
            {
                "last_name": "Ghanbarzadeh",
                "first_name": "Armin"
            },
            {
                "last_name": "Najafi",
                "first_name": "Esmaeil"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The proposed YOLO-Former method seamlessly integrates the ideas of transformer and YOLOv4 to create a highly accurate and efficient object detection system. The method leverages the fast inference speed of YOLOv4 and incorporates the advantages of the transformer architecture through the integration of convolutional attention and transformer modules. The results demonstrate the effectiveness of the proposed approach, with a mean average precision (mAP) of 85.76\\% on the Pascal VOC dataset, while maintaining high prediction speed with a frame rate of 10.85 frames per second. The contribution of this work lies in the demonstration of how the innovative combination of these two state-of-the-art techniques can lead to further improvements in the field of object detection. ",
        "title": "YOLO-Former: YOLO Shakes Hand With ViT",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06247",
        "abstract_url": "http://arxiv.org/abs/2401.06247",
        "authors": [
            {
                "last_name": "Kronhardt",
                "first_name": "Kirill"
            },
            {
                "last_name": "Rolfes",
                "first_name": "Kevin"
            },
            {
                "last_name": "Gerken",
                "first_name": "Jens"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Dark patterns are often used in interface design to manipulate users into performing actions they would otherwise not take, such as consenting to excessive data collection. We present a narrative serious game concept, along with seven game-adapted dark patterns designed to create awareness of and bolster resistance against dark patterns through direct consequences of player actions. We performed a qualitative, exploratory study investigating player behavior when confronted with game-adapted dark patterns. A thematic analysis provides insights into influencing factors for adapting dark patterns into gameplay, as well as player motivations and driving forces influencing player behavior. ",
        "title": "Player Beware: Driving Forces and Influencing Factors for Game-Adapted  Dark Patterns",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06251",
        "abstract_url": "http://arxiv.org/abs/2401.06251",
        "authors": [
            {
                "last_name": "Khorshidi",
                "first_name": "Mohammad Sadegh"
            },
            {
                "last_name": "Yazdanjue",
                "first_name": "Navid"
            },
            {
                "last_name": "Gharoun",
                "first_name": "Hassan"
            },
            {
                "last_name": "Yazdani",
                "first_name": "Danial"
            },
            {
                "last_name": "Nikoo",
                "first_name": "Mohammad Reza"
            },
            {
                "last_name": "Chen",
                "first_name": "Fang"
            },
            {
                "last_name": "Gandomi",
                "first_name": "Amir H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IT"
        ],
        "abstract": "  In machine learning, the exponential growth of data and the associated ``curse of dimensionality'' pose significant challenges, particularly with expansive yet sparse datasets. Addressing these challenges, multi-view ensemble learning (MEL) has emerged as a transformative approach, with feature partitioning (FP) playing a pivotal role in constructing artificial views for MEL. Our study introduces the Semantic-Preserving Feature Partitioning (SPFP) algorithm, a novel method grounded in information theory. The SPFP algorithm effectively partitions datasets into multiple semantically consistent views, enhancing the MEL process. Through extensive experiments on eight real-world datasets, ranging from high-dimensional with limited instances to low-dimensional with high instances, our method demonstrates notable efficacy. It maintains model accuracy while significantly improving uncertainty measures in scenarios where high generalization performance is achievable. Conversely, it retains uncertainty metrics while enhancing accuracy where high generalization accuracy is less attainable. An effect size analysis further reveals that the SPFP algorithm outperforms benchmark models by large effect size and reduces computational demands through effective dimensionality reduction. The substantial effect sizes observed in most experiments underscore the algorithm's significant improvements in model performance. ",
        "title": "Semantic-Preserving Feature Partitioning for Multi-View Ensemble  Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06252",
        "abstract_url": "http://arxiv.org/abs/2401.06252",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shaochun"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanjun"
            },
            {
                "last_name": "Cai",
                "first_name": "Hengfan"
            },
            {
                "last_name": "Deng",
                "first_name": "Lina"
            },
            {
                "last_name": "Lin",
                "first_name": "Yunhao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Real-time and accurate information on fine-grained changes in crop cultivation is of great significance for crop growth monitoring, yield prediction and agricultural structure adjustment. Aiming at the problems of serious spectral confusion in visible high-resolution unmanned aerial vehicle (UAV) images of different phases, interference of large complex background and salt-and-pepper noise by existing semantic change detection (SCD) algorithms, in order to effectively extract deep image features of crops and meet the demand of agricultural practical engineering applications, this paper designs and proposes an agricultural geographic scene and parcel-scale constrained SCD framework for crops (AGSPNet). AGSPNet framework contains three parts: agricultural geographic scene (AGS) division module, parcel edge extraction module and crop SCD module. Meanwhile, we produce and introduce an UAV image SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple semantic variation types of crops in complex geographical scene. We conduct comparative experiments and accuracy evaluations in two test areas of this dataset, and the results show that the crop SCD results of AGSPNet consistently outperform other deep learning SCD models in terms of quantity and quality, with the evaluation metrics F1-score, kappa, OA, and mIoU obtaining improvements of 0.038, 0.021, 0.011 and 0.062, respectively, on average over the sub-optimal method. The method proposed in this paper can clearly detect the fine-grained change information of crop types in complex scenes, which can provide scientific and technical support for smart agriculture monitoring and management, food policy formulation and food security assurance. ",
        "title": "AGSPNet: A framework for parcel-scale crop fine-grained semantic change  detection from UAV high-resolution imagery with agricultural geographic scene  constraints",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06256",
        "abstract_url": "http://arxiv.org/abs/2401.06256",
        "authors": [
            {
                "last_name": "Sukhobokov",
                "first_name": "Artem"
            },
            {
                "last_name": "Belousov",
                "first_name": "Evgeny"
            },
            {
                "last_name": "Gromozdov",
                "first_name": "Danila"
            },
            {
                "last_name": "Zenger",
                "first_name": "Anna"
            },
            {
                "last_name": "Popov",
                "first_name": "Ilya"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The article identified 42 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a new cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph models are used, constructed as a development of annotated metagraphs. As components, the cognitive architecture being developed includes machine consciousness, machine subconsciousness, blocks of interaction with the external environment, a goal management block, an emotional control system, a block of social interaction, a block of reflection, an ethics block and a worldview block, a learning block, a monitoring block, blocks of statement and solving problems, self-organization and meta learning block. ",
        "title": "A Universal Knowledge Model and Cognitive Architecture for Prototyping  AGI",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06262",
        "abstract_url": "http://arxiv.org/abs/2401.06262",
        "authors": [
            {
                "last_name": "Koike",
                "first_name": "Amy"
            },
            {
                "last_name": "Wehner",
                "first_name": "Michael"
            },
            {
                "last_name": "Mutlu",
                "first_name": "Bilge"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we explore how techniques from soft robotics can help create a new form of robot expression. We present Sprout, a soft expressive robot that conveys its internal states by changing its body shape. Sprout can extend, bend, twist, and expand using fiber-embedded actuators integrated into its construction. These deformations enable Sprout to express its internal states, for example, by expanding to express anger and bending its body sideways to express curiosity. Through two user studies, we investigated how users interpreted Sprout's expressions, their perceptions of Sprout, and their expectations from future iterations of Sprout's design. We argue that the use of soft actuators opens a novel design space for robot expressions to convey internal states, emotions, and intent. ",
        "title": "Sprout: Designing Expressivity for Robots Using Fiber-Embedded Actuator",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06263",
        "abstract_url": "http://arxiv.org/abs/2401.06263",
        "authors": [
            {
                "last_name": "Sattarov",
                "first_name": "Timur"
            },
            {
                "last_name": "Schreyer",
                "first_name": "Marco"
            },
            {
                "last_name": "Borth",
                "first_name": "Damian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Realistic synthetic tabular data generation encounters significant challenges in preserving privacy, especially when dealing with sensitive information in domains like finance and healthcare. In this paper, we introduce \\textit{Federated Tabular Diffusion} (FedTabDiff) for generating high-fidelity mixed-type tabular data without centralized access to the original tabular datasets. Leveraging the strengths of \\textit{Denoising Diffusion Probabilistic Models} (DDPMs), our approach addresses the inherent complexities in tabular data, such as mixed attribute types and implicit relationships. More critically, FedTabDiff realizes a decentralized learning scheme that permits multiple entities to collaboratively train a generative model while respecting data privacy and locality. We extend DDPMs into the federated setting for tabular data generation, which includes a synchronous update scheme and weighted averaging for effective model aggregation. Experimental evaluations on real-world financial and medical datasets attest to the framework's capability to produce synthetic data that maintains high fidelity, utility, privacy, and coverage. ",
        "title": "FedTabDiff: Federated Learning of Diffusion Probabilistic Models for  Synthetic Mixed-Type Tabular Data Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06268",
        "abstract_url": "http://arxiv.org/abs/2401.06268",
        "authors": [
            {
                "last_name": "Amiriara",
                "first_name": "Hamid"
            },
            {
                "last_name": "Mirmohseni",
                "first_name": "Mahtab"
            },
            {
                "last_name": "Ashtiani",
                "first_name": "Farid"
            },
            {
                "last_name": "Nasiri-Kenari",
                "first_name": "Masoumeh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper presents exact formulas for the probability distribution function (PDF) and moment generating function (MGF) of the sum-product of statistically independent but not necessarily identically distributed (i.n.i.d.) Nakagami-$m$ random variables (RVs) in terms of Meijer's G-function. Additionally, exact series representations are also derived for the sum of double-Nakagami RVs, providing useful insights on the trade-off between accuracy and computational cost. Simple asymptotic analytical expressions are provided to gain further insight into the derived formula, and the achievable diversity order is obtained. The suggested statistical properties are proved to be a highly useful tool for modeling parallel cascaded Nakagami-$m$ fading channels. The application of these new results is illustrated by deriving exact expressions and simple tight upper bounds for the outage probability (OP) and average symbol error rate (ASER) of several binary and multilevel modulation signals in intelligent reflecting surfaces (IRSs)-assisted communication systems operating over Nakagami-$m$ fading channels. It is demonstrated that the new asymptotic expression is highly accurate and can be extended to encompass a wider range of scenarios. To validate the theoretical frameworks and formulations, Monte-Carlo simulation results are presented. Additionally, supplementary simulations are provided to compare the derived results with two common types of approximations available in the literature, namely the central limit theorem (CLT) and gamma distribution. ",
        "title": "A Novel Stochastic Model for IRS-Assisted Communication Systems Based on  the Sum-Product of Nakagami-$m$ Random Variables",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06270",
        "abstract_url": "http://arxiv.org/abs/2401.06270",
        "authors": [
            {
                "last_name": "Ji",
                "first_name": "Shixin"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhuoping"
            },
            {
                "last_name": "Cahoon",
                "first_name": "Stephen"
            },
            {
                "last_name": "Jones",
                "first_name": "Alex K."
            },
            {
                "last_name": "Zhou",
                "first_name": "Peipei"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Embodied carbon has been widely reported as a significant component in the full system lifecycle of various computing systems green house gas emissions. Many efforts have been undertaken to quantify the elements that comprise this embodied carbon, from tools that evaluate semiconductor manufacturing to those that can quantify different elements of the computing system from commercial and academic sources. However, these tools cannot easily reproduce results reported by server vendors' product carbon reports and the accuracy can vary substantially due to various assumptions. Furthermore, attempts to determine green house gas contributions using bottom-up methodologies often do not agree with system-level studies and are hard to rectify. Nonetheless, given there is a need to consider all contributions to green house gas emissions in datacenters, we propose the Server Carbon including Accelerator Reporter with Intelligence-based Formulation (SCARIF) tool. SCARIF has three main contributions: (1) We first collect reported carbon cost data from server vendors and design learning models to predict the embodied carbon cost so that users can get the embodied carbon cost for their server configurations. (2) We provide embodied carbon cost if users configure servers with accelerators including GPUs, and FPGAs. (3) We provide an interface of SCARIF to the ACT and GreenChip tools and demonstrate the end-to-end system flow through indifference analysis considering the embodied and operational energy and green house gas emissions on different years servers with or without accelerators. Thus, SCARIF provides an opportunity for large-scale datacenter and hyperscaler design. ",
        "title": "Towards Carbon Modeling of Cloud Servers with Accelerators",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06272",
        "abstract_url": "http://arxiv.org/abs/2401.06272",
        "authors": [
            {
                "last_name": "Mathai",
                "first_name": "Tejas Sudharshan"
            },
            {
                "last_name": "Liu",
                "first_name": "Bohan"
            },
            {
                "last_name": "Summers",
                "first_name": "Ronald M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Purpose: Lymph nodes (LNs) in the chest have a tendency to enlarge due to various pathologies, such as lung cancer or pneumonia. Clinicians routinely measure nodal size to monitor disease progression, confirm metastatic cancer, and assess treatment response. However, variations in their shapes and appearances make it cumbersome to identify LNs, which reside outside of most organs. Methods: We propose to segment LNs in the mediastinum by leveraging the anatomical priors of 28 different structures (e.g., lung, trachea etc.) generated by the public TotalSegmentator tool. The CT volumes from 89 patients available in the public NIH CT Lymph Node dataset were used to train three 3D nnUNet models to segment LNs. The public St. Olavs dataset containing 15 patients (out-of-training-distribution) was used to evaluate the segmentation performance. Results: For the 15 test patients, the 3D cascade nnUNet model obtained the highest Dice score of 72.2 +- 22.3 for mediastinal LNs with short axis diameter $\\geq$ 8mm and 54.8 +- 23.8 for all LNs respectively. These results represent an improvement of 10 points over a current approach that was evaluated on the same test dataset. Conclusion: To our knowledge, we are the first to harness 28 distinct anatomical priors to segment mediastinal LNs, and our work can be extended to other nodal zones in the body. The proposed method has immense potential for improved patient outcomes through the identification of enlarged nodes in initial staging CT scans. ",
        "title": "Segmentation of Mediastinal Lymph Nodes in CT with Anatomical Priors",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06273",
        "abstract_url": "http://arxiv.org/abs/2401.06273",
        "authors": [
            {
                "last_name": "Grislain",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Roussel",
                "first_name": "Paul"
            },
            {
                "last_name": "Agathe",
                "first_name": "Victoria de Sainte"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  This paper introduces Qrlew, an open source library that can parse SQL queries into Relations -- an intermediate representation -- that keeps track of rich data types, value ranges, and row ownership; so that they can easily be rewritten into differentially-private equivalent and turned back into SQL queries for execution in a variety of standard data stores.   With Qrlew, a data practitioner can express their data queries in standard SQL; the data owner can run the rewritten query without any technical integration and with strong privacy guarantees on the output; and the query rewriting can be operated by a privacy-expert who must be trusted by the owner, but may belong to a separate organization. ",
        "title": "Qrlew: Rewriting SQL into Differentially Private SQL",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06274",
        "abstract_url": "http://arxiv.org/abs/2401.06274",
        "authors": [
            {
                "last_name": "Zayat",
                "first_name": "Abdullah"
            },
            {
                "last_name": "Hasabelnaby",
                "first_name": "Mahmoud A."
            },
            {
                "last_name": "Obeed",
                "first_name": "Mohanad"
            },
            {
                "last_name": "Chaaban",
                "first_name": "Anas"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Next-generation communication networks are expected to exploit recent advances in data science and cutting-edge communications technologies to improve the utilization of the available communications resources. In this article, we introduce an emerging deep learning (DL) architecture, the transformer-masked autoencoder (TMAE), and discuss its potential in next-generation wireless networks. We discuss the limitations of current DL techniques in meeting the requirements of 5G and beyond 5G networks, and how the TMAE differs from the classical DL techniques can potentially address several wireless communication problems. We highlight various areas in next-generation mobile networks which can be addressed using a TMAE, including source and channel coding, estimation, and security. Furthermore, we demonstrate a case study showing how a TMAE can improve data compression performance and complexity compared to existing schemes. Finally, we discuss key challenges and open future research directions for deploying the TMAE in intelligent next-generation mobile networks. ",
        "title": "Transformer Masked Autoencoders for Next-Generation Wireless  Communications: Architecture and Opportunities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06275",
        "abstract_url": "http://arxiv.org/abs/2401.06275",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Siyi"
            },
            {
                "last_name": "He",
                "first_name": "Zihao"
            },
            {
                "last_name": "Rao",
                "first_name": "Ashwin"
            },
            {
                "last_name": "Morstatter",
                "first_name": "Fred"
            },
            {
                "last_name": "Brantingham",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Lerman",
                "first_name": "Kristina"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  The rich and dynamic information environment of social media provides researchers, policy makers, and entrepreneurs with opportunities to learn about social phenomena in a timely manner. However, using these data to understand social behavior is difficult due to heterogeneity of topics and events discussed in the highly dynamic online information environment. To address these challenges, we present a method for systematically detecting and measuring emotional reactions to offline events using change point detection on the time series of collective affect, and further explaining these reactions using a transformer-based topic model. We demonstrate the utility of the method by successfully detecting major and smaller events on three different datasets, including (1) a Los Angeles Tweet dataset between Jan. and Aug. 2020, in which we revealed the complex psychological impact of the BlackLivesMatter movement and the COVID-19 pandemic, (2) a dataset related to abortion rights discussions in USA, in which we uncovered the strong emotional reactions to the overturn of Roe v. Wade and state abortion bans, and (3) a dataset about the 2022 French presidential election, in which we discovered the emotional and moral shift from positive before voting to fear and criticism after voting. The capability of our method allows for better sensing and monitoring of population's reactions during crises using online data. ",
        "title": "The Pulse of Mood Online: Unveiling Emotional Reactions in a Dynamic  Social Media Landscape",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06277",
        "abstract_url": "http://arxiv.org/abs/2401.06277",
        "authors": [
            {
                "last_name": "Spies",
                "first_name": "Lukas"
            },
            {
                "last_name": "Olson",
                "first_name": "Luke"
            },
            {
                "last_name": "MacLachlan",
                "first_name": "Scott"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years, solvers for finite-element discretizations of linear or linearized saddle-point problems, like the Stokes and Oseen equations, have become well established. There are two main classes of preconditioners for such systems: those based on block-factorization approach and those based on monolithic multigrid. Both classes of preconditioners have several critical choices to be made in their composition, such as the selection of a suitable relaxation scheme for monolithic multigrid. From existing studies, some insight can be gained as to what options are preferable in low-performance computing settings, but there are very few fair comparisons of these approaches in the literature, particularly for modern architectures, such as GPUs. In this paper, we perform a comparison between a block-triangular preconditioner and a monolithic multigrid method with the three most common choices of relaxation scheme - Braess-Sarazin, Vanka, and Schur-Uzawa. We develop a performant Vanka relaxation algorithm for structured-grid discretizations, which takes advantage of memory efficiencies in this setting. We detail the behavior of the various CUDA kernels for the multigrid relaxation schemes and evaluate their individual arithmetic intensity, performance, and runtime. Running a preconditioned FGMRES solver for the Stokes equations with these preconditioners allows us to compare their efficiency in a practical setting. We show monolithic multigrid can outperform block-triangular preconditioning, and that using Vanka or Braess-Sarazin relaxation is most efficient. Even though multigrid with Vanka relaxation exhibits reduced performance on the CPU (up to $100\\%$ slower than Braess-Sarazin), it is able to outperform Braess-Sarazin by more than $20\\%$ on the GPU, making it a competitive algorithm, especially given the high amount of algorithmic tuning needed for effective Braess-Sarazin relaxation. ",
        "title": "Exploiting mesh structure to improve multigrid performance for saddle  point problems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06278",
        "abstract_url": "http://arxiv.org/abs/2401.06278",
        "authors": [
            {
                "last_name": "Sanderson",
                "first_name": "Edward"
            },
            {
                "last_name": "Matuszewski",
                "first_name": "Bogdan J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exception of monocular depth estimation in colonoscopy; and that ViT-Bs are more suitable in polyp segmentation and monocular depth estimation in colonoscopy, ResNet50s are more suitable in polyp detection, and both architectures perform similarly in anatomical landmark recognition and pathological finding characterisation. We hope this work draws attention to the complexity of pretraining for GIE vision tasks, informs this development of more suitable approaches than the convention, and inspires further research on this topic to help advance this development. Code available: \\underline{github.com/ESandML/SSL4GIE} ",
        "title": "A Study on Self-Supervised Pretraining for Vision Problems in  Gastrointestinal Endoscopy",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06279",
        "abstract_url": "http://arxiv.org/abs/2401.06279",
        "authors": [
            {
                "last_name": "Parada-Mayorga",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Ribeiro",
                "first_name": "Alejandro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this work, we study the properties of sampling sets on families of large graphs by leveraging the theory of graphons and graph limits. To this end, we extend to graphon signals the notion of removable and uniqueness sets, which was developed originally for the analysis of signals on graphs. We state the formal definition of a $\\Lambda-$removable set and conditions under which a bandlimited graphon signal can be represented in a unique way when its samples are obtained from the complement of a given $\\Lambda-$removable set in the graphon. By leveraging such results we show that graphon representations of graphs and graph signals can be used as a common framework to compare sampling sets between graphs with different numbers of nodes and edges, and different node labelings. Additionally, given a sequence of graphs that converges to a graphon, we show that the sequences of sampling sets whose graphon representation is identical in $[0,1]$ are convergent as well. We exploit the convergence results to provide an algorithm that obtains approximately close to optimal sampling sets. Performing a set of numerical experiments, we evaluate the quality of these sampling sets. Our results open the door for the efficient computation of optimal sampling sets in graphs of large size. ",
        "title": "Sampling and Uniqueness Sets in Graphon Signal Processing",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06280",
        "abstract_url": "http://arxiv.org/abs/2401.06280",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Alvin"
            },
            {
                "last_name": "Bishop",
                "first_name": "Joseph E."
            },
            {
                "last_name": "Sukumar",
                "first_name": "N."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we present a first-order Stress-Hybrid Virtual Element Method (SH-VEM) on six-noded triangular meshes for linear plane elasticity. We adopt the Hellinger--Reissner variational principle to construct a weak equilibrium condition and a stress based projection operator. On applying the divergence theorem to the weak strain-displacement relations, the stress projection operator is expressed in terms of the nodal displacements, which leads to a displacement-based formulation. This stress-hybrid approach assumes a globally continuous displacement field while the stress field is discontinuous across each element. The stress field is initially represented by divergence-free tensor polynomials based on Airy stress functions. However, for flexibility in choosing basis functions, we also present a formulation that uses a penalty term to enforce the element equilibrium conditions. This method is referred to as the Penalty Stress-Hybrid Virtual Element Method (PSH-VEM). Numerical results are presented for PSH-VEM and SH-VEM, and we compare their convergence to the composite triangle FEM and B-bar VEM on benchmark problems in linear elasticity. The SH-VEM converges optimally in the $L^2$ norm of the displacement, energy seminorm, and the $L^2$ norm of hydrostatic stress. Furthermore, the results reveal that PSH-VEM converges in most cases at a faster rate than the expected optimal rate, but it requires the selection of a suitably chosen penalty parameter. ",
        "title": "Stress-hybrid virtual element method on six-noded triangular meshes for  compressible and nearly-incompressible linear elasticity",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06281",
        "abstract_url": "http://arxiv.org/abs/2401.06281",
        "authors": [
            {
                "last_name": "Ribeiro",
                "first_name": "Fabio De Sousa"
            },
            {
                "last_name": "Glocker",
                "first_name": "Ben"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Despite the growing popularity of diffusion models, gaining a deep understanding of the model class remains somewhat elusive for the uninitiated in non-equilibrium statistical physics. With that in mind, we present what we believe is a more straightforward introduction to diffusion models using directed graphical modelling and variational Bayesian principles, which imposes relatively fewer prerequisites on the average reader. Our exposition constitutes a comprehensive technical review spanning from foundational concepts like deep latent variable models to recent advances in continuous-time diffusion-based modelling, highlighting theoretical connections between model classes along the way. We provide additional mathematical insights that were omitted in the seminal works whenever possible to aid in understanding, while avoiding the introduction of new notation. We envision this article serving as a useful educational supplement for both researchers and practitioners in the area, and we welcome feedback and contributions from the community at https://github.com/biomedia-mira/demystifying-diffusion. ",
        "title": "Demystifying Variational Diffusion Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06287",
        "abstract_url": "http://arxiv.org/abs/2401.06287",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Yukun"
            },
            {
                "last_name": "Yao",
                "first_name": "Hantao"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Liansheng"
            },
            {
                "last_name": "Xu",
                "first_name": "Changsheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Audio-visual video recognition (AVVR) aims to integrate audio and visual clues to categorize videos accurately. While existing methods train AVVR models using provided datasets and achieve satisfactory results, they struggle to retain historical class knowledge when confronted with new classes in real-world situations. Currently, there are no dedicated methods for addressing this problem, so this paper concentrates on exploring Class Incremental Audio-Visual Video Recognition (CIAVVR). For CIAVVR, since both stored data and learned model of past classes contain historical knowledge, the core challenge is how to capture past data knowledge and past model knowledge to prevent catastrophic forgetting. We introduce Hierarchical Augmentation and Distillation (HAD), which comprises the Hierarchical Augmentation Module (HAM) and Hierarchical Distillation Module (HDM) to efficiently utilize the hierarchical structure of data and models, respectively. Specifically, HAM implements a novel augmentation strategy, segmental feature augmentation, to preserve hierarchical model knowledge. Meanwhile, HDM introduces newly designed hierarchical (video-distribution) logical distillation and hierarchical (snippet-video) correlative distillation to capture and maintain the hierarchical intra-sample knowledge of each data and the hierarchical inter-sample knowledge between data, respectively. Evaluations on four benchmarks (AVE, AVK-100, AVK-200, and AVK-400) demonstrate that the proposed HAD effectively captures hierarchical information in both data and models, resulting in better preservation of historical class knowledge and improved performance. Furthermore, we provide a theoretical analysis to support the necessity of the segmental feature augmentation strategy. ",
        "title": "Hierarchical Augmentation and Distillation for Class Incremental  Audio-Visual Video Recognition",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06289",
        "abstract_url": "http://arxiv.org/abs/2401.06289",
        "authors": [
            {
                "last_name": "O'Connell",
                "first_name": "Amy"
            },
            {
                "last_name": "Banga",
                "first_name": "Ashveen"
            },
            {
                "last_name": "Ayissi",
                "first_name": "Jennifer"
            },
            {
                "last_name": "Yaminrafie",
                "first_name": "Nikki"
            },
            {
                "last_name": "Ko",
                "first_name": "Ellen"
            },
            {
                "last_name": "Le",
                "first_name": "Andrew"
            },
            {
                "last_name": "Cislowski",
                "first_name": "Bailey"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  College students with ADHD respond positively to simple socially assistive robots (SARs) that monitor attention and provide non-verbal feedback, but studies have been done only in brief in-lab sessions. We present an initial design and evaluation of an in-dorm SAR study companion for college students with ADHD. This work represents the introductory stages of an ongoing user-centered, participatory design process. In a three-week within-subjects user study, university students (N=11) with self-reported symptoms of adult ADHD had a SAR study companion in their dorm room for two weeks and a computer-based system for one week. Toward developing SARs for long-term, in-dorm use, we focus on 1) evaluating the usability and desire for SAR study companions by college students with ADHD and 2) collecting participant feedback about the SAR design and functionality. Participants responded positively to the robot; after one week of regular use, 91% (10 of 11) chose to continue using the robot voluntarily in the second week. ",
        "title": "Design and Evaluation of a Socially Assistive Robot Schoolwork Companion  for College Students with ADHD",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06291",
        "abstract_url": "http://arxiv.org/abs/2401.06291",
        "authors": [
            {
                "last_name": "Kalkhof",
                "first_name": "John"
            },
            {
                "last_name": "K\u00fchn",
                "first_name": "Arlene"
            },
            {
                "last_name": "Frisch",
                "first_name": "Yannik"
            },
            {
                "last_name": "Mukhopadhyay",
                "first_name": "Anirban"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Denoising Diffusion Models (DDMs) have become the leading generative technique for synthesizing high-quality images but are often constrained by their UNet-based architectures that impose certain limitations. In particular, the considerable size of often hundreds of millions of parameters makes them impractical when hardware resources are limited. However, even with powerful hardware, processing images in the gigapixel range is difficult. This is especially true in fields such as microscopy or satellite imaging, where such challenges arise from the limitation to a predefined generative size and the inefficient scaling to larger images. We present two variations of Neural Cellular Automata (NCA)-based DDM methods to address these challenges and jumpstart NCA-based DDMs: Diff-NCA and FourierDiff-NCA. Diff-NCA performs diffusion by using only local features of the underlying distribution, making it suitable for applications where local features are critical. To communicate global knowledge in image space, naive NCA setups require timesteps that increase with the image scale. We solve this bottleneck of current NCA architectures by introducing FourierDiff-NCA, which advances Diff-NCA by adding a Fourier-based diffusion process and combines the frequency-organized Fourier space with the image space. By initiating diffusion in the Fourier domain and finalizing it in the image space, FourierDiff-NCA accelerates global communication. We validate our techniques by using Diff-NCA (208k parameters) to generate high-resolution digital pathology scans at 576x576 resolution and FourierDiff-NCA (887k parameters) to synthesize CelebA images at 64x64, outperforming VNCA and five times bigger UNet-based DDMs. In addition, we demonstrate FourierDiff-NCA's capabilities in super-resolution, OOD image synthesis, and inpainting without additional training. ",
        "title": "Frequency-Time Diffusion with Neural Cellular Automata",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06293",
        "abstract_url": "http://arxiv.org/abs/2401.06293",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Qiang Charles"
            },
            {
                "last_name": "Muralidharan",
                "first_name": "Ajith"
            },
            {
                "last_name": "Tiwana",
                "first_name": "Birjodh"
            },
            {
                "last_name": "Jia",
                "first_name": "Johnson"
            },
            {
                "last_name": "Borisyuk",
                "first_name": "Fedor"
            },
            {
                "last_name": "Gupta",
                "first_name": "Aman"
            },
            {
                "last_name": "Woodard",
                "first_name": "Dawn"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In this paper, we propose a generic model-based re-ranking framework, MultiSlot ReRanker, which simultaneously optimizes relevance, diversity, and freshness. Specifically, our Sequential Greedy Algorithm (SGA) is efficient enough (linear time complexity) for large-scale production recommendation engines. It achieved a lift of $+6\\%$ to $ +10\\%$ offline Area Under the receiver operating characteristic Curve (AUC) which is mainly due to explicitly modeling mutual influences among items of a list, and leveraging the second pass ranking scores of multiple objectives. In addition, we have generalized the offline replay theory to multi-slot re-ranking scenarios, with trade-offs among multiple objectives. The offline replay results can be further improved by Pareto Optimality. Moreover, we've built a multi-slot re-ranking simulator based on OpenAI Gym integrated with the Ray framework. It can be easily configured for different assumptions to quickly benchmark both reinforcement learning and supervised learning algorithms. ",
        "title": "MultiSlot ReRanker: A Generic Model-based Re-Ranking Framework in  Recommendation Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06299",
        "abstract_url": "http://arxiv.org/abs/2401.06299",
        "authors": [
            {
                "last_name": "Estiri",
                "first_name": "Elham"
            },
            {
                "last_name": "Mirinejad",
                "first_name": "Hossein"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Fluid administration, also called fluid resuscitation, is a medical treatment to restore the lost blood volume and optimize cardiac functions in critical care scenarios such as burn, hemorrhage, and septic shock. Automated fluid administration systems (AFAS), a potential means to improve the treatment, employ computational control algorithms to automatically adjust optimal fluid infusion dosages by targeting physiological variables (e.g., blood volume or blood pressure). Most of the existing AFAS control algorithms are model-based approaches, and their performance is highly dependent on the model accuracy, making them less desirable in real-world care of critically ill patients due to complexity and variability of modeling patients physiological states. This work presents a novel model-free reinforcement learning (RL) approach for the control of fluid infusion dosages in AFAS systems. The proposed RL agent learns to adjust the blood volume to a desired value by choosing the optimal infusion dosages using a Q-learning algorithm. The RL agent learns the optimal actions by interacting with the environment (without having the knowledge of system dynamics). The proposed methodology (i) overcomes the need for a precise mathematical model in AFAS systems and (ii) provides a robust performance in rejecting clinical noises and reaching desired hemodynamic states, as will be shown by simulation results. ",
        "title": "Model-Free Reinforcement Learning for Automated Fluid Administration in  Critical Care",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06300",
        "abstract_url": "http://arxiv.org/abs/2401.06300",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Weishun"
            },
            {
                "last_name": "Shtanko",
                "first_name": "Oles"
            },
            {
                "last_name": "Movassagh",
                "first_name": "Ramis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A promising strategy to protect quantum information from noise-induced errors is to encode it into the low-energy states of a topological quantum memory device. However, readout errors from such memory under realistic settings is less understood. We study the problem of decoding quantum information encoded in the groundspaces of topological stabilizer Hamiltonians in the presence of generic perturbations, such as quenched disorder. We first prove that the standard stabilizer-based error correction and decoding schemes work adequately well in such perturbed quantum codes by showing that the decoding error diminishes exponentially in the distance of the underlying unperturbed code. We then prove that Quantum Neural Network (QNN) decoders provide an almost quadratic improvement on the readout error. Thus, we demonstrate provable advantage of using QNNs for decoding realistic quantum error-correcting codes, and our result enables the exploration of a wider range of non-stabilizer codes in the near-term laboratory settings. ",
        "title": "Advantage of Quantum Neural Networks as Quantum Information Decoders",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06301",
        "abstract_url": "http://arxiv.org/abs/2401.06301",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Shangqing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In-context learning with large language models (LLMs) excels at adapting to various tasks rapidly. However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice. Current approaches to this problem either rely on hard-to-acquire external supervision or require frequent interactions with LLMs, resulting in high costs. We propose a new method called In-Context Reflection (ICR) to overcome these challenges. ICR strategically selects demonstrations to reduce the discrepancy between the LLM's outputs and the actual input-output mappings. Specifically, ICR starts with a random set of initial demonstrations, then iteratively refines it. In each step, it analyzes a pool of candidate examples and identifies the ones most likely to challenge the LLM's current understanding, measured by a new metric called misconfidence. These most confusing examples are then selected to replace the less informative demonstrations in the current set. Our comprehensive evaluation across five diverse datasets encompassing 13 subtasks shows the efficacy of ICR. Compared to existing methods, ICR achieves an average performance boost of 4%, while demonstrating remarkable cross-task generalization capabilities. ",
        "title": "Misconfidence-based Demonstration Selection for LLM In-Context Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06305",
        "abstract_url": "http://arxiv.org/abs/2401.06305",
        "authors": [
            {
                "last_name": "Anon",
                "first_name": "Alexandre Miranda"
            },
            {
                "last_name": "Bae",
                "first_name": "Sangjae"
            },
            {
                "last_name": "Saroya",
                "first_name": "Manish"
            },
            {
                "last_name": "Isele",
                "first_name": "David"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Smooth and safe speed planning is imperative for the successful deployment of autonomous vehicles. This paper presents a mathematical formulation for the optimal speed planning of autonomous driving, which has been validated in high-fidelity simulations and real-road demonstrations with practical constraints. The algorithm explores the inter-traffic gaps in the time and space domain using a breadth-first search. For each gap, quadratic programming finds an optimal speed profile, synchronizing the time and space pair along with dynamic obstacles. Qualitative and quantitative analysis in Carla is reported to discuss the smoothness and robustness of the proposed algorithm. Finally, we present a road demonstration result for urban city driving. ",
        "title": "Multi-Profile Quadratic Programming (MPQP) for Optimal Gap Selection and  Speed Planning of Autonomous Driving",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06306",
        "abstract_url": "http://arxiv.org/abs/2401.06306",
        "authors": [
            {
                "last_name": "Sasan",
                "first_name": "Zeinab"
            },
            {
                "last_name": "Shokrnezhad",
                "first_name": "Masoud"
            },
            {
                "last_name": "Khorsandi",
                "first_name": "Siavash"
            },
            {
                "last_name": "Taleb",
                "first_name": "Tarik"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "DC",
            "PF"
        ],
        "abstract": "  To address the evolving landscape of next-generation mobile networks, characterized by an increasing number of connected users, surging traffic demands, and the continuous emergence of new services, a novel communication paradigm is essential. One promising candidate is the integration of network slicing and in-network computing, offering resource isolation, deterministic networking, enhanced resource efficiency, network expansion, and energy conservation. Although prior research has explored resource allocation within network slicing, routing, and in-network computing independently, a comprehensive investigation into their joint approach has been lacking. This paper tackles the joint problem of network slicing, path selection, and the allocation of in-network and cloud computing resources, aiming to maximize the number of accepted users while minimizing energy consumption. First, we introduce a Mixed-Integer Linear Programming (MILP) formulation of the problem and analyze its complexity, proving that the problem is NP-hard. Next, a Water Filling-based Joint Slicing, Routing, and In-Network Computing (WF-JSRIN) heuristic algorithm is proposed to solve it. Finally, a comparative analysis was conducted among WF-JSRIN, a random allocation technique, and two optimal approaches, namely Opt-IN (utilizing in-network computation) and Opt-C (solely relying on cloud node resources). The results emphasize WF-JSRIN's efficiency in delivering highly efficient near-optimal solutions with significantly reduced execution times, solidifying its suitability for practical real-world applications. ",
        "title": "Joint Network Slicing, Routing, and In-Network Computing for  Energy-Efficient 6G",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06308",
        "abstract_url": "http://arxiv.org/abs/2401.06308",
        "authors": [
            {
                "last_name": "Mazandarani",
                "first_name": "Hamidreza"
            },
            {
                "last_name": "Shokrnezhad",
                "first_name": "Masoud"
            },
            {
                "last_name": "Taleb",
                "first_name": "Tarik"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG",
            "MA"
        ],
        "abstract": "  The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL), enabling the user equipment to autonomously make decisions regarding wireless spectrum access based solely on their local individual observations. The efficiency of the proposed technique is evaluated through two scenarios: single-channel and multi-channel. The findings illustrate that, across a spectrum of $\\alpha$ values, association matrices, and channels, SAMA-D3QL consistently outperforms alternative approaches. This establishes it as a promising candidate for facilitating the realization of future federated, dynamically evolving applications. ",
        "title": "A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic  6G-Based Applications",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06309",
        "abstract_url": "http://arxiv.org/abs/2401.06309",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shian"
            },
            {
                "last_name": "Shang",
                "first_name": "Mingfeng"
            },
            {
                "last_name": "Stern",
                "first_name": "Raphael"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  While automated vehicles (AVs) are expected to revolutionize future transportation systems, emerging AV technologies open a door for malicious actors to compromise intelligent vehicles. As the first generation of AVs, adaptive cruise control (ACC) vehicles are vulnerable to cyberattacks. While recent effort has been made to understanding the impact of attacks on transportation systems, little work has been done to systematically model and characterize the malicious nature of candidate attacks. In this study, we develop a general framework for modeling and synthesizing two types of candidate attacks on ACC vehicles, namely direct attacks on vehicle control commands and false data injection attacks on sensor measurement, with explicit characterization of their adverse effects. Based on linear stability analysis of car-following dynamics, we derive a series of analytical conditions characterizing the malicious nature of potential attacks. This ensures a higher degree of realism in modeling attacks with adverse effects, as opposed to simply considering attacks as constants or random variables. Notably, the conditions derived provide an effective method for strategically synthesizing an array of candidate attacks on ACC vehicles. We conduct extensive simulation to examine the impacts of intelligently designed attacks on microscopic car-following dynamics and macroscopic traffic flow. Numerical results illustrate the mechanism of candidate attacks, offering useful insights into understanding the vulnerability of future transportation systems. The methodology developed allows for further study of the widespread impact of strategically designed attacks on traffic cybersecurity, and is expected to inspire the development of efficient attack detection techniques and advanced vehicle controls. ",
        "title": "Cyberattacks on Adaptive Cruise Control Vehicles: An Analytical  Characterization",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06310",
        "abstract_url": "http://arxiv.org/abs/2401.06310",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Akshita"
            },
            {
                "last_name": "Prabhakaran",
                "first_name": "Vinodkumar"
            },
            {
                "last_name": "Denton",
                "first_name": "Remi"
            },
            {
                "last_name": "Laszlo",
                "first_name": "Sarah"
            },
            {
                "last_name": "Dave",
                "first_name": "Shachi"
            },
            {
                "last_name": "Qadri",
                "first_name": "Rida"
            },
            {
                "last_name": "Reddy",
                "first_name": "Chandan K."
            },
            {
                "last_name": "Dev",
                "first_name": "Sunipa"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL",
            "CY"
        ],
        "abstract": "  Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images of these identities as compared to other attributes. We further investigate how disparately offensive the depictions of generated images are for different nationalities. Finally, through a detailed case study, we reveal how the 'default' representations of all identity groups have a stereotypical appearance. Moreover, for the Global South, images across different attributes are visually similar, even when explicitly prompted otherwise. CONTENT WARNING: Some examples may contain offensive stereotypes. ",
        "title": "Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in  Text-to-Image Generation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06311",
        "abstract_url": "http://arxiv.org/abs/2401.06311",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Le"
            },
            {
                "last_name": "Wu",
                "first_name": "Yihong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Large Language Models (LLMs) have emerged as a pivotal force in language technology. Their robust reasoning capabilities and expansive knowledge repositories have enabled exceptional zero-shot generalization abilities across various facets of the natural language processing field, including information retrieval (IR). In this paper, we conduct an in-depth investigation into the utility of documents generated by LLMs for IR. We introduce a simple yet effective framework, Multi-Text Generation Integration (MuGI), to augment existing IR methodologies. Specifically, we prompt LLMs to generate multiple pseudo references and integrate with query for retrieval. The training-free MuGI model eclipses existing query expansion strategies, setting a new standard in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR, achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a 7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity re-ranking pipeline. This allows a relatively small 110M parameter retriever to surpass the performance of larger 3B models in in-domain evaluations, while also bridging the gap in out-of-distribution situations. We release our code and all generated references at https://github.com/lezhang7/Retrieval_MuGI. ",
        "title": "MuGI: Enhancing Information Retrieval through Multi-Text Generation  Intergration with Large Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06318",
        "abstract_url": "http://arxiv.org/abs/2401.06318",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Yaowei"
            },
            {
                "last_name": "Lear",
                "first_name": "Jacob"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  While significant advancements have been made in the field of fair machine learning, the majority of studies focus on scenarios where the decision model operates on a static population. In this paper, we study fairness in dynamic systems where sequential decisions are made. Each decision may shift the underlying distribution of features or user behavior. We model the dynamic system through a Markov Decision Process (MDP). By acknowledging that traditional fairness notions and long-term fairness are distinct requirements that may not necessarily align with one another, we propose an algorithmic framework to integrate various fairness considerations with reinforcement learning using both pre-processing and in-processing approaches. Three case studies show that our method can strike a balance between traditional fairness notions, long-term fairness, and utility. ",
        "title": "Striking a Balance in Fairness for Dynamic Systems Through Reinforcement  Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06320",
        "abstract_url": "http://arxiv.org/abs/2401.06320",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Scells",
                "first_name": "Harrisen"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Shengyao"
            },
            {
                "last_name": "Potthast",
                "first_name": "Martin"
            },
            {
                "last_name": "Koopman",
                "first_name": "Bevan"
            },
            {
                "last_name": "Zuccon",
                "first_name": "Guido"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches. ",
        "title": "Zero-shot Generative Large Language Models for Systematic Review  Screening Automation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06321",
        "abstract_url": "http://arxiv.org/abs/2401.06321",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Wonjune"
            },
            {
                "last_name": "Wang",
                "first_name": "Yun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shun"
            },
            {
                "last_name": "Hinsvark",
                "first_name": "Arthur"
            },
            {
                "last_name": "He",
                "first_name": "Qing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that incorporating this dataset into training significantly improves HD performance over only using a commonly used, but imbalanced, pre-existing dataset. ",
        "title": "Multi-Task Learning for Front-End Text Processing in TTS",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06323",
        "abstract_url": "http://arxiv.org/abs/2401.06323",
        "authors": [
            {
                "last_name": "Abate",
                "first_name": "Marcus"
            },
            {
                "last_name": "Chang",
                "first_name": "Yun"
            },
            {
                "last_name": "Hughes",
                "first_name": "Nathan"
            },
            {
                "last_name": "Carlone",
                "first_name": "Luca"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We present improvements to Kimera, an open-source metric-semantic visual-inertial SLAM library. In particular, we enhance Kimera-VIO, the visual-inertial odometry pipeline powering Kimera, to support better feature tracking, more efficient keyframe selection, and various input modalities (eg monocular, stereo, and RGB-D images, as well as wheel odometry). Additionally, Kimera-RPGO and Kimera-PGMO, Kimera's pose-graph optimization backends, are updated to support modern outlier rejection methods - specifically, Graduated-Non-Convexity - for improved robustness to spurious loop closures. These new features are evaluated extensively on a variety of simulated and real robotic platforms, including drones, quadrupeds, wheeled robots, and simulated self-driving cars. We present comparisons against several state-of-the-art visual-inertial SLAM pipelines and discuss strengths and weaknesses of the new release of Kimera. The newly added features have been released open-source at https://github.com/MIT-SPARK/Kimera. ",
        "title": "Kimera2: Robust and Accurate Metric-Semantic SLAM in the Real World",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06325",
        "abstract_url": "http://arxiv.org/abs/2401.06325",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Xunpeng"
            },
            {
                "last_name": "Zou",
                "first_name": "Difan"
            },
            {
                "last_name": "Dong",
                "first_name": "Hanze"
            },
            {
                "last_name": "Ma",
                "first_name": "Yian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  To sample from a general target distribution $p_*\\propto e^{-f_*}$ beyond the isoperimetric condition, Huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of DMC originates from its redundant design of score estimation, and proposed a more efficient algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation and sampling subproblems accordingly, which are correlated in a recursive manner. Importantly, we show that with a proper design of the segment decomposition, all sampling subproblems will only need to tackle a strongly log-concave distribution, which can be very efficient to solve using the Langevin-based samplers with a provably rapid convergence rate. As a result, we prove that the gradient complexity of RS-DMC only has a quasi-polynomial dependency on $\\epsilon$, which significantly improves exponential gradient complexity in Huang et al. (2023). Furthermore, under commonly used dissipative conditions, our algorithm is provably much faster than the popular Langevin-based algorithms. Our algorithm design and theoretical framework illuminate a novel direction for addressing sampling problems, which could be of broader applicability in the community. ",
        "title": "Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06327",
        "abstract_url": "http://arxiv.org/abs/2401.06327",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lingling"
            },
            {
                "last_name": "Liu",
                "first_name": "Jun"
            },
            {
                "last_name": "Guo",
                "first_name": "Tianlin"
            },
            {
                "last_name": "Wu",
                "first_name": "Wenjun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We introduce a novel task, called Generalized Relation Discovery (GRD), for open-world relation extraction. GRD aims to identify unlabeled instances in existing pre-defined relations or discover novel relations by assigning instances to clusters as well as providing specific meanings for these clusters. The key challenges of GRD are how to mitigate the serious model biases caused by labeled pre-defined relations to learn effective relational representations and how to determine the specific semantics of novel relations during classifying or clustering unlabeled instances. We then propose a novel framework, SFGRD, for this task to solve the above issues by learning from semi-factuals in two stages. The first stage is semi-factual generation implemented by a tri-view debiased relation representation module, in which we take each original sentence as the main view and design two debiased views to generate semi-factual examples for this sentence. The second stage is semi-factual thinking executed by a dual-space tri-view collaborative relation learning module, where we design a cluster-semantic space and a class-index space to learn relational semantics and relation label indices, respectively. In addition, we devise alignment and selection strategies to integrate two spaces and establish a self-supervised learning loop for unlabeled data by doing semi-factual thinking across three views. Extensive experimental results show that SFGRD surpasses state-of-the-art models in terms of accuracy by 2.36\\% $\\sim$5.78\\% and cosine similarity by 32.19\\%$\\sim$ 84.45\\% for relation label index and relation semantic quality, respectively. To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals in relation extraction. ",
        "title": "Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for  Generalized Relation Discovery",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06328",
        "abstract_url": "http://arxiv.org/abs/2401.06328",
        "authors": [
            {
                "last_name": "Eppstein",
                "first_name": "David"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  The Erd\\H{o}s--Anning theorem states that every point set in the Euclidean plane with integer distances must be either collinear or finite. More strongly, for any (non-degenerate) triangle of diameter $\\delta$, at most $O(\\delta^2)$ points can have integer distances from all three triangle vertices. We prove the same results for any strictly convex distance function on the plane, and analogous results for every two-dimensional complete Riemannian manifold of bounded genus and for geodesic distance on the boundary of every three-dimensional Euclidean convex set. Our proofs are based on the properties of additively weighted Voronoi diagrams of these distances. ",
        "title": "Non-Euclidean Erd\\H{o}s--Anning Theorems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06331",
        "abstract_url": "http://arxiv.org/abs/2401.06331",
        "authors": [
            {
                "last_name": "Felfeliyan",
                "first_name": "Banafshe"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuyue"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Shrimanti"
            },
            {
                "last_name": "Kupper",
                "first_name": "Jessica"
            },
            {
                "last_name": "Liu",
                "first_name": "Shaobo"
            },
            {
                "last_name": "Hareendranathan",
                "first_name": "Abhilash"
            },
            {
                "last_name": "Jaremko",
                "first_name": "Jacob L."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Osteoarthritis (OA) poses a global health challenge, demanding precise diagnostic methods. Current radiographic assessments are time consuming and prone to variability, prompting the need for automated solutions. The existing deep learning models for OA assessment are unimodal single task systems and they don't incorporate relevant text information such as patient demographics, disease history, or physician reports. This study investigates employing Vision Language Processing (VLP) models to predict OA severity using Xray images and corresponding reports. Our method leverages Xray images of the knee and diverse report templates generated from tabular OA scoring values to train a CLIP (Contrastive Language Image PreTraining) style VLP model. Furthermore, we incorporate additional contrasting captions to enforce the model to discriminate between positive and negative reports. Results demonstrate the efficacy of these models in learning text image representations and their contextual relationships, showcase potential advancement in OA assessment, and establish a foundation for specialized vision language models in medical contexts. ",
        "title": "Application Of Vision-Language Models For Assessing Osteoarthritis  Disease Severity",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06332",
        "abstract_url": "http://arxiv.org/abs/2401.06332",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lei"
            },
            {
                "last_name": "Ren",
                "first_name": "Zihao"
            },
            {
                "last_name": "Yuan",
                "first_name": "Deming"
            },
            {
                "last_name": "Shi",
                "first_name": "Guodong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we study distributed solvers for network linear equations over a network with node-to-node communication messages compressed as scalar values. Our key idea lies in a dimension compression scheme including a dimension compressing vector that applies to individual node states to generate a real-valued message for node communication as an inner product, and a data unfolding step in the local computations where the scalar message is plotted along the subspace generated by the compression vector. We first present a compressed average consensus flow that relies only on such scalar communication, and show that exponential convergence can be achieved with well excited signals for the compression vector. We then employ such a compressed consensus flow as a fundamental consensus subroutine to develop distributed continuous-time and discrete-time solvers for network linear equations, and prove their exponential convergence properties under scalar node communications. With scalar communications, a direct benefit would be the reduced node-to-node communication channel capacity requirement for distributed computing. Numerical examples are presented to illustrate the effectiveness of the established theoretical results. ",
        "title": "Distributed Solvers for Network Linear Equations with Scalarized  Compression",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06336",
        "abstract_url": "http://arxiv.org/abs/2401.06336",
        "authors": [
            {
                "last_name": "Sivakumar",
                "first_name": "Suharsh"
            },
            {
                "last_name": "Shen",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Monga",
                "first_name": "Rajat"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "DB"
        ],
        "abstract": "  A large class of data questions can be modeled as identifying important slices of data driven by user defined metrics. This paper presents TRACE, a Time-Relational Approximate Cubing Engine that enables interactive analysis on such slices with a low upfront cost - both in space and computation. It does this by materializing the most important parts of the cube over time enabling interactive querying for a large class of analytical queries e.g. what part of my business has the highest revenue growth ([SubCategory=Sports Equipment, Gender=Female]), what slices are lagging in revenue per user ([State=CA, Age=20-30]). Many user defined metrics are supported including common aggregations such as SUM, COUNT, DISTINCT COUNT and more complex ones such as AVERAGE. We implemented and deployed TRACE for a variety of business use cases. ",
        "title": "TRACE: A Time-Relational Approximate Cubing Engine for Fast Data  Insights",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06337",
        "abstract_url": "http://arxiv.org/abs/2401.06337",
        "authors": [
            {
                "last_name": "Lv",
                "first_name": "Zhaoming"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  User interaction is one of the most effective ways to improve the ontology alignment quality. However, this approach faces the challenge of how users can participate effectively in the matching process. To solve this challenge. In this paper, an interactive ontology alignment approach using compact differential evolution algorithm with adaptive parameter control (IOACDE) is proposed. In this method, the ontology alignment process is modeled as an interactive optimization problem and users are allowed to intervene in matching in two ways. One is that the mapping suggestions generated by IOACDE as a complete candidate alignment is evaluated by user during optimization process. The other is that the user ameliorates the alignment results by evaluating single mapping after the automatic matching process. To demonstrate the effectiveness of the proposed algorithm, the neural embedding model and K nearest neighbor (KNN) is employed to simulate user for the ontologies of the real world. The experimental results show that the proposed interactive approach can improve the alignment quality compared to the non-interactive. Compared with the state-of-the-art methods from OAEI, the results show that the proposed algorithm has a better performance under the same error rate. ",
        "title": "An ontology alignment method with user intervention using compact  differential evolution with adaptive parameter control",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06340",
        "abstract_url": "http://arxiv.org/abs/2401.06340",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Xujin"
            },
            {
                "last_name": "Wei",
                "first_name": "Wei"
            },
            {
                "last_name": "Qiu",
                "first_name": "Shuang"
            },
            {
                "last_name": "He",
                "first_name": "Huiguang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an efficient technology for target retrieval using electroencephalography (EEG) signals. The performance improvement of traditional decoding methods relies on a substantial amount of training data from new test subjects, which increases preparation time for BCI systems. Several studies introduce data from existing subjects to reduce the dependence of performance improvement on data from new subjects, but their optimization strategy based on adversarial learning with extensive data increases training time during the preparation procedure. Moreover, most previous methods only focus on the single-view information of EEG signals, but ignore the information from other views which may further improve performance. To enhance decoding performance while reducing preparation time, we propose a Temporal-Spectral fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a cross-view interaction module is proposed to facilitate information transfer and extract common representations across two-view features extracted from EEG temporal signals and spectrogram images. Then, an attention-based fusion module fuses the features of two views to obtain comprehensive discriminative features for classification. Furthermore, a multi-view consistency loss is proposed to maximize the feature similarity between two views of the same EEG signal. Finally, we propose a subject-specific adapter to rapidly transfer the knowledge of the model trained on data from existing subjects to decode data from new subjects. Experimental results show that TSformer-SA significantly outperforms comparison methods and achieves outstanding performance with limited training data from new subjects. This facilitates efficient decoding and rapid deployment of BCI systems in practical use. ",
        "title": "A Temporal-Spectral Fusion Transformer with Subject-specific Adapter for  Enhancing RSVP-BCI Decoding",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06341",
        "abstract_url": "http://arxiv.org/abs/2401.06341",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Shengyi"
            },
            {
                "last_name": "Chen",
                "first_name": "Weifeng"
            },
            {
                "last_name": "Bai",
                "first_name": "Min"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiong"
            },
            {
                "last_name": "Tu",
                "first_name": "Zhuowen"
            },
            {
                "last_name": "Li",
                "first_name": "Li Erran"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Affordance grounding refers to the task of finding the area of an object with which one can interact. It is a fundamental but challenging task, as a successful solution requires the comprehensive understanding of a scene in multiple aspects including detection, localization, and recognition of objects with their parts, of geo-spatial configuration/layout of the scene, of 3D shapes and physics, as well as of the functionality and potential interaction of the objects and humans. Much of the knowledge is hidden and beyond the image content with the supervised labels from a limited training set. In this paper, we make an attempt to improve the generalization capability of the current affordance grounding by taking the advantage of the rich world, abstract, and human-object-interaction knowledge from pretrained large-scale vision language models. Under the AGD20K benchmark, our proposed model demonstrates a significant performance gain over the competing methods for in-the-wild object affordance grounding. We further demonstrate it can ground affordance for objects from random Internet images, even if both objects and actions are unseen during training. Project site: https://jasonqsy.github.io/AffordanceLLM/ ",
        "title": "AffordanceLLM: Grounding Affordance from Vision Language Models",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06344",
        "abstract_url": "http://arxiv.org/abs/2401.06344",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weizheng"
            },
            {
                "last_name": "Mao",
                "first_name": "Le"
            },
            {
                "last_name": "Yang",
                "first_name": "Baijian"
            },
            {
                "last_name": "Chen",
                "first_name": "Guohua"
            },
            {
                "last_name": "Min",
                "first_name": "Byung-Cheol"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer network. Hyper-STTN outperformes other state-of-the-art baselines and ablation models on 5 real-world pedestrian motion datasets. ",
        "title": "Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for  Human Trajectory Prediction with Hypergraph Reasoning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06345",
        "abstract_url": "http://arxiv.org/abs/2401.06345",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Chang"
            },
            {
                "last_name": "Peng",
                "first_name": "Junran"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiangyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoxiang"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            },
            {
                "last_name": "Lei",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The text-to-image synthesis by diffusion models has recently shown remarkable performance in generating high-quality images. Although performs well for simple texts, the models may get confused when faced with complex texts that contain multiple objects or spatial relationships. To get the desired images, a feasible way is to manually adjust the textual descriptions, i.e., narrating the texts or adding some words, which is labor-consuming. In this paper, we propose a framework to learn the proper textual descriptions for diffusion models through prompt learning. By utilizing the quality guidance and the semantic guidance derived from the pre-trained diffusion model, our method can effectively learn the prompts to improve the matches between the input text and the generated images. Extensive experiments and analyses have validated the effectiveness of the proposed method. ",
        "title": "Seek for Incantations: Towards Accurate Text-to-Image Diffusion  Synthesis through Prompt Engineering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06349",
        "abstract_url": "http://arxiv.org/abs/2401.06349",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yifeng"
            },
            {
                "last_name": "Chen",
                "first_name": "Ke"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yihan"
            },
            {
                "last_name": "Wang",
                "first_name": "Haohan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Automated diagnosis of AD in brain images is becoming a clinically important technique to support precision and efficient diagnosis and treatment planning. A few efforts have been made to automatically diagnose AD in magnetic resonance imaging (MRI) using three-dimensional CNNs. However, due to the complexity of 3D models, the performance is still unsatisfactory, both in terms of accuracy and efficiency. To overcome the complexities of 3D images and 3D models, in this study, we aim to attack this problem with 2D vision Transformers. We propose a 2D transformer-based medical image model with various transformer attention encoders to diagnose AD in 3D MRI images, by cutting the 3D images into multiple 2D slices.The model consists of four main components: shared encoders across three dimensions, dimension-specific encoders, attention across images from the same dimension, and attention across three dimensions. It is used to obtain attention relationships among multiple sequences from different dimensions (axial, coronal, and sagittal) and multiple slices. We also propose morphology augmentation, an erosion and dilation based method to increase the structural difference between AD and normal images. In this experiment, we use multiple datasets from ADNI, AIBL, MIRAID, OASIS to show the performance of our model. Our proposed MedTransformer demonstrates a strong ability in diagnosing AD. These results demonstrate the effectiveness of MedTransformer in learning from 3D data using a much smaller model and its capability to generalize among different medical tasks, which provides a possibility to help doctors diagnose AD in a simpler way. ",
        "title": "MedTransformer: Accurate AD Diagnosis for 3D MRI Images through 2D  Vision Transformers",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06354",
        "abstract_url": "http://arxiv.org/abs/2401.06354",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Jungpyo"
            },
            {
                "last_name": "Lee",
                "first_name": "Sebastian D."
            },
            {
                "last_name": "Huh",
                "first_name": "Tae Myung"
            },
            {
                "last_name": "Stuart",
                "first_name": "Hannah S."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Suction cups offer a useful gripping solution, particularly in industrial robotics and warehouse applications. Vision-based grasp algorithms, like Dex-Net, show promise but struggle to accurately perceive dark or reflective objects, sub-resolution features, and occlusions, resulting in suction cup grip failures. In our prior work, we designed the Smart Suction Cup, which estimates the flow state within the cup and provides a mechanically resilient end-effector that can inform arm feedback control through a sense of touch. We then demonstrated how this cup's signals enable haptically-driven search behaviors for better grasping points on adversarial objects. This prior work uses a model-based approach to predict the desired motion direction, which opens up the question: does a data-driven approach perform better? This technical report provides an initial analysis harnessing the data previously collected. Specifically, we compare the model-based method with a preliminary data-driven approach to accurately estimate lateral pose adjustment direction for improved grasp success. ",
        "title": "Initial Analysis of Data-Driven Haptic Search for the Smart Suction Cup",
        "date": "2023-10-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06356",
        "abstract_url": "http://arxiv.org/abs/2401.06356",
        "authors": [
            {
                "last_name": "Sultan",
                "first_name": "Md Arafat"
            },
            {
                "last_name": "Trivedi",
                "first_name": "Aashka"
            },
            {
                "last_name": "Awasthy",
                "first_name": "Parul"
            },
            {
                "last_name": "Sil",
                "first_name": "Avirup"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board. ",
        "title": "An Empirical Investigation into the Effect of Parameter Choices in  Knowledge Distillation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06361",
        "abstract_url": "http://arxiv.org/abs/2401.06361",
        "authors": [
            {
                "last_name": "Sola",
                "first_name": "Mar Canet"
            },
            {
                "last_name": "Guljajeva",
                "first_name": "Varvara"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper discusses the artwork \"Visions of Destruction\", with a primary conceptual focus on the Anthropocene, which is communicated through audience interaction and generative AI as artistic research methods. Gaze-based interaction transitions the audience from mere observers to agents of landscape transformation, fostering a profound, on-the-edge engagement with pressing issues such as climate change and planetary destruction. The paper looks into early references of interactive art history that deploy eye-tracking as a method for audience interaction, and presents recent AI-aided artworks that demonstrate interactive latent space navigation. ",
        "title": "Visions Of Destruction: Exploring Human Impact on Nature by Navigating  the Latent Space of a Diffusion Model via Gaze",
        "date": "2023-12-28",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06366",
        "abstract_url": "http://arxiv.org/abs/2401.06366",
        "authors": [
            {
                "last_name": "Lyu",
                "first_name": "Minzhao"
            },
            {
                "last_name": "Madanapalli",
                "first_name": "Sharat Chandra"
            },
            {
                "last_name": "Vishwanath",
                "first_name": "Arun"
            },
            {
                "last_name": "Sivaraman",
                "first_name": "Vijay"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "PF"
        ],
        "abstract": "  Cloud gaming, wherein game graphics is rendered in the cloud and streamed back to the user as real-time video, expands the gaming market to billions of users who do not have gaming consoles or high-power graphics PCs. Companies like Nvidia, Amazon, Sony and Microsoft are investing in building cloud gaming platforms to tap this large unserved market. However, cloud gaming requires the user to have high bandwidth and stable network connectivity - whereas a typical console game needs about 100-200 kbps, a cloud game demands minimum 10-20 Mbps. This makes the Internet Service Provider (ISP) a key player in ensuring the end-user's good gaming experience. In this paper we develop a method to detect user experience to detect Nvidia's GeForce NOW cloud gaming sessions over their network infrastructure, and measure associated user experience. In particular, we envision ISPs taking advantage of our method to provision network capacity at the right time and in the right place to support growth in cloud gaming at the right experience level; as well as identify the role of contextual factors such as user setup (browser vs app) and connectivity type (wired vs wireless) in performance degradation. We first present a detailed anatomy of flow establishment and volumetric profiles of cloud gaming sessions over multiple platforms, followed by a method to detect gameplay and measure key experience aspects such as latency, frame rate and resolution via real-time analysis of network traffic. The insights and methods are also validated in the lab for XBox Cloud Gaming platform. We then implement and deploy our method in a campus network to capture gameplay behaviors and experience measures across various user setups and connectivity types which we believe are valuable for network operators. ",
        "title": "Network Anatomy and Real-Time Measurement of Nvidia GeForce NOW Cloud  Gaming",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06370",
        "abstract_url": "http://arxiv.org/abs/2401.06370",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yueyi"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Huang",
                "first_name": "Wei"
            },
            {
                "last_name": "Hu",
                "first_name": "Bo"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Wu",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Instance-aware embeddings predicted by deep neural networks have revolutionized biomedical instance segmentation, but its resource requirements are substantial. Knowledge distillation offers a solution by transferring distilled knowledge from heavy teacher networks to lightweight yet high-performance student networks. However, existing knowledge distillation methods struggle to extract knowledge for distinguishing instances and overlook global relation information. To address these challenges, we propose a graph relation distillation approach for efficient biomedical instance segmentation, which considers three essential types of knowledge: instance-level features, instance relations, and pixel-level boundaries. We introduce two graph distillation schemes deployed at both the intra-image level and the inter-image level: instance graph distillation (IGD) and affinity graph distillation (AGD). IGD constructs a graph representing instance features and relations, transferring these two types of knowledge by enforcing instance graph consistency. AGD constructs an affinity graph representing pixel relations to capture structured knowledge of instance boundaries, transferring boundary-related knowledge by ensuring pixel affinity consistency. Experimental results on a number of biomedical datasets validate the effectiveness of our approach, enabling student models with less than $ 1\\%$ parameters and less than $10\\%$ inference time while achieving promising performance compared to teacher models. ",
        "title": "Graph Relation Distillation for Efficient Biomedical Instance  Segmentation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06373",
        "abstract_url": "http://arxiv.org/abs/2401.06373",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Yi"
            },
            {
                "last_name": "Lin",
                "first_name": "Hongpeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Yang",
                "first_name": "Diyi"
            },
            {
                "last_name": "Jia",
                "first_name": "Ruoxi"
            },
            {
                "last_name": "Shi",
                "first_name": "Weiyan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs ",
        "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to  Challenge AI Safety by Humanizing LLMs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06374",
        "abstract_url": "http://arxiv.org/abs/2401.06374",
        "authors": [
            {
                "last_name": "Ding",
                "first_name": "Haoxuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Junyu"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the emergence of foundation model, this novel paradigm of deep learning has encouraged many powerful achievements in natural language processing and computer vision. There are many advantages of foundation model, such as excellent feature extraction power, mighty generalization ability, great few-shot and zero-shot learning capacity, etc. which are beneficial to vision tasks. As the unique identity of vehicle, different countries and regions have diverse license plate (LP) styles and appearances, and even different types of vehicles have different LPs. However, recent deep learning based license plate detectors are mainly trained on specific datasets, and these limited datasets constrain the effectiveness and robustness of LP detectors. To alleviate the negative impact of limited data, an attempt to exploit the advantages of foundation model is implement in this paper. We customize a vision foundation model, i.e. Segment Anything Model (SAM), for LP detection task and propose the first LP detector based on vision foundation model, named SamLP. Specifically, we design a Low-Rank Adaptation (LoRA) fine-tuning strategy to inject extra parameters into SAM and transfer SAM into LP detection task. And then, we further propose a promptable fine-tuning step to provide SamLP with prompatable segmentation capacity. The experiments show that our proposed SamLP achieves promising detection performance compared to other LP detectors. Meanwhile, the proposed SamLP has great few-shot and zero-shot learning ability, which shows the potential of transferring vision foundation model. The code is available at https://github.com/Dinghaoxuan/SamLP ",
        "title": "SamLP: A Customized Segment Anything Model for License Plate Detection",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06375",
        "abstract_url": "http://arxiv.org/abs/2401.06375",
        "authors": [
            {
                "last_name": "Banks",
                "first_name": "Gordon"
            },
            {
                "last_name": "Bierhuizen",
                "first_name": "Gates"
            },
            {
                "last_name": "McCrum",
                "first_name": "Katherine"
            },
            {
                "last_name": "Wengert",
                "first_name": "Ellen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We examine ProcessGPT, an AI model designed to automate, augment, and improve business processes, to study the challenges of managing business processes within the cognitive limitations of the human workforce, particularly individuals with cognitive disabilities. ProcessGPT provides a blueprint for designing efficient business processes that take into account human cognitive limitations. By viewing this through the lens of cognitive disabilities, we show that ProcessGPT improves process usability for individuals with and without cognitive disabilities. We also demonstrate that organizations implementing ProcessGPT-like capabilities will realize increased productivity, morale, and inclusion. ",
        "title": "Cognitive BPM as an Equalizer: Improving Access and Efficiency for  Employees with (and without) Cognitive Disabilities",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06377",
        "abstract_url": "http://arxiv.org/abs/2401.06377",
        "authors": [
            {
                "last_name": "Qi",
                "first_name": "Xinda"
            },
            {
                "last_name": "Mei",
                "first_name": "Yu"
            },
            {
                "last_name": "Chen",
                "first_name": "Dong"
            },
            {
                "last_name": "Li",
                "first_name": "Zhaojian"
            },
            {
                "last_name": "Tan",
                "first_name": "Xiaobo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We propose a novel multi-section cable-driven soft robotic arm inspired by octopus tentacles along with a new modeling approach. Each section of the modular manipulator is made of a soft tubing backbone, a soft silicon arm body, and two rigid endcaps, which connect adjacent sections and decouple the actuation cables of different sections. The soft robotic arm is made with casting after the rigid endcaps are 3D-printed, achieving low-cost and convenient fabrication. To capture the nonlinear effect of cables pushing into the soft silicon arm body, which results from the absence of intermediate rigid cable guides for higher compliance, an analytical static model is developed to capture the relationship between the bending curvature and the cable lengths. The proposed model shows superior prediction performance in experiments over that of a baseline model, especially under large bending conditions. Based on the nonlinear static model, a kinematic model of a multi-section arm is further developed and used to derive a motion planning algorithm. Experiments show that the proposed soft arm has high flexibility and a large workspace, and the tracking errors under the algorithm based on the proposed modeling approach are up to 52$\\%$ smaller than those with the algorithm derived from the baseline model. The presented modeling approach is expected to be applicable to a broad range of soft cable-driven actuators and manipulators. ",
        "title": "Design and Nonlinear Modeling of a Modular Cable Driven Soft Robotic Arm",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06378",
        "abstract_url": "http://arxiv.org/abs/2401.06378",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Prantar"
            },
            {
                "last_name": "Shah",
                "first_name": "Vihan"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We show new lower bounds in the \\emph{Merlin-Arthur} (MA) communication model and the related \\emph{annotated streaming} or stream verification model. The MA communication model is an enhancement of the classical communication model, where in addition to the usual players Alice and Bob, there is an all-powerful but untrusted player Merlin who knows their inputs and tries to convince them about the output. Most functions have MA protocols with total communication significantly smaller than what would be needed without Merlin. We focus on the online MA (OMA) model, which is the MA analogue of one-way communication, and introduce the notion of \\emph{non-trivial-OMA} complexity of a function. This is the minimum total communication needed by any non-trivial OMA protocol computing that function, where a trivial OMA protocol is one where Alice sends Bob roughly as many bits as she would have sent without Merlin. We prove a lower bound on the non-trivial-OMA complexity of a natural function \\emph{Equals-Index} (basically the well-known Index problem on large domains) and identify it as a canonical problem for proving strong lower bounds on this complexity: reductions from it (i) reproduce and/or improve upon the lower bounds for all functions that were previously known to have large non-trivial-OMA complexity, (ii) exhibit the first explicit functions whose non-trivial-OMA complexity is superlinear, and even exponential, in their classical one-way complexity, and (iii) show functions on input size $n$ for which this complexity is as large as $n/\\log n$. While exhibiting a function with $\\omega(\\sqrt{n})$ (standard) OMA complexity is a longstanding open problem, we did not even know of any function with $\\omega(\\sqrt{n})$ non-trivial-OMA complexity. We further extend the lower bounds to a related streaming model called annotated streaming. ",
        "title": "New Lower Bounds in Merlin-Arthur Communication and Graph Streaming  Verification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06379",
        "abstract_url": "http://arxiv.org/abs/2401.06379",
        "authors": [
            {
                "last_name": "Daggitt",
                "first_name": "Matthew L."
            },
            {
                "last_name": "Kokke",
                "first_name": "Wen"
            },
            {
                "last_name": "Atkey",
                "first_name": "Robert"
            },
            {
                "last_name": "Slusarz",
                "first_name": "Natalia"
            },
            {
                "last_name": "Arnaboldi",
                "first_name": "Luca"
            },
            {
                "last_name": "Komendantskaya",
                "first_name": "Ekaterina"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Neuro-symbolic programs -- programs containing both machine learning components and traditional symbolic code -- are becoming increasingly widespread. However, we believe that there is still a lack of a general methodology for verifying these programs whose correctness depends on the behaviour of the machine learning components. In this paper, we identify the ``embedding gap'' -- the lack of techniques for linking semantically-meaningful ``problem-space'' properties to equivalent ``embedding-space'' properties -- as one of the key issues, and describe Vehicle, a tool designed to facilitate the end-to-end verification of neural-symbolic programs in a modular fashion. Vehicle provides a convenient language for specifying ``problem-space'' properties of neural networks and declaring their relationship to the ``embedding-space\", and a powerful compiler that automates interpretation of these properties in the language of a chosen machine-learning training environment, neural network verifier, and interactive theorem prover. We demonstrate Vehicle's utility by using it to formally verify the safety of a simple autonomous car equipped with a neural network controller. ",
        "title": "Vehicle: Bridging the Embedding Gap in the Verification of  Neuro-Symbolic Programs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06382",
        "abstract_url": "http://arxiv.org/abs/2401.06382",
        "authors": [
            {
                "last_name": "Adkins",
                "first_name": "Mark"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CL"
        ],
        "abstract": "  As Artificial Intelligence (AI) technology becomes more and more prevalent, it becomes increasingly important to explore how we as humans interact with AI. The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer Interaction (HCI) field and aims to examine this very notion. Many interaction patterns have been implemented without fully understanding the changes in required cognition as well as the cognitive science implications of using these alternative interfaces that aim to be more human-like in nature. Prior research suggests that theory of mind representations are crucial to successful and effortless communication, however very little is understood when it comes to how theory of mind representations are established when interacting with AI. ",
        "title": "What should I say? -- Interacting with AI and Natural Language  Interfaces",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06384",
        "abstract_url": "http://arxiv.org/abs/2401.06384",
        "authors": [
            {
                "last_name": "Mollah",
                "first_name": "Muhammad Baqer"
            },
            {
                "last_name": "Azad",
                "first_name": "Md Abul Kalam"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yinghui"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  Smart devices are considered as an integral part of Internet of Things (IoT), have an aim to make a dynamic network to exchange information, collect data, analysis, and make optimal decisions in an autonomous way to achieve more efficient, automatic, and economical services. Message dissemination among these smart devices allows adding new features, sending updated instructions, alerts or safety messages, informing the pricing information or billing amount, incentives, and installing security patches. On one hand, such message disseminations are directly beneficial to the all parties involved in the IoT system. On the other hand, due to remote procedure, smart devices, vendors, and other involved authorities might have to meet a number of security, privacy, and performance related concerns while disseminating messages among targeted devices. To this end, in this paper, we design STarEdgeChain, a security and privacy aware targeted message dissemination in IoT to show how blockchain along with advanced cryptographic techniques are devoted to address such concerns. In fact, the STarEdgeChain employs a permissioned blockchain assisted edge computing in order to expedite a single signcrypted message dissemination among targeted groups of devices, at the same time avoiding the dependency of utilizing multiple unicasting approaches. Finally, we develop a software prototype of STarEdgeChain and show it's practicability for smart devices. The codes are publicly available at https://github.com/mbaqer/Blockchain-IoT ",
        "title": "Secure Targeted Message Dissemination in IoT Using Blockchain Enabled  Edge Computing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06385",
        "abstract_url": "http://arxiv.org/abs/2401.06385",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Zhenlong"
            },
            {
                "last_name": "Cao",
                "first_name": "Jiakai"
            },
            {
                "last_name": "Li",
                "first_name": "Zhaoxin"
            },
            {
                "last_name": "Jiang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaoqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we introduce Segmentation-Driven Deformation Multi-View Stereo (SD-MVS), a method that can effectively tackle challenges in 3D reconstruction of textureless areas. We are the first to adopt the Segment Anything Model (SAM) to distinguish semantic instances in scenes and further leverage these constraints for pixelwise patch deformation on both matching cost and propagation. Concurrently, we propose a unique refinement strategy that combines spherical coordinates and gradient descent on normals and pixelwise search interval on depths, significantly improving the completeness of reconstructed 3D model. Furthermore, we adopt the Expectation-Maximization (EM) algorithm to alternately optimize the aggregate matching cost and hyperparameters, effectively mitigating the problem of parameters being excessively dependent on empirical tuning. Evaluations on the ETH3D high-resolution multi-view stereo benchmark and the Tanks and Temples dataset demonstrate that our method can achieve state-of-the-art results with less time consumption. ",
        "title": "SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical  Refinement and EM optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06386",
        "abstract_url": "http://arxiv.org/abs/2401.06386",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Minrui"
            },
            {
                "last_name": "Niyato",
                "first_name": "Dusit"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zehui"
            },
            {
                "last_name": "Guo",
                "first_name": "Song"
            },
            {
                "last_name": "Fang",
                "first_name": "Yuguang"
            },
            {
                "last_name": "Kim",
                "first_name": "Dong In"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Mobile multimedia networks (MMNs) demonstrate great potential in delivering low-latency and high-quality entertainment and tactical applications, such as short-video sharing, online conferencing, and battlefield surveillance. For instance, in tactical surveillance of battlefields, scalability and sustainability are indispensable for maintaining large-scale military multimedia applications in MMNs. Therefore, many data-driven networking solutions are leveraged to optimize streaming strategies based on real-time traffic analysis and resource monitoring. In addition, generative AI (GAI) can not only increase the efficiency of existing data-driven solutions through data augmentation but also develop potential capabilities for MMNs, including AI-generated content (AIGC) and AI-aided perception. In this article, we propose the framework of GAI-enabled MMNs that leverage the capabilities of GAI in data and content synthesis to distribute high-quality and immersive interactive content in wireless networks. Specifically, we outline the framework of GAI-enabled MMNs and then introduce its three main features, including distribution, generation, and perception. Furthermore, we propose a second-score auction mechanism for allocating network resources by considering GAI model values and other metrics jointly. The experimental results show that the proposed auction mechanism can effectively increase social welfare by allocating resources and models with the highest user satisfaction. ",
        "title": "Generative AI-enabled Mobile Tactical Multimedia Networks: Distribution,  Generation, and Perception",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06387",
        "abstract_url": "http://arxiv.org/abs/2401.06387",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Ye-Xin"
            },
            {
                "last_name": "Ai",
                "first_name": "Yang"
            },
            {
                "last_name": "Du",
                "first_name": "Hui-Peng"
            },
            {
                "last_name": "Ling",
                "first_name": "Zhen-Hua"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Speech bandwidth extension (BWE) refers to widening the frequency bandwidth range of speech signals, enhancing the speech quality towards brighter and fuller. This paper proposes a generative adversarial network (GAN) based BWE model with parallel prediction of Amplitude and Phase spectra, named AP-BWE, which achieves both high-quality and efficient wideband speech waveform generation. The proposed AP-BWE generator is entirely based on convolutional neural networks (CNNs). It features a dual-stream architecture with mutual interaction, where the amplitude stream and the phase stream communicate with each other and respectively extend the high-frequency components from the input narrowband amplitude and phase spectra. To improve the naturalness of the extended speech signals, we employ a multi-period discriminator at the waveform level and design a pair of multi-resolution amplitude and phase discriminators at the spectral level, respectively. Experimental results demonstrate that our proposed AP-BWE achieves state-of-the-art performance in terms of speech quality for BWE tasks targeting sampling rates of both 16 kHz and 48 kHz. In terms of generation efficiency, due to the all-convolutional architecture and all-frame-level operations, the proposed AP-BWE can generate 48 kHz waveform samples 292.3 times faster than real-time on a single RTX 4090 GPU and 18.1 times faster than real-time on a single CPU. Notably, to our knowledge, AP-BWE is the first to achieve the direct extension of the high-frequency phase spectrum, which is beneficial for improving the effectiveness of existing BWE methods. ",
        "title": "Towards High-Quality and Efficient Speech Bandwidth Extension with  Parallel Amplitude and Phase Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06390",
        "abstract_url": "http://arxiv.org/abs/2401.06390",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Fan"
            },
            {
                "last_name": "Wang",
                "first_name": "Haoxu"
            },
            {
                "last_name": "Shi",
                "first_name": "Xian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shiliang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "MM"
        ],
        "abstract": "  The growing prevalence of online conferences and courses presents a new challenge in improving automatic speech recognition (ASR) with enriched textual information from video slides. In contrast to rare phrase lists, the slides within videos are synchronized in real-time with the speech, enabling the extraction of long contextual bias. Therefore, we propose a novel long-context biasing network (LCB-net) for audio-visual speech recognition (AVSR) to leverage the long-context information available in videos effectively. Specifically, we adopt a bi-encoder architecture to simultaneously model audio and long-context biasing. Besides, we also propose a biasing prediction module that utilizes binary cross entropy (BCE) loss to explicitly determine biased phrases in the long-context biasing. Furthermore, we introduce a dynamic contextual phrases simulation to enhance the generalization and robustness of our LCB-net. Experiments on the SlideSpeech, a large-scale audio-visual corpus enriched with slides, reveal that our proposed LCB-net outperforms general ASR model by 9.4%/9.1%/10.9% relative WER/U-WER/B-WER reduction on test set, which enjoys high unbiased and biased performance. Moreover, we also evaluate our model on LibriSpeech corpus, leading to 23.8%/19.2%/35.4% relative WER/U-WER/B-WER reduction over the ASR model. ",
        "title": "LCB-net: Long-Context Biasing for Audio-Visual Speech Recognition",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06391",
        "abstract_url": "http://arxiv.org/abs/2401.06391",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jian"
            },
            {
                "last_name": "Feng",
                "first_name": "Yebo"
            },
            {
                "last_name": "Li",
                "first_name": "Tianlin"
            },
            {
                "last_name": "Sun",
                "first_name": "Weisong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            },
            {
                "last_name": "Peng",
                "first_name": "Xin"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Recent code large language models (LLMs) have shown promising performance in generating standalone functions but face limitations in repository-level code generation due to their lack of awareness of repository-level dependencies (e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen, an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Data Augmentation and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding docstrings, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one.   We conduct comprehensive experiments to evaluate ToolGen's effectiveness in repository-level code generation. To facilitate this evaluation, we create a benchmark comprising 680 real-world code repositories and introduce two new repository-level metrics: Dependency Coverage and Success Rate. The results demonstrate that ToolGen significantly improves dependency coverage by 15.2% to 45.8% and success rates by 10.9% to 42.2% across three distinct code LLMs, while maintaining competitive performance in widely-recognized similarity metrics. Furthermore, our generalizability evaluation confirms ToolGen's consistent performance when applied to diverse code LLMs, including various model architectures and scales. ",
        "title": "Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code  Generation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06394",
        "abstract_url": "http://arxiv.org/abs/2401.06394",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Wenyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xinghua"
            },
            {
                "last_name": "Cui",
                "first_name": "Shiyao"
            },
            {
                "last_name": "Huang",
                "first_name": "Kun"
            },
            {
                "last_name": "Wang",
                "first_name": "Xuebin"
            },
            {
                "last_name": "Liu",
                "first_name": "Tingwen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment elements for a given sentence, which is a critical task in the field of aspect-based sentiment analysis. However, the data imbalance issue has not received sufficient attention in ASQP task. In this paper, we divide the issue into two-folds, quad-pattern imbalance and aspect-category imbalance, and propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance issue. Specifically, a data augmentation process with a condition function adaptively enhances the tail quad patterns and aspect categories, alleviating the data imbalance in ASQP. Following previous studies, we also further explore the generative framework for extracting complete quads by introducing the category prior knowledge and syntax-guided decoding target. Experimental results demonstrate that data augmentation for imbalance in ASQP task can improve the performance, and the proposed ADA method is superior to naive data oversampling. ",
        "title": "Adaptive Data Augmentation for Aspect Sentiment Quad Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06395",
        "abstract_url": "http://arxiv.org/abs/2401.06395",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Bohan"
            },
            {
                "last_name": "Wu",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Humans possess the capability to comprehend diverse modalities and seamlessly transfer information between them. In this work, we introduce ModaVerse, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio. Predominant MLLM frameworks have largely relied on the alignment of latent spaces of textual and non-textual features. This alignment process, which synchronizes a language model trained on textual data with encoders and decoders trained on multi-modal data, often necessitates extensive training of several projection layers in multiple stages. Inspired by LLM-as-agent methodologies, we propose a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language. It aligns the LLM's output with the input of generative models, avoiding the complexities associated with latent feature alignments, and simplifying the multiple training stages of existing MLLMs into a single, efficient process. This conceptual advancement leads to significant reductions in both data and computational costs. By conducting experiments on several benchmarks, we demonstrate that our approach attains comparable performance with the state of the art while achieving considerable efficiencies in data usage and training duration. ",
        "title": "ModaVerse: Efficiently Transforming Modalities with LLMs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06397",
        "abstract_url": "http://arxiv.org/abs/2401.06397",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Bowen"
            },
            {
                "last_name": "Zhao",
                "first_name": "Peisen"
            },
            {
                "last_name": "Wang",
                "first_name": "Zichen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaoming"
            },
            {
                "last_name": "Li",
                "first_name": "Jin"
            },
            {
                "last_name": "Dai",
                "first_name": "Wenrui"
            },
            {
                "last_name": "Zou",
                "first_name": "Junni"
            },
            {
                "last_name": "Xiong",
                "first_name": "Hongkai"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaopeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vision-language foundation models, represented by Contrastive language-image pre-training (CLIP), have gained increasing attention for jointly understanding both vision and textual tasks. However, existing approaches primarily focus on training models to match global image representations with textual descriptions, thereby overlooking the critical alignment between local regions and corresponding text tokens. This paper extends CLIP with multi-granularity alignment. Notably, we deliberately construct a new dataset comprising pseudo annotations at various levels of granularities, encompassing image-level, region-level, and pixel-level captions/tags. Accordingly, we develop a unified multi-granularity learning framework, named UMG-CLIP, that simultaneously empowers the model with versatile perception abilities across different levels of detail. Equipped with parameter efficient tuning, UMG-CLIP surpasses current widely used CLIP models and achieves state-of-the-art performance on diverse image understanding benchmarks, including open-world recognition, retrieval, semantic segmentation, and panoptic segmentation tasks. We hope UMG-CLIP can serve as a valuable option for advancing vision-language foundation models. ",
        "title": "UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World  Understanding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06398",
        "abstract_url": "http://arxiv.org/abs/2401.06398",
        "authors": [
            {
                "last_name": "Das",
                "first_name": "Sudhansu Bala"
            },
            {
                "last_name": "Rodrigues",
                "first_name": "Leo Raphael"
            },
            {
                "last_name": "Mishra",
                "first_name": "Tapas Kumar"
            },
            {
                "last_name": "Patra",
                "first_name": "Bidyut Kr."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The conversion of content from one language to another utilizing a computer system is known as Machine Translation (MT). Various techniques have come up to ensure effective translations that retain the contextual and lexical interpretation of the source language. End-to-end Neural Machine Translation (NMT) is a popular technique and it is now widely used in real-world MT systems. Massive amounts of parallel datasets (sentences in one language alongside translations in another) are required for MT systems. These datasets are crucial for an MT system to learn linguistic structures and patterns of both languages during the training phase. One such dataset is Samanantar, the largest publicly accessible parallel dataset for Indian languages (ILs). Since the corpus has been gathered from various sources, it contains many incorrect translations. Hence, the MT systems built using this dataset cannot perform to their usual potential. In this paper, we propose an algorithm to remove mistranslations from the training corpus and evaluate its performance and efficiency. Two Indic languages (ILs), namely, Hindi (HIN) and Odia (ODI) are chosen for the experiment. A baseline NMT system is built for these two ILs, and the effect of different dataset sizes is also investigated. The quality of the translations in the experiment is evaluated using standard metrics such as BLEU, METEOR, and RIBES. From the results, it is observed that removing the incorrect translation from the dataset makes the translation quality better. It is also noticed that, despite the fact that the ILs-English and English-ILs systems are trained using the same corpus, ILs-English works more effectively across all the evaluation metrics. ",
        "title": "An approach for mistranslation removal from popular dataset for Indic MT  Task",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06401",
        "abstract_url": "http://arxiv.org/abs/2401.06401",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Li",
                "first_name": "Ge"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Li",
                "first_name": "Yongmin"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hao"
            },
            {
                "last_name": "Liu",
                "first_name": "Huanyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Kaibo"
            },
            {
                "last_name": "Wang",
                "first_name": "Lecheng"
            },
            {
                "last_name": "Fang",
                "first_name": "Zheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Lanshen"
            },
            {
                "last_name": "Ding",
                "first_name": "Jiazheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xuanming"
            },
            {
                "last_name": "Dong",
                "first_name": "Yihong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Gu",
                "first_name": "Bin"
            },
            {
                "last_name": "Yang",
                "first_name": "Mengfei"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL"
        ],
        "abstract": "  How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experiments. We also discuss the challenges and future directions of code generation in practical projects. We open-source DevEval and hope it can facilitate the development of code generation in practical projects. ",
        "title": "DevEval: Evaluating Code Generation in Practical Software Projects",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06405",
        "abstract_url": "http://arxiv.org/abs/2401.06405",
        "authors": [
            {
                "last_name": "Kimura",
                "first_name": "Kei"
            },
            {
                "last_name": "Makino",
                "first_name": "Kazuhisa"
            },
            {
                "last_name": "Yamada",
                "first_name": "Shota"
            },
            {
                "last_name": "Yoshizumi",
                "first_name": "Ryo"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Characterizing the solution sets in a problem by closedness under operations is recognized as one of the key aspects of algorithm development, especially in constraint satisfaction. An example from the Boolean satisfiability problem is that the solution set of a Horn conjunctive normal form (CNF) is closed under the minimum operation, and this property implies that minimizing a nonnegative linear function over a Horn CNF can be done in polynomial time. In this paper, we focus on the set of integer points (vectors) in a polyhedron, and study the relation between these sets and closedness under operations from the viewpoint of 2-decomposability. By adding further conditions to the 2-decomposable polyhedra, we show that important classes of sets of integer vectors in polyhedra are characterized by 2-decomposability and closedness under certain operations, and in some classes, by closedness under operations alone. The most prominent result we show is that the set of integer vectors in a unit-two-variable-per-inequality polyhedron can be characterized by closedness under the median and directed discrete midpoint operations, each of these operations was independently considered in constraint satisfaction and discrete convex analysis. ",
        "title": "Characterizing the integer points in 2-decomposable polyhedra by  closedness under operations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06406",
        "abstract_url": "http://arxiv.org/abs/2401.06406",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Lingchao"
            },
            {
                "last_name": "Wang",
                "first_name": "Hairong"
            },
            {
                "last_name": "Hu",
                "first_name": "Leland S."
            },
            {
                "last_name": "Tran",
                "first_name": "Nhan L"
            },
            {
                "last_name": "Canoll",
                "first_name": "Peter D"
            },
            {
                "last_name": "Swanson",
                "first_name": "Kristin R"
            },
            {
                "last_name": "Li",
                "first_name": "Jing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning has enabled in-depth analysis of rich multi-omics profiles and medical imaging for cancer diagnosis and prognosis. Despite these advancements, machine learning models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to surmount these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art machine learning studies that adopted the fusion of biomedical knowledge and data, termed knowledge-informed machine learning, for cancer diagnosis and prognosis. Emphasizing the properties inherent in four primary data types including clinical, imaging, molecular, and treatment data, we highlight modeling considerations relevant to these contexts. We provide an overview of diverse forms of knowledge representation and current strategies of knowledge integration into machine learning pipelines with concrete examples. We conclude the review article by discussing future directions to advance cancer research through knowledge-informed machine learning. ",
        "title": "Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis:  A review",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06407",
        "abstract_url": "http://arxiv.org/abs/2401.06407",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jincheng"
            },
            {
                "last_name": "Wolek",
                "first_name": "Artur"
            },
            {
                "last_name": "Willis",
                "first_name": "Andrew R."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  This article presents a comprehensive review of and analysis of state-of-the-art mapping algorithms for UAV (Unmanned Aerial Vehicle) applications, focusing on canopy-level and high-speed scenarios. This article presents a comprehensive exploration of sensor technologies suitable for UAV mapping, assessing their capabilities to provide measurements that meet the requirements of fast UAV mapping. Furthermore, the study conducts extensive experiments in a simulated environment to evaluate the performance of three distinct mapping algorithms: Direct Sparse Odometry (DSO), Stereo DSO (SDSO), and DSO Lite (DSOL). The experiments delve into mapping accuracy and mapping speed, providing valuable insights into the strengths and limitations of each algorithm. The results highlight the versatility and shortcomings of these algorithms in meeting the demands of modern UAV applications. The findings contribute to a nuanced understanding of UAV mapping dynamics, emphasizing their applicability in complex environments and high-speed scenarios. This research not only serves as a benchmark for mapping algorithm comparisons but also offers practical guidance for selecting sensors tailored to specific UAV mapping applications. ",
        "title": "UAV-borne Mapping Algorithms for Canopy-Level and High-Speed Drone  Applications",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06408",
        "abstract_url": "http://arxiv.org/abs/2401.06408",
        "authors": [
            {
                "last_name": "Lucy",
                "first_name": "Li"
            },
            {
                "last_name": "Gururangan",
                "first_name": "Suchin"
            },
            {
                "last_name": "Soldaini",
                "first_name": "Luca"
            },
            {
                "last_name": "Strubell",
                "first_name": "Emma"
            },
            {
                "last_name": "Bamman",
                "first_name": "David"
            },
            {
                "last_name": "Klein",
                "first_name": "Lauren"
            },
            {
                "last_name": "Dodge",
                "first_name": "Jesse"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten \"quality\" and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications. ",
        "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of  English Pretraining Data Filters",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06411",
        "abstract_url": "http://arxiv.org/abs/2401.06411",
        "authors": [
            {
                "last_name": "Aviles",
                "first_name": "Robert S."
            },
            {
                "last_name": "Li",
                "first_name": "Xi"
            },
            {
                "last_name": "Lu",
                "first_name": "Lei"
            },
            {
                "last_name": "Ni",
                "first_name": "Zhaorui"
            },
            {
                "last_name": "Beerel",
                "first_name": "Peter A."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  A key distinguishing feature of single flux quantum (SFQ) circuits is that each logic gate is clocked. This feature forces the introduction of path-balancing flip-flops to ensure proper synchronization of inputs at each gate. This paper proposes a polynomial time complexity approximation algorithm for clocking assignments that minimizes the insertion of path balancing buffers for multi-threaded multi-phase clocking of SFQ circuits. Existing SFQ multi-phase clocking solutions have been shown to effectively reduce the number of required buffers inserted while maintaining high throughput, however, the associated clock assignment algorithms have exponential complexity and can have prohibitively long runtimes for large circuits, limiting the scalability of this approach. Our proposed algorithm is based on a linear program (LP) that leads to solutions that are experimentally on average within 5% of the optimum and helps accelerate convergence towards the optimal integer linear program (ILP) based solution. The improved LP and ILP runtimes permit multi-phase clocking schemes to scale to larger SFQ circuits than previous state of the art clocking assignment methods. We further extend the existing algorithm to support fanout sharing of the added buffers, saving, on average, an additional 10% of the inserted DFFs. Compared to traditional full path balancing (FPB) methods across 10 benchmarks, our enhanced LP saves 79.9%, 87.8%, and 91.2% of the inserted buffers for 2, 3, and 4 clock phases respectively. Finally, we extend this approach to the generation of circuits that completely mitigate potential hold-time violations at the cost of either adding on average less than 10% more buffers (for designs with 3 or more clock phases) or, more generally, adding a clock phase and thereby reducing throughput. ",
        "title": "An Efficient and Scalable Clocking Assignment Algorithm for  Multi-Threaded Multi-Phase Single Flux Quantum Circuits",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06412",
        "abstract_url": "http://arxiv.org/abs/2401.06412",
        "authors": [
            {
                "last_name": "Takamido",
                "first_name": "Ryota"
            },
            {
                "last_name": "Suzuki",
                "first_name": "Chiharu"
            },
            {
                "last_name": "Ota",
                "first_name": "Jun"
            },
            {
                "last_name": "Nakamoto",
                "first_name": "Hiroki"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Background: Simultaneously focusing on intra- and inter-individual body dynamics and elucidating how these affect each other will help understand human inter-personal coordination behavior. However, this association has not been investigated previously owing to difficulties in analyzing complex causal relations among several body components.To address this issue, this study proposes a new analytical framework that attempts to understand the underlying causal structures behind each joint movement of individual baseball players using neural Granger causality (NGC) as the explainable AI. Methods: In the NGC analysis, causal relationships were defined as the size of the weight parameters of the first layer of a machine-learning model trained to predict the future state of a specific time-series variable. To verify the approach in a practical context, we conducted an experiment with 16 pairs of expert baseball pitchers and batters; input datasets with 27 joint resultant velocity data (joints of 13 pitchers and 14 batters) were generated and used for model training.Results: NGC analysis revealed significant causal relations among intra- and inter-individual body components such as the batter's hands having a causal effect from the pitcher's throwing arm. Remarkably, although the causality from the batter's body to pitcher's body is much lower than the reverse, it is significantly correlated with batter performance outcomes. Conclusions: The above results suggest the effectiveness of NGC analysis for understanding whole-body inter-personal coordination dynamics and that of the AI technique as a new approach for analyzing complex human behavior from a different perspective than conventional techniques. ",
        "title": "Understanding whole-body inter-personal dynamics between two players  using neural Granger causality as the explainable AI (XAI)",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06413",
        "abstract_url": "http://arxiv.org/abs/2401.06413",
        "authors": [
            {
                "last_name": "Ahuja",
                "first_name": "Sanju"
            },
            {
                "last_name": "Jain",
                "first_name": "Ridhi"
            },
            {
                "last_name": "Kumar",
                "first_name": "Jyoti"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  'Automating the user away' has been designated as a dark pattern in literature for performing tasks without user consent or confirmation. However, limited studies have been reported on how users experience the sense of autonomy when digital systems fully or partially bypass consent. More research is required to understand what makes automaticity a threat to autonomy. To address this gap, a qualitative interview study with 10 users was conducted to investigate the user experience of Microsoft Windows updates. It was found that ten design features of Windows updates impact the autonomy experience. For each design feature, the contextual factors which influence its impact on autonomy were also noted. The findings of this paper can help designers understand the ethical concerns posed by automaticity in design and identify measures to mitigate these concerns. ",
        "title": "Why Doesn't Microsoft Let Me Sleep? How Automaticity of Windows Updates  Impacts User Autonomy",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06415",
        "abstract_url": "http://arxiv.org/abs/2401.06415",
        "authors": [
            {
                "last_name": "Cha",
                "first_name": "Junuk"
            },
            {
                "last_name": "Lee",
                "first_name": "Hansol"
            },
            {
                "last_name": "Kim",
                "first_name": "Jaewon"
            },
            {
                "last_name": "Truong",
                "first_name": "Nhat Nguyen Bao"
            },
            {
                "last_name": "Yoon",
                "first_name": "Jae Shin"
            },
            {
                "last_name": "Baek",
                "first_name": "Seungryul"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces a novel pipeline to reconstruct the geometry of interacting multi-person in clothing on a globally coherent scene space from a single image. The main challenge arises from the occlusion: a part of a human body is not visible from a single view due to the occlusion by others or the self, which introduces missing geometry and physical implausibility (e.g., penetration). We overcome this challenge by utilizing two human priors for complete 3D geometry and surface contacts. For the geometry prior, an encoder learns to regress the image of a person with missing body parts to the latent vectors; a decoder decodes these vectors to produce 3D features of the associated geometry; and an implicit network combines these features with a surface normal map to reconstruct a complete and detailed 3D humans. For the contact prior, we develop an image-space contact detector that outputs a probability distribution of surface contacts between people in 3D. We use these priors to globally refine the body poses, enabling the penetration-free and accurate reconstruction of interacting multi-person in clothing on the scene space. The results demonstrate that our method is complete, globally coherent, and physically plausible compared to existing methods. ",
        "title": "3D Reconstruction of Interacting Multi-Person in Clothing from a Single  Image",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06416",
        "abstract_url": "http://arxiv.org/abs/2401.06416",
        "authors": [
            {
                "last_name": "Kallini",
                "first_name": "Julie"
            },
            {
                "last_name": "Papadimitriou",
                "first_name": "Isabel"
            },
            {
                "last_name": "Futrell",
                "first_name": "Richard"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            },
            {
                "last_name": "Potts",
                "first_name": "Christopher"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations. ",
        "title": "Mission: Impossible Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06421",
        "abstract_url": "http://arxiv.org/abs/2401.06421",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Geethen"
            },
            {
                "last_name": "Moncrieff",
                "first_name": "Glenn"
            },
            {
                "last_name": "Venter",
                "first_name": "Zander"
            },
            {
                "last_name": "Cawse-Nicholson",
                "first_name": "Kerry"
            },
            {
                "last_name": "Slingsby",
                "first_name": "Jasper"
            },
            {
                "last_name": "Robinson",
                "first_name": "Tamara B"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Unreliable predictions can occur when using artificial intelligence (AI) systems with negative consequences for downstream applications, particularly when employed for decision-making. Conformal prediction provides a model-agnostic framework for uncertainty quantification that can be applied to any dataset, irrespective of its distribution, post hoc. In contrast to other pixel-level uncertainty quantification methods, conformal prediction operates without requiring access to the underlying model and training dataset, concurrently offering statistically valid and informative prediction regions, all while maintaining computational efficiency. In response to the increased need to report uncertainty alongside point predictions, we bring attention to the promise of conformal prediction within the domain of Earth Observation (EO) applications. To accomplish this, we assess the current state of uncertainty quantification in the EO domain and found that only 20% of the reviewed Google Earth Engine (GEE) datasets incorporated a degree of uncertainty information, with unreliable methods prevalent. Next, we introduce modules that seamlessly integrate into existing GEE predictive modelling workflows and demonstrate the application of these tools for datasets spanning local to global scales, including the Dynamic World and Global Ecosystem Dynamics Investigation (GEDI) datasets. These case studies encompass regression and classification tasks, featuring both traditional and deep learning-based workflows. Subsequently, we discuss the opportunities arising from the use of conformal prediction in EO. We anticipate that the increased availability of easy-to-use implementations of conformal predictors, such as those provided here, will drive wider adoption of rigorous uncertainty quantification in EO, thereby enhancing the reliability of uses such as operational monitoring and decision making. ",
        "title": "Uncertainty quantification for probabilistic machine learning in earth  observation using conformal prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06422",
        "abstract_url": "http://arxiv.org/abs/2401.06422",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Doyoung"
            },
            {
                "last_name": "Jeong",
                "first_name": "Seongah"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this letter, we propose a joint mechanical and electrical adjustment of intelligent reflecting surface (IRS) for the performance improvements of low-earth orbit (LEO) satellite multiple-input multiple-output (MIMO) communications. In particular, we construct a three-dimensional (3D) MIMO channel model for the mechanically-tilted IRS, and consider two types of scenarios with and without the direct path of LEO-ground user link due to the orbital flight. With the aim of maximizing the end-to-end performance, we jointly optimize tilting angle and phase shift of IRS along with the transceiver beamforming, whose performance superiority is verified via simulations. ",
        "title": "Joint Mechanical and Electrical Adjustment of IRS-aided LEO Satellite  MIMO Communications",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06426",
        "abstract_url": "http://arxiv.org/abs/2401.06426",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Ji"
            },
            {
                "last_name": "Tang",
                "first_name": "Dehua"
            },
            {
                "last_name": "Huang",
                "first_name": "Yuanxian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Li"
            },
            {
                "last_name": "Zeng",
                "first_name": "Xiaocheng"
            },
            {
                "last_name": "Li",
                "first_name": "Dong"
            },
            {
                "last_name": "Lu",
                "first_name": "Mingjie"
            },
            {
                "last_name": "Peng",
                "first_name": "Jinzhang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Fan"
            },
            {
                "last_name": "Tian",
                "first_name": "Lu"
            },
            {
                "last_name": "Sirasao",
                "first_name": "Ashish"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet by directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 models with our method applying on ConvNeXtV1, which surpass most SOTA efficient models with comparable inference performance. Our method also achieves state-of-the-art pruning performance on the vision transformer model. ",
        "title": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06430",
        "abstract_url": "http://arxiv.org/abs/2401.06430",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Huiyuan"
            },
            {
                "last_name": "Cui",
                "first_name": "Kuilong"
            },
            {
                "last_name": "Wang",
                "first_name": "Chuanming"
            },
            {
                "last_name": "Qi",
                "first_name": "Mengshi"
            },
            {
                "last_name": "Ma",
                "first_name": "Huadong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the rapid advancements in deep learning technologies, person re-identification (ReID) has witnessed remarkable performance improvements. However, the majority of prior works have traditionally focused on solving the problem via extracting features solely from a single perspective, such as uniform partitioning, hard attention mechanisms, or semantic masks. While these approaches have demonstrated efficacy within specific contexts, they fall short in diverse situations. In this paper, we propose a novel approach, Mutual Distillation Learning For Person Re-identification (termed as MDPR), which addresses the challenging problem from multiple perspectives within a single unified model, leveraging the power of mutual distillation to enhance the feature representations collectively. Specifically, our approach encompasses two branches: a hard content branch to extract local features via a uniform horizontal partitioning strategy and a Soft Content Branch to dynamically distinguish between foreground and background and facilitate the extraction of multi-granularity features via a carefully designed attention mechanism. To facilitate knowledge exchange between these two branches, a mutual distillation and fusion process is employed, promoting the capability of the outputs of each branch. Extensive experiments are conducted on widely used person ReID datasets to validate the effectiveness and superiority of our approach. Notably, our method achieves an impressive $88.7\\%/94.4\\%$ in mAP/Rank-1 on the DukeMTMC-reID dataset, surpassing the current state-of-the-art results. Our source code is available at https://github.com/KuilongCui/MDPR. ",
        "title": "Mutual Distillation Learning For Person Re-Identification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06431",
        "abstract_url": "http://arxiv.org/abs/2401.06431",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Changrong"
            },
            {
                "last_name": "Ma",
                "first_name": "Wenxing"
            },
            {
                "last_name": "Xu",
                "first_name": "Sean Xin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Yufang"
            },
            {
                "last_name": "Fu",
                "first_name": "Qi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more efficient and maintain greater consistency in their assessments. These results underscore the potential of LLMs in educational technology, paving the way for effective collaboration between humans and AI, ultimately leading to transformative learning experiences through AI-generated feedback. ",
        "title": "From Automation to Augmentation: Large Language Models Elevating Essay  Scoring Landscape",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06432",
        "abstract_url": "http://arxiv.org/abs/2401.06432",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Yae Jee"
            },
            {
                "last_name": "Liu",
                "first_name": "Luyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Fahrezi",
                "first_name": "Aldi"
            },
            {
                "last_name": "Joshi",
                "first_name": "Gauri"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we combine the advantages of high and low-rank LoRAs, which achieves improved convergence speed and final performance compared to homogeneous LoRA. Furthermore, it offers enhanced computation efficiency compared to full fine-tuning, making it suitable for heterogeneous devices while preserving data privacy. ",
        "title": "Heterogeneous Low-Rank Approximation for Federated Fine-tuning of  On-Device Foundation Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06435",
        "abstract_url": "http://arxiv.org/abs/2401.06435",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Jiaming"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Xu",
                "first_name": "Jialong"
            },
            {
                "last_name": "Guo",
                "first_name": "Yiran"
            },
            {
                "last_name": "Li",
                "first_name": "Lun"
            },
            {
                "last_name": "Ai",
                "first_name": "Bo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  For massive multiple-input multiple-output systems in the frequency division duplex (FDD) mode, accurate downlink channel state information (CSI) is required at the base station (BS). However, the increasing number of transmit antennas aggravates the feedback overhead of CSI. Recently, deep learning (DL) has shown considerable potential to reduce CSI feedback overhead. In this paper, we propose a Swin Transformer-based autoencoder network called SwinCFNet for the CSI feedback task. In particular, the proposed method can effectively capture the long-range dependence information of CSI. Moreover, we explore the impact of the number of Swin Transformer blocks and the dimension of feature channels on the performance of SwinCFNet. Experimental results show that SwinCFNet significantly outperforms other DL-based methods with comparable model sizes, especially for the outdoor scenario. ",
        "title": "Swin Transformer-Based CSI Feedback for Massive MIMO",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06436",
        "abstract_url": "http://arxiv.org/abs/2401.06436",
        "authors": [
            {
                "last_name": "Hoang",
                "first_name": "Thi Linh"
            },
            {
                "last_name": "Pham",
                "first_name": "Tuan Dung"
            },
            {
                "last_name": "Ta",
                "first_name": "Viet Cuong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  In this work, we have proposed an approach for improving the GCN for predicting ratings in social networks. Our model is expanded from the standard model with several layers of transformer architecture. The main focus of the paper is on the encoder architecture for node embedding in the network. Using the embedding layer from the graph-based convolution layer, the attention mechanism could rearrange the feature space to get a more efficient embedding for the downstream task. The experiments showed that our proposed architecture achieves better performance than GCN on the traditional link prediction task. ",
        "title": "Improving Graph Convolutional Networks with Transformer Layer in  social-based items recommendation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06437",
        "abstract_url": "http://arxiv.org/abs/2401.06437",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Zeqing"
            },
            {
                "last_name": "Lan",
                "first_name": "Haoxuan"
            },
            {
                "last_name": "Zou",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Junbo"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CL"
        ],
        "abstract": "  Recent advancements in implicit 3D representations and generative models have markedly propelled the field of 3D object generation forward. However, it remains a significant challenge to accurately model geometries with defined sharp features under parametric controls, which is crucial in fields like industrial design and manufacturing. To bridge this gap, we introduce a framework that employs Large Language Models (LLMs) to generate text-driven 3D shapes, manipulating 3D software via program synthesis. We present 3D-PreMise, a dataset specifically tailored for 3D parametric modeling of industrial shapes, designed to explore state-of-the-art LLMs within our proposed pipeline. Our work reveals effective generation strategies and delves into the self-correction capabilities of LLMs using a visual interface. Our work highlights both the potential and limitations of LLMs in 3D parametric modeling for industrial applications. ",
        "title": "3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp  Features and Parametric Control?",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06438",
        "abstract_url": "http://arxiv.org/abs/2401.06438",
        "authors": [
            {
                "last_name": "Ono",
                "first_name": "Seitaro"
            },
            {
                "last_name": "Ogino",
                "first_name": "Yuka"
            },
            {
                "last_name": "Toizumi",
                "first_name": "Takahiro"
            },
            {
                "last_name": "Ito",
                "first_name": "Atsushi"
            },
            {
                "last_name": "Tsukada",
                "first_name": "Masato"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, significant progress has been made in image recognition technology based on deep neural networks. However, improving recognition performance under low-light conditions remains a significant challenge. This study addresses the enhancement of recognition model performance in low-light conditions. We propose an image-adaptive learnable module which apply appropriate image processing on input images and a hyperparameter predictor to forecast optimal parameters used in the module. Our proposed approach allows for the enhancement of recognition performance under low-light conditions by easily integrating as a front-end filter without the need to retrain existing recognition models designed for low-light conditions. Through experiments, our proposed method demonstrates its contribution to enhancing image recognition performance under low-light conditions. ",
        "title": "Improving Low-Light Image Recognition Performance Based on  Image-adaptive Learnable Module",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06439",
        "abstract_url": "http://arxiv.org/abs/2401.06439",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Bin-Bin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yanxin"
            },
            {
                "last_name": "Wei",
                "first_name": "Henglai"
            },
            {
                "last_name": "Wang",
                "first_name": "Yan"
            },
            {
                "last_name": "Lv",
                "first_name": "Chen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, we propose a cooperative long-term task execution (LTTE) algorithm for protecting a moving target into the interior of an ordering-flexible convex hull by a team of robots resiliently in the changing environments. Particularly, by designing target-approaching and sensing-neighbor collision-free subtasks, and incorporating these subtasks into the constraints rather than the traditional cost function in an online constraint-based optimization framework, the proposed LTTE can systematically guarantee long-term target convoying under changing environments in the n-dimensional Euclidean space. Then, the introduction of slack variables allow for the constraint violation of different subtasks; i.e., the attraction from target-approaching constraints and the repulsion from time-varying collision-avoidance constraints, which results in the desired formation with arbitrary spatial ordering sequences. Rigorous analysis is provided to guarantee asymptotical convergence with challenging nonlinear couplings induced by time-varying collision-free constraints. Finally, 2D experiments using three autonomous mobile robots (AMRs) are conducted to validate the effectiveness of the proposed algorithm, and 3D simulations tackling changing environmental elements, such as different initial positions, some robots suddenly breakdown and static obstacles are presented to demonstrate the multi-dimensional adaptability, robustness and the ability of obstacle avoidance of the proposed method. ",
        "title": "Ordering-Flexible Multi-Robot Coordination for MovingTarget Convoying  Using Long-TermTask Execution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06441",
        "abstract_url": "http://arxiv.org/abs/2401.06441",
        "authors": [
            {
                "last_name": "Pirayeshshirazinezhad",
                "first_name": "Reza"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This study introduces lateral pendulum as an innovative balancer design for bicycle stabilization. This pendulum, operating in the bicycle's vertical plane, enables the bicycle to remain stationary. The paper develops a dynamic model for a bicycle equipped with this lateral pendulum, using Lagrange's method, where the equations are validated with ADAMS software. The stabilization is demonstrated with traditional vertical and novel lateral pendulums, managed through a genetic-pole placement control algorithm. This approach showcases the superiority of the lateral pendulum over traditional methods, including vertical pendulums and steering the handlebar. Additionally, a Digital Linear Quadratic Regulator controller is implemented for practical application, further enhancing system stability. ",
        "title": "Bicycle Stabilization using mechanism optimization and Digital LQR",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06442",
        "abstract_url": "http://arxiv.org/abs/2401.06442",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Minxing"
            },
            {
                "last_name": "Cheng",
                "first_name": "Wentao"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A precise and user-friendly manipulation of image content while preserving image fidelity has always been crucial to the field of image editing. Thanks to the power of generative models, recent point-based image editing methods allow users to interactively change the image content with high generalizability by clicking several control points. But the above mentioned editing process is usually based on the assumption that features stay constant in the motion supervision step from initial to target points. In this work, we conduct a comprehensive investigation in the feature space of diffusion models, and find that features change acutely under in-plane rotation. Based on this, we propose a novel approach named RotationDrag, which significantly improves point-based image editing performance when users intend to in-plane rotate the image content. Our method tracks handle points more precisely by utilizing the feature map of the rotated images, thus ensuring precise optimization and high image fidelity. Furthermore, we build a in-plane rotation focused benchmark called RotateBench, the first benchmark to evaluate the performance of point-based image editing method under in-plane rotation scenario on both real images and generated images. A thorough user study demonstrates the superior capability in accomplishing in-plane rotation that users intend to achieve, comparing the DragDiffusion baseline and other existing diffusion-based methods. See the project page https://github.com/Tony-Lowe/RotationDrag for code and experiment results. ",
        "title": "RotationDrag: Point-based Image Editing with Rotated Diffusion Features",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06443",
        "abstract_url": "http://arxiv.org/abs/2401.06443",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Minjun"
            },
            {
                "last_name": "Song",
                "first_name": "Seungwoo"
            },
            {
                "last_name": "Lee",
                "first_name": "Youhan"
            },
            {
                "last_name": "Jang",
                "first_name": "Haneol"
            },
            {
                "last_name": "Lim",
                "first_name": "Kyungtae"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data on VQA. ",
        "title": "BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via  Graph Representation Pretraining",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06445",
        "abstract_url": "http://arxiv.org/abs/2401.06445",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Chenwei"
            },
            {
                "last_name": "Ke",
                "first_name": "Qiao"
            },
            {
                "last_name": "Chen",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Chuang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Xiu-Xiu"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Analyzing and characterizing the differences between networks is a fundamental and challenging problem in network science. Previously, most network comparison methods that rely on topological properties have been restricted to measuring differences between two undirected networks. However, many networks, such as biological networks, social networks, and transportation networks, exhibit inherent directionality and higher-order attributes that should not be ignored when comparing networks. Therefore, we propose a motif-based directed network comparison method that captures local, global, and higher-order differences between two directed networks. Specifically, we first construct a motif distribution vector for each node, which captures the information of a node's involvement in different directed motifs. Then, the dissimilarity between two directed networks is defined on the basis of a matrix which is composed of the motif distribution vector of every node and Jensen-Shannon divergence. The performance of our method is evaluated via the comparison of six real directed networks with their null models as well as their perturbed networks based on edge perturbation. Our method is superior to the state-of-the-art baselines and is robust with different parameter settings. ",
        "title": "Directed network comparison using motifs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06451",
        "abstract_url": "http://arxiv.org/abs/2401.06451",
        "authors": [
            {
                "last_name": "van Ditmarsch",
                "first_name": "Hans"
            },
            {
                "last_name": "Fruzsa",
                "first_name": "Krisztina"
            },
            {
                "last_name": "Kuznets",
                "first_name": "Roman"
            },
            {
                "last_name": "Schmid",
                "first_name": "Ulrich"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We provide an epistemic logical language and semantics for the modeling and analysis of byzantine fault-tolerant multi-agent systems. This not only facilitates reasoning about the agents' fault status but also supports model updates for implementing repair and state recovery. For each agent, besides the standard knowledge modality our logic provides an additional modality called hope, which is capable of expressing that the agent is correct (not faulty), and also dynamic modalities enabling change of the agents' correctness status. These dynamic modalities are interpreted as model updates that come in three flavours: fully public, more private, or involving factual change. We provide complete axiomatizations for all these variants in the form of reduction systems: formulas with dynamic modalities are equivalent to formulas without. Therefore, they have the same expressivity as the logic of knowledge and hope. Multiple examples are provided to demonstrate the utility and flexibility of our logic for modeling a wide range of repair and state recovery techniques that have been implemented in the context of fault-detection, isolation, and recovery (FDIR) approaches in fault-tolerant distributed computing with byzantine agents. ",
        "title": "A Logic for Repair and State Recovery in Byzantine Fault-tolerant  Multi-agent Systems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06452",
        "abstract_url": "http://arxiv.org/abs/2401.06452",
        "authors": [
            {
                "last_name": "Saunders",
                "first_name": "Jack D."
            },
            {
                "last_name": "Freitas",
                "first_name": "Alex A."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Positive-Unlabelled (PU) learning is a growing field of machine learning that aims to learn classifiers from data consisting of labelled positive and unlabelled instances, which can be in reality positive or negative, but whose label is unknown. An extensive number of methods have been proposed to address PU learning over the last two decades, so many so that selecting an optimal method for a given PU learning task presents a challenge. Our previous work has addressed this by proposing GA-Auto-PU, the first Automated Machine Learning (Auto-ML) system for PU learning. In this work, we propose two new Auto-ML systems for PU learning: BO-Auto-PU, based on a Bayesian Optimisation approach, and EBO-Auto-PU, based on a novel evolutionary/Bayesian optimisation approach. We also present an extensive evaluation of the three Auto-ML systems, comparing them to each other and to well-established PU learning methods across 60 datasets (20 real-world datasets, each with 3 versions in terms of PU learning characteristics). ",
        "title": "Automated Machine Learning for Positive-Unlabelled Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06453",
        "abstract_url": "http://arxiv.org/abs/2401.06453",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuyao"
            },
            {
                "last_name": "Guo",
                "first_name": "Ke"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiao"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Artificial light plays an integral role in modern cities, significantly enhancing human productivity and the efficiency of civilization. However, excessive illumination can lead to light pollution, posing non-negligible threats to economic burdens, ecosystems, and human health. Despite its critical importance, the exploration of its causes remains relatively limited within the field of artificial intelligence, leaving an incomplete understanding of the factors contributing to light pollution and sustainable illumination planning distant. To address this gap, we introduce a novel framework named Causally Aware Generative Adversarial Networks (CAGAN). This innovative approach aims to uncover the fundamental drivers of light pollution within cities and offer intelligent solutions for optimal illumination resource allocation in the context of sustainable urban development. We commence by examining light pollution across 33,593 residential areas in seven global metropolises. Our findings reveal substantial influences on light pollution levels from various building types, notably grasslands, commercial centers and residential buildings as significant contributors. These discovered causal relationships are seamlessly integrated into the generative modeling framework, guiding the process of generating light pollution maps for diverse residential areas. Extensive experiments showcase CAGAN's potential to inform and guide the implementation of effective strategies to mitigate light pollution. Our code and data are publicly available at https://github.com/zhangyuuao/Light_Pollution_CAGAN. ",
        "title": "Causally Aware Generative Adversarial Networks for Light Pollution  Control",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06461",
        "abstract_url": "http://arxiv.org/abs/2401.06461",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Yuling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongyu"
            },
            {
                "last_name": "Wan",
                "first_name": "Chengcheng"
            },
            {
                "last_name": "Gu",
                "first_name": "Xiaodong"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL"
        ],
        "abstract": "  Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by capturing the distinct structural patterns of code. Diverging from conventional techniques that depend on external LLMs for perturbations, DetectCodeGPT perturbs the code corpus by strategically inserting spaces and newlines, ensuring both efficacy and efficiency. Experiment results show that our approach significantly outperforms state-of-the-art techniques in detecting machine-generated code. ",
        "title": "Between Lines of Code: Unraveling the Distinct Patterns of Machine and  Human Programmers",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06462",
        "abstract_url": "http://arxiv.org/abs/2401.06462",
        "authors": [
            {
                "last_name": "Xuan",
                "first_name": "Xiwei"
            },
            {
                "last_name": "Ono",
                "first_name": "Jorge Piazentin"
            },
            {
                "last_name": "Gou",
                "first_name": "Liang"
            },
            {
                "last_name": "Ma",
                "first_name": "Kwan-Liu"
            },
            {
                "last_name": "Ren",
                "first_name": "Liu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC"
        ],
        "abstract": "  Data slice-finding is an emerging technique for evaluating machine learning models. It works by identifying subgroups within a specified dataset that exhibit poor performance, often defined by distinct feature sets or meta-information. However, in the context of unstructured image data, data slice-finding poses two notable challenges: it requires additional metadata -- a laborious and costly requirement, and also demands non-trivial efforts for interpreting the root causes of the underperformance within data slices. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for data-slicing-based machine learning (ML) model validation. Our approach excels in identifying interpretable data slices, employing explainable features extracted through the lens of Explainable AI (XAI) techniques, and removing the necessity for additional metadata of textual annotations or cross-model embeddings. AttributionScanner demonstrates proficiency in pinpointing critical model issues, including spurious correlations and mislabeled data. Our novel VA interface visually summarizes data slices, enabling users to gather insights into model behavior patterns effortlessly. Furthermore, our framework closes the ML Development Cycle by empowering domain experts to address model issues by using a cutting-edge neural network regularization technique. The efficacy of AttributionScanner is underscored through two prototype use cases, elucidating its substantial effectiveness in model validation for vision-centric tasks. Our approach paves the way for ML researchers and practitioners to drive interpretable model validation in a data-efficient way, ultimately leading to more reliable and accurate models. ",
        "title": "AttributionScanner: A Visual Analytics System for Metadata-Free  Data-Slicing Based Model Validation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06465",
        "abstract_url": "http://arxiv.org/abs/2401.06465",
        "authors": [
            {
                "last_name": "Hedstr\u00f6m",
                "first_name": "Anna"
            },
            {
                "last_name": "Weber",
                "first_name": "Leander"
            },
            {
                "last_name": "Lapuschkin",
                "first_name": "Sebastian"
            },
            {
                "last_name": "H\u00f6hne",
                "first_name": "Marina MC"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the eXplainable Artificial Intelligence (XAI) community for its well-motivated evaluative principle: that the explanation function should be sensitive to changes in the parameters of the model function. However, recent works have identified several methodological caveats for the empirical interpretation of MPRT. To address these caveats, we introduce two adaptations to the original MPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact that noise has on the evaluation results through sampling and the latter circumvents the need for biased similarity measurements by re-interpreting the test through the explanation's rise in complexity, after full parameter randomisation. Our experimental results demonstrate that these proposed variants lead to improved metric reliability, thus enabling a more trustworthy application of XAI methods. ",
        "title": "Sanity Checks Revisited: An Exploration to Repair the Model Parameter  Randomisation Test",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06466",
        "abstract_url": "http://arxiv.org/abs/2401.06466",
        "authors": [
            {
                "last_name": "Rostami",
                "first_name": "Pedram"
            },
            {
                "last_name": "Salemi",
                "first_name": "Ali"
            },
            {
                "last_name": "Dousti",
                "first_name": "Mohammad Javad"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another. ",
        "title": "PersianMind: A Cross-Lingual Persian-English Large Language Model",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06468",
        "abstract_url": "http://arxiv.org/abs/2401.06468",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Minghao"
            },
            {
                "last_name": "Vu",
                "first_name": "Thuy-Trang"
            },
            {
                "last_name": "Qu",
                "first_name": "Lizhen"
            },
            {
                "last_name": "Foster",
                "first_name": "George"
            },
            {
                "last_name": "Haffari",
                "first_name": "Gholamreza"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as translation errors, the scaling law of parallel documents, out-of-domain generalization, and the impact of zero-shot crosslingual transfer. The findings of this research not only shed light on the strengths and limitations of LLM-based DocMT models but also provide a foundation for future research in DocMT. ",
        "title": "Adapting Large Language Models for Document-Level Machine Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06469",
        "abstract_url": "http://arxiv.org/abs/2401.06469",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Kaiyi"
            },
            {
                "last_name": "Lv",
                "first_name": "Ang"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuhan"
            },
            {
                "last_name": "Ha",
                "first_name": "Hansen"
            },
            {
                "last_name": "Xu",
                "first_name": "Tao"
            },
            {
                "last_name": "Yan",
                "first_name": "Rui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple \"epochs\" of meta-optimization. This variant implicitly explores permutations of ICL examples, further enhancing ICL performance. ",
        "title": "Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06470",
        "abstract_url": "http://arxiv.org/abs/2401.06470",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Gengrui"
            },
            {
                "last_name": "Wang",
                "first_name": "Yao"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoshuang"
            },
            {
                "last_name": "Qian",
                "first_name": "Hongyi"
            },
            {
                "last_name": "Zhan",
                "first_name": "Kaiqiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Ben"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In recent years, there has been a growing interest in utilizing reinforcement learning (RL) to optimize long-term rewards in recommender systems. Since industrial recommender systems are typically designed as multi-stage systems, RL methods with a single agent face challenges when optimizing multiple stages simultaneously. The reason is that different stages have different observation spaces, and thus cannot be modeled by a single agent. To address this issue, we propose a novel UNidirectional-EXecution-based multi-agent Reinforcement Learning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage recommender systems. We show that the unidirectional execution is a key feature of multi-stage recommender systems, bringing new challenges to the applications of multi-agent reinforcement learning (MARL), namely the observation dependency and the cascading effect. To tackle these challenges, we provide a cascading information chain (CIC) method to separate the independent observations from action-dependent observations and use CIC to train UNEX-RL effectively. We also discuss practical variance reduction techniques for UNEX-RL. Finally, we show the effectiveness of UNEX-RL on both public datasets and an online recommender system with over 100 million users. Specifically, UNEX-RL reveals a 0.558% increase in users' usage time compared with single-agent RL algorithms in online A/B experiments, highlighting the effectiveness of UNEX-RL in industrial recommender systems. ",
        "title": "UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender  Systems with UNidirectional EXecution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06471",
        "abstract_url": "http://arxiv.org/abs/2401.06471",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yuwei"
            },
            {
                "last_name": "Zeng",
                "first_name": "Yi"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Concept learning is a fundamental aspect of human cognition and plays a critical role in mental processes such as categorization, reasoning, memory, and decision-making. Researchers across various disciplines have shown consistent interest in the process of concept acquisition in individuals. To elucidate the mechanisms involved in human concept learning, this study examines the findings from computational neuroscience and cognitive psychology. These findings indicate that the brain's representation of concepts relies on two essential components: multisensory representation and text-derived representation. These two types of representations are coordinated by a semantic control system, ultimately leading to the acquisition of concepts. Drawing inspiration from this mechanism, the study develops a human-like computational model for concept learning based on spiking neural networks. By effectively addressing the challenges posed by diverse sources and imbalanced dimensionality of the two forms of concept representations, the study successfully attains human-like concept representations. Tests involving similar concepts demonstrate that our model, which mimics the way humans learn concepts, yields representations that closely align with human cognition. ",
        "title": "A Brain-inspired Computational Model for Human-like Concept Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06473",
        "abstract_url": "http://arxiv.org/abs/2401.06473",
        "authors": [
            {
                "last_name": "Kats",
                "first_name": "Eytan"
            },
            {
                "last_name": "Hirsch",
                "first_name": "Jochen G."
            },
            {
                "last_name": "Heinrich",
                "first_name": "Mattias P."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper demonstrates a self-supervised framework for learning voxel-wise coarse-to-fine representations tailored for dense downstream tasks. Our approach stems from the observation that existing methods for hierarchical representation learning tend to prioritize global features over local features due to inherent architectural bias. To address this challenge, we devise a training strategy that balances the contributions of features from multiple scales, ensuring that the learned representations capture both coarse and fine-grained details. Our strategy incorporates 3-fold improvements: (1) local data augmentations, (2) a hierarchically balanced architecture, and (3) a hybrid contrastive-restorative loss function. We evaluate our method on CT and MRI data and demonstrate that our new approach particularly beneficial for fine-tuning with limited annotated data and consistently outperforms the baseline counterpart in linear evaluation settings. ",
        "title": "Self-supervised Learning of Dense Hierarchical Representations for  Medical Image Segmentation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06477",
        "abstract_url": "http://arxiv.org/abs/2401.06477",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Guo",
                "first_name": "Shuyue"
            },
            {
                "last_name": "Qu",
                "first_name": "Xingwei"
            },
            {
                "last_name": "Guo",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weixu"
            },
            {
                "last_name": "Du",
                "first_name": "Xinrun"
            },
            {
                "last_name": "Lin",
                "first_name": "Chenghua"
            },
            {
                "last_name": "Huang",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenhu"
            },
            {
                "last_name": "Fu",
                "first_name": "Jie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ge"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a scalable and efficient solution for improving the instruction-following capabilities of LLMs, with significant implications for their application across diverse fields. The code and dataset can be found at https://github.com/Zheng0428/COIG-Kun ",
        "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction  Back-Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06481",
        "abstract_url": "http://arxiv.org/abs/2401.06481",
        "authors": [
            {
                "last_name": "Holland",
                "first_name": "Kieran"
            },
            {
                "last_name": "Ipp",
                "first_name": "Andreas"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "David I."
            },
            {
                "last_name": "Wenger",
                "first_name": "Urs"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Fixed point lattice actions are designed to have continuum classical properties unaffected by discretization effects and reduced lattice artifacts at the quantum level. They provide a possible way to extract continuum physics with coarser lattices, thereby allowing to circumvent problems with critical slowing down and topological freezing toward the continuum limit. A crucial ingredient for practical applications is to find an accurate and compact parametrization of a fixed point action, since many of its properties are only implicitly defined. Here we use machine learning methods to revisit the question of how to parametrize fixed point actions. In particular, we obtain a fixed point action for four-dimensional SU(3) gauge theory using convolutional neural networks with exact gauge invariance. The large operator space allows us to find superior parametrizations compared to previous studies, a necessary first step for future Monte Carlo simulations. ",
        "title": "Machine learning a fixed point action for SU(3) gauge theory with a  gauge equivariant convolutional neural network",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06482",
        "abstract_url": "http://arxiv.org/abs/2401.06482",
        "authors": [
            {
                "last_name": "Rani",
                "first_name": "Pooja"
            },
            {
                "last_name": "Zellweger",
                "first_name": "Jonas"
            },
            {
                "last_name": "Kousadianos",
                "first_name": "Veronika"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "Kehrer",
                "first_name": "Timo"
            },
            {
                "last_name": "Bacchelli",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "PF"
        ],
        "abstract": "  As the energy footprint generated by software is increasing at an alarming rate, understanding how to develop energy-efficient applications has become a necessity. Previous work has introduced catalogs of coding practices, also known as energy patterns. These patterns are yet limited to Mobile or third-party libraries. In this study, we focus on the Web domain--a main source of energy consumption. First, we investigated whether and how Mobile energy patterns could be ported to this domain and found that 20 patterns could be ported. Then, we interviewed six expert web developers from different companies to challenge the ported patterns. Most developers expressed concerns for antipatterns, specifically with functional antipatterns, and were able to formulate guidelines to locate these patterns in the source code. Finally, to quantify the effect of Web energy patterns on energy consumption, we set up an automated pipeline to evaluate two ported patterns: 'Dynamic Retry Delay' (DRD) and 'Open Only When Necessary' (OOWN). With this, we found no evidence that the DRD pattern consumes less energy than its antipattern, while the opposite is true for OOWN. Data and Material: https://doi.org/10.5281/zenodo.8404487 ",
        "title": "Energy Patterns for Web: An Exploratory Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06484",
        "abstract_url": "http://arxiv.org/abs/2401.06484",
        "authors": [
            {
                "last_name": "Khadem",
                "first_name": "Mina"
            },
            {
                "last_name": "Zeinali",
                "first_name": "Farshad"
            },
            {
                "last_name": "Mokari",
                "first_name": "Nader"
            },
            {
                "last_name": "Saeedi",
                "first_name": "Hamid"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  In this paper, we present a quality of service (QoS)-aware priority-based spectrum management scheme to guarantee the minimum required bit rate of vertical sector players (VSPs) in the 5G and beyond generation, including the 6th generation (6G). VSPs are considered as spectrum leasers to optimize the overall spectrum efficiency of the network from the perspective of the mobile network operator (MNO) as the spectrum licensee and auctioneer. We exploit a modified Vickrey-Clarke-Groves (VCG) auction mechanism to allocate the spectrum to them where the QoS and the truthfulness of bidders are considered as two important parameters for prioritization of VSPs. The simulation is done with the help of deep deterministic policy gradient (DDPG) as a deep reinforcement learning (DRL)-based algorithm. Simulation results demonstrate that deploying the DDPG algorithm results in significant advantages. In particular, the efficiency of the proposed spectrum management scheme is about %85 compared to the %35 efficiency in traditional auction methods. ",
        "title": "AI-enabled Priority and Auction-Based Spectrum Management for 6G",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06485",
        "abstract_url": "http://arxiv.org/abs/2401.06485",
        "authors": [
            {
                "last_name": "Xi",
                "first_name": "Yu"
            },
            {
                "last_name": "Yang",
                "first_name": "Baochen"
            },
            {
                "last_name": "Li",
                "first_name": "Hao"
            },
            {
                "last_name": "Guo",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Yu",
                "first_name": "Kai"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Customizable keyword spotting (KWS) in continuous speech has attracted increasing attention due to its real-world application potential. While contrastive learning (CL) has been widely used to extract keyword representations, previous CL approaches all operate on pre-segmented isolated words and employ only audio-text representations matching strategy. However, for KWS in continuous speech, co-articulation and streaming word segmentation can easily yield similar audio patterns for different texts, which may consequently trigger false alarms. To address this issue, we propose a novel CL with Audio Discrimination (CLAD) approach to learning keyword representation with both audio-text matching and audio-audio discrimination ability. Here, an InfoNCE loss considering both audio-audio and audio-text CL data pairs is employed for each sliding window during training. Evaluations on the open-source LibriPhrase dataset show that the use of sliding-window level InfoNCE loss yields comparable performance compared to previous CL approaches. Furthermore, experiments on the continuous speech dataset LibriSpeech demonstrate that, by incorporating audio discrimination, CLAD achieves significant performance gain over CL without audio discrimination. Meanwhile, compared to two-stage KWS approaches, the end-to-end KWS with CLAD achieves not only better performance, but also significant speed-up. ",
        "title": "Contrastive Learning With Audio Discrimination For Customizable Keyword  Spotting In Continuous Speech",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06486",
        "abstract_url": "http://arxiv.org/abs/2401.06486",
        "authors": [
            {
                "last_name": "Brunner",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Praetorius",
                "first_name": "Dirk"
            },
            {
                "last_name": "Streitberger",
                "first_name": "Julian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider scalar semilinear elliptic PDEs, where the nonlinearity is strongly monotone, but only locally Lipschitz continuous. To linearize the arising discrete nonlinear problem, we employ a damped Zarantonello iteration, which leads to a linear Poisson-type equation that is symmetric and positive definite. The resulting system is solved by a contractive algebraic solver such as a multigrid method with local smoothing. We formulate a fully adaptive algorithm that equibalances the various error components coming from mesh refinement, iterative linearization, and algebraic solver. We prove that the proposed adaptive iteratively linearized finite element method (AILFEM) guarantees convergence with optimal complexity, where the rates are understood with respect to the overall computational cost (i.e., the computational time). Numerical experiments investigate the involved adaptivity parameters. ",
        "title": "Cost-optimal adaptive FEM with linearization and algebraic solver for  semilinear elliptic PDEs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06492",
        "abstract_url": "http://arxiv.org/abs/2401.06492",
        "authors": [
            {
                "last_name": "D\u00f6rich",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Nikoli\u0107",
                "first_name": "Vanja"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Kuznetsov equation is a classical wave model of acoustics that incorporates quadratic gradient nonlinearities. When its strong damping vanishes, it undergoes a singular behavior change, switching from a parabolic-like to a hyperbolic quasilinear evolution. In this work, we establish for the first time the optimal error bounds for its finite element approximation as well as a semi-implicit fully discrete approximation that are robust with respect to the vanishing damping parameter. The core of the new arguments lies in devising energy estimates directly for the error equation where one can more easily exploit the polynomial structure of the nonlinearities and compensate inverse estimates with smallness conditions on the error. Numerical experiments are included to illustrate the theoretical results. ",
        "title": "Robust fully discrete error bounds for the Kuznetsov equation in the  inviscid limit",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06493",
        "abstract_url": "http://arxiv.org/abs/2401.06493",
        "authors": [
            {
                "last_name": "Karmakar",
                "first_name": "Pratik"
            },
            {
                "last_name": "Monet",
                "first_name": "Mika\u00ebl"
            },
            {
                "last_name": "Senellart",
                "first_name": "Pierre"
            },
            {
                "last_name": "Bressan",
                "first_name": "St\u00e9phane"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "CC"
        ],
        "abstract": "  Shapley values, originating in game theory and increasingly prominent in explainable AI, have been proposed to assess the contribution of facts in query answering over databases, along with other similar power indices such as Banzhaf values. In this work we adapt these Shapley-like scores to probabilistic settings, the objective being to compute their expected value. We show that the computations of expected Shapley values and of the expected values of Boolean functions are interreducible in polynomial time, thus obtaining the same tractability landscape. We investigate the specific tractable case where Boolean functions are represented as deterministic decomposable circuits, designing a polynomial-time algorithm for this setting. We present applications to probabilistic databases through database provenance, and an effective implementation of this algorithm within the ProvSQL system, which experimentally validates its feasibility over a standard benchmark. ",
        "title": "Expected Shapley-Like Scores of Boolean Functions: Complexity and  Applications to Probabilistic Databases",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06495",
        "abstract_url": "http://arxiv.org/abs/2401.06495",
        "authors": [
            {
                "last_name": "Leteno",
                "first_name": "Thibaud"
            },
            {
                "last_name": "Gourru",
                "first_name": "Antoine"
            },
            {
                "last_name": "Laclau",
                "first_name": "Charlotte"
            },
            {
                "last_name": "Gravier",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY",
            "LG"
        ],
        "abstract": "  In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate gender bias (e.g. is DistilBERT more prone to gender bias than its uncompressed version, BERT)? Our findings are the following: (I) one cannot identify a specific layer that produces bias; (II) every attention head uniformly encodes bias; except in the context of underrepresented classes with a high imbalance of the sensitive attribute; (III) this subset of heads is different as we re-fine tune the network; (IV) bias is more homogeneously produced by the heads in the distilled model. ",
        "title": "An investigation of structures responsible for gender bias in BERT and  DistilBERT",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06498",
        "abstract_url": "http://arxiv.org/abs/2401.06498",
        "authors": [
            {
                "last_name": "Glandorf",
                "first_name": "Dominik"
            },
            {
                "last_name": "Lee",
                "first_name": "Hye Rin"
            },
            {
                "last_name": "Orona",
                "first_name": "Gabe Avakian"
            },
            {
                "last_name": "Pumptow",
                "first_name": "Marina"
            },
            {
                "last_name": "Yu",
                "first_name": "Renzhe"
            },
            {
                "last_name": "Fischer",
                "first_name": "Christian"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  Large-scale administrative data is a common input in early warning systems for college dropout in higher education. Still, the terminology and methodology vary significantly across existing studies, and the implications of different modeling decisions are not fully understood. This study provides a systematic evaluation of contributing factors and predictive performance of machine learning models over time and across different student groups. Drawing on twelve years of administrative data at a large public university in the US, we find that dropout prediction at the end of the second year has a 20% higher AUC than at the time of enrollment in a Random Forest model. Also, most predictive factors at the time of enrollment, including demographics and high school performance, are quickly superseded in predictive importance by college performance and in later stages by enrollment behavior. Regarding variability across student groups, college GPA has more predictive value for students from traditionally disadvantaged backgrounds than their peers. These results can help researchers and administrators understand the comparative value of different data sources when building early warning systems and optimizing decisions under specific policy goals. ",
        "title": "Temporal and Between-Group Variability in College Dropout Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06499",
        "abstract_url": "http://arxiv.org/abs/2401.06499",
        "authors": [
            {
                "last_name": "Pandey",
                "first_name": "Sumit"
            },
            {
                "last_name": "Changdar",
                "first_name": "Satyasaran"
            },
            {
                "last_name": "Perslev",
                "first_name": "Mathias"
            },
            {
                "last_name": "Dam",
                "first_name": "Erik B"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Automated segmentation of distinct tumor regions is critical for accurate diagnosis and treatment planning in pediatric brain tumors. This study evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in segmenting different tumor subregions across three challenging datasets: Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse scenarios and anatomical variations, making them suitable for assessing the robustness and generalization capabilities of the MPUnet model. By utilizing multi-planar information, the MPUnet architecture aims to enhance segmentation accuracy. Our results show varying performance levels across the evaluated challenges, with the tumor core (TC) class demonstrating relatively higher segmentation accuracy. However, variability is observed in the segmentation of other classes, such as the edema and enhancing tumor (ET) regions. These findings emphasize the complexity of brain tumor segmentation and highlight the potential for further refinement of the MPUnet approach and inclusion of MRI more data and preprocessing. ",
        "title": "Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner  UNet",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06503",
        "abstract_url": "http://arxiv.org/abs/2401.06503",
        "authors": [
            {
                "last_name": "Doloriel",
                "first_name": "Chandler Timm C."
            },
            {
                "last_name": "Cajote",
                "first_name": "Rhandley D."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Small oriented objects that represent tiny pixel-area in large-scale aerial images are difficult to detect due to their size and orientation. Existing oriented aerial detectors have shown promising results but are mainly focused on orientation modeling with less regard to the size of the objects. In this work, we proposed a method to accurately detect small oriented objects in aerial images by enhancing the classification and regression tasks of the oriented object detection model. We designed the Attention-Points Network consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss (BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn the attention features needed to improve the detection of small objects. These attention features are then used to predict box points for BPLoss, which determines the points' position relative to the target oriented bounding box. Experimental results show the effectiveness of our Attention-Points Network on a standard oriented aerial dataset with small object instances (DOTA-v1.5) and on a maritime-related dataset (HRSC2016). The code is publicly available. ",
        "title": "Improving the Detection of Small Oriented Objects in Aerial Images",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06508",
        "abstract_url": "http://arxiv.org/abs/2401.06508",
        "authors": [
            {
                "last_name": "Aljafar",
                "first_name": "Muayad J."
            },
            {
                "last_name": "Azais",
                "first_name": "Florence"
            },
            {
                "last_name": "Flottes",
                "first_name": "Marie-Lise"
            },
            {
                "last_name": "Pagliarini",
                "first_name": "Samuel"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "AR"
        ],
        "abstract": "  While numerous obfuscation techniques are available for securing digital assets in the digital domain, there has been a notable lack of focus on protecting Intellectual Property (IP) in the analog domain. This is primarily due to the relatively smaller footprint of analog components within an Integrated Circuit (IC), with the majority of the surface dedicated to digital elements. However, despite their smaller nature, analog components are highly valuable IP and warrant effective protection. In this paper, we present a groundbreaking method for safeguarding analog IP by harnessing layout-based effects that are typically considered undesirable in IC design. Specifically, we exploit the impact of Length of Oxide Diffusion and Well Proximity Effect on transistors to fine-tune critical parameters such as transconductance (gm) and threshold voltage (Vth). These parameters remain concealed behind key inputs, akin to the logic locking approach employed in digital ICs. Our research explores the application of layout-based effects in two commercial CMOS technologies, namely a 28nm and a 65nm node. To demonstrate the efficacy of our proposed technique, we implement it for locking an Operational Transconductance Amplifier. Extensive simulations are performed, evaluating the obfuscation strength by applying a large number of key sets (over 50,000 and 300,000). The results exhibit a significant degradation in performance metrics, such as open-loop gain (up to 130dB), phase margin (up to 50 degrees), 3dB bandwidth (approximately 2.5MHz), and power consumption (around 1mW) when incorrect keys are employed. Our findings highlight the advantages of our approach as well as the associated overhead. ",
        "title": "Utilizing Layout Effects for Analog Logic Locking",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06509",
        "abstract_url": "http://arxiv.org/abs/2401.06509",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Yuanzhi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Linchao"
            },
            {
                "last_name": "Yang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions, we introduce the Agent interaction Evaluation framework (AntEval), targeting the qualitative evaluation of interaction informativeness and expressiveness. Specifically, we propose two novel evaluation metrics: Information Exchanging Precision (IEP) and Interaction Expressiveness Gap (IEG). These metrics are designed to assess interactions in scenarios focused on information exchange and intention expression, respectively. Our experimental results demonstrate the effectiveness of these metrics in evaluating interaction quality. Notably, we identify significant areas for improvement in LLMs regarding social interactions, as highlighted by our metrics. We believe AntEval will guide further exploration in complex agent interactions, bringing them closer to emulating real human behavior and enhancing their integration and utility in real-world applications. ",
        "title": "AntEval: Quantitatively Evaluating Informativeness and Expressiveness of  Agent Social Interactions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06512",
        "abstract_url": "http://arxiv.org/abs/2401.06512",
        "authors": [
            {
                "last_name": "Dallant",
                "first_name": "Justin"
            },
            {
                "last_name": "Haagensen",
                "first_name": "Frederik"
            },
            {
                "last_name": "Jacob",
                "first_name": "Riko"
            },
            {
                "last_name": "Kozma",
                "first_name": "L\u00e1szl\u00f3"
            },
            {
                "last_name": "Wild",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "DS"
        ],
        "abstract": "  A \\emph{saddlepoint} of an $n \\times n$ matrix is an entry that is the maximum of its row and the minimum of its column. Saddlepoints give the \\emph{value} of a two-player zero-sum game, corresponding to its pure-strategy Nash equilibria; efficiently finding a saddlepoint is thus a natural and fundamental algorithmic task.   For finding a \\emph{strict saddlepoint} (an entry that is the strict maximum of its row and the strict minimum of its column) we recently gave an $O({n\\log^*{n}})$-time algorithm, improving the $O({n\\log{n}})$ bounds from 1991 of Bienstock, Chung, Fredman, Sch\\\"affer, Shor, Suri and of Byrne and Vaserstein.   In this paper we present an optimal $O({n})$-time algorithm for finding a strict saddlepoint based on random sampling. Our algorithm, like earlier approaches, accesses matrix entries only via unit-cost binary comparisons. For finding a (non-strict) saddlepoint, we extend an existing lower bound to randomized algorithms, showing that the trivial $O(n^2)$ runtime cannot be improved even with the use of randomness. ",
        "title": "An Optimal Randomized Algorithm for Finding the Saddlepoint",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06513",
        "abstract_url": "http://arxiv.org/abs/2401.06513",
        "authors": [
            {
                "last_name": "Abdelkader",
                "first_name": "Hala"
            },
            {
                "last_name": "Abdelrazek",
                "first_name": "Mohamed"
            },
            {
                "last_name": "Barnett",
                "first_name": "Scott"
            },
            {
                "last_name": "Schneider",
                "first_name": "Jean-Guy"
            },
            {
                "last_name": "Rani",
                "first_name": "Priya"
            },
            {
                "last_name": "Vasa",
                "first_name": "Rajesh"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in production. ",
        "title": "ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A  Case Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06514",
        "abstract_url": "http://arxiv.org/abs/2401.06514",
        "authors": [
            {
                "last_name": "Ivanov",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Ben-Porat",
                "first_name": "Omer"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Personalization in machine learning (ML) tailors models' decisions to the individual characteristics of users. While this approach has seen success in areas like recommender systems, its expansion into high-stakes fields such as healthcare and autonomous driving is hindered by the extensive regulatory approval processes involved. To address this challenge, we propose a novel framework termed represented Markov Decision Processes (r-MDPs) that is designed to balance the need for personalization with the regulatory constraints. In an r-MDP, we cater to a diverse user population, each with unique preferences, through interaction with a small set of representative policies. Our objective is twofold: efficiently match each user to an appropriate representative policy and simultaneously optimize these policies to maximize overall social welfare. We develop two deep reinforcement learning algorithms that efficiently solve r-MDPs. These algorithms draw inspiration from the principles of classic K-means clustering and are underpinned by robust theoretical foundations. Our empirical investigations, conducted across a variety of simulated environments, showcase the algorithms' ability to facilitate meaningful personalization even under constrained policy budgets. Furthermore, they demonstrate scalability, efficiently adapting to larger policy budgets. ",
        "title": "Personalized Reinforcement Learning with a Budget of Policies",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06518",
        "abstract_url": "http://arxiv.org/abs/2401.06518",
        "authors": [
            {
                "last_name": "S\u00e1nchez",
                "first_name": "Jos\u00e9 Manuel Gaspar"
            },
            {
                "last_name": "Bruns",
                "first_name": "Leonard"
            },
            {
                "last_name": "Tumova",
                "first_name": "Jana"
            },
            {
                "last_name": "Jensfelt",
                "first_name": "Patric"
            },
            {
                "last_name": "T\u00f6rngren",
                "first_name": "Martin"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Autonomous agents rely on sensor data to construct representations of their environment, essential for predicting future events and planning their own actions. However, sensor measurements suffer from limited range, occlusions, and sensor noise. These challenges become more evident in dynamic environments, where efficiently inferring the state of the environment based on sensor readings from different times is still an open problem. This work focuses on inferring the state of the dynamic part of the environment, i.e., where dynamic objects might be, based on previous observations and constraints on their dynamics. We formalize the problem and introduce Transitional Grid Maps (TGMs), an efficient analytical solution. TGMs are based on a set of novel assumptions that hold in many practical scenarios. They significantly reduce the complexity of the problem, enabling continuous prediction and updating of the entire dynamic map based on the known static map (see Fig.1), differentiating them from other alternatives. We compare our approach with a state-of-the-art particle filter, obtaining more prudent predictions in occluded scenarios and on-par results on unoccluded tracking. ",
        "title": "Transitional Grid Maps: Efficient Analytical Inference of Dynamic  Environments under Limited Sensing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06519",
        "abstract_url": "http://arxiv.org/abs/2401.06519",
        "authors": [
            {
                "last_name": "Ahvonen",
                "first_name": "Veeti"
            },
            {
                "last_name": "Heiman",
                "first_name": "Damian"
            },
            {
                "last_name": "Kuusisto",
                "first_name": "Antti"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "DC"
        ],
        "abstract": "  We examine the relationship of graded (multi)modal logic to counting (multichannel) message passing automata with applications to the Weisfeiler-Leman algorithm. We introduce the notion of graded multimodal types, which are formulae of graded multimodal logic that encode the local information of a pointed Kripke-model. We also introduce message passing automata that carry out a generalization of the Weisfeiler-Leman algorithm for distinguishing non-isomorphic graph nodes. We show that the classes of pointed Kripke-models recognizable by these automata are definable by a countable (possibly infinite) disjunction of graded multimodal formulae and vice versa. In particular, this equivalence also holds between recursively enumerable disjunctions and recursively enumerable automata. We also show a way of carrying out the Weisfeiler-Leman algorithm with a formula of first order logic that has been augmented with H\\\"artig's quantifier and greatest fixed points. ",
        "title": "Graded modal logic and counting message passing automata",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06521",
        "abstract_url": "http://arxiv.org/abs/2401.06521",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Mu",
                "first_name": "Junxian"
            },
            {
                "last_name": "Zhu",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Hu",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Open set recognition (OSR) requires the model to classify samples that belong to closed sets while rejecting unknown samples during test. Currently, generative models often perform better than discriminative models in OSR, but recent studies show that generative models may be computationally infeasible or unstable on complex tasks. In this paper, we provide insights into OSR and find that learning supplementary representations can theoretically reduce the open space risk. Based on the analysis, we propose a new model, namely Multi-Expert Diverse Attention Fusion (MEDAF), that learns diverse representations in a discriminative way. MEDAF consists of multiple experts that are learned with an attention diversity regularization term to ensure the attention maps are mutually different. The logits learned by each expert are adaptively fused and used to identify the unknowns through the score function. We show that the differences in attention maps can lead to diverse representations so that the fused representations can well handle the open space. Extensive experiments are conducted on standard and OSR large-scale benchmarks. Results show that the proposed discriminative method can outperform existing generative models by up to 9.5% on AUROC and achieve new state-of-the-art performance with little computational cost. Our method can also seamlessly integrate existing classification models. Code is available at https://github.com/Vanixxz/MEDAF. ",
        "title": "Exploring Diverse Representations for Open Set Recognition",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06523",
        "abstract_url": "http://arxiv.org/abs/2401.06523",
        "authors": [
            {
                "last_name": "Kertel",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Klein",
                "first_name": "Nadja"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We present a boosting-based method to learn additive Structural Equation Models (SEMs) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. We introduce a family of score functions based on arbitrary regression techniques, for which we establish necessary conditions to consistently favor the true causal ordering. Our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. To address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive SEMs. Our simulation study underlines our theoretical results for lower dimensions and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. In addition, it exhibits robustness with respect to the choice of the hyperparameters making the procedure easy to tune. ",
        "title": "Boosting Causal Additive Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06524",
        "abstract_url": "http://arxiv.org/abs/2401.06524",
        "authors": [
            {
                "last_name": "Khanal",
                "first_name": "Subina"
            },
            {
                "last_name": "Tirupathi",
                "first_name": "Seshu"
            },
            {
                "last_name": "Zizzo",
                "first_name": "Giulio"
            },
            {
                "last_name": "Rawat",
                "first_name": "Ambrish"
            },
            {
                "last_name": "Pedersen",
                "first_name": "Torben Bach"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the \\emph{One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with diverse time series instances. We then fine-tune the pre-trained model using a gradual unfreezing technique. This helps enhance the model's performance in time series prediction for domains with limited data. Extensive experimental results on two real-world datasets show that our approach improves over the state-of-the-art baselines by 4.35% and 11.54% for indoor temperature and wind power prediction, respectively. ",
        "title": "Domain Adaptation for Time series Transformers using One-step  fine-tuning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06526",
        "abstract_url": "http://arxiv.org/abs/2401.06526",
        "authors": [
            {
                "last_name": "Piot",
                "first_name": "Paloma"
            },
            {
                "last_name": "Mart\u00edn-Rodilla",
                "first_name": "Patricia"
            },
            {
                "last_name": "Parapar",
                "first_name": "Javier"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SI"
        ],
        "abstract": "  Hate speech represents a pervasive and detrimental form of online discourse, often manifested through an array of slurs, from hateful tweets to defamatory posts. As such speech proliferates, it connects people globally and poses significant social, psychological, and occasionally physical threats to targeted individuals and communities. Current computational linguistic approaches for tackling this phenomenon rely on labelled social media datasets for training. For unifying efforts, our study advances in the critical need for a comprehensive meta-collection, advocating for an extensive dataset to help counteract this problem effectively. We scrutinized over 60 datasets, selectively integrating those pertinent into MetaHate. This paper offers a detailed examination of existing collections, highlighting their strengths and limitations. Our findings contribute to a deeper understanding of the existing datasets, paving the way for training more robust and adaptable models. These enhanced models are essential for effectively combating the dynamic and complex nature of hate speech in the digital realm. ",
        "title": "MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06528",
        "abstract_url": "http://arxiv.org/abs/2401.06528",
        "authors": [
            {
                "last_name": "Arbash",
                "first_name": "Elias"
            },
            {
                "last_name": "Fuchs",
                "first_name": "Margret"
            },
            {
                "last_name": "Rasti",
                "first_name": "Behnood"
            },
            {
                "last_name": "Lorenz",
                "first_name": "Sandra"
            },
            {
                "last_name": "Ghamisi",
                "first_name": "Pedram"
            },
            {
                "last_name": "Gloaguen",
                "first_name": "Richard"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Addressing the critical theme of recycling electronic waste (E-waste), this contribution is dedicated to developing advanced automated data processing pipelines as a basis for decision-making and process control. Aligning with the broader goals of the circular economy and the United Nations (UN) Sustainable Development Goals (SDG), our work leverages non-invasive analysis methods utilizing RGB and hyperspectral imaging data to provide both quantitative and qualitative insights into the E-waste stream composition for optimizing recycling efficiency. In this paper, we introduce 'PCB-Vision'; a pioneering RGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53 RGB images of high spatial resolution paired with their corresponding high spectral resolution hyperspectral data cubes in the visible and near-infrared (VNIR) range. Grounded in open science principles, our dataset provides a comprehensive resource for researchers through high-quality ground truths, focusing on three primary PCB components: integrated circuits (IC), capacitors, and connectors. We provide extensive statistical investigations on the proposed dataset together with the performance of several state-of-the-art (SOTA) models, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and DeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the baseline codes, we hope to foster transparent, traceable, and comparable developments of advanced data processing across various scientific communities, including, but not limited to, computer vision and remote sensing. Emphasizing our commitment to supporting a collaborative and inclusive scientific community, all materials, including code, data, ground truth, and masks, will be accessible at https://github.com/hifexplo/PCBVision. ",
        "title": "PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed  Circuit Boards",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06529",
        "abstract_url": "http://arxiv.org/abs/2401.06529",
        "authors": [
            {
                "last_name": "Moy\u00f3n",
                "first_name": "Fabiola"
            },
            {
                "last_name": "Angermeir",
                "first_name": "Florian"
            },
            {
                "last_name": "Mendez",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The intersection between security and continuous software engineering has been of great interest since the early years of the agile development movement, and it remains relevant as software development processes are more frequently guided by agility and the adoption of DevOps. Several authors have contributed studies about the framing of secure agile development and secure DevOps, motivating academic contributions to methods and practices, but also discussions around benefits and challenges. Especially the challenges captured also our interest since, for the last few years, we are conducting research on secure continuous software engineering from a more applied, practical perspective with the overarching aim to introduce solutions that can be adopted at scale. The short positioning at hands summarizes a relevant part of our endeavors in which we validated challenges with several practitioners of different roles. More than framing a set of challenges, we conclude by presenting four key research directions we identified for practitioners and researchers to delineate future work. ",
        "title": "Industrial Challenges in Secure Continuous Development",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06532",
        "abstract_url": "http://arxiv.org/abs/2401.06532",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Yutao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peitian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yifei"
            },
            {
                "last_name": "Xie",
                "first_name": "Binyu"
            },
            {
                "last_name": "Dou",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Wen",
                "first_name": "Ji-Rong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly available LLMs, such as LLaMA, Mistral, and Phi, in search-related tasks. Furthermore, we conduct a comprehensive analysis to ascertain the effects of base model selection, instruction design, volume of instructions, and task variety on performance. We make our dataset and the models fine-tuned on it publicly accessible at https://github.com/DaoD/INTERS. ",
        "title": "INTERS: Unlocking the Power of Large Language Models in Search with  Instruction Tuning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06538",
        "abstract_url": "http://arxiv.org/abs/2401.06538",
        "authors": [
            {
                "last_name": "Moreira",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Silva",
                "first_name": "Flavio de Oliveira"
            },
            {
                "last_name": "Carvalho",
                "first_name": "Tereza Cristina Melo de Brito"
            },
            {
                "last_name": "Martins",
                "first_name": "Joberto S. B."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "LG"
        ],
        "abstract": "  Network slicing is a crucial enabler and a trend for the Next Generation Mobile Network (NGMN) and various other new systems like the Internet of Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning are key elements with a crucial role in the network-slicing processes since the NS process needs to orchestrate resources and functionalities, and machine learning can potentially optimize the orchestration process. However, existing network-slicing architectures lack the ability to define intelligent approaches to orchestrate features and resources in the slicing process. This paper discusses machine learning-based orchestration of features and capabilities in network slicing architectures. Initially, the slice resource orchestration and allocation in the slicing planning, configuration, commissioning, and operation phases are analyzed. In sequence, we highlight the need for optimized architectural feature orchestration and recommend using ML-embed agents, federated learning intrinsic mechanisms for knowledge acquisition, and a data-driven approach embedded in the network slicing architecture. We further develop an architectural features orchestration case embedded in the SFI2 network slicing architecture. An attack prevention security mechanism is developed for the SFI2 architecture using distributed embedded and cooperating ML agents. The case presented illustrates the architectural feature's orchestration process and benefits, highlighting its importance for the network slicing process. ",
        "title": "Intelligent Data-Driven Architectural Features Orchestration for Network  Slicing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06541",
        "abstract_url": "http://arxiv.org/abs/2401.06541",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Kaishuai"
            },
            {
                "last_name": "Hou",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Medical dialogue systems have attracted growing research attention as they have the potential to provide rapid diagnoses, treatment plans, and health consultations. In medical dialogues, a proper diagnosis is crucial as it establishes the foundation for future consultations. Clinicians typically employ both intuitive and analytic reasoning to formulate a differential diagnosis. This reasoning process hypothesizes and verifies a variety of possible diseases and strives to generate a comprehensive and rigorous diagnosis. However, recent studies on medical dialogue generation have overlooked the significance of modeling a differential diagnosis, which hinders the practical application of these systems. To address the above issue, we propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced analytic procedure. The resulting differential diagnosis is then used to retrieve medical knowledge and guide response generation. Experimental results on two datasets validate the efficacy of our method. Besides, we demonstrate how our framework assists both clinicians and patients in understanding the diagnostic process, for instance, by producing intermediate results and graph-based diagnosis paths. ",
        "title": "Medical Dialogue Generation via Intuitive-then-Analytical Differential  Diagnosis",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06542",
        "abstract_url": "http://arxiv.org/abs/2401.06542",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ziying"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Jia",
                "first_name": "Feiyang"
            },
            {
                "last_name": "Luo",
                "first_name": "Yadan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Yang",
                "first_name": "Lei"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            },
            {
                "last_name": "Jia",
                "first_name": "Caiyan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the realm of modern autonomous driving, the perception system is indispensable for accurately assessing the state of the surrounding environment, thereby enabling informed prediction and planning. Key to this system is 3D object detection methods, that utilize vehicle-mounted sensors such as LiDAR and cameras to identify the size, category, and location of nearby objects. Despite the surge in 3D object detection methods aimed at enhancing detection precision and efficiency, there is a gap in the literature that systematically examines their resilience against environmental variations, noise, and weather changes. This study emphasizes the importance of robustness, alongside accuracy and latency, in evaluating perception systems under practical scenarios. Our work presents an extensive survey of camera-based, LiDAR-based, and multimodal 3D object detection algorithms, thoroughly evaluating their trade-off between accuracy, latency, and robustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair comparisons. Among these,multimodal 3D detection approaches exhibit superior robustness and a novel taxonomy is introduced to reorganize its literature for enhanced clarity. This survey aims to offer a more practical perspective on the current capabilities and constraints of 3D object detection algorithms in real-world applications, thus steering future research towards robustness-centric advancements ",
        "title": "Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and  Outlook",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06546",
        "abstract_url": "http://arxiv.org/abs/2401.06546",
        "authors": [
            {
                "last_name": "Imani",
                "first_name": "Vandad"
            },
            {
                "last_name": "Moradi",
                "first_name": "Elaheh"
            },
            {
                "last_name": "Sevilla-Salcedo",
                "first_name": "Carlos"
            },
            {
                "last_name": "Fortino",
                "first_name": "Vittorio"
            },
            {
                "last_name": "Tohka",
                "first_name": "Jussi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV",
            "NE"
        ],
        "abstract": "  Feature selection in noisy label scenarios remains an understudied topic. We propose a novel genetic algorithm-based approach, the Noise-Aware Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting optimal feature subsets in binary classification with noisy labels. NMFS-GA offers a unified framework for selecting feature subsets that are both accurate and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise, a Breast Cancer dataset enriched with noisy features, and a real-world ADNI dataset for dementia conversion prediction. Our results indicate that NMFS-GA can effectively select feature subsets that improve the accuracy and interpretability of binary classifiers in scenarios with noisy labels. ",
        "title": "Optimizing Feature Selection for Binary Classification with Noisy  Labels: A Genetic Algorithm Approach",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06547",
        "abstract_url": "http://arxiv.org/abs/2401.06547",
        "authors": [
            {
                "last_name": "Magen",
                "first_name": "Roey"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  The missing item problem, as introduced by Stoeckl in his work at SODA 23, focuses on continually identifying a missing element $e$ in a stream of elements ${e_1, ..., e_{\\ell}}$ from the set $\\{1,2,...,n\\}$, such that $e \\neq e_i$ for any $i \\in \\{1,...,n\\}$. Stoeckl's investigation primarily delves into scenarios with $\\ell<n$, providing bounds for the (i) deterministic case, (ii) the static case -- where the algorithm might be randomized but the stream is fixed in advanced) and (iii) the adversarially robust case -- where the algorithm is randomized and each stream element can be chosen depending on earlier algorithm outputs. Building upon this foundation, our paper addresses previously unexplored aspects of the missing item problem.   In the first segment, we examine the static setting with a long stream, where the length of the steam $\\ell$ is close to or even exceeds the size of the universe $n$. We present an algorithm demonstrating that even when $\\ell$ is very close to $n$ (say $\\ell=n-1$), polylog($n$) bits of memory suffice to identify the missing item. Additionally, we establish tight bounds of $\\tilde{\\Theta(k)}$ for the scenario of $\\ell = n+k$.   The second segment of this part of our work focuses on the {\\em adversarially robust setting}. We show a lower bound for a pseudo-deterministic error-zero (where the algorithm reports its errors) algorithm of approximating $\\Omega(\\ell)$, up to polylog factors. Based on Stoeckl's work, we establish a lower bound for a random-start (only use randomness at initialization) error-zero streaming algorithm.   In the final segment, we explore streaming algorithms with randomness-on-the-fly, where the random bits that are saved for future use are included in the space cost. For streams with length $\\ell = O(\\sqrt{n})$, we provide an upper bound of $O(log n)$. This establishes a gap between randomness-on-the-fly to random-start. ",
        "title": "Are We Still Missing an Item?",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06548",
        "abstract_url": "http://arxiv.org/abs/2401.06548",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Chenyang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Hu",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            },
            {
                "last_name": "Ji",
                "first_name": "Xiangyang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning systems are prone to catastrophic forgetting when learning from a sequence of tasks, where old data from experienced tasks is unavailable when learning from a new task. To mitigate the problem, a line of methods propose to replay the data of experienced tasks when learning new tasks. These methods usually adopt an extra memory to store the data for replay. However, it is not expected in practice considering the memory constraint or data privacy issue. As a replacement, data-free data replay methods are proposed by inverting samples from the classification model. Though achieving good results, these methods still suffer from the inconsistency of the inverted and real training data, which is neglected in the inversion stage in recent works. To that effect, we propose to measure the data consistency quantitatively by some simplification and assumptions. Using the measurement, we analyze existing techniques for inverting samples and get some insightful information that inspires a novel loss function to reduce the inconsistency. Specifically, the loss minimizes the KL divergence of the distributions of inverted and real data under the tied multivariate Gaussian assumption, which is easy to implement in continual learning. In addition, we observe that the norms of old class weights turn to decrease continually as learning progresses. We thus analyze the underlying reasons and propose a simple regularization term to balance the class weights so that the samples of old classes are more distinguishable. To conclude, we propose the Consistency enhanced data replay with debiased classifier for Class Incremental Learning (CCIL). Extensive experiments on CIFAR-100, Tiny-ImageNet, and ImageNet100 show consistently improved performance of CCIL compared to previous approaches. ",
        "title": "Enhancing Consistency and Mitigating Bias: A Data Replay Approach for  Incremental Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06550",
        "abstract_url": "http://arxiv.org/abs/2401.06550",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Chuanji"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yingying"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiaotuan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Qiqi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road nodes, human mobility, and logistics addresses to build a multimodal detection model based on transformer encoder-decoder architecture, titled AOITR. In the model, in addition to the remote sensing images, multi-semantic information including core POI and road nodes is embedded and reorganized as the query content part for the transformer decoder to generate the AOI polygon. Meanwhile, relatively dynamic distribution features of human mobility, nearby POIs, and logistics addresses are used for AOI reliability evaluation through a cascaded feedforward network. The experimental results demonstrate that our algorithm significantly outperforms two existing methods. ",
        "title": "Multimodal Learning for detecting urban functional zones using remote  sensing image and multi-semantic information",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06557",
        "abstract_url": "http://arxiv.org/abs/2401.06557",
        "authors": [
            {
                "last_name": "Cui",
                "first_name": "Ziqiang"
            },
            {
                "last_name": "Tang",
                "first_name": "Xing"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yang"
            },
            {
                "last_name": "He",
                "first_name": "Bowei"
            },
            {
                "last_name": "Chen",
                "first_name": "Liang"
            },
            {
                "last_name": "He",
                "first_name": "Xiuqiang"
            },
            {
                "last_name": "Ma",
                "first_name": "Chen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Estimating the individual treatment effect (ITE) from observational data is a crucial research topic that holds significant value across multiple domains. How to identify hidden confounders poses a key challenge in ITE estimation. Recent studies have incorporated the structural information of social networks to tackle this challenge, achieving notable advancements. However, these methods utilize graph neural networks to learn the representation of hidden confounders in Euclidean space, disregarding two critical issues: (1) the social networks often exhibit a scalefree structure, while Euclidean embeddings suffer from high distortion when used to embed such graphs, and (2) each ego-centric network within a social network manifests a treatment-related characteristic, implying significant patterns of hidden confounders. To address these issues, we propose a novel method called Treatment-Aware Hyperbolic Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic space to encode the social networks, thereby effectively reducing the distortion of confounder representation caused by Euclidean embeddings. Secondly, we design a treatment-aware relationship identification module that enhances the representation of hidden confounders by identifying whether an individual and her neighbors receive the same treatment. Extensive experiments on two benchmark datasets are conducted to demonstrate the superiority of our method. ",
        "title": "Treatment-Aware Hyperbolic Representation Learning for Causal Effect  Estimation with Social Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06559",
        "abstract_url": "http://arxiv.org/abs/2401.06559",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yusen"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Dynamic graph learning is crucial for modeling real-world systems with evolving relationships and temporal dynamics. However, the lack of a unified benchmark framework in current research has led to inaccurate evaluations of dynamic graph models. This paper highlights the significance of dynamic graph learning and its applications in various domains. It emphasizes the need for a standardized benchmark framework that captures temporal dynamics, evolving graph structures, and downstream task requirements. Establishing a unified benchmark will help researchers understand the strengths and limitations of existing models, foster innovation, and advance dynamic graph learning. In conclusion, this paper identifies the lack of a standardized benchmark framework as a current limitation in dynamic graph learning research . Such a framework will facilitate accurate model evaluation, drive advancements in dynamic graph learning techniques, and enable the development of more effective models for real-world applications. ",
        "title": "A General Benchmark Framework is Dynamic Graph Neural Network Need",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06561",
        "abstract_url": "http://arxiv.org/abs/2401.06561",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lefei"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alphadl/SafeLLM_with_IntentionAnalysis ",
        "title": "Intention Analysis Prompting Makes Large Language Models A Good  Jailbreak Defender",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06563",
        "abstract_url": "http://arxiv.org/abs/2401.06563",
        "authors": [
            {
                "last_name": "Safa",
                "first_name": "Ali"
            },
            {
                "last_name": "Mommen",
                "first_name": "Wout"
            },
            {
                "last_name": "Keuninckx",
                "first_name": "Lars"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "HC"
        ],
        "abstract": "  This work proposes a novel approach for hand gesture recognition using an inexpensive, low-resolution (24 x 32) thermal sensor processed by a Spiking Neural Network (SNN) followed by Sparse Segmentation and feature-based gesture classification via Robust Principal Component Analysis (R-PCA). Compared to the use of standard RGB cameras, the proposed system is insensitive to lighting variations while being significantly less expensive compared to high-frequency radars, time-of-flight cameras and high-resolution thermal sensors previously used in literature. Crucially, this paper shows that the innovative use of the recently proposed Monostable Multivibrator (MMV) neural networks as a new class of SNN achieves more than one order of magnitude smaller memory and compute complexity compared to deep learning approaches, while reaching a top gesture recognition accuracy of 93.9% using a 5-class thermal camera dataset acquired in a car cabin, within an automotive context. Our dataset is released for helping future research. ",
        "title": "Resource-Efficient Gesture Recognition using Low-Resolution Thermal  Camera via Spiking Neural Networks and Sparse Segmentation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06566",
        "abstract_url": "http://arxiv.org/abs/2401.06566",
        "authors": [
            {
                "last_name": "Anahtarci",
                "first_name": "Berkay"
            },
            {
                "last_name": "Kariksiz",
                "first_name": "Can Deha"
            },
            {
                "last_name": "Saldi",
                "first_name": "Naci"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) for the forward RL problem. This method is employed to produce data for a numerical example. We note that this novel algorithm is also applicable to general MFE computations. ",
        "title": "Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field  Games",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06568",
        "abstract_url": "http://arxiv.org/abs/2401.06568",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Xu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhirui"
            },
            {
                "last_name": "Geng",
                "first_name": "Xiang"
            },
            {
                "last_name": "Du",
                "first_name": "Yichao"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks. ",
        "title": "Lost in the Source Language: How Large Language Models Evaluate the  Quality of Machine Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06574",
        "abstract_url": "http://arxiv.org/abs/2401.06574",
        "authors": [
            {
                "last_name": "Badings",
                "first_name": "Thom"
            },
            {
                "last_name": "Volk",
                "first_name": "Matthias"
            },
            {
                "last_name": "Junges",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Stoelinga",
                "first_name": "Marielle"
            },
            {
                "last_name": "Jansen",
                "first_name": "Nils"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Labeled continuous-time Markov chains (CTMCs) describe processes subject to random timing and partial observability. In applications such as runtime monitoring, we must incorporate past observations. The timing of these observations matters but may be uncertain. Thus, we consider a setting in which we are given a sequence of imprecisely timed labels called the evidence. The problem is to compute reachability probabilities, which we condition on this evidence. Our key contribution is a method that solves this problem by unfolding the CTMC states over all possible timings for the evidence. We formalize this unfolding as a Markov decision process (MDP) in which each timing for the evidence is reflected by a scheduler. This MDP has infinitely many states and actions in general, making a direct analysis infeasible. Thus, we abstract the continuous MDP into a finite interval MDP (iMDP) and develop an iterative refinement scheme to upper-bound conditional probabilities in the CTMC. We show the feasibility of our method on several numerical benchmarks and discuss key challenges to further enhance the performance. ",
        "title": "CTMCs with Imprecisely Timed Observations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06576",
        "abstract_url": "http://arxiv.org/abs/2401.06576",
        "authors": [
            {
                "last_name": "Theisel",
                "first_name": "Holger"
            },
            {
                "last_name": "Motejat",
                "first_name": "Michael"
            },
            {
                "last_name": "Zimmermann",
                "first_name": "Janos"
            },
            {
                "last_name": "R\u00f6ssl",
                "first_name": "Christian"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  We introduce a representation of a 2D steady vector field ${{\\mathbf v}}$ by two scalar fields $a$, $b$, such that the isolines of $a$ correspond to stream lines of ${{\\mathbf v}}$, and $b$ increases with constant speed under integration of ${{\\mathbf v}}$. This way, we get a direct encoding of stream lines, i.e., a numerical integration of ${{\\mathbf v}}$ can be replaced by a local isoline extraction of $a$. To guarantee a solution in every case, gradient-preserving cuts are introduced such that the scalar fields are allowed to be discontinuous in the values but continuous in the gradient. Along with a piecewise linear discretization and a proper placement of the cuts, the fields $a$ and $b$ can be computed. We show several evaluations on non-trivial vector fields. ",
        "title": "Scalar Representation of 2D Steady Vector Fields",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06578",
        "abstract_url": "http://arxiv.org/abs/2401.06578",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qian"
            },
            {
                "last_name": "Li",
                "first_name": "Weiqi"
            },
            {
                "last_name": "Mou",
                "first_name": "Chong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xinhua"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  360-degree panoramic videos recently attract more interest in both studies and applications, courtesy of the heightened immersive experiences they engender. Due to the expensive cost of capturing 360-degree panoramic videos, generating desirable panoramic videos by given prompts is urgently required. Recently, the emerging text-to-video (T2V) diffusion methods demonstrate notable effectiveness in standard video generation. However, due to the significant gap in content and motion patterns between panoramic and standard videos, these methods encounter challenges in yielding satisfactory 360-degree panoramic videos. In this paper, we propose a controllable panorama video generation pipeline named 360-Degree Video Diffusion model (360DVD) for generating panoramic videos based on the given prompts and motion conditions. Concretely, we introduce a lightweight module dubbed 360-Adapter and assisted 360 Enhancement Techniques to transform pre-trained T2V models for 360-degree video generation. We further propose a new panorama dataset named WEB360 consisting of 360-degree video-text pairs for training 360DVD, addressing the absence of captioned panoramic video datasets. Extensive experiments demonstrate the superiority and effectiveness of 360DVD for panorama video generation. The code and dataset will be released soon. ",
        "title": "360DVD: Controllable Panorama Video Generation with 360-Degree Video  Diffusion Model",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06579",
        "abstract_url": "http://arxiv.org/abs/2401.06579",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Yaoxu"
            },
            {
                "last_name": "Li",
                "first_name": "Hongyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Peng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Time-Triggered Ethernet (TTEthernet) has been widely applied in many scenarios such as industrial internet, automotive electronics, and aerospace, where offline routing and scheduling for TTEthernet has been largely investigated. However, predetermined routes and schedules cannot meet the demands in some agile scenarios, such as smart factories, autonomous driving, and satellite network switching, where the transmission requests join in and leave the network frequently. Thus, we study the online joint routing and scheduling problem for TTEthernet. However, balancing efficient and effective routing and scheduling in an online environment can be quite challenging. To ensure high-quality and fast routing and scheduling, we first design a time-slot expanded graph (TSEG) to model the available resources of TTEthernet over time. The fine-grained representation of TSEG allows us to select a time slot via selecting an edge, thus transforming the scheduling problem into a simple routing problem. Next, we design a dynamic weighting method for each edge in TSEG and further propose an algorithm to co-optimize the routing and scheduling. Our scheme enhances the TTEthernet throughput by co-optimizing the routing and scheduling to eliminate potential conflicts among flow requests, as compared to existing methods. The extensive simulation results show that our scheme runs >400 times faster than standard solutions (i.e., ILP solver), while the gap is only 2% to the optimally scheduled number of flow requests. Besides, as compared to existing schemes, our method can improve the successfully scheduled number of flows by more than 18%. ",
        "title": "Enhancing Throughput for TTEthernet via Co-optimizing Routing and  Scheduling: An Online Time-Varying Graph-based Method",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06580",
        "abstract_url": "http://arxiv.org/abs/2401.06580",
        "authors": [
            {
                "last_name": "Sapozhnikov",
                "first_name": "Arkadii"
            },
            {
                "last_name": "Olsthoorn",
                "first_name": "Mitchell"
            },
            {
                "last_name": "Panichella",
                "first_name": "Annibale"
            },
            {
                "last_name": "Kovalenko",
                "first_name": "Vladimir"
            },
            {
                "last_name": "Derakhshanfar",
                "first_name": "Pouria"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces TestSpark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo ",
        "title": "TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06582",
        "abstract_url": "http://arxiv.org/abs/2401.06582",
        "authors": [
            {
                "last_name": "Ng",
                "first_name": "Lynnette Hui Xian"
            },
            {
                "last_name": "Robertson",
                "first_name": "Dawn C."
            },
            {
                "last_name": "Carley",
                "first_name": "Kathleen M."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Social media platforms are a key ground of information consumption and dissemination. Key figures like politicians, celebrities and activists have leveraged on its wide user base for strategic communication. Strategic communications, or StratCom, is the deliberate act of information creation and distribution. Its techniques are used by these key figures for establishing their brand and amplifying their messages. Automated scripts are used on top of personal touches to quickly and effectively perform these tasks. The combination of automation and manual online posting creates a Cyborg social media profile, which is a hybrid between bot and human. In this study, we establish a quantitative definition for a Cyborg account, which is an account that are detected as bots in one time window, and identified as humans in another. This definition makes use of frequent changes of bot classification labels and large differences in bot likelihood scores to identify Cyborgs. We perform a large-scale analysis across over 3.1 million users from Twitter collected from two key events, the 2020 Coronavirus pandemic and 2020 US Elections. We extract Cyborgs from two datasets and employ tools from network science, natural language processing and manual annotation to characterize Cyborg accounts. Our analyses identify Cyborg accounts are mostly constructed for strategic communication uses, have a strong duality in their bot/human classification and are tactically positioned in the social media network, aiding these accounts to promote their desired content. Cyborgs are also discovered to have long online lives, indicating their ability to evade bot detectors, or the graciousness of platforms to allow their operations. ",
        "title": "Cyborgs for strategic communication on social media",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06583",
        "abstract_url": "http://arxiv.org/abs/2401.06583",
        "authors": [
            {
                "last_name": "Tashu",
                "first_name": "Tsegaye Misikir"
            },
            {
                "last_name": "Kontos",
                "first_name": "Eduard-Raul"
            },
            {
                "last_name": "Sabatelli",
                "first_name": "Matthia"
            },
            {
                "last_name": "Valdenegro-Toro",
                "first_name": "Matias"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR",
            "LG"
        ],
        "abstract": "  Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding beyond language connections, between two specific languages. ",
        "title": "Mapping Transformer Leveraged Embeddings for Cross-Lingual Document  Representation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06588",
        "abstract_url": "http://arxiv.org/abs/2401.06588",
        "authors": [
            {
                "last_name": "Salvi",
                "first_name": "Giampiero"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "SD"
        ],
        "abstract": "  This paper describes the use of connectionist techniques in phonetic speech recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular attention has been paid to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency. ",
        "title": "Dynamic Behaviour of Connectionist Speech Recognition with Strong  Latency Constraints",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06591",
        "abstract_url": "http://arxiv.org/abs/2401.06591",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Seongyun"
            },
            {
                "last_name": "Kim",
                "first_name": "Seungone"
            },
            {
                "last_name": "Park",
                "first_name": "Sue Hyun"
            },
            {
                "last_name": "Kim",
                "first_name": "Geewook"
            },
            {
                "last_name": "Seo",
                "first_name": "Minjoon"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision ",
        "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained  Evaluation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06592",
        "abstract_url": "http://arxiv.org/abs/2401.06592",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Hang"
            },
            {
                "last_name": "Li",
                "first_name": "Song"
            },
            {
                "last_name": "Lin",
                "first_name": "Junhong"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We study deterministic matrix completion problem, i.e., recovering a low-rank matrix from a few observed entries where the sampling set is chosen as the edge set of a Ramanujan graph. We first investigate projected gradient descent (PGD) applied to a Burer-Monteiro least-squares problem and show that it converges linearly to the incoherent ground-truth with respect to the condition number \\k{appa} of ground-truth under a benign initialization and large samples. We next apply the scaled variant of PGD to deal with the ill-conditioned case when \\k{appa} is large, and we show the algorithm converges at a linear rate independent of the condition number \\k{appa} under similar conditions. Finally, we provide numerical experiments to corroborate our results. ",
        "title": "Nonconvex Deterministic Matrix Completion by Projected Gradient Descent  Methods",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06595",
        "abstract_url": "http://arxiv.org/abs/2401.06595",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Wang",
                "first_name": "Qian"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Li",
                "first_name": "Jialu"
            },
            {
                "last_name": "Hu",
                "first_name": "Qinghua"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Attributed graph clustering is an unsupervised task that partitions nodes into different groups. Self-supervised learning (SSL) shows great potential in handling this task, and some recent studies simultaneously learn multiple SSL tasks to further boost performance. Currently, different SSL tasks are assigned the same set of weights for all graph nodes. However, we observe that some graph nodes whose neighbors are in different groups require significantly different emphases on SSL tasks. In this paper, we propose to dynamically learn the weights of SSL tasks for different nodes and fuse the embeddings learned from different SSL tasks to boost performance. We design an innovative graph clustering approach, namely Dynamically Fusing Self-Supervised Learning (DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks using distinct weights derived from a gating network. To effectively learn the gating network, we design a dual-level self-supervised strategy that incorporates pseudo labels and the graph structure. Extensive experiments on five datasets show that DyFSS outperforms the state-of-the-art multi-task SSL methods by up to 8.66% on the accuracy metric. The code of DyFSS is available at: https://github.com/q086/DyFSS. ",
        "title": "Every Node is Different: Dynamically Fusing Self-Supervised Tasks for  Attributed Graph Clustering",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06596",
        "abstract_url": "http://arxiv.org/abs/2401.06596",
        "authors": [
            {
                "last_name": "L\u00f6ding",
                "first_name": "Christof"
            },
            {
                "last_name": "Thomas",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  The class of Boolean combinations of tree languages recognized by deterministic top-down tree automata (also known as deterministic root-to-frontier automata) is studied. The problem of determining for a given regular tree language whether it belongs to this class is open. We provide some progress by two results: First, a characterization of this class by a natural extension of deterministic top-down tree automata is presented, and as an application we obtain a convenient method to show that certain regular tree languages are outside this class. In the second result, it is shown that, for fixed $k$, it is decidable whether a regular tree language is a Boolean combination of $k$ tree languages recognized by deterministic top-down tree automata. ",
        "title": "On the Boolean Closure of Deterministic Top-Down Tree Automata",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06601",
        "abstract_url": "http://arxiv.org/abs/2401.06601",
        "authors": [
            {
                "last_name": "Nunes",
                "first_name": "Henry C."
            },
            {
                "last_name": "da Silva",
                "first_name": "Marlon P."
            },
            {
                "last_name": "Neu",
                "first_name": "Charles V."
            },
            {
                "last_name": "Zorzo",
                "first_name": "Avelino F."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "DB"
        ],
        "abstract": "  This paper presents ongoing research focused on improving the utility of data protected by Global Differential Privacy(DP) in the scenario of summary statistics. Our approach is based on predictions on how an analyst will use statistics released under DP protection, so that a developer can optimise data utility on further usage of the data in the privacy budget allocation. This novel approach can potentially improve the utility of data without compromising privacy constraints. We also propose a metric that can be used by the developer to optimise the budget allocation process. ",
        "title": "A proposal to increase data utility on Global Differential Privacy data  based on data use predictions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06602",
        "abstract_url": "http://arxiv.org/abs/2401.06602",
        "authors": [
            {
                "last_name": "Voggenreiter",
                "first_name": "Markus"
            },
            {
                "last_name": "Angermeir",
                "first_name": "Florian"
            },
            {
                "last_name": "Moy\u00f3n",
                "first_name": "Fabiola"
            },
            {
                "last_name": "Sch\u00f6pp",
                "first_name": "Ulrich"
            },
            {
                "last_name": "Bonvin",
                "first_name": "Pierre"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  In recent years, DevOps, the unification of development and operation workflows, has become a trend for the industrial software development lifecycle. Security activities turned into an essential field of application for DevOps principles as they are a fundamental part of secure software development in the industry. A common practice arising from this trend is the automation of security tests that analyze a software product from several perspectives. To effectively improve the security of the analyzed product, the identified security findings must be managed and looped back to the project team for stakeholders to take action. This management must cope with several challenges ranging from low data quality to a consistent prioritization of findings while following DevOps aims. To manage security findings with the same efficiency as other activities in DevOps projects, a methodology for the management of industrial security findings minding DevOps principles is essential.   In this paper, we propose a methodology for the management of security findings in industrial DevOps projects, summarizing our research in this domain and presenting the resulting artifact. As an instance of the methodology, we developed the Security Flama, a semantic knowledge base for the automated management of security findings. To analyze the impact of our methodology on industrial practice, we performed a case study on two DevOps projects of a multinational industrial enterprise. The results emphasize the importance of using such an automated methodology in industrial DevOps projects, confirm our approach's usefulness and positive impact on the studied projects, and identify the communication strategy as a crucial factor for usability in practice. ",
        "title": "Automated Security Findings Management: A Case Study in Industrial  DevOps",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06603",
        "abstract_url": "http://arxiv.org/abs/2401.06603",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Shangding"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as \"I help you help I help.\" The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization, exploration, and mutual improvement for both agents, enabling them to accomplish increasingly challenging tasks. Remarkably, we propose a practical algorithm to address the problem and conduct empirical experiments to evaluate the effectiveness of our method. ",
        "title": "Mutual Enhancement of Large Language and Reinforcement Learning Models  through Bi-Directional Feedback Mechanisms: A Case Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06610",
        "abstract_url": "http://arxiv.org/abs/2401.06610",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jingyi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper addresses the planar finger kinematics for seeking optimized manipulation strategies. The first step is to model based on geometric features of linear and rotation motion so that the robot can select the fingers configurations. This kinematic model considers the motion between hands and object. Based on 2-finger manipulation cases, this model can output the strategies for bimanual manipulation. For executing strategies, the second step is to seek the appropriate values of finger joints according to the ending orientation of fingers. The simulation shows that the computed solutions can complete the relative rotation and linear motion of unknown objects. ",
        "title": "The Hand-object Kinematic Model for Bimanual Manipulation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06612",
        "abstract_url": "http://arxiv.org/abs/2401.06612",
        "authors": [
            {
                "last_name": "AlQahtani",
                "first_name": "Ali Abdullah S."
            },
            {
                "last_name": "Alshayeb",
                "first_name": "Thamraa"
            },
            {
                "last_name": "Nabil",
                "first_name": "Mahmoud"
            },
            {
                "last_name": "Patooghy",
                "first_name": "Ahmad"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The traditional two-factor authentication (2FA) methods primarily rely on the user manually entering a code or token during the authentication process. This can be burdensome and time-consuming, particularly for users who must be authenticated frequently. To tackle this challenge, we present a novel 2FA approach replacing the user's input with decisions made by Machine Learning (ML) that continuously verifies the user's identity with zero effort. Our system exploits unique environmental features associated with the user, such as beacon frame characteristics and Received Signal Strength Indicator (RSSI) values from Wi-Fi Access Points (APs). These features are gathered and analyzed in real-time by our ML algorithm to ascertain the user's identity. For enhanced security, our system mandates that the user's two devices (i.e., a login device and a mobile device) be situated within a predetermined proximity before granting access. This precaution ensures that unauthorized users cannot access sensitive information or systems, even with the correct login credentials. Through experimentation, we have demonstrated our system's effectiveness in determining the location of the user's devices based on beacon frame characteristics and RSSI values, achieving an accuracy of 92.4%. Additionally, we conducted comprehensive security analysis experiments to evaluate the proposed 2FA system's resilience against various cyberattacks. Our findings indicate that the system exhibits robustness and reliability in the face of these threats. The scalability, flexibility, and adaptability of our system render it a promising option for organizations and users seeking a secure and convenient authentication system. ",
        "title": "Leveraging Machine Learning for Wi-Fi-based Environmental Continuous  Two-Factor Authentication",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06614",
        "abstract_url": "http://arxiv.org/abs/2401.06614",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Wei"
            },
            {
                "last_name": "Luo",
                "first_name": "Chang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Biao"
            },
            {
                "last_name": "Nie\u00dfner",
                "first_name": "Matthias"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiapeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based prior enables more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent vector sets instead of using a global latent. This novel 4D representation allows us to learn local surface shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporal-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid the computational overhead, we design an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against the state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations, notably achieving a 19% improvement in Intersection over Union (IoU) compared to CaDex for reconstructing unseen individuals from sparse point clouds on the DeformingThings4D-Animals dataset. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/. ",
        "title": "Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape  Reconstruction and Tracking",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06618",
        "abstract_url": "http://arxiv.org/abs/2401.06618",
        "authors": [
            {
                "last_name": "Ball",
                "first_name": "Simeon"
            },
            {
                "last_name": "Moreno",
                "first_name": "Edgar"
            },
            {
                "last_name": "Simoens",
                "first_name": "Robin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We prove that the natural isomorphism between GF(2^h) and GF(2)^h induces a bijection between stabiliser codes on n quqits with local dimension q=2^h and binary stabiliser codes on hn qubits. This allows us to describe these codes geometrically: a stabiliser code over a field of even order corresponds to a so-called quantum set of symplectic polar spaces. Moreover, equivalent stabiliser codes have a similar geometry, which can be used to prove the uniqueness of a [[4,0,3]]_4 stabiliser code and the nonexistence of both a [[7,1,4]]_4 and an [[8,0,5]]_4 stabiliser code. ",
        "title": "Stabiliser codes over fields of even order",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06619",
        "abstract_url": "http://arxiv.org/abs/2401.06619",
        "authors": [
            {
                "last_name": "Chow",
                "first_name": "Yiu Wai"
            },
            {
                "last_name": "Di Grazia",
                "first_name": "Luca"
            },
            {
                "last_name": "Pradel",
                "first_name": "Michael"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice. ",
        "title": "PyTy: Repairing Static Type Errors in Python",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06620",
        "abstract_url": "http://arxiv.org/abs/2401.06620",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yihong"
            },
            {
                "last_name": "Ma",
                "first_name": "Chunlan"
            },
            {
                "last_name": "Ye",
                "first_name": "Haotian"
            },
            {
                "last_name": "Sch\u00fctze",
                "first_name": "Hinrich"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\\%) of its training data, and refer to the resulting model as Furina. We show that Furina not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages are highly related but use different scripts. We make our code and models publicly available. ",
        "title": "TransliCo: A Contrastive Learning Framework to Address the Script  Barrier in Multilingual Pretrained Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06626",
        "abstract_url": "http://arxiv.org/abs/2401.06626",
        "authors": [
            {
                "last_name": "Bursuc",
                "first_name": "Sergiu"
            },
            {
                "last_name": "Gil-Pons",
                "first_name": "Reynaldo"
            },
            {
                "last_name": "Mauw",
                "first_name": "Sjouke"
            },
            {
                "last_name": "Trujillo-Rasua",
                "first_name": "Rolando"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  A Proof of Secure Erasure (PoSE) is a communication protocol where a verifier seeks evidence that a prover has erased its memory within the time frame of the protocol execution. Designers of PoSE protocols have long been aware that, if a prover can outsource the computation of the memory erasure proof to another device, then their protocols are trivially defeated. As a result, most software-based PoSE protocols in the literature assume that provers are isolated during the protocol execution, that is, provers cannot receive help from a network adversary. Our main contribution is to show that this assumption is not necessary. We introduce formal models for PoSE protocols playing against provers aided by external conspirators and develop three PoSE protocols that we prove secure in this context. We reduce the requirement of isolation to the more realistic requirement that the communication with the external conspirator is relatively slow. Software-based protocols with such relaxed isolation assumptions are especially pertinent for low-end devices, where it is too costly to deploy sophisticated protection methods. ",
        "title": "Software-Based Memory Erasure with relaxed isolation requirements:  Extended Version",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06628",
        "abstract_url": "http://arxiv.org/abs/2401.06628",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            },
            {
                "last_name": "Luo",
                "first_name": "Yong"
            },
            {
                "last_name": "Du",
                "first_name": "Bo"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field. Our benchmark and scripts are publicly released at: https://github.com/alphadl/OOP-eval. ",
        "title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language  Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06633",
        "abstract_url": "http://arxiv.org/abs/2401.06633",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Lei"
            },
            {
                "last_name": "Lian",
                "first_name": "Jianxun"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiao"
            },
            {
                "last_name": "Xie",
                "first_name": "Xing"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We perform experiments on three widely used public datasets, incorporating five powerful sequential recommenders as backbone models. Our results demonstrate that Ada-Retrieval significantly enhances the performance of various base models, with consistent improvements observed across different datasets. Our code and data are publicly available at: https://github.com/ll0ruc/Ada-Retrieval. ",
        "title": "Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential  Recommendations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06634",
        "abstract_url": "http://arxiv.org/abs/2401.06634",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Jie"
            },
            {
                "last_name": "Liu",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhong-Yuan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCFC also shows superior performance in handling device failures from a practical viewpoint. ",
        "title": "CCFC: Bridging Federated Clustering and Contrastive Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06635",
        "abstract_url": "http://arxiv.org/abs/2401.06635",
        "authors": [
            {
                "last_name": "Iserles",
                "first_name": "Arieh"
            },
            {
                "last_name": "Kropielnicka",
                "first_name": "Karolina"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Using elementary means, we derive the three most popular splittings of $e^{(A+B)}$ and their error bounds in the case when $A$ and $B$ are (possibly unbounded) operators in a Hilbert space, generating strongly continuous semigroups, $e^{tA}$, $e^{tB}$ and $e^{t(A+B)}$. The error of these splittings is bounded in terms of the norm of the commutators $[A, B]$, $[A, [A, B]]$ and $[B, [A, B]]$. ",
        "title": "An elementary approach to splittings of unbounded operators",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06638",
        "abstract_url": "http://arxiv.org/abs/2401.06638",
        "authors": [
            {
                "last_name": "Bansal",
                "first_name": "Manish"
            },
            {
                "last_name": "Shrivastava",
                "first_name": "Pramsu"
            },
            {
                "last_name": "Harshan",
                "first_name": "J."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CR"
        ],
        "abstract": "  In Vehicle-to-Everything (V2X) networks that involve multi-hop communication, the Road Side Units (RSUs) typically desire to gather the location information of the participating vehicles to provide security and network-diagnostics features. Although Global Positioning System (GPS) based localization is widely used by vehicles for navigation; they may not forward their exact GPS coordinates to the RSUs due to privacy issues. Therefore, to balance the high-localization requirements of RSU and the privacy of the vehicles, we demonstrate a new spatial-provenance framework wherein the vehicles agree to compromise their privacy to a certain extent and share a low-precision variant of its coordinates in agreement with the demands of the RSU. To study the deployment feasibility of the proposed framework in state-of-the-art wireless standards, we propose a testbed of ZigBee and LoRa devices and implement the underlying protocols on their stack using correlated Bloom filters and Rake compression algorithms. Our demonstrations reveal that low-to-moderate precision localization can be achieved in fewer packets, thus making an appealing case for next-generation vehicular networks to include our methods for providing real-time security and network-diagnostics features. ",
        "title": "A Prototype on the Feasibility of Learning Spatial Provenance in XBee  and LoRa Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06640",
        "abstract_url": "http://arxiv.org/abs/2401.06640",
        "authors": [
            {
                "last_name": "Misra",
                "first_name": "Kanishka"
            },
            {
                "last_name": "Ettinger",
                "first_name": "Allyson"
            },
            {
                "last_name": "Mahowald",
                "first_name": "Kyle"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs. ",
        "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference  in Language Models, but Inconsistently",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06643",
        "abstract_url": "http://arxiv.org/abs/2401.06643",
        "authors": [
            {
                "last_name": "Cegin",
                "first_name": "Jan"
            },
            {
                "last_name": "Pecher",
                "first_name": "Branislav"
            },
            {
                "last_name": "Simko",
                "first_name": "Jakub"
            },
            {
                "last_name": "Srba",
                "first_name": "Ivan"
            },
            {
                "last_name": "Bielikova",
                "first_name": "Maria"
            },
            {
                "last_name": "Brusilovsky",
                "first_name": "Peter"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune the model. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs and 6 datasets. We show that diversity is most increased by taboo words, while downstream model performance is highest when previously created paraphrases are used as hints. ",
        "title": "Effects of diversity incentives on sample diversity and downstream model  performance in LLM-based text augmentation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06644",
        "abstract_url": "http://arxiv.org/abs/2401.06644",
        "authors": [
            {
                "last_name": "Saeizadeh",
                "first_name": "Ali"
            },
            {
                "last_name": "Schonholtz",
                "first_name": "Douglas"
            },
            {
                "last_name": "Uvaydov",
                "first_name": "Daniel"
            },
            {
                "last_name": "Guida",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Demirors",
                "first_name": "Emrecan"
            },
            {
                "last_name": "Johari",
                "first_name": "Pedram"
            },
            {
                "last_name": "Jimenez",
                "first_name": "Jorge M."
            },
            {
                "last_name": "Neimat",
                "first_name": "Joseph S."
            },
            {
                "last_name": "Melodia",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we introduce SeizNet, a closed-loop system for predicting epileptic seizures through the use of Deep Learning (DL) method and implantable sensor networks. While pharmacological treatment is effective for some epilepsy patients (with ~65M people affected worldwide), one out of three suffer from drug-resistant epilepsy. To alleviate the impact of seizure, predictive systems have been developed that can notify such patients of an impending seizure, allowing them to take precautionary measures. SeizNet leverages DL techniques and combines data from multiple recordings, specifically intracranial electroencephalogram (iEEG) and electrocardiogram (ECG) sensors, that can significantly improve the specificity of seizure prediction while preserving very high levels of sensitivity. SeizNet DL algorithms are designed for efficient real-time execution at the edge, minimizing data privacy concerns, data transmission overhead, and power inefficiencies associated with cloud-based solutions. Our results indicate that SeizNet outperforms traditional single-modality and non-personalized prediction systems in all metrics, achieving up to 99% accuracy in predicting seizure, offering a promising new avenue in refractory epilepsy treatment. ",
        "title": "SeizNet: An AI-enabled Implantable Sensor Network System for Seizure  Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06646",
        "abstract_url": "http://arxiv.org/abs/2401.06646",
        "authors": [
            {
                "last_name": "Hien",
                "first_name": "Le Thi Khanh"
            },
            {
                "last_name": "Leplat",
                "first_name": "Valentin"
            },
            {
                "last_name": "Gillis",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a Block Majorization Minimization method with Extrapolation (BMMe) for solving a class of multi-convex optimization problems. The extrapolation parameters of BMMe are updated using a novel adaptive update rule. By showing that block majorization minimization can be reformulated as a block mirror descent method, with the Bregman divergence adaptively updated at each iteration, we establish subsequential convergence for BMMe. We use this method to design efficient algorithms to tackle nonnegative matrix factorization problems with the $\\beta$-divergences ($\\beta$-NMF) for $\\beta\\in [1,2]$. These algorithms, which are multiplicative updates with extrapolation, benefit from our novel results that offer convergence guarantees. We also empirically illustrate the significant acceleration of BMMe for $\\beta$-NMF through extensive experiments. ",
        "title": "Block Majorization Minimization with Extrapolation and Application to  $\\beta$-NMF",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06648",
        "abstract_url": "http://arxiv.org/abs/2401.06648",
        "authors": [
            {
                "last_name": "Allamaa",
                "first_name": "Jean Pierre"
            },
            {
                "last_name": "Patrinos",
                "first_name": "Panagiotis"
            },
            {
                "last_name": "Ohtsuka",
                "first_name": "Toshiyuki"
            },
            {
                "last_name": "Son",
                "first_name": "Tong Duy"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The autonomous driving industry is continuously dealing with more safety-critical scenarios, and nonlinear model predictive control (NMPC) is a powerful control strategy for handling such situations. However, standard safety constraints are not scalable and require a long NMPC horizon. Moreover, the adoption of NMPC in the automotive industry is limited by the heavy computation of numerical optimization routines. To address those issues, this paper presents a real-time capable NMPC for automated driving in urban environments, using control barrier functions (CBFs). Furthermore, the designed NMPC is based on a novel collocation transcription approach, named RESAFE/COL, that allows to reduce the number of optimization variables while still guaranteeing the continuous time (nonlinear) inequality constraints satisfaction, through regional convex hull approximation. RESAFE/COL is proven to be 5 times faster than multiple shooting and more tractable for embedded hardware without a decrease in the performance, nor accuracy and safety of the numerical solution. We validate our NMPC-CBF with RESAFE/COL approach with highly accurate digital twins of the vehicle and the urban environment and show the safe controller's ability to improve crash avoidance by 91%. ",
        "title": "Real-time MPC with Control Barrier Functions for Autonomous Driving  using Safety Enhanced Collocation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06649",
        "abstract_url": "http://arxiv.org/abs/2401.06649",
        "authors": [
            {
                "last_name": "Heidari",
                "first_name": "Arash"
            },
            {
                "last_name": "Gonzalez",
                "first_name": "Sebastian Rojas"
            },
            {
                "last_name": "Dhaene",
                "first_name": "Tom"
            },
            {
                "last_name": "Couckuyt",
                "first_name": "Ivo"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Multi-objective optimization is a widely studied problem in diverse fields, such as engineering and finance, that seeks to identify a set of non-dominated solutions that provide optimal trade-offs among competing objectives. However, the computation of the entire Pareto front can become prohibitively expensive, both in terms of computational resources and time, particularly when dealing with a large number of objectives. In practical applications, decision-makers (DMs) will select a single solution of the Pareto front that aligns with their preferences to be implemented; thus, traditional multi-objective algorithms invest a lot of budget sampling solutions that are not interesting for the DM. In this paper, we propose two novel algorithms that employ Gaussian Processes and advanced discretization methods to efficiently locate the most preferred region of the Pareto front in expensive-to-evaluate problems. Our approach involves interacting with the decision-maker to guide the optimization process towards their preferred trade-offs. Our experimental results demonstrate that our proposed algorithms are effective in finding non-dominated solutions that align with the decision-maker's preferences while maintaining computational efficiency. ",
        "title": "Data-Efficient Interactive Multi-Objective Optimization Using ParEGO",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06650",
        "abstract_url": "http://arxiv.org/abs/2401.06650",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Zilin"
            },
            {
                "last_name": "Georgiou",
                "first_name": "Anastasis"
            },
            {
                "last_name": "Yu",
                "first_name": "Min"
            },
            {
                "last_name": "Evangelou",
                "first_name": "Simos A."
            },
            {
                "last_name": "Jaimoukha",
                "first_name": "Imad M"
            },
            {
                "last_name": "Dini",
                "first_name": "Daniele"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a robust model predictive control-based solution for the recently introduced series active variable geometry suspension (SAVGS) to improve the ride comfort and road holding of a quarter car. In order to close the gap between the nonlinear multi-body SAVGS model and its linear equivalent, a new uncertain system characterization is proposed that captures unmodeled dynamics, parameter variation, and external disturbances. Based on the newly proposed linear uncertain model for the quarter car SAVGS system, a constrained optimal control problem (OCP) is presented in the form of a linear matrix inequality (LMI) optimization. More specifically, utilizing semidefinite relaxation techniques a state-feedback robust model predictive control (RMPC) scheme is presented and integrated with the nonlinear multi-body SAVGS model, where state-feedback gain and control perturbation are computed online to optimise performance, while physical and design constraints are preserved. Numerical simulation results with different ISO-defined road events demonstrate the robustness and significant performance improvement in terms of ride comfort and road holding of the proposed approach, as compared to the conventional passive suspension, as well as, to actively controlled SAVGS by a previously developed conventional H-infinity control scheme. ",
        "title": "LMI-based robust model predictive control for a quarter car with series  active variable geometry suspension",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06653",
        "abstract_url": "http://arxiv.org/abs/2401.06653",
        "authors": [
            {
                "last_name": "Georgescu",
                "first_name": "Calin"
            },
            {
                "last_name": "Olsthoorn",
                "first_name": "Mitchell"
            },
            {
                "last_name": "Derakhshanfar",
                "first_name": "Pouria"
            },
            {
                "last_name": "Akhin",
                "first_name": "Marat"
            },
            {
                "last_name": "Panichella",
                "first_name": "Annibale"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Compiler correctness is a cornerstone of reliable software development. However, systematic testing of compilers is infeasible, given the vast space of possible programs and the complexity of modern programming languages. In this context, differential testing offers a practical methodology as it addresses the oracle problem by comparing the output of alternative compilers given the same set of programs as input. In this paper, we investigate the effectiveness of differential testing in finding bugs within the Kotlin compilers developed at JetBrains. We propose a black-box generative approach that creates input programs for the K1 and K2 compilers. First, we build workable models of Kotlin semantic (semantic interface) and syntactic (enriched context-free grammar) language features, which are subsequently exploited to generate random code snippets. Second, we extend random sampling by introducing two genetic algorithms (GAs) that aim to generate more diverse input programs. Our case study shows that the proposed approach effectively detects bugs in K1 and K2; these bugs have been confirmed and (some) fixed by JetBrains developers. While we do not observe a significant difference w.r.t. the number of defects uncovered by the different search algorithms, random search and GAs are complementary as they find different categories of bugs. Finally, we provide insights into the relationships between the size, complexity, and fault detection capability of the generated input programs. ",
        "title": "Evolutionary Generative Fuzzing for Differential Testing of the Kotlin  Compiler",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06654",
        "abstract_url": "http://arxiv.org/abs/2401.06654",
        "authors": [
            {
                "last_name": "Bl\u00fccher",
                "first_name": "Stefan"
            },
            {
                "last_name": "Vielhaben",
                "first_name": "Johanna"
            },
            {
                "last_name": "Strodthoff",
                "first_name": "Nils"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-based explanations (Shapley values) as well as their evaluation (pixel flipping, PF). However, occlusion strategies can vary significantly from simple mean replacement up to inpainting with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-based approaches. For example, PF benchmarks lead to contradicting rankings. This is amplified by competing PF measures: Features are either removed starting with most influential first (MIF) or least influential first (LIF). This study proposes two complementary perspectives to resolve this disagreement problem. Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead to unreliable model evaluations. We propose to measure the reliability by the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of occlusion strategies and resolves the disagreement problem by grouping consistent PF rankings. Secondly, we show that the insightfulness of MIF and LIF is conversely dependent on the R-OMS score. To leverage this, we combine the MIF and LIF measures into the symmetric relevance gain (SRG) measure. This breaks the inherent connection to the underlying occlusion strategy and leads to consistent rankings. This resolves the disagreement problem, which we verify for a set of 40 different occlusion strategies. ",
        "title": "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI  Benchmarks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06656",
        "abstract_url": "http://arxiv.org/abs/2401.06656",
        "authors": [
            {
                "last_name": "Opschoor",
                "first_name": "Joost A. A."
            },
            {
                "last_name": "Schwab",
                "first_name": "Christoph"
            },
            {
                "last_name": "Xenophontos",
                "first_name": "Christos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We prove deep neural network (DNN for short) expressivity rate bounds for solution sets of a model class of singularly perturbed, elliptic two-point boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We assume that the given source term and reaction coefficient are analytic in $[-1,1]$.   We establish expression rate bounds in Sobolev norms in terms of the NN size which are uniform with respect to the singular perturbation parameter for several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and $\\tanh$- and sigmoid-activated NNs. The latter activations can represent ``exponential boundary layer solution features'' explicitly, in the last hidden layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust expression rate bounds in terms of the NN size.   We prove that all DNN architectures allow robust exponential solution expression in so-called `energy' as well as in `balanced' Sobolev norms, for analytic input data. ",
        "title": "Neural Networks for Singular Perturbations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06657",
        "abstract_url": "http://arxiv.org/abs/2401.06657",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Jayasree"
            },
            {
                "last_name": "Dey",
                "first_name": "Debasmita"
            },
            {
                "last_name": "Ferlin",
                "first_name": "Simone"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Nirnay"
            },
            {
                "last_name": "Bajpai",
                "first_name": "Vaibhav"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  The Tactile Internet paradigm is set to revolutionize human society by enabling skill-set delivery and haptic communication over ultra-reliable, low-latency networks. The emerging sixth-generation (6G) mobile communication systems are envisioned to underpin this Tactile Internet ecosystem at the network edge by providing ubiquitous global connectivity. However, apart from a multitude of opportunities of the Tactile Internet, security and privacy challenges emerge at the forefront. We believe that the recently standardized QUIC protocol, characterized by end-to-end encryption and reduced round-trip delay would serve as the backbone of Tactile Internet. In this article, we envision a futuristic scenario where a QUIC-enabled network uses the underlying 6G communication infrastructure to achieve the requirements for Tactile Internet. Interestingly this requires a deeper investigation of a wide range of security and privacy challenges in QUIC, that need to be mitigated for its adoption in Tactile Internet. Henceforth, this article reviews the existing security and privacy attacks in QUIC and their implication on users. Followed by that, we discuss state-of-the-art attack mitigation strategies and investigate some of their drawbacks with possible directions for future work ",
        "title": "Accelerating Tactile Internet with QUIC: A Security and Privacy  Perspective",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06658",
        "abstract_url": "http://arxiv.org/abs/2401.06658",
        "authors": [
            {
                "last_name": "Nasuto",
                "first_name": "Andrea"
            },
            {
                "last_name": "Rowe",
                "first_name": "Francisco"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Immigration is one of the most salient topics in public debate. Social media heavily influences opinions on immigration, often sparking polarized debates and offline tensions. Studying 220,870 immigration-related tweets in the UK, we assessed the extent of polarization, key content creators and disseminators, and the speed of content dissemination. We identify a high degree of online polarization between pro and anti-immigration communities. We found that the anti-migration community is small but denser and more active than the pro-immigration community with the top 1% of users responsible for over 23% of anti-immigration tweets and 21% of retweets. We also discovered that anti-immigration content spreads also 1.66 times faster than pro-immigration messages and bots have minimal impact on content dissemination. Our findings suggest that identifying and tracking highly active users could curb anti-immigration sentiment, potentially easing social polarization and shaping broader societal attitudes toward migration. ",
        "title": "Exposing Hate -- Understanding Anti-Immigration Sentiment Spreading on  Twitter",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06659",
        "abstract_url": "http://arxiv.org/abs/2401.06659",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wenbin"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            },
            {
                "last_name": "Luo",
                "first_name": "Yong"
            },
            {
                "last_name": "Hu",
                "first_name": "Han"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (brings an average +1.89 F1 score among five advanced methods) over several state-of-the-art methods. Code will be released. ",
        "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual  World Knowledge",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06665",
        "abstract_url": "http://arxiv.org/abs/2401.06665",
        "authors": [
            {
                "last_name": "Consolaro",
                "first_name": "Gianpietro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhen"
            },
            {
                "last_name": "Razanajato",
                "first_name": "Harenome"
            },
            {
                "last_name": "Lossing",
                "first_name": "Nelson"
            },
            {
                "last_name": "Tchoulak",
                "first_name": "Nassim"
            },
            {
                "last_name": "Susungi",
                "first_name": "Adilla"
            },
            {
                "last_name": "Alves",
                "first_name": "Artur Cesar Araujo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Renwei"
            },
            {
                "last_name": "Barthou",
                "first_name": "Denis"
            },
            {
                "last_name": "Ancourt",
                "first_name": "Corinne"
            },
            {
                "last_name": "Bastoul",
                "first_name": "Cedric"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "CL",
            "PF"
        ],
        "abstract": "  Polyhedral techniques have been widely used for automatic code optimization in low-level compilers and higher-level processes. Loop optimization is central to this technique, and several polyhedral schedulers like Feautrier, Pluto, isl and Tensor Scheduler have been proposed, each of them targeting a different architecture, parallelism model, or application scenario. The need for scenario-specific optimization is growing due to the heterogeneity of architectures. One of the most critical cases is represented by NPUs (Neural Processing Units) used for AI, which may require loop optimization with different objectives. Another factor to be considered is the framework or compiler in which polyhedral optimization takes place. Different scenarios, depending on the target architecture, compilation environment, and application domain, may require different kinds of optimization to best exploit the architecture feature set.   We introduce a new configurable polyhedral scheduler, PolyTOPS, that can be adjusted to various scenarios with straightforward, high-level configurations. This scheduler allows the creation of diverse scheduling strategies that can be both scenario-specific (like state-of-the-art schedulers) and kernel-specific, breaking the concept of a one-size-fits-all scheduler approach. PolyTOPS has been used with isl and CLooG as code generators and has been integrated in MindSpore AKG deep learning compiler. Experimental results in different scenarios show good performance: a geomean speedup of 7.66x on MindSpore (for the NPU Ascend architecture) hybrid custom operators over isl scheduling, a geomean speedup up to 1.80x on PolyBench on different multicore architectures over Pluto scheduling. Finally, some comparisons with different state-of-the-art tools are presented in the PolyMage scenario. ",
        "title": "PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06667",
        "abstract_url": "http://arxiv.org/abs/2401.06667",
        "authors": [
            {
                "last_name": "Arazzi",
                "first_name": "Marco"
            },
            {
                "last_name": "Nocera",
                "first_name": "Antonino"
            },
            {
                "last_name": "Storti",
                "first_name": "Emanuele"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recently, the Industry 5.0 is gaining attention as a novel paradigm, defining the next concrete steps toward more and more intelligent, green-aware and user-centric digital systems. In an era in which smart devices typically adopted in the industry domain are more and more sophisticated and autonomous, the Internet of Things and its evolution, known as the Internet of Everything (IoE, for short), involving also people, robots, processes and data in the network, represent the main driver to allow industries to put the experiences and needs of human beings at the center of their ecosystems. However, due to the extreme heterogeneity of the involved entities, their intrinsic need and capability to cooperate, and the aim to adapt to a dynamic user-centric context, special attention is required for the integration and processing of the data produced by such an IoE. This is the objective of the present paper, in which we propose a novel semantic model that formalizes the fundamental actors, elements and information of an IoE, along with their relationships. In our design, we focus on state-of-the-art design principles, in particular reuse, and abstraction, to build ``SemIoE'', a lightweight ontology inheriting and extending concepts from well-known and consolidated reference ontologies. The defined semantic layer represents a core data model that can be extended to embrace any modern industrial scenario. It represents the base of an IoE Knowledge Graph, on top of which, as an additional contribution, we analyze and define some essential services for an IoE-based industry. ",
        "title": "The SemIoE Ontology: A Semantic Model Solution for an IoE-based Industry",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06669",
        "abstract_url": "http://arxiv.org/abs/2401.06669",
        "authors": [
            {
                "last_name": "G\u00f6ttsch",
                "first_name": "Fabian"
            },
            {
                "last_name": "Caire",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Xu",
                "first_name": "Wen"
            },
            {
                "last_name": "Schubert",
                "first_name": "Martin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper presents a comprehensive communication theoretic model for the physical layer of a cell-free user-centric network, formed by user equipments (UEs), radio units (RUs), and decentralized units (DUs), uniformly spatially distributed over a given coverage area. We consider RUs equipped with multiple antennas, and focus on the regime where the UE, RU, and DU densities are constant and therefore the number of such nodes grows with the coverage area. A system is said scalable if the computing load and information rate at any node in the network converges to a constant as the network size (coverage area) grows to infinity. This imposes that each UE must be processed by a (user-centric) finite-size cluster of RUs, and that such cluster processors are dynamically allocated to the DUs (e.g., as software defined virtual network functions) in order to achieve a balanced computation load. We also assume that the RUs are connected to the DUs through a packet switching network, in order to achieve adaptive routing and load balance. For this model, we define in details the dynamic cluster formation and uplink pilot allocation. As a consequence of the pilot allocation and the scalability constraint, each cluster processor has a partial view of the network channel state information. We define the condition of ``ideal partial CSI'' when the channel vectors that can be estimated are perfectly known (while the ones that cannot be estimated are not know at all). We develop two attractive cluster-based linear receiver schemes for the uplink, and an uplink-downlink duality that allows to reuse such vectors as precoders for the downlink. ",
        "title": "User-Centric Cell-Free Wireless Networks for 6G: Communication Theoretic  Models and Research Challenges",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06671",
        "abstract_url": "http://arxiv.org/abs/2401.06671",
        "authors": [
            {
                "last_name": "Razmjoo",
                "first_name": "Amirreza"
            },
            {
                "last_name": "Brecelj",
                "first_name": "Tilen"
            },
            {
                "last_name": "Savevska",
                "first_name": "Kristina"
            },
            {
                "last_name": "Ude",
                "first_name": "Ale\u0161"
            },
            {
                "last_name": "Petri\u010d",
                "first_name": "Tadej"
            },
            {
                "last_name": "Calinon",
                "first_name": "Sylvain"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper presents a study on the use of the Talos humanoid robot for performing assistive sit-to-stand or stand-to-sit tasks. In such tasks, the human exerts a large amount of force (100--200 N) within a very short time (2--8 s), posing significant challenges in terms of human unpredictability and robot stability control. To address these challenges, we propose an approach for finding a spatial reference for the robot, which allows the robot to move according to the force exerted by the human and control its stability during the task. Specifically, we focus on the problem of finding a 1D manifold for the robot, while assuming a simple controller to guide its movement on this manifold. To achieve this, we use a functional representation to parameterize the manifold and solve an optimization problem that takes into account the robot's stability and the unpredictability of human behavior. We demonstrate the effectiveness of our approach through simulations and experiments with the Talos robot, showing robustness and adaptability. ",
        "title": "Learning Joint Space Reference Manifold for Reliable Physical Assistance",
        "date": "2023-08-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06672",
        "abstract_url": "http://arxiv.org/abs/2401.06672",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Sangung"
            },
            {
                "last_name": "Xue",
                "first_name": "Jiawei"
            },
            {
                "last_name": "Ukkusuri",
                "first_name": "Satish V."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Frequent and intensive disasters make the repeated and uncertain post-disaster recovery process. Despite the importance of the successful recovery process, previous simulation studies on the post-disaster recovery process did not explore the sufficient number of household return decision model types, population sizes, and the corresponding critical transition conditions of the system. This paper simulates the recovery process in the agent-based model with multilayer networks to reveal the impact of household return decision model types and population sizes in a toy network. After that, this paper applies the agent-based model to the five selected counties affected by Hurricane Harvey in 2017 to check the urban-rural recovery differences by types of household return decision models. The agent-based model yields three conclusions. First, the threshold model can successfully substitute the binary logit model. Second, high thresholds and less than 1,000 populations perturb the recovery process, yielding critical transitions during the recovery process. Third, this study checks the urban-rural recovery value differences by different decision model types. This study highlights the importance of the threshold models and population sizes to check the critical transitions and urban-rural differences in the recovery process. ",
        "title": "Finding critical transitions of the post-disaster recovery using the  sensitivity analysis of agent-based models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06676",
        "abstract_url": "http://arxiv.org/abs/2401.06676",
        "authors": [
            {
                "last_name": "John",
                "first_name": "Angela"
            },
            {
                "last_name": "Aidoo",
                "first_name": "Theophilus"
            },
            {
                "last_name": "Behmanush",
                "first_name": "Hamayoon"
            },
            {
                "last_name": "Gunduz",
                "first_name": "Irem B."
            },
            {
                "last_name": "Shrestha",
                "first_name": "Hewan"
            },
            {
                "last_name": "Rahman",
                "first_name": "Maxx Richard"
            },
            {
                "last_name": "Maa\u00df",
                "first_name": "Wolfgang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recommendation systems are ubiquitous, from Spotify playlist suggestions to Amazon product suggestions. Nevertheless, depending on the methodology or the dataset, these systems typically fail to capture user preferences and generate general recommendations. Recent advancements in Large Language Models (LLM) offer promising results for analyzing user queries. However, employing these models to capture user preferences and efficiency remains an open question. In this paper, we propose LLMRS, an LLM-based zero-shot recommender system where we employ pre-trained LLM to encode user reviews into a review score and generate user-tailored recommendations. We experimented with LLMRS on a real-world dataset, the Amazon product reviews, for software purchase use cases. The results show that LLMRS outperforms the ranking-based baseline model while successfully capturing meaningful information from product reviews, thereby providing more reliable recommendations. ",
        "title": "LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for  Software Purchase",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06683",
        "abstract_url": "http://arxiv.org/abs/2401.06683",
        "authors": [
            {
                "last_name": "Cambrin",
                "first_name": "Daniele Rege"
            },
            {
                "last_name": "Cagliero",
                "first_name": "Luca"
            },
            {
                "last_name": "Garza",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL",
            "LG"
        ],
        "abstract": "  Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark. ",
        "title": "DQNC2S: DQN-based Cross-stream Crisis event Summarizer",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06684",
        "abstract_url": "http://arxiv.org/abs/2401.06684",
        "authors": [
            {
                "last_name": "Frommer",
                "first_name": "Andreas"
            },
            {
                "last_name": "Ramirez-Hidalgo",
                "first_name": "Gustavo"
            },
            {
                "last_name": "Schweitzer",
                "first_name": "Marcel"
            },
            {
                "last_name": "Tsolakis",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  While preconditioning is a long-standing concept to accelerate iterative methods for linear systems, generalizations to matrix functions are still in their infancy. We go a further step in this direction, introducing polynomial preconditioning for Krylov subspace methods which approximate the action of the matrix square root and inverse square root on a vector. Preconditioning reduces the subspace size and therefore avoids the storage problem together with -- for non-Hermitian matrices -- the increased computational cost per iteration that arises in the unpreconditioned case. Polynomial preconditioning is an attractive alternative to current restarting or sketching approaches since it is simpler and computationally more efficient. We demonstrate this for several numerical examples. ",
        "title": "Polynomial Preconditioning for the Action of the Matrix Square Root and  Inverse Square Root",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06686",
        "abstract_url": "http://arxiv.org/abs/2401.06686",
        "authors": [
            {
                "last_name": "Pilli",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Heuristics and cognitive biases are an integral part of human decision-making. Automatically detecting a particular cognitive bias could enable intelligent tools to provide better decision-support. Detecting the presence of a cognitive bias currently requires a hand-crafted experiment and human interpretation. Our research aims to explore conversational agents as an effective tool to measure various cognitive biases in different domains. Our proposed conversational agent incorporates a bias measurement mechanism that is informed by the existing experimental designs and various experimental tasks identified in the literature. Our initial experiments to measure framing and loss-aversion biases indicate that the conversational agents can be effectively used to measure the biases. ",
        "title": "Exploring Conversational Agents as an Effective Tool for Measuring  Cognitive Biases in Decision-Making",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06687",
        "abstract_url": "http://arxiv.org/abs/2401.06687",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jacob M."
            },
            {
                "last_name": "Bhattacharya",
                "first_name": "Rohit"
            },
            {
                "last_name": "Keith",
                "first_name": "Katherine A."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-shot classifiers is novel (to our knowledge) and expands the set of text-specific causal methods available to practitioners. ",
        "title": "Proximal Causal Inference With Text Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06688",
        "abstract_url": "http://arxiv.org/abs/2401.06688",
        "authors": [
            {
                "last_name": "Vernikos",
                "first_name": "Giorgos"
            },
            {
                "last_name": "Popescu-Belis",
                "first_name": "Andrei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5-200). Furthermore, we empirically establish that QE-fusion scales linearly with the number of candidates in the pool. QE-fusion proves effective in enhancing LLM-based translation without the need for costly retraining of LLMs. ",
        "title": "Don't Rank, Combine! Combining Machine Translation Hypotheses Using  Quality Estimation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06690",
        "abstract_url": "http://arxiv.org/abs/2401.06690",
        "authors": [
            {
                "last_name": "Y\u00fccel",
                "first_name": "M. Erkin"
            },
            {
                "last_name": "Topalo\u011flu",
                "first_name": "Serkan"
            },
            {
                "last_name": "\u00dcnsalan",
                "first_name": "Cem"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The retail sector presents several open and challenging problems that could benefit from advanced pattern recognition and computer vision techniques. One such critical challenge is planogram compliance control. In this study, we propose a complete embedded system to tackle this issue. Our system consists of four key components as image acquisition and transfer via stand-alone embedded camera module, object detection via computer vision and deep learning methods working on single board computers, planogram compliance control method again working on single board computers, and energy harvesting and power management block to accompany the embedded camera modules. The image acquisition and transfer block is implemented on the ESP-EYE camera module. The object detection block is based on YOLOv5 as the deep learning method and local feature extraction. We implement these methods on Raspberry Pi 4, NVIDIA Jetson Orin Nano, and NVIDIA Jetson AGX Orin as single board computers. The planogram compliance control block utilizes sequence alignment through a modified Needleman-Wunsch algorithm. This block is also working along with the object detection block on the same single board computers. The energy harvesting and power management block consists of solar and RF energy harvesting modules with suitable battery pack for operation. We tested the proposed embedded planogram compliance control system on two different datasets to provide valuable insights on its strengths and weaknesses. The results show that our method achieves F1 scores of 0.997 and 1.0 in object detection and planogram compliance control blocks, respectively. Furthermore, we calculated that the complete embedded system can work in stand-alone form up to two years based on battery. This duration can be further extended with the integration of the proposed solar and RF energy harvesting options. ",
        "title": "Embedded Planogram Compliance Control System",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06692",
        "abstract_url": "http://arxiv.org/abs/2401.06692",
        "authors": [
            {
                "last_name": "Bhatt",
                "first_name": "Gantavya"
            },
            {
                "last_name": "Chen",
                "first_name": "Yifang"
            },
            {
                "last_name": "Das",
                "first_name": "Arnav M."
            },
            {
                "last_name": "Zhang",
                "first_name": "Jifan"
            },
            {
                "last_name": "Truong",
                "first_name": "Sang T."
            },
            {
                "last_name": "Mussmann",
                "first_name": "Stephen"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yinglun"
            },
            {
                "last_name": "Bilmes",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Du",
                "first_name": "Simon S."
            },
            {
                "last_name": "Jamieson",
                "first_name": "Kevin"
            },
            {
                "last_name": "Ash",
                "first_name": "Jordan T."
            },
            {
                "last_name": "Nowak",
                "first_name": "Robert D."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimental design techniques and find that these methods consistently yield significant gains in label efficiency with little computational overhead. On generative tasks, our methods achieve the same generalization performance with only $50\\%$ of annotation cost required by random sampling. ",
        "title": "An Experimental Design Framework for Label-Efficient Supervised  Finetuning of Large Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06697",
        "abstract_url": "http://arxiv.org/abs/2401.06697",
        "authors": [
            {
                "last_name": "Akpinar",
                "first_name": "Emine"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Alzheimer's disease (AD) is the most prevalent neurodegenerative brain disorder, which results in significant cognitive impairments, especially in the elderly population. Cognitive impairments can manifest as a decline in various mental faculties, such as concentration, memory, and other higher-order cognitive abilities. These deficits can significantly impact an individual's capacity to comprehend information, acquire new knowledge, and communicate effectively. One of the affected activities due to cognitive impairments is handwriting. By analyzing different aspects of handwriting, including pressure, velocity, and spatial organization, researchers can detect subtle alterations that might indicate early-stage cognitive impairments, especially AD. Recently, several classical artificial intelligence (AI) approaches have been proposed for detecting AD in elderly individuals through handwriting analysis. However, advanced AI methods require more computational power as the size of the data increases. Additionally, diagnoses can be influenced by factors such as limited relevant classical vector space and correlations between features. Recent studies have shown that using quantum computing technologies in healthcare can not only address these problems but also accelerate complex data analysis and process large datasets more efficiently. In this study, we introduced a variational quantum classifier with fewer circuit elements to facilitate the early diagnosis of AD in elderly individuals based on handwriting data. We employed ZZFeatureMap for encoding features. To classify AD, a parameterized quantum circuit consisting of repeated Ry and Rz rotation gates, as well as CY and CZ two-qubit entangling gates, was designed and implemented. The proposed model achieved an accuracy of 0.75 in classifying AD. ",
        "title": "Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease  Study",
        "date": "2023-09-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06699",
        "abstract_url": "http://arxiv.org/abs/2401.06699",
        "authors": [
            {
                "last_name": "Tomic",
                "first_name": "Slavisa"
            },
            {
                "last_name": "Matos-Carvalho",
                "first_name": "Jo\u00e3o Pedro"
            },
            {
                "last_name": "Beko",
                "first_name": "Marko"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carried out in parallel to optimize all weights in a given layer simultaneously. Furthermore, its running time is deterministic in the sense that one can obtain the exact number of computations necessary to optimize the weights in all network layers (per iteration, in the case of non-injective mapping). Our simulation and empirical results show that the proposed scheme, BPLS, works well and is competitive with existing ones in terms of accuracy, but significantly surpasses them in terms of running time. To summarize, the new method is straightforward to implement, is competitive and computationally more efficient than the existing ones, and is well-tailored for parallel implementation. ",
        "title": "A Closed-form Solution for Weight Optimization in Fully-connected  Feed-forward Neural Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06703",
        "abstract_url": "http://arxiv.org/abs/2401.06703",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Puxuan"
            },
            {
                "last_name": "Mallia",
                "first_name": "Antonio"
            },
            {
                "last_name": "Petri",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems. ",
        "title": "Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06704",
        "abstract_url": "http://arxiv.org/abs/2401.06704",
        "authors": [
            {
                "last_name": "Robert",
                "first_name": "Damien"
            },
            {
                "last_name": "Raguet",
                "first_name": "Hugo"
            },
            {
                "last_name": "Landrieu",
                "first_name": "Loic"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a highly efficient method for panoptic segmentation of large 3D point clouds by redefining this task as a scalable graph clustering problem. This approach can be trained using only local auxiliary tasks, thereby eliminating the resource-intensive instance-matching step during training. Moreover, our formulation can easily be adapted to the superpoint paradigm, further increasing its efficiency. This allows our model to process scenes with millions of points and thousands of objects in a single inference. Our method, called SuperCluster, achieves a new state-of-the-art panoptic segmentation performance for two indoor scanning datasets: $50.1$ PQ ($+7.8$) for S3DIS Area~5, and $58.7$ PQ ($+25.2$) for ScanNetV2. We also set the first state-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and DALES. With only $209$k parameters, our model is over $30$ times smaller than the best-competing method and trains up to $15$ times faster. Our code and pretrained models are available at https://github.com/drprojects/superpoint_transformer. ",
        "title": "Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06706",
        "abstract_url": "http://arxiv.org/abs/2401.06706",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Sen"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            },
            {
                "last_name": "Dai",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding. ",
        "title": "Multi-Candidate Speculative Decoding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06709",
        "abstract_url": "http://arxiv.org/abs/2401.06709",
        "authors": [
            {
                "last_name": "Garg",
                "first_name": "Muskan"
            },
            {
                "last_name": "Sathvik",
                "first_name": "MSVPJ"
            },
            {
                "last_name": "Chadha",
                "first_name": "Amrit"
            },
            {
                "last_name": "Raza",
                "first_name": "Shaina"
            },
            {
                "last_name": "Sohn",
                "first_name": "Sunghwan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The social NLP research community witness a recent surge in the computational advancements of mental health analysis to build responsible AI models for a complex interplay between language use and self-perception. Such responsible AI models aid in quantifying the psychological concepts from user-penned texts on social media. On thinking beyond the low-level (classification) task, we advance the existing binary classification dataset, towards a higher-level task of reliability analysis through the lens of explanations, posing it as one of the safety measures. We annotate the LoST dataset to capture nuanced textual cues that suggest the presence of low self-esteem in the posts of Reddit users. We further state that the NLP models developed for determining the presence of low self-esteem, focus more on three types of textual cues: (i) Trigger: words that triggers mental disturbance, (ii) LoST indicators: text indicators emphasizing low self-esteem, and (iii) Consequences: words describing the consequences of mental disturbance. We implement existing classifiers to examine the attention mechanism in pre-trained language models (PLMs) for a domain-specific psychology-grounded task. Our findings suggest the need of shifting the focus of PLMs from Trigger and Consequences to a more comprehensive explanation, emphasizing LoST indicators while determining low self-esteem in Reddit posts. ",
        "title": "Reliability Analysis of Psychological Concept Extraction and  Classification in User-penned Text",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06710",
        "abstract_url": "http://arxiv.org/abs/2401.06710",
        "authors": [
            {
                "last_name": "Iyengar",
                "first_name": "Garud"
            },
            {
                "last_name": "Singal",
                "first_name": "Raghav"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  The flexibility of choosing the ad action as a function of the consumer state is critical for modern-day marketing campaigns. We study the problem of identifying the optimal sequential personalized interventions that maximize the adoption probability for a new product. We model consumer behavior by a conversion funnel that captures the state of each consumer (e.g., interaction history with the firm) and allows the consumer behavior to vary as a function of both her state and firm's sequential interventions. We show our model captures consumer behavior with very high accuracy (out-of-sample AUC of over 0.95) in a real-world email marketing dataset. However, it results in a very large-scale learning problem, where the firm must learn the state-specific effects of various interventions from consumer interactions. We propose a novel attribution-based decision-making algorithm for this problem that we call model-free approximate Bayesian learning. Our algorithm inherits the interpretability and scalability of Thompson sampling for bandits and maintains an approximate belief over the value of each state-specific intervention. The belief is updated as the algorithm interacts with the consumers. Despite being an approximation to the Bayes update, we prove the asymptotic optimality of our algorithm and analyze its convergence rate. We show that our algorithm significantly outperforms traditional approaches on extensive simulations calibrated to a real-world email marketing dataset. ",
        "title": "Model-Free Approximate Bayesian Learning for Large-Scale Conversion  Funnel Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06712",
        "abstract_url": "http://arxiv.org/abs/2401.06712",
        "authors": [
            {
                "last_name": "Soto",
                "first_name": "Rafael Rivera"
            },
            {
                "last_name": "Koch",
                "first_name": "Kailin"
            },
            {
                "last_name": "Khan",
                "first_name": "Aleem"
            },
            {
                "last_name": "Chen",
                "first_name": "Barry"
            },
            {
                "last_name": "Bishop",
                "first_name": "Marcus"
            },
            {
                "last_name": "Andrews",
                "first_name": "Nicholas"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally different approach not relying on samples from language models of concern at training time. Instead, we propose to leverage representations of writing style estimated from human-authored text. Indeed, we find that features effective at distinguishing among human authors are also effective at distinguishing human from machine authors, including state of the art large language models like Llama 2, ChatGPT, and GPT-4. Furthermore, given a handful of examples composed by each of several specific language models of interest, our approach affords the ability to predict which model generated a given document. ",
        "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06713",
        "abstract_url": "http://arxiv.org/abs/2401.06713",
        "authors": [
            {
                "last_name": "Ferdous",
                "first_name": "S M"
            },
            {
                "last_name": "Neff",
                "first_name": "Reece"
            },
            {
                "last_name": "Peng",
                "first_name": "Bo"
            },
            {
                "last_name": "Shuvo",
                "first_name": "Salman"
            },
            {
                "last_name": "Minutoli",
                "first_name": "Marco"
            },
            {
                "last_name": "Mukherjee",
                "first_name": "Sayak"
            },
            {
                "last_name": "Kowalski",
                "first_name": "Karol"
            },
            {
                "last_name": "Becchi",
                "first_name": "Michela"
            },
            {
                "last_name": "Halappanavar",
                "first_name": "Mahantesh"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  A \\emph{coloring} of a graph is an assignment of colors to vertices such that no two neighboring vertices have the same color. The need for memory-efficient coloring algorithms is motivated by their application in computing clique partitions of graphs arising in quantum computations where the objective is to map a large set of Pauli strings into a compact set of unitaries. We present \\texttt{Picasso}, a randomized memory-efficient iterative parallel graph coloring algorithm with theoretical sublinear space guarantees under practical assumptions. The parameters of our algorithm provide a trade-off between coloring quality and resource consumption. To assist the user, we also propose a machine learning model to predict the coloring algorithm's parameters considering these trade-offs. We provide a sequential and a parallel implementation of the proposed algorithm.   We perform an experimental evaluation on a 64-core AMD CPU equipped with 512 GB of memory and an Nvidia A100 GPU with 40GB of memory. For a small dataset where existing coloring algorithms can be executed within the 512 GB memory budget, we show up to {\\bf 68$\\times$} memory savings. On massive datasets we demonstrate that GPU-accelerated \\pic{} can process inputs with {\\bf 49.5$\\times$} more Pauli strings (vertex set in our graph) and {\\bf 2,478$\\times$} more edges than state-of-the-art parallel approaches. ",
        "title": "\\texttt{Picasso}: Memory-Efficient Graph Coloring Using Palettes With  Applications in Quantum Computing",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06714",
        "abstract_url": "http://arxiv.org/abs/2401.06714",
        "authors": [
            {
                "last_name": "Jaiswal",
                "first_name": "Ragesh"
            },
            {
                "last_name": "Kumar",
                "first_name": "Amit"
            },
            {
                "last_name": "Yadav",
                "first_name": "Jatin"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We consider the capacitated clustering problem in general metric spaces where the goal is to identify $k$ clusters and minimize the sum of the radii of the clusters (we call this the Capacitated-$k$-sumRadii problem). We are interested in fixed-parameter tractable (FPT) approximation algorithms where the running time is of the form $f(k) \\cdot \\text{poly}(n)$, where $f(k)$ can be an exponential function of $k$ and $n$ is the number of points in the input. In the uniform capacity case, Bandyapadhyay et al. recently gave a $4$-approximation algorithm for this problem. Our first result improves this to an FPT $3$-approximation and extends to a constant factor approximation for any $L_p$ norm of the cluster radii. In the general capacities version, Bandyapadhyay et al. gave an FPT $15$-approximation algorithm. We extend their framework to give an FPT $(4 + \\sqrt{13})$-approximation algorithm for this problem. Our framework relies on a novel idea of identifying approximations to optimal clusters by carefully pruning points from an initial candidate set of points. This is in contrast to prior results that rely on guessing suitable points and building balls of appropriate radii around them.   On the hardness front, we show that assuming the Exponential Time Hypothesis, there is a constant $c > 1$ such that any $c$-approximation algorithm for the non-uniform capacity version of this problem requires running time $2^{\\Omega \\left(\\frac{k}{polylog(k)} \\right)}$. ",
        "title": "FPT Approximation for Capacitated Sum of Radii",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06715",
        "abstract_url": "http://arxiv.org/abs/2401.06715",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Xinrui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ming"
            },
            {
                "last_name": "Weir",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Van Durme",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Holzenberger",
                "first_name": "Nils"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Statutory reasoning refers to the application of legislative provisions to a series of case facts described in natural language. We re-frame statutory reasoning as an analogy task, where each instance of the analogy task involves a combination of two instances of statutory reasoning. This increases the dataset size by two orders of magnitude, and introduces an element of interpretability. We show that this task is roughly as difficult to Natural Language Processing models as the original task. Finally, we come back to statutory reasoning, solving it with a combination of a retrieval mechanism and analogy models, and showing some progress on prior comparable work. ",
        "title": "Reframing Tax Law Entailment as Analogical Reasoning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06717",
        "abstract_url": "http://arxiv.org/abs/2401.06717",
        "authors": [
            {
                "last_name": "Costa",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Duarte",
                "first_name": "Pedro"
            },
            {
                "last_name": "Coelho",
                "first_name": "Andr\u00e9"
            },
            {
                "last_name": "Campos",
                "first_name": "Rui"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The 6G paradigm and the massive usage of interconnected wireless devices introduced the need for flexible wireless networks. A promising approach lies in employing Mobile Robotic Platforms (MRPs) to create communications cells on-demand. The challenge consists in positioning the MRPs to improve the wireless connectivity offered. This is exacerbated in millimeter wave (mmWave), Terahertz (THz), and visible light-based networks, which imply the establishment of short-range, Line of Sight (LoS) wireless links to take advantage of the ultra-high bandwidth channels available.   This paper proposes a solution to enable the obstacle-aware, autonomous positioning of MRPs and provide LoS wireless connectivity to communications devices. It consists of 1) a Vision Module that uses video data gathered by the MRP to determine the location of obstacles, wireless devices and users, and 2) a Control Module, which autonomously positions the MRP based on the information provided by the Vision Module. The proposed solution was validated in simulation and through experimental testing, showing that it is able to position an MRP while ensuring LoS wireless links between a mobile communications cell and wireless devices or users. ",
        "title": "Obstacle-Aware Positioning of a Mobile Robotic Platform for 6G Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06721",
        "abstract_url": "http://arxiv.org/abs/2401.06721",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Bowen"
            },
            {
                "last_name": "Iannelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The goal of this article is to study fundamental mechanisms behind so-called indirect and direct data-driven control for unknown systems. Specifically, we consider policy iteration applied to the linear quadratic regulator problem. Two iterative procedures, where data collected from the system are repeatedly used to compute new estimates of the desired optimal controller, are considered. In indirect policy iteration, data are used to obtain an updated model estimate through a recursive identification scheme, which is used in a certainty-equivalent fashion to perform the classic policy iteration update. By casting the concurrent model identification and control design as a feedback interconnection between two algorithmic systems, we provide a closed-loop analysis that shows convergence and robustness properties for arbitrary levels of excitation in the data. In direct policy iteration, data are used to approximate the value function and design the associated controller without requiring the intermediate identification step. After proposing an extension to a recently proposed scheme that overcomes potential identifiability issues, we establish under which conditions this procedure is guaranteed to deliver the optimal controller. Based on these analyses we are able to compare the strengths and limitations of the two approaches, highlighting aspects such as the required samples, convergence properties, and excitation requirement. Simulations are also provided to illustrate the results. ",
        "title": "The Role of Identification in Data-driven Policy Iteration: A System  Theoretic Study",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06723",
        "abstract_url": "http://arxiv.org/abs/2401.06723",
        "authors": [
            {
                "last_name": "Ferreira",
                "first_name": "Diogo"
            },
            {
                "last_name": "Coelho",
                "first_name": "Andr\u00e9"
            },
            {
                "last_name": "Campos",
                "first_name": "Rui"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The emerging 6G paradigm and the proliferation of wireless devices require flexible network infrastructures capable of meeting the increasing Quality of Service (QoS) requirements. Mobile Robotic Platforms (MRPs) acting as mobile communications cells are a promising solution to provide on-demand wireless connectivity in dynamic networking scenarios. However, the energy consumption of MRPs is a challenge that must be considered, in order to maximize the availability of the wireless networks created.   The main contribution of this paper is the experimental evaluation of the energy consumption of an MRP acting as a mobile communications cell. The evaluation considers different actions performed by a real MRP, showing that the energy consumption varies significantly with the type of action performed. The obtained results pave the way for optimizing the MRP movement in dynamic networking scenarios so that the wireless network's availability is maximized while minimizing the MRP's energy consumption. ",
        "title": "Evaluation of the Energy Consumption of a Mobile Robotic Platform for  Sustainable 6G Networks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06725",
        "abstract_url": "http://arxiv.org/abs/2401.06725",
        "authors": [
            {
                "last_name": "Kallaugher",
                "first_name": "John"
            },
            {
                "last_name": "Parekh",
                "first_name": "Ojas"
            },
            {
                "last_name": "Thompson",
                "first_name": "Kevin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yipu"
            },
            {
                "last_name": "Yirka",
                "first_name": "Justin"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  Product states, unentangled tensor products of single qubits, are a ubiquitous ansatz in quantum computation, including for state-of-the-art Hamiltonian approximation algorithms. A natural question is whether we should expect to efficiently solve product state problems on any interesting families of Hamiltonians.   We completely classify the complexity of finding minimum-energy product states for Hamiltonians defined by any fixed set of allowed 2-qubit interactions. Our results follow a line of work classifying the complexity of solving Hamiltonian problems and classical constraint satisfaction problems based on the allowed constraints. We prove that estimating the minimum energy of a product state is in P if and only if all allowed interactions are 1-local, and NP-complete otherwise. Equivalently, any family of non-trivial two-body interactions generates Hamiltonians with NP-complete product-state problems. Our hardness constructions only require coupling strengths of constant magnitude.   A crucial component of our proofs is a collection of hardness results for a new variant of the Vector Max-Cut problem, which should be of independent interest. Our definition involves sums of distances rather than squared distances and allows linear stretches.   A corollary of our classification is a new proof that optimizing product states in the Quantum Max-Cut model (the quantum Heisenberg model) is NP-complete. ",
        "title": "Complexity Classification of Product State Problems for Local  Hamiltonians",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06727",
        "abstract_url": "http://arxiv.org/abs/2401.06727",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Bozhen"
            },
            {
                "last_name": "Zang",
                "first_name": "Zelin"
            },
            {
                "last_name": "Xia",
                "first_name": "Jun"
            },
            {
                "last_name": "Wu",
                "first_name": "Lirong"
            },
            {
                "last_name": "Tan",
                "first_name": "Cheng"
            },
            {
                "last_name": "Li",
                "first_name": "Stan Z."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Representing graph data in a low-dimensional space for subsequent tasks is the purpose of attributed graph embedding. Most existing neural network approaches learn latent representations by minimizing reconstruction errors. Rare work considers the data distribution and the topological structure of latent codes simultaneously, which often results in inferior embeddings in real-world graph data. This paper proposes a novel Deep Manifold (Variational) Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improve the stability and quality of learned representations to tackle the crowding problem. The node-to-node geodesic similarity is preserved between the original and latent space under a pre-defined distribution. The proposed method surpasses state-of-the-art baseline algorithms by a significant margin on different downstream tasks across popular datasets, which validates our solutions. We promise to release the code after acceptance. ",
        "title": "Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06730",
        "abstract_url": "http://arxiv.org/abs/2401.06730",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Kaitlyn"
            },
            {
                "last_name": "Hwang",
                "first_name": "Jena D."
            },
            {
                "last_name": "Ren",
                "first_name": "Xiang"
            },
            {
                "last_name": "Sap",
                "first_name": "Maarten"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward. ",
        "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to  Express Uncertainty",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06738",
        "abstract_url": "http://arxiv.org/abs/2401.06738",
        "authors": [
            {
                "last_name": "Dang",
                "first_name": "Anh"
            },
            {
                "last_name": "Babanezhad",
                "first_name": "Reza"
            },
            {
                "last_name": "Vaswani",
                "first_name": "Sharan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\\left(\\exp(-\\frac{T}{\\sqrt{\\kappa}}) + \\sigma \\right)$ convergence rate, where $T$ is the number of iterations and $\\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\\left(\\exp\\left(-\\frac{T}{\\sqrt{\\kappa}} \\right) + \\frac{\\sigma}{T}\\right)$ rate. For general strongly-convex functions, we use the averaging interpretation of SHB along with exponential step-sizes to prove an $O\\left(\\exp\\left(-\\frac{T}{\\kappa} \\right) + \\frac{\\sigma^2}{T} \\right)$ convergence to the minimizer in a noise-adaptive manner. Finally, we empirically demonstrate the effectiveness of the proposed algorithms. ",
        "title": "Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06740",
        "abstract_url": "http://arxiv.org/abs/2401.06740",
        "authors": [
            {
                "last_name": "Georgoulis",
                "first_name": "Emmanuil H."
            },
            {
                "last_name": "Papapantoleon",
                "first_name": "Antonis"
            },
            {
                "last_name": "Smaragdakis",
                "first_name": "Costas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model. ",
        "title": "A deep implicit-explicit minimizing movement method for option pricing  in jump-diffusion models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06742",
        "abstract_url": "http://arxiv.org/abs/2401.06742",
        "authors": [
            {
                "last_name": "DeLucia",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Zhao",
                "first_name": "Mengjie"
            },
            {
                "last_name": "Maeda",
                "first_name": "Yoshinori"
            },
            {
                "last_name": "Yoda",
                "first_name": "Makoto"
            },
            {
                "last_name": "Yamada",
                "first_name": "Keiichi"
            },
            {
                "last_name": "Wakaki",
                "first_name": "Hiromi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the \"real\" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new setting. We draw inspiration from the literature of dialog natural language inference (NLI), and devise NLI-reranking methods to extract structured persona information from dialogue. Compared to existing persona extraction models, our method returns higher-quality extracted persona and requires less human annotation. ",
        "title": "Using Natural Language Inference to Improve Persona Extraction from  Dialogue in a New Domain",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06744",
        "abstract_url": "http://arxiv.org/abs/2401.06744",
        "authors": [
            {
                "last_name": "K\u00e4mper",
                "first_name": "Niklas"
            },
            {
                "last_name": "Chizhov",
                "first_name": "Vassillen"
            },
            {
                "last_name": "Weickert",
                "first_name": "Joachim"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years inpainting-based compression methods have been shown to be a viable alternative to classical codecs such as JPEG and JPEG2000. Unlike transform-based codecs, which store coefficients in the transform domain, inpainting-based approaches store a small subset of the original image pixels and reconstruct the image from those by using a suitable inpainting operator. A good candidate for such an inpainting operator is homogeneous diffusion inpainting, as it is simple, theoretically well-motivated, and can achieve good reconstruction quality for optimized data. However, a major challenge has been to design fast solvers for homogeneous diffusion inpainting that scale to 4K image resolution ($3840 \\times 2160$ pixels) and are real-time capable. We overcome this with a careful adaptation and fusion of two of the most efficient concept from numerical analysis: multigrid and domain decomposition. Our domain decomposition algorithm efficiently utilizes GPU parallelism by solving inpainting problems on small overlapping blocks. Unlike simple block decomposition strategies such as the ones in JPEG, our approach yields block artifact-free reconstructions. Furthermore, embedding domain decomposition in a full multigrid scheme provides global interactions and allows us to achieve optimal convergence by reducing both low- and high-frequency errors at the same rate. We are able to achieve 4K color image reconstruction at more than $60$ frames per second even from very sparse data - something which has been previously unfeasible. ",
        "title": "Efficient Parallel Algorithms for Inpainting-Based Representations of 4K  Images -- Part I: Homogeneous Diffusion Inpainting",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06747",
        "abstract_url": "http://arxiv.org/abs/2401.06747",
        "authors": [
            {
                "last_name": "K\u00e4mper",
                "first_name": "Niklas"
            },
            {
                "last_name": "Chizhov",
                "first_name": "Vassillen"
            },
            {
                "last_name": "Weickert",
                "first_name": "Joachim"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Homogeneous diffusion inpainting can reconstruct missing image areas with high quality from a sparse subset of known pixels, provided that their location as well as their gray or color values are well optimized. This property is exploited in inpainting-based image compression, which is a promising alternative to classical transform-based codecs such as JPEG and JPEG2000. However, optimizing the inpainting data is a challenging task. Current approaches are either quite slow or do not produce high quality results. As a remedy we propose fast spatial and tonal optimization algorithms for homogeneous diffusion inpainting that efficiently utilize GPU parallelism, with a careful adaptation of some of the most successful numerical concepts. We propose a densification strategy using ideas from error-map dithering combined with a Delaunay triangulation for the spatial optimization. For the tonal optimization we design a domain decomposition solver that solves the corresponding normal equations in a matrix-free fashion and supplement it with a Voronoi-based initialization strategy. With our proposed methods we are able to generate high quality inpainting masks for homogeneous diffusion and optimized tonal values in a runtime that outperforms prior state-of-the-art by a wide margin. ",
        "title": "Efficient Parallel Algorithms for Inpainting-Based Representations of 4K  Images -- Part II: Spatial and Tonal Data Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06748",
        "abstract_url": "http://arxiv.org/abs/2401.06748",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qingsong"
            },
            {
                "last_name": "Ma",
                "first_name": "Guanquan"
            },
            {
                "last_name": "Sridharamurthy",
                "first_name": "Raghavendra"
            },
            {
                "last_name": "Wang",
                "first_name": "Bei"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  A Reeb graph is a graphical representation of a scalar function $f: X \\to \\mathbb{R}$ on a topological space $X$ that encodes the topology of the level sets. A Reeb space is a generalization of the Reeb graph to a multivariate function $f: X \\to \\mathbb{R}^d$. In this paper, we propose novel constructions of Reeb graphs and Reeb spaces that incorporate the use of a measure. Specifically, we introduce measure theoretic Reeb graphs and Reeb spaces when the domain or the range is modeled as a metric measure space (i.e.,~a metric space equipped with a measure). Our main goal is to enhance the robustness of the Reeb graph and Reeb space in representing the topological features of a scalar field while accounting for the distribution of the measure. We first prove the stability of our measure theoretic constructions with respect to the interleaving distance. We then prove their stability with respect to the measure, defined using the distance to a measure or the kernel distance to a measure, respectively. ",
        "title": "Measure Theoretic Reeb Graphs and Reeb Spaces",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06749",
        "abstract_url": "http://arxiv.org/abs/2401.06749",
        "authors": [
            {
                "last_name": "Garcia-Archilla",
                "first_name": "Bosco"
            },
            {
                "last_name": "Li",
                "first_name": "Xuejian"
            },
            {
                "last_name": "Novo",
                "first_name": "Julia"
            },
            {
                "last_name": "Rebholz",
                "first_name": "Leo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider nonlinear solvers for the incompressible, steady (or at a fixed time step for unsteady) Navier-Stokes equations in the setting where partial measurement data of the solution is available. The measurement data is incorporated/assimilated into the solution through a nudging term addition to the the Picard iteration that penalized the difference between the coarse mesh interpolants of the true solution and solver solution, analogous to how continuous data assimilation (CDA) is implemented for time dependent PDEs. This was considered in the paper [Li et al. {\\it CMAME} 2023], and we extend the methodology by improving the analysis to be in the $L^2$ norm instead of a weighted $H^1$ norm where the weight depended on the coarse mesh width, and to the case of noisy measurement data. For noisy measurement data, we prove that the CDA-Picard method is stable and convergent, up to the size of the noise. Numerical tests illustrate the results, and show that a very good strategy when using noisy data is to use CDA-Picard to generate an initial guess for the classical Newton iteration. ",
        "title": "Enhancing nonlinear solvers for the Navier-Stokes equations with  continuous (noisy) data assimilation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06751",
        "abstract_url": "http://arxiv.org/abs/2401.06751",
        "authors": [
            {
                "last_name": "Hase",
                "first_name": "Peter"
            },
            {
                "last_name": "Bansal",
                "first_name": "Mohit"
            },
            {
                "last_name": "Clark",
                "first_name": "Peter"
            },
            {
                "last_name": "Wiegreffe",
                "first_name": "Sarah"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as \"oracle\" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied, suggesting the scalable oversight problem may be easier than previously thought. Our code is available at https://github.com/allenai/easy-to-hard-generalization ",
        "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06752",
        "abstract_url": "http://arxiv.org/abs/2401.06752",
        "authors": [
            {
                "last_name": "Zamir",
                "first_name": "Muhammad Tayyab"
            },
            {
                "last_name": "Ayub",
                "first_name": "Muhammad Asif"
            },
            {
                "last_name": "Gul",
                "first_name": "Asma"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Nasir"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Kashif"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent years, the increasing use of Artificial Intelligence based text generation tools has posed new challenges in document provenance, authentication, and authorship detection. However, advancements in stylometry have provided opportunities for automatic authorship and author change detection in multi-authored documents using style analysis techniques. Style analysis can serve as a primary step toward document provenance and authentication through authorship detection. This paper investigates three key tasks of style analysis: (i) classification of single and multi-authored documents, (ii) single change detection, which involves identifying the point where the author switches, and (iii) multiple author-switching detection in multi-authored documents. We formulate all three tasks as classification problems and propose a merit-based fusion framework that integrates several state-of-the-art natural language processing (NLP) algorithms and weight optimization techniques. We also explore the potential of special characters, which are typically removed during pre-processing in NLP applications, on the performance of the proposed methods for these tasks by conducting extensive experiments on both cleaned and raw datasets. Experimental results demonstrate significant improvements over existing solutions for all three tasks on a benchmark dataset. ",
        "title": "Stylometry Analysis of Multi-authored Documents for Authorship and  Author Style Change Detection",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06755",
        "abstract_url": "http://arxiv.org/abs/2401.06755",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Boyang"
            },
            {
                "last_name": "Heaney",
                "first_name": "Claire E."
            },
            {
                "last_name": "Gomes",
                "first_name": "Jefferson L. M. A."
            },
            {
                "last_name": "Matar",
                "first_name": "Omar K."
            },
            {
                "last_name": "Pain",
                "first_name": "Christopher C."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper solves the multiphase flow equations with interface capturing using the AI4PDEs approach (Artificial Intelligence for Partial Differential Equations). The solver within AI4PDEs uses tools from machine learning (ML) libraries to solve (exactly) partial differential equations (PDEs) that have been discretised using numerical methods. Convolutional layers can be used to express the discretisations as a neural network, whose weights are determined by the numerical method, rather than by training. To solve the system, a multigrid solver is implemented through a neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier-Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov-Galerkin for accuracy and designed with AI4PDEs in mind. High-order finite-element based schemes are chosen to model a collapsing water column and a rising bubble. Results compare well with experimental data and other numerical results from the literature, demonstrating that, for the first time, finite element discretisations of multiphase flows can be solved using the neural network solver from the AI4PDEs approach. A benefit of expressing numerical discretisations as neural networks is that the code can run, without modification, on CPUs, GPUs or the latest accelerators designed especially to run AI codes. ",
        "title": "Solving the Discretised Multiphase Flow Equations with Interface  Capturing on Structured Grids Using Machine Learning Libraries",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06757",
        "abstract_url": "http://arxiv.org/abs/2401.06757",
        "authors": [
            {
                "last_name": "Riaz",
                "first_name": "Muhammad Naveed"
            },
            {
                "last_name": "Wielgosz",
                "first_name": "Maciej"
            },
            {
                "last_name": "Romera",
                "first_name": "Abel Garcia"
            },
            {
                "last_name": "Lopez",
                "first_name": "Antonio M."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to predict crossing intentions. ",
        "title": "Synthetic Data Generation Framework, Dataset, and Efficient Deep Model  for Pedestrian Intention Prediction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06760",
        "abstract_url": "http://arxiv.org/abs/2401.06760",
        "authors": [
            {
                "last_name": "Kocmi",
                "first_name": "Tom"
            },
            {
                "last_name": "Zouhar",
                "first_name": "Vil\u00e9m"
            },
            {
                "last_name": "Federmann",
                "first_name": "Christian"
            },
            {
                "last_name": "Post",
                "first_name": "Matt"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the \"dynamic range\" of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask what point difference X in metric Y is required between two systems for humans to notice? We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size. Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness. ",
        "title": "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06761",
        "abstract_url": "http://arxiv.org/abs/2401.06761",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mingdao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Aohan"
            },
            {
                "last_name": "Wang",
                "first_name": "Bowen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peng"
            },
            {
                "last_name": "Tang",
                "first_name": "Jie"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuxiao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks. ",
        "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06762",
        "abstract_url": "http://arxiv.org/abs/2401.06762",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Caleb"
            },
            {
                "last_name": "Corley",
                "first_name": "Isaac"
            },
            {
                "last_name": "Ortiz",
                "first_name": "Anthony"
            },
            {
                "last_name": "Dodhia",
                "first_name": "Rahul"
            },
            {
                "last_name": "Ferres",
                "first_name": "Juan M. Lavista"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to segment roads from background in aerial imagery achieves an 84% recall on unoccluded roads, but just 63.5% recall on roads covered by tree canopy despite being trained to model both the same way. We further analyze how the performance of models changes as the relevant context for a decision (unoccluded roads in our case) varies in distance. We release the code to reproduce our experiments and dataset of imagery and masks to encourage future research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC. ",
        "title": "Seeing the roads through the trees: A benchmark for modeling spatial  dependencies with aerial imagery",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06763",
        "abstract_url": "http://arxiv.org/abs/2401.06763",
        "authors": [
            {
                "last_name": "Zaman",
                "first_name": "Md Mahabub Uz"
            },
            {
                "last_name": "Tao",
                "first_name": "Liangde"
            },
            {
                "last_name": "Maldonado",
                "first_name": "Mark"
            },
            {
                "last_name": "Liu",
                "first_name": "Chang"
            },
            {
                "last_name": "Sunny",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Xu",
                "first_name": "Shouhuai"
            },
            {
                "last_name": "Chen",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CC"
        ],
        "abstract": "  Honeypot is an important cyber defense technique that can expose attackers new attacks. However, the effectiveness of honeypots has not been systematically investigated, beyond the rule of thumb that their effectiveness depends on how they are deployed. In this paper, we initiate a systematic study on characterizing the cybersecurity effectiveness of a new paradigm of deploying honeypots: blending honeypot computers (or IP addresses) into production computers. This leads to the following Honeypot Deployment (HD) problem, How should the defender blend honeypot computers into production computers to maximize the utility in forcing attackers to expose their new attacks while minimizing the loss to the defender in terms of the digital assets stored in the compromised production computers? We formalize HD as a combinatorial optimization problem, prove its NP hardness, provide a near optimal algorithm (i.e., polynomial time approximation scheme). We also conduct simulations to show the impact of attacker capabilities. ",
        "title": "Optimally Blending Honeypots into Production Networks: Hardness and  Algorithms",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06765",
        "abstract_url": "http://arxiv.org/abs/2401.06765",
        "authors": [
            {
                "last_name": "Yaraghi",
                "first_name": "Ahmadreza Saboor"
            },
            {
                "last_name": "Holden",
                "first_name": "Darren"
            },
            {
                "last_name": "Kahani",
                "first_name": "Nafiseh"
            },
            {
                "last_name": "Briand",
                "first_name": "Lionel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. The need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. Further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers' time. To address this challenge, we present TaRGet (Test Repair GEneraTor), a novel approach leveraging pre-trained code language models for automated test case repair. TaRGet treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. To evaluate our approach, we introduce TaRBench, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. Our results demonstrate TaRGet's effectiveness, achieving a 66.1% exact match accuracy. Furthermore, our study examines the effectiveness of TaRGet across different test repair scenarios. We provide a practical guide to predict situations where the generated test repairs might be less reliable. We also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects. ",
        "title": "Automated Test Case Repair Using Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06766",
        "abstract_url": "http://arxiv.org/abs/2401.06766",
        "authors": [
            {
                "last_name": "Voronov",
                "first_name": "Anton"
            },
            {
                "last_name": "Wolf",
                "first_name": "Lena"
            },
            {
                "last_name": "Ryabinin",
                "first_name": "Max"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates. ",
        "title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning  Improvements",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06769",
        "abstract_url": "http://arxiv.org/abs/2401.06769",
        "authors": [
            {
                "last_name": "Wastl",
                "first_name": "Michelle"
            },
            {
                "last_name": "Vamvas",
                "first_name": "Jannis"
            },
            {
                "last_name": "Sennrich",
                "first_name": "Rico"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82-96% for NMT-produced translations, and 60-81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection ",
        "title": "Machine Translation Models are Zero-Shot Detectors of Translation  Direction",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06771",
        "abstract_url": "http://arxiv.org/abs/2401.06771",
        "authors": [
            {
                "last_name": "Chadi",
                "first_name": "Mohamed-Amine"
            },
            {
                "last_name": "Mousannif",
                "first_name": "Hajar"
            },
            {
                "last_name": "Aamouche",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, deep learning has demonstrated promising results in de novo drug design. However, the proposed techniques still lack an efficient exploration of the large chemical space. Most of these methods explore a small fragment of the chemical space of known drugs, if the desired molecules were not found, the process ends. In this work, we introduce a curiosity-driven method to force the model to navigate many parts of the chemical space, therefore, achieving higher desirability and diversity as well. At first, we train a recurrent neural network-based general molecular generator (G), then we fine-tune G to maximize curiosity and desirability. We define curiosity as the Tanimoto similarity between two generated molecules, a first molecule generated by G, and a second one generated by a copy of G (Gcopy). We only backpropagate the loss through G while keeping Gcopy unchanged. We benchmarked our approach against two desirable chemical properties related to drug-likeness and showed that the discovered chemical space can be significantly expanded, thus, discovering a higher number of desirable molecules with more diversity and potentially easier to synthesize. All Code and data used in this paper are available at https://github.com/amine179/Curiosity-RL-for-Drug-Design. ",
        "title": "Curiosity as a Self-Supervised Method to Improve Exploration in De novo  Drug Design",
        "date": "2023-09-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06772",
        "abstract_url": "http://arxiv.org/abs/2401.06772",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Sijia"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenwen"
            },
            {
                "last_name": "Li",
                "first_name": "Qisong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we introduce a novel method named \"graph-to-segment\" for question answering over knowledge graphs, focusing on understanding question utterances. This method centers on semantic parsing, a key approach for interpreting these utterances. Our primary challenge lies in comprehending implicit entities, relationships, and complex constraints like time, ordinality, and aggregation within questions, contextualized by the knowledge graph. Our framework employs a combination of rule-based and neural-based techniques to parse and construct highly accurate and comprehensive semantic segment sequences. These sequences form semantic query graphs, effectively representing question utterances. We approach question semantic parsing as a sequence generation task, utilizing an encoder-decoder neural network to transform natural language questions into semantic segments. Moreover, to enhance the parsing of implicit entities and relations, we incorporate a graph neural network that leverages the context of the knowledge graph to better understand question representations. Our experimental evaluations on two datasets demonstrate the effectiveness and superior performance of our model in semantic parsing for question answering. ",
        "title": "Semantic Segment Based Semantic Parsing for Question Answering over  Knowledge Graphs",
        "date": "2023-12-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06774",
        "abstract_url": "http://arxiv.org/abs/2401.06774",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Rumeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xun"
            },
            {
                "last_name": "Yu",
                "first_name": "Hong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is underexplored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge, which guides LLMs to generate synthetic data following two different directions: \"data-to-label\", which labels sentences from a public EHR collection with AD-related signs and symptoms; and \"label-to-data\", which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method. We find that using the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality. ",
        "title": "Two Directions for Clinical Data Generation with Large Language Models:  Data-to-Label and Label-to-Data",
        "date": "2023-12-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06775",
        "abstract_url": "http://arxiv.org/abs/2401.06775",
        "authors": [
            {
                "last_name": "Nazi",
                "first_name": "Zabir Al"
            },
            {
                "last_name": "Peng",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The deployment of large language models (LLMs) within the healthcare sector has sparked both enthusiasm and apprehension. These models exhibit the remarkable capability to provide proficient responses to free-text queries, demonstrating a nuanced understanding of professional medical knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for healthcare applications, elucidating the trajectory of their development, starting from traditional Pretrained Language Models (PLMs) to the present state of LLMs in healthcare sector. First, we explore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare applications, particularly focusing on clinical language understanding tasks. These tasks encompass a wide spectrum, ranging from named entity recognition and relation extraction to natural language inference, multi-modal medical applications, document classification, and question-answering. Additionally, we conduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain, while also assessing the utilization of various open-source LLMs and highlighting their significance in healthcare applications. Furthermore, we present the essential performance metrics employed to evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations. Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings. This review provides a comprehensive exploration of the current landscape of LLMs in healthcare, addressing their role in transforming medical applications and the areas that warrant further research and development. ",
        "title": "Large language models in healthcare and medical domain: A review",
        "date": "2023-12-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06777",
        "abstract_url": "http://arxiv.org/abs/2401.06777",
        "authors": [
            {
                "last_name": "Vo",
                "first_name": "Jamie"
            },
            {
                "last_name": "Sharif",
                "first_name": "Naeha"
            },
            {
                "last_name": "Hassan",
                "first_name": "Ghulam Mubashar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The early detection of Alzheimer's Disease is imperative to ensure early treatment and improve patient outcomes. There has consequently been extenstive research into detecting AD and its intermediate phase, mild cognitive impairment (MCI). However, there is very small literature in predicting the conversion to AD and MCI from normal cognitive condition. Recently, multiple studies have applied convolutional neural networks (CNN) which integrate Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) to classify MCI and AD. However, in these works, the fusion of MRI and PET features are simply achieved through concatenation, resulting in a lack of cross-modal interactions. In this paper, we propose a novel multimodal neuroimaging attention-based CNN architecture, MNA-net, to predict whether cognitively normal (CN) individuals will develop MCI or AD within a period of 10 years. To address the lack of interactions across neuroimaging modalities seen in previous works, MNA-net utilises attention mechanisms to form shared representations of the MRI and PET images. The proposed MNA-net is tested in OASIS-3 dataset and is able to predict CN individuals who converted to MCI or AD with an accuracy of 83%, true negative rate of 80%, and true positive rate of 86%. The new state of the art results improved by 5% and 10% for accuracy and true negative rate by the use of attention mechanism. These results demonstrate the potential of the proposed model to predict cognitive impairment and attention based mechanisms in the fusion of different neuroimaging modalities to improve the prediction of cognitive decline. ",
        "title": "Multimodal Neuroimaging Attention-Based architecture for Cognitive  Decline Prediction",
        "date": "2023-12-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06779",
        "abstract_url": "http://arxiv.org/abs/2401.06779",
        "authors": [
            {
                "last_name": "El-Awady",
                "first_name": "Khalid"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We investigate the construction of generative models capable of encoding physical constraints that can be hard to express explicitly. For the problem of inverse material design, where one seeks to design a material with a prescribed set of properties, a significant challenge is ensuring synthetic viability of a proposed new material. We encode an implicit dataset relationships, namely that certain materials can be decomposed into other ones in the dataset, and present a VAE model capable of preserving this property in the latent space and generating new samples with the same. This is particularly useful in sequential inverse material design, an emergent research area that seeks to design a material with specific properties by sequentially adding (or removing) elements using policies trained through deep reinforcement learning. ",
        "title": "VAE for Modified 1-Hot Generative Materials Modeling, A Step Towards  Inverse Material Design",
        "date": "2023-12-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06780",
        "abstract_url": "http://arxiv.org/abs/2401.06780",
        "authors": [
            {
                "last_name": "Shen",
                "first_name": "Xiongri"
            },
            {
                "last_name": "Song",
                "first_name": "Zhenxi"
            },
            {
                "last_name": "Li",
                "first_name": "Linling"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            },
            {
                "last_name": "Liu",
                "first_name": "Lingyan Liang Honghai"
            },
            {
                "last_name": "Deng",
                "first_name": "Demao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhiguo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Early diagnosis of mild cognitive impairment (MCI) and subjective cognitive decline (SCD) utilizing multi-modal magnetic resonance imaging (MRI) is a pivotal area of research. While various regional and connectivity features from functional MRI (fMRI) and diffusion tensor imaging (DTI) have been employed to develop diagnosis models, most studies integrate these features without adequately addressing their alignment and interactions. This limits the potential to fully exploit the synergistic contributions of combined features and modalities. To solve this gap, our study introduces a novel Hierarchical Alignments and Hierarchical Interactions (HA-HI) method for MCI and SCD classification, leveraging the combined strengths of fMRI and DTI. HA-HI efficiently learns significant MCI- or SCD- related regional and connectivity features by aligning various feature types and hierarchically maximizing their interactions. Furthermore, to enhance the interpretability of our approach, we have developed the Synergistic Activation Map (SAM) technique, revealing the critical brain regions and connections that are indicative of MCI/SCD. Comprehensive evaluations on the ADNI dataset and our self-collected data demonstrate that HA-HI outperforms other existing methods in diagnosing MCI and SCD, making it a potentially vital and interpretable tool for early detection. The implementation of this method is publicly accessible at https://github.com/ICI-BCI/Dual-MRI-HA-HI.git. ",
        "title": "HA-HI: Synergising fMRI and DTI through Hierarchical Alignments and  Hierarchical Interactions for Mild Cognitive Impairment Diagnosis",
        "date": "2024-01-02",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06781",
        "abstract_url": "http://arxiv.org/abs/2401.06781",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Cao",
                "first_name": "Yanbo"
            },
            {
                "last_name": "Wen",
                "first_name": "Yinlong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yanru"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Poker, also known as Texas Hold'em, has always been a typical research target within imperfect information games (IIGs). IIGs have long served as a measure of artificial intelligence (AI) development. Representative prior works, such as DeepStack and Libratus heavily rely on counterfactual regret minimization (CFR) to tackle heads-up no-limit Poker. However, it is challenging for subsequent researchers to learn CFR from previous models and apply it to other real-world applications due to the expensive computational cost of CFR iterations. Additionally, CFR is difficult to apply to multi-player games due to the exponential growth of the game tree size. In this work, we introduce PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number of players and gaining high win rates, established on a lightweight large language model (LLM). PokerGPT only requires simple textual information of Poker games for generating decision-making advice, thus guaranteeing the convenient interaction between AI and humans. We mainly transform a set of textual records acquired from real games into prompts, and use them to fine-tune a lightweight pre-trained LLM using reinforcement learning human feedback technique. To improve fine-tuning performance, we conduct prompt engineering on raw data, including filtering useful information, selecting behaviors of players with high win rates, and further processing them into textual instruction using multiple prompt engineering techniques. Through the experiments, we demonstrate that PokerGPT outperforms previous approaches in terms of win rate, model size, training time, and response speed, indicating the great potential of LLMs in solving IIGs. ",
        "title": "PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas  Hold'em via Large Language Model",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06782",
        "abstract_url": "http://arxiv.org/abs/2401.06782",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Liqiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Bo"
            },
            {
                "last_name": "Lin",
                "first_name": "Qunwei"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Che",
                "first_name": "Chang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the realm of patent document analysis, assessing semantic similarity between phrases presents a significant challenge, notably amplifying the inherent complexities of Cooperative Patent Classification (CPC) research. Firstly, this study addresses these challenges, recognizing early CPC work while acknowledging past struggles with language barriers and document intricacy. Secondly, it underscores the persisting difficulties of CPC research.   To overcome these challenges and bolster the CPC system, This paper presents two key innovations. Firstly, it introduces an ensemble approach that incorporates four BERT-related models, enhancing semantic similarity accuracy through weighted averaging. Secondly, a novel text preprocessing method tailored for patent documents is introduced, featuring a distinctive input structure with token scoring that aids in capturing semantic relationships during CPC context training, utilizing BCELoss. Our experimental findings conclusively establish the effectiveness of both our Ensemble Model and novel text processing strategies when deployed on the U.S. Patent Phrase to Phrase Matching dataset. ",
        "title": "Semantic Similarity Matching for Patent Documents Using Ensemble  BERT-related Model and Novel Text Processing Method",
        "date": "2024-01-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06783",
        "abstract_url": "http://arxiv.org/abs/2401.06783",
        "authors": [
            {
                "last_name": "Bhoi",
                "first_name": "Sudhanshu"
            },
            {
                "last_name": "Markhedkar",
                "first_name": "Swapnil"
            },
            {
                "last_name": "Phadke",
                "first_name": "Shruti"
            },
            {
                "last_name": "Agrawal",
                "first_name": "Prashant"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  Social media accounts post increasingly similar content, creating a chaotic experience across platforms, which makes accessing desired information difficult. These posts can be organized by categorizing and grouping duplicates across social handles and accounts. There can be more than one duplicate of a post, however, a conventional Siamese neural network only considers a pair of inputs for duplicate text detection. In this paper, we first propose a multiple-input Siamese network, MultiSiam. This condensed network is then used to propose another model, SMCD (Social Media Classification and Duplication Model) to perform both duplicate text grouping and categorization. The MultiSiam network, just like the Siamese, can be used in multiple applications by changing the sub-network appropriately. ",
        "title": "MultiSiam: A Multiple Input Siamese Network For Social Media Text  Classification And Duplicate Text Detection",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06785",
        "abstract_url": "http://arxiv.org/abs/2401.06785",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongyi"
            },
            {
                "last_name": "Yao",
                "first_name": "Yuanshun"
            },
            {
                "last_name": "Shen",
                "first_name": "Wei"
            },
            {
                "last_name": "Wei",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiaoying"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaoran"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aligning large language models (LLMs) with human values is a vital task for LLM practitioners. Current alignment techniques have several limitations: (1) requiring a large amount of annotated data; (2) demanding heavy human involvement; (3) lacking a systematic mechanism to continuously improve. In this work, we study aligning LLMs to a new domain with limited samples (e.g. < 100). We propose an algorithm that can self-align LLMs iteratively without active human involvement. Unlike existing works, our algorithm relies on neither human-crafted instructions nor labeled rewards, significantly reducing human involvement. In addition, our algorithm can self-improve the alignment continuously. The key idea is to first retrieve high-quality samples related to the target domain and use them as In-context Learning examples to generate more samples. Then we use the self-generated samples to finetune the LLM iteratively. We show that our method can unlock the LLMs' self-generalization ability to perform alignment with near-zero human supervision. We test our algorithm on three benchmarks in safety, truthfulness, and instruction-following, and show good performance in alignment, domain adaptability, and scalability. ",
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06786",
        "abstract_url": "http://arxiv.org/abs/2401.06786",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Yifei"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuning"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xumiao"
            },
            {
                "last_name": "Lin",
                "first_name": "Xianshang"
            },
            {
                "last_name": "Hu",
                "first_name": "Pan"
            },
            {
                "last_name": "Ma",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Lu",
                "first_name": "Songwu"
            },
            {
                "last_name": "Du",
                "first_name": "Wan"
            },
            {
                "last_name": "Mao",
                "first_name": "Zhuoqing"
            },
            {
                "last_name": "Zhai",
                "first_name": "Ennan"
            },
            {
                "last_name": "Cai",
                "first_name": "Dennis"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Among the thriving ecosystem of cloud computing and the proliferation of Large Language Model (LLM)-based code generation tools, there is a lack of benchmarking for code generation in cloud-native applications. In response to this need, we present CloudEval-YAML, a practical benchmark for cloud configuration generation. CloudEval-YAML tackles the diversity challenge by focusing on YAML, the de facto standard of numerous cloud-native tools. We develop the CloudEval-YAML benchmark with practicality in mind: the dataset consists of hand-written problems with unit tests targeting practical scenarios. We further enhanced the dataset to meet practical needs by rephrasing questions in a concise, abbreviated, and bilingual manner. The dataset consists of 1011 problems that take more than 1200 human hours to complete. To improve practicality during evaluation, we build a scalable evaluation platform for CloudEval-YAML that achieves a 20 times speedup over a single machine. To the best of our knowledge, the CloudEval-YAML dataset is the first hand-written dataset targeting cloud-native applications. We present an in-depth evaluation of 12 LLMs, leading to a deeper understanding of the problems and LLMs, as well as effective methods to improve task performance and reduce cost. ",
        "title": "CloudEval-YAML: A Practical Benchmark for Cloud Configuration Generation",
        "date": "2023-11-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06787",
        "abstract_url": "http://arxiv.org/abs/2401.06787",
        "authors": [
            {
                "last_name": "Nath",
                "first_name": "Sristy Shidul"
            },
            {
                "last_name": "Karim",
                "first_name": "Razuan"
            },
            {
                "last_name": "Miraz",
                "first_name": "Mahdi H."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  The Internet is currently the largest platform for global communication including expressions of opinions, reviews, contents, images, videos and so forth. Moreover, social media has now become a very broad and highly engaging platform due to its immense popularity and swift adoption trend. Increased social networking, however, also has detrimental impacts on the society leading to a range of unwanted phenomena, such as online assault, intimidation, digital bullying, criminality and trolling. Hence, cyberbullying has become a pervasive and worrying problem that poses considerable psychological and emotional harm to the people, particularly amongst the teens and the young adults. In order to lessen its negative effects and provide victims with prompt support, a great deal of research to identify cyberbullying instances at various online platforms is emerging. In comparison to other languages, Bangla (also known as Bengali) has fewer research studies in this domain. This study demonstrates a deep learning strategy for identifying cyberbullying in Bengali, using a dataset of 12282 versatile comments from multiple social media sites. In this study, a two-layer bidirectional long short-term memory (Bi-LSTM) model has been built to identify cyberbullying, using a variety of optimisers as well as 5-fold cross validation. To evaluate the functionality and efficacy of the proposed system, rigorous assessment and validation procedures have been employed throughout the project. The results of this study reveals that the proposed model's accuracy, using momentum-based stochastic gradient descent (SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a F1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31% in 5-fold cross validation. ",
        "title": "Deep Learning Based Cyberbullying Detection in Bangla Language",
        "date": "2024-01-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06788",
        "abstract_url": "http://arxiv.org/abs/2401.06788",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "He"
            },
            {
                "last_name": "Guo",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Pan"
            },
            {
                "last_name": "Xie",
                "first_name": "Lei"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual Speech Recognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks of Single-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multi-scale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, comprising a ResNet3D visual frontend, an E-Branchformer encoder, and a Transformer decoder. Experiments show that our system achieves 34.76% CER for the Single-Speaker Task and 41.06% CER for the Multi-Speaker Task after multi-system fusion, ranking first place in all three tracks we participate. ",
        "title": "The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in  CNVSRC 2023",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06789",
        "abstract_url": "http://arxiv.org/abs/2401.06789",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Tingting"
            },
            {
                "last_name": "Tian",
                "first_name": "Shubo"
            },
            {
                "last_name": "Daly",
                "first_name": "Jordan"
            },
            {
                "last_name": "Geiger",
                "first_name": "Melissa"
            },
            {
                "last_name": "Jia",
                "first_name": "Minna"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jinfeng"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL",
            "LG"
        ],
        "abstract": "  For an approaching disaster, the tracking of time-sensitive critical information such as hurricane evacuation notices is challenging in the United States. These notices are issued and distributed rapidly by numerous local authorities that may spread across multiple states. They often undergo frequent updates and are distributed through diverse online portals lacking standard formats. In this study, we developed an approach to timely detect and track the locally issued hurricane evacuation notices. The text data were collected mainly with a spatially targeted web scraping method. They were manually labeled and then classified using natural language processing techniques with deep learning models. The classification of mandatory evacuation notices achieved a high accuracy (recall = 96%). We used Hurricane Ian (2022) to illustrate how real-time evacuation notices extracted from local government sources could be redistributed with a Web GIS system. Our method applied to future hurricanes provides live data for situation awareness to higher-level government agencies and news media. The archived data helps scholars to study government responses toward weather warnings and individual behaviors influenced by evacuation history. The framework may be applied to other types of disasters for rapid and targeted retrieval, classification, redistribution, and archiving of real-time government orders and notifications. ",
        "title": "Information Retrieval and Classification of Real-Time Multi-Source  Hurricane Evacuation Notices",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06790",
        "abstract_url": "http://arxiv.org/abs/2401.06790",
        "authors": [
            {
                "last_name": "Moraes",
                "first_name": "Daniel de S."
            },
            {
                "last_name": "Santos",
                "first_name": "Pedro T. C."
            },
            {
                "last_name": "da Costa",
                "first_name": "Polyana B."
            },
            {
                "last_name": "Pinto",
                "first_name": "Matheus A. S."
            },
            {
                "last_name": "Pinto",
                "first_name": "Ivan de J. P."
            },
            {
                "last_name": "da Veiga",
                "first_name": "\u00c1lvaro M. G."
            },
            {
                "last_name": "Colcher",
                "first_name": "Sergio"
            },
            {
                "last_name": "Busson",
                "first_name": "Antonio J. G."
            },
            {
                "last_name": "Rocha",
                "first_name": "Rafael H."
            },
            {
                "last_name": "Gaio",
                "first_name": "Rennan"
            },
            {
                "last_name": "Miceli",
                "first_name": "Rafael"
            },
            {
                "last_name": "Tourinho",
                "first_name": "Gabriela"
            },
            {
                "last_name": "Rabaioli",
                "first_name": "Marcos"
            },
            {
                "last_name": "Santos",
                "first_name": "Leandro"
            },
            {
                "last_name": "Marques",
                "first_name": "Fellipe"
            },
            {
                "last_name": "Favaro",
                "first_name": "David"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This work presents an unsupervised method for automatically constructing and expanding topic taxonomies by using instruction-based fine-tuned LLMs (Large Language Models). We apply topic modeling and keyword extraction techniques to create initial topic taxonomies and LLMs to post-process the resulting terms and create a hierarchy. To expand an existing taxonomy with new terms, we use zero-shot prompting to find out where to add new nodes, which, to our knowledge, is the first work to present such an approach to taxonomy tasks. We use the resulting taxonomies to assign tags that characterize merchants from a retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a two-part form in which we first assessed the quality of the taxonomies created and then the tags assigned to merchants based on that taxonomy. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies, while the average coherence for merchant tagging surpassed 80%. ",
        "title": "Using Zero-shot Prompting in the Automatic Creation and Expansion of  Topic Taxonomies for Tagging Retail Banking Transactions",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06791",
        "abstract_url": "http://arxiv.org/abs/2401.06791",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Gongbo"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiliang"
            },
            {
                "last_name": "Hu",
                "first_name": "Yan"
            },
            {
                "last_name": "Xu",
                "first_name": "Hua"
            },
            {
                "last_name": "Weng",
                "first_name": "Chunhua"
            },
            {
                "last_name": "Peng",
                "first_name": "Yifan"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Objectives Extraction of PICO (Populations, Interventions, Comparison, and Outcomes) entities is fundamental to evidence retrieval. We present a novel method PICOX to extract overlapping PICO entities.   Materials and Methods PICOX first identifies entities by assessing whether a word marks the beginning or conclusion of an entity. Then it uses a multi-label classifier to assign one or more PICO labels to a span candidate. PICOX was evaluated using one of the best-performing baselines, EBM-NLP, and three more datasets, i.e., PICO-Corpus, and RCT publications on Alzheimer's Disease or COVID-19, using entity-level precision, recall, and F1 scores.   Results PICOX achieved superior precision, recall, and F1 scores across the board, with the micro F1 score improving from 45.05 to 50.87 (p << 0.01). On the PICO-Corpus, PICOX obtained higher recall and F1 scores than the baseline and improved the micro recall score from 56.66 to 67.33. On the COVID-19 dataset, PICOX also outperformed the baseline and improved the micro F1 score from 77.10 to 80.32. On the AD dataset, PICOX demonstrated comparable F1 scores with higher precision when compared to the baseline.   Conclusion PICOX excels in identifying overlapping entities and consistently surpasses a leading baseline across multiple datasets. Ablation studies reveal that its data augmentation strategy effectively minimizes false positives and improves precision. ",
        "title": "A Span-based Model for Extracting Overlapping PICO Entities from RCT  Publications",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06792",
        "abstract_url": "http://arxiv.org/abs/2401.06792",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  With the development of artificial intelligence, large-scale models have become increasingly intelligent. However, numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence, a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI, research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI, summarizing the current work on AGI hallucinations and proposing some directions for future research. ",
        "title": "LightHouse: A Survey of AGI Hallucination",
        "date": "2024-01-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06793",
        "abstract_url": "http://arxiv.org/abs/2401.06793",
        "authors": [
            {
                "last_name": "Durdymyradov",
                "first_name": "Kerven"
            },
            {
                "last_name": "Moshkov",
                "first_name": "Mikhail"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Decision trees and decision rule systems play important roles as classifiers, knowledge representation tools, and algorithms. They are easily interpretable models for data analysis, making them widely used and studied in computer science. Understanding the relationships between these two models is an important task in this field. There are well-known methods for converting decision trees into systems of decision rules. In this paper, we consider the inverse transformation problem, which is not so simple. Instead of constructing an entire decision tree, our study focuses on a greedy polynomial time algorithm that simulates the operation of a decision tree on a given tuple of attribute values. ",
        "title": "Greedy Algorithm for Inference of Decision Trees from Decision Rule  Systems",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06794",
        "abstract_url": "http://arxiv.org/abs/2401.06794",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yurui"
            },
            {
                "last_name": "Ma",
                "first_name": "Langtian"
            },
            {
                "last_name": "Tian",
                "first_name": "Chaolin"
            },
            {
                "last_name": "Jiang",
                "first_name": "Xunyi"
            },
            {
                "last_name": "Sinatra",
                "first_name": "Roberta"
            },
            {
                "last_name": "Ma",
                "first_name": "Yifang"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  Human behaviors, including scientific activities, are shaped by the hierarchical divisions of geography. As a result, researchers' mobility patterns vary across regions, influencing several aspects of the scientific community. These aspects encompass career trajectories, knowledge transfer, international collaborations, talent circulation, innovation diffusion, resource distribution, and policy development. However, our understanding of the relationship between the hierarchical regional scale and scientific movements is limited. This study aims to understand the subtle role of the geographical scales on scientists' mobility patterns across cities, countries, and continents. To this end, we analyzed 2.03 million scientists from 1960 to 2021, spanning institutions, cities, countries, and continents. We built a model based on hierarchical regions with different administrative levels and assessed the tendency for mobility from one region to another and the attractiveness of each region. Our findings reveal distinct nested hierarchies of regional scales and the dynamic of scientists' relocation patterns. This study sheds light on the complex dynamics of scientists' mobility and offers insights into how geographical scale and administrative divisions influence career decisions. ",
        "title": "Quantifying the hierarchical scales of scientists'mobility",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06795",
        "abstract_url": "http://arxiv.org/abs/2401.06795",
        "authors": [
            {
                "last_name": "Glickman",
                "first_name": "Mark"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simulate abductive reasoning, which provides researchers the ability to make connections among related technical topics, which can also be used for research discovery. We review the developments in AI and generative AI for research discovery and summarization, and propose directions where these types of tools are likely to head in the future that may be of interest to statistician and data scientists. ",
        "title": "AI and Generative AI for Research Discovery and Summarization",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06796",
        "abstract_url": "http://arxiv.org/abs/2401.06796",
        "authors": [
            {
                "last_name": "Maleki",
                "first_name": "Negar"
            },
            {
                "last_name": "Padmanabhan",
                "first_name": "Balaji"
            },
            {
                "last_name": "Dutta",
                "first_name": "Kaushik"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As large language models continue to advance in Artificial Intelligence (AI), text generation systems have been shown to suffer from a problematic phenomenon termed often as \"hallucination.\" However, with AI's increasing presence across various domains including medicine, concerns have arisen regarding the use of the term itself. In this study, we conducted a systematic review to identify papers defining \"AI hallucination\" across fourteen databases. We present and analyze definitions obtained across all databases, categorize them based on their applications, and extract key points within each category. Our results highlight a lack of consistency in how the term is used, but also help identify several alternative terms in the literature. We discuss implications of these and call for a more unified effort to bring consistency to an important contemporary AI issue that can affect multiple domains significantly. ",
        "title": "AI Hallucinations: A Misnomer Worth Clarifying",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06799",
        "abstract_url": "http://arxiv.org/abs/2401.06799",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Youngjae"
            },
            {
                "last_name": "Bae",
                "first_name": "HeeSun"
            },
            {
                "last_name": "Shin",
                "first_name": "Seungjae"
            },
            {
                "last_name": "Youn",
                "first_name": "Yeo Dong"
            },
            {
                "last_name": "Joo",
                "first_name": "Weonyoung"
            },
            {
                "last_name": "Moon",
                "first_name": "Il-Chul"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Recent Vision-Language Pretrained (VLP) models have become the backbone for many downstream tasks, but they are utilized as frozen model without learning. Prompt learning is a method to improve the pre-trained VLP model by adding a learnable context vector to the inputs of the text encoder. In a few-shot learning scenario of the downstream task, MLE training can lead the context vector to over-fit dominant image features in the training data. This overfitting can potentially harm the generalization ability, especially in the presence of a distribution shift between the training and test dataset. This paper presents a Bayesian-based framework of prompt learning, which could alleviate the overfitting issues on few-shot learning application and increase the adaptability of prompts on unseen instances. Specifically, modeling data-dependent prior enhances the adaptability of text features for both seen and unseen image features without the trade-off of performance between them. Based on the Bayesian framework, we utilize the Wasserstein Gradient Flow in the estimation of our target posterior distribution, which enables our prompt to be flexible in capturing the complex modes of image features. We demonstrate the effectiveness of our method on benchmark datasets for several experiments by showing statistically significant improvements on performance compared to existing methods. The code is available at https://github.com/youngjae-cho/APP. ",
        "title": "Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt  Learning with Data-Dependent Prior",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06800",
        "abstract_url": "http://arxiv.org/abs/2401.06800",
        "authors": [
            {
                "last_name": "Kulkarni",
                "first_name": "Mandar"
            },
            {
                "last_name": "Tangarajan",
                "first_name": "Praveen"
            },
            {
                "last_name": "Kim",
                "first_name": "Kyung"
            },
            {
                "last_name": "Trivedi",
                "first_name": "Anusua"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline. ",
        "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06801",
        "abstract_url": "http://arxiv.org/abs/2401.06801",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ye"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow's potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development. ",
        "title": "Graph-of-Thought: Utilizing Large Language Models to Solve Complex and  Dynamic Business Problems",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06802",
        "abstract_url": "http://arxiv.org/abs/2401.06802",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Quan"
            },
            {
                "last_name": "Jing",
                "first_name": "Shixiong"
            },
            {
                "last_name": "Chen",
                "first_name": "Lingwei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  The popularization of social media increases user engagements and generates a large amount of user-oriented data. Among them, text data (e.g., tweets, blogs) significantly attracts researchers and speculators to infer user attributes (e.g., age, gender, location) for fulfilling their intents. Generally, this line of work casts attribute inference as a text classification problem, and starts to leverage graph neural networks (GNNs) to utilize higher-level representations of source texts. However, these text graphs are constructed over words, suffering from high memory consumption and ineffectiveness on few labeled texts. To address this challenge, we design a text-graph-based few-shot learning model for attribute inferences on social media text data. Our model first constructs and refines a text graph using manifold learning and message passing, which offers a better trade-off between expressiveness and complexity. Afterwards, to further use cross-domain texts and unlabeled texts to improve few-shot performance, a hierarchical knowledge distillation is devised over text graph to optimize the problem, which derives better text representations, and advances model generalization ability. Experiments on social media datasets demonstrate the state-of-the-art performance of our model on attribute inferences with considerably fewer labeled texts. ",
        "title": "Hierarchical Knowledge Distillation on Text Graph for Data-limited  Attribute Inference",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06803",
        "abstract_url": "http://arxiv.org/abs/2401.06803",
        "authors": [
            {
                "last_name": "Grassucci",
                "first_name": "Eleonora"
            },
            {
                "last_name": "Park",
                "first_name": "Jihong"
            },
            {
                "last_name": "Barbarossa",
                "first_name": "Sergio"
            },
            {
                "last_name": "Kim",
                "first_name": "Seong-Lyun"
            },
            {
                "last_name": "Choi",
                "first_name": "Jinho"
            },
            {
                "last_name": "Comminiello",
                "first_name": "Danilo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  While deep generative models are showing exciting abilities in computer vision and natural language processing, their adoption in communication frameworks is still far underestimated. These methods are demonstrated to evolve solutions to classic communication problems such as denoising, restoration, or compression. Nevertheless, generative models can unveil their real potential in semantic communication frameworks, in which the receiver is not asked to recover the sequence of bits used to encode the transmitted (semantic) message, but only to regenerate content that is semantically consistent with the transmitted message. Disclosing generative models capabilities in semantic communication paves the way for a paradigm shift with respect to conventional communication systems, which has great potential to reduce the amount of data traffic and offers a revolutionary versatility to novel tasks and applications that were not even conceivable a few years ago. In this paper, we present a unified perspective of deep generative models in semantic communication and we unveil their revolutionary role in future communication frameworks, enabling emerging applications and tasks. Finally, we analyze the challenges and opportunities to face to develop generative models specifically tailored for communication systems. ",
        "title": "Generative AI Meets Semantic Communication: Evolution and Revolution of  Communication Tasks",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06804",
        "abstract_url": "http://arxiv.org/abs/2401.06804",
        "authors": [
            {
                "last_name": "Shahin",
                "first_name": "Nada"
            },
            {
                "last_name": "Ismail",
                "first_name": "Leila"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  ChatGPT is a language model based on Generative AI. Existing research work on ChatGPT focused on its use in various domains. However, its potential for Sign Language Translation (SLT) is yet to be explored. This paper addresses this void. Therefore, we present GPT's evolution aiming a retrospective analysis of the improvements to its architecture for SLT. We explore ChatGPT's capabilities in translating different sign languages in paving the way to better accessibility for deaf and hard-of-hearing community. Our experimental results indicate that ChatGPT can accurately translate from English to American (ASL), Australian (AUSLAN), and British (BSL) sign languages and from Arabic Sign Language (ArSL) to English with only one prompt iteration. However, the model failed to translate from Arabic to ArSL and ASL, AUSLAN, and BSL to Arabic. Consequently, we present challenges and derive insights for future research directions. ",
        "title": "ChatGPT, Let us Chat Sign Language: Experiments, Architectural Elements,  Challenges and Research Directions",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06805",
        "abstract_url": "http://arxiv.org/abs/2401.06805",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yiqi"
            },
            {
                "last_name": "Chen",
                "first_name": "Wentao"
            },
            {
                "last_name": "Han",
                "first_name": "Xiaotian"
            },
            {
                "last_name": "Lin",
                "first_name": "Xudong"
            },
            {
                "last_name": "Zhao",
                "first_name": "Haiteng"
            },
            {
                "last_name": "Liu",
                "first_name": "Yongfei"
            },
            {
                "last_name": "Zhai",
                "first_name": "Bohan"
            },
            {
                "last_name": "Yuan",
                "first_name": "Jianbo"
            },
            {
                "last_name": "You",
                "first_name": "Quanzeng"
            },
            {
                "last_name": "Yang",
                "first_name": "Hongxia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning. ",
        "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models  (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06806",
        "abstract_url": "http://arxiv.org/abs/2401.06806",
        "authors": [
            {
                "last_name": "Jung",
                "first_name": "Jee-weon"
            },
            {
                "last_name": "Sharma",
                "first_name": "Roshan"
            },
            {
                "last_name": "Chen",
                "first_name": "William"
            },
            {
                "last_name": "Raj",
                "first_name": "Bhiksha"
            },
            {
                "last_name": "Watanabe",
                "first_name": "Shinji"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Abstractive speech summarization (SSUM) aims to generate human-like summaries from speech. Given variations in information captured and phrasing, recordings can be summarized in multiple ways. Therefore, it is more reasonable to consider a probabilistic distribution of all potential summaries rather than a single summary. However, conventional SSUM models are mostly trained and evaluated with a single ground-truth (GT) human-annotated deterministic summary for every recording. Generating multiple human references would be ideal to better represent the distribution statistically, but is impractical because annotation is expensive. We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation. First, we explore prompting strategies to generate synthetic summaries from ChatGPT. We validate the quality of synthetic summaries using multiple metrics including human evaluation, where we find that summaries generated using AugSumm are perceived as more valid to humans. Second, we develop methods to utilize synthetic summaries in training and evaluation. Experiments on How2 demonstrate that pre-training on synthetic summaries and fine-tuning on GT summaries improves ROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries are available at https://github.com/Jungjee/AugSumm. ",
        "title": "AugSumm: towards generalizable speech summarization using synthetic  labels from large language model",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06807",
        "abstract_url": "http://arxiv.org/abs/2401.06807",
        "authors": [
            {
                "last_name": "Tomar",
                "first_name": "Mohit"
            },
            {
                "last_name": "Tiwari",
                "first_name": "Abhisek"
            },
            {
                "last_name": "Saha",
                "first_name": "Tulika"
            },
            {
                "last_name": "Jha",
                "first_name": "Prince"
            },
            {
                "last_name": "Saha",
                "first_name": "Sriparna"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent times, there has been an increasing awareness about imminent environmental challenges, resulting in people showing a stronger dedication to taking care of the environment and nurturing green life. The current $19.6 billion indoor gardening industry, reflective of this growing sentiment, not only signifies a monetary value but also speaks of a profound human desire to reconnect with the natural world. However, several recent surveys cast a revealing light on the fate of plants within our care, with more than half succumbing primarily due to the silent menace of improper care. Thus, the need for accessible expertise capable of assisting and guiding individuals through the intricacies of plant care has become paramount more than ever. In this work, we make the very first attempt at building a plant care assistant, which aims to assist people with plant(-ing) concerns through conversations. We propose a plant care conversational dataset named Plantational, which contains around 1K dialogues between users and plant care experts. Our end-to-end proposed approach is two-fold : (i) We first benchmark the dataset with the help of various large language models (LLMs) and visual language model (VLM) by studying the impact of instruction tuning (zero-shot and few-shot prompting) and fine-tuning techniques on this task; (ii) finally, we build EcoSage, a multi-modal plant care assisting dialogue generation framework, incorporating an adapter-based modality infusion using a gated mechanism. We performed an extensive examination (both automated and manual evaluation) of the performance exhibited by various LLMs and VLM in the generation of the domain-specific dialogue responses to underscore the respective strengths and weaknesses of these diverse models. ",
        "title": "An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue  Assistant",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06808",
        "abstract_url": "http://arxiv.org/abs/2401.06808",
        "authors": [
            {
                "last_name": "Lewis",
                "first_name": "Martha"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "NE"
        ],
        "abstract": "  Categorical compositional distributional semantics is an approach to modelling language that combines the success of vector-based models of meaning with the compositional power of formal semantics. However, this approach was developed without an eye to cognitive plausibility. Vector representations of concepts and concept binding are also of interest in cognitive science, and have been proposed as a way of representing concepts within a biologically plausible spiking neural network. This work proposes a way for compositional distributional semantics to be implemented within a spiking neural network architecture, with the potential to address problems in concept binding, and give a small implementation. We also describe a means of training word representations using labelled images. ",
        "title": "Grounded learning for compositional vector semantics",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06810",
        "abstract_url": "http://arxiv.org/abs/2401.06810",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Srishti"
            },
            {
                "last_name": "Garg",
                "first_name": "Piyush Kumar"
            },
            {
                "last_name": "Dandapat",
                "first_name": "Sourav Kumar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Emotions have played an important part in many sectors, including psychology, medicine, mental health, computer science, and so on, and categorizing them has proven extremely useful in separating one emotion from another. Emotions can be classified using the following two methods: (1) The supervised method's efficiency is strongly dependent on the size and domain of the data collected. A categorization established using relevant data from one domain may not work well in another. (2) An unsupervised method that uses either domain expertise or a knowledge base of emotion types already exists. Though this second approach provides a suitable and generic categorization of emotions and is cost-effective, the literature doesn't possess a publicly available knowledge base that can be directly applied to any emotion categorization-related task. This pushes us to create a knowledge base that can be used for emotion classification across domains, and ontology is often used for this purpose. In this study, we provide TONE, an emotion-based ontology that effectively creates an emotional hierarchy based on Dr. Gerrod Parrot's group of emotions. In addition to ontology development, we introduce a semi-automated vocabulary construction process to generate a detailed collection of terms for emotions at each tier of the hierarchy. We also demonstrate automated methods for establishing three sorts of dependencies in order to develop linkages between different emotions. Our human and automatic evaluation results show the ontology's quality. Furthermore, we describe three distinct use cases that demonstrate the applicability of our ontology. ",
        "title": "TONE: A 3-Tiered ONtology for Emotion analysis",
        "date": "2024-01-10",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06811",
        "abstract_url": "http://arxiv.org/abs/2401.06811",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Zhongtian"
            },
            {
                "last_name": "Chen",
                "first_name": "Yangqi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Meng"
            },
            {
                "last_name": "Li",
                "first_name": "Ronghan"
            },
            {
                "last_name": "Wang",
                "first_name": "Lifang"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL"
        ],
        "abstract": "  Knowledge-based dialogue systems with internet retrieval have recently attracted considerable attention from researchers. The dialogue systems overcome a major limitation of traditional knowledge dialogue systems, where the timeliness of knowledge cannot be assured, hence providing greater practical application value. Knowledge-based dialogue systems with internet retrieval can be typically segmented into three tasks: Retrieval Decision, Query Generation, and Response Generation. However, many of studies assumed that all conversations require external knowledge to continue, neglecting the critical step of determining when retrieval is necessary. This assumption often leads to an over-dependence on external knowledge, even when it may not be required. Our work addresses this oversight by employing a single unified model facilitated by prompt and multi-task learning approaches. This model not only decides whether retrieval is necessary but also generates retrieval queries and responses. By integrating these functions, our system leverages the full potential of pre-trained models and reduces the complexity and costs associated with deploying multiple models. We conducted extensive experiments to investigate the mutual enhancement among the three tasks in our system. What is more, the experiment results on the Wizint and Dusinc datasets not only demonstrate that our unified model surpasses the baseline performance for individual tasks, but also reveal that it achieves comparable results when contrasted with SOTA systems that deploy separate, specialized models for each task. ",
        "title": "UniRQR: A Unified Model for Retrieval Decision, Query, and Response  Generation in Internet-Based Knowledge Dialogue Systems",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06816",
        "abstract_url": "http://arxiv.org/abs/2401.06816",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Qinghan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiyong"
            },
            {
                "last_name": "Huang",
                "first_name": "Jihao"
            },
            {
                "last_name": "Li",
                "first_name": "Guiquan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  ChatGPT has been evidenced to enhance human performance in creative tasks. Yet, it is still unclear if this boosting effect sustains with and without ChatGPT. In a pre-registered seven-day lab experiment and a follow-up survey after 30 days of experiment completion, we examined the impacts of ChatGPT presence and absence on sustained creativity using a text dataset of 3302 creative ideas and 427 creative solutions from 61 college students. Participants in the treatment group used ChatGPT in creative tasks, while those in the control group completed the tasks by themselves. The findings show that although the boosting effect of ChatGPT was consistently observed over a five-day creative journey, human creative performance reverted to baseline when ChatGPT was down on the 7th and the 30th day. More critically, the use of ChatGPT in creative tasks resulted in increasingly homogenized contents, and this homogenization effect persisted even when ChatGPT was absence. These findings pose a challenge to the prevailing argument that ChatGPT can enhance human creativity. In fact, generative AI like ChatGPT lends to human with a temporary rise in creative performance but boxes human creative capability in the long run, highlighting the imperative for cautious generative AI integration in creative endeavors. ",
        "title": "When ChatGPT is gone: Creativity reverts and homogeneity persists",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06817",
        "abstract_url": "http://arxiv.org/abs/2401.06817",
        "authors": [
            {
                "last_name": "Mallick",
                "first_name": "Tanwi"
            },
            {
                "last_name": "Murphy",
                "first_name": "John"
            },
            {
                "last_name": "Bergerson",
                "first_name": "Joshua David"
            },
            {
                "last_name": "Verner",
                "first_name": "Duane R."
            },
            {
                "last_name": "Hutchison",
                "first_name": "John K"
            },
            {
                "last_name": "Levy",
                "first_name": "Leslie-Anne"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Understanding the multifaceted effects of climate change across diverse geographic locations is crucial for timely adaptation and the development of effective mitigation strategies. As the volume of scientific literature on this topic continues to grow exponentially, manually reviewing these documents has become an immensely challenging task. Utilizing Natural Language Processing (NLP) techniques to analyze this wealth of information presents an efficient and scalable solution. By gathering extensive amounts of peer-reviewed articles and studies, we can extract and process critical information about the effects of climate change in specific regions. We employ BERT (Bidirectional Encoder Representations from Transformers) for Named Entity Recognition (NER), which enables us to efficiently identify specific geographies within the climate literature. This, in turn, facilitates location-specific analyses. We conduct region-specific climate trend analyses to pinpoint the predominant themes or concerns related to climate change within a particular area, trace the temporal progression of these identified issues, and evaluate their frequency, severity, and potential development over time. These in-depth examinations of location-specific climate data enable the creation of more customized policy-making, adaptation, and mitigation strategies, addressing each region's unique challenges and providing more effective solutions rooted in data-driven insights. This approach, founded on a thorough exploration of scientific texts, offers actionable insights to a wide range of stakeholders, from policymakers to engineers to environmentalists. By proactively understanding these impacts, societies are better positioned to prepare, allocate resources wisely, and design tailored strategies to cope with future climate conditions, ensuring a more resilient future for all. ",
        "title": "Analyzing Regional Impacts of Climate Change using Natural Language  Processing Techniques",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06820",
        "abstract_url": "http://arxiv.org/abs/2401.06820",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Sihan"
            },
            {
                "last_name": "Kim",
                "first_name": "Youngdae"
            },
            {
                "last_name": "Ren",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Kim",
                "first_name": "Kibaek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  At the heart of power system operations, alternating current optimal power flow (ACOPF) studies the generation of electric power in the most economical way under network-wide load requirement, and can be formulated as a highly structured non-convex quadratically constrained quadratic program (QCQP). Optimization-based solutions to ACOPF (such as ADMM or interior-point method), as the classic approach, require large amount of computation and cannot meet the need to repeatedly solve the problem as load requirement frequently changes. On the other hand, learning-based methods that directly predict the ACOPF solution given the load input incur little computational cost but often generates infeasible solutions (i.e. violate the constraints of ACOPF). In this work, we combine the best of both worlds -- we propose an innovated framework for learning ACOPF, where the input load is mapped to the ACOPF solution through a neural network in a computationally efficient and reliable manner. Key to our innovation is a specific-purpose \"activation function\" defined implicitly by a QCQP and a novel loss, which enforce constraint satisfaction. We show through numerical simulations that our proposed method achieves superior feasibility rate and generation cost in situations where the existing learning-based approaches fail. ",
        "title": "QCQP-Net: Reliably Learning Feasible Alternating Current Optimal Power  Flow Solutions Under Constraints",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06821",
        "abstract_url": "http://arxiv.org/abs/2401.06821",
        "authors": [
            {
                "last_name": "Ducoffe",
                "first_name": "M\u00e9lanie"
            },
            {
                "last_name": "Pov\u00e9da",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Galametz",
                "first_name": "Audrey"
            },
            {
                "last_name": "Boumazouza",
                "first_name": "Ryma"
            },
            {
                "last_name": "Martin",
                "first_name": "Marion-C\u00e9cile"
            },
            {
                "last_name": "Baris",
                "first_name": "Julien"
            },
            {
                "last_name": "Daverschot",
                "first_name": "Derk"
            },
            {
                "last_name": "O'Higgins",
                "first_name": "Eugene"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Surrogate Neural Networks (NN) now routinely serve as substitutes for computationally demanding simulations (e.g., finite element). They enable faster analyses in industrial applications e.g., manufacturing processes, performance assessment. The verification of surrogate models is a critical step to assess their robustness under different scenarios. We explore the combination of empirical and formal methods in one NN verification pipeline. We showcase its efficiency on an industrial use case of aircraft predictive maintenance. We assess the local stability of surrogate NN designed to predict the stress sustained by an aircraft part from external loads. Our contribution lies in the complete verification of the surrogate models that possess a high-dimensional input and output space, thus accommodating multi-objective constraints. We also demonstrate the pipeline effectiveness in substantially decreasing the runtime needed to assess the targeted property. ",
        "title": "Surrogate Neural Networks Local Stability for Aircraft Predictive  Maintenance",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06822",
        "abstract_url": "http://arxiv.org/abs/2401.06822",
        "authors": [
            {
                "last_name": "Sammany",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Steef",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Agami",
                "first_name": "Nedaa"
            },
            {
                "last_name": "Medhat",
                "first_name": "T."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  It is well known over the recent years that measuring the success of projects under the umbrella of project management is inextricably linked with the associated cost, time, and quality. Most of the previous researches in the field assigned a separate mathematical model for each criterion, then numerical methods or search techniques were applied to obtain the optimal trade-off between the three criteria. However in this paper, the problem was addressed by linear multi-objective optimization using only one fuzzy mathematical model. The three criteria were merged in a single non-linear membership function to find the optimal trade-off. Finally, the proposed model is tested and validated using numerical examples. ",
        "title": "Fuzzy Mathematical Model For Optimizing Success Criteria Of Projects: A  Project Management Application",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06823",
        "abstract_url": "http://arxiv.org/abs/2401.06823",
        "authors": [
            {
                "last_name": "Wagle",
                "first_name": "Manoj M"
            },
            {
                "last_name": "Long",
                "first_name": "Siqu"
            },
            {
                "last_name": "Chen",
                "first_name": "Carissa"
            },
            {
                "last_name": "Liu",
                "first_name": "Chunlei"
            },
            {
                "last_name": "Yang",
                "first_name": "Pengyi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recent developments in single-cell omics technologies have enabled the quantification of molecular profiles in individual cells at an unparalleled resolution. Deep learning, a rapidly evolving sub-field of machine learning, has instilled a significant interest in single-cell omics research due to its remarkable success in analysing heterogeneous high-dimensional single-cell omics data. Nevertheless, the inherent multi-layer nonlinear architecture of deep learning models often makes them `black boxes' as the reasoning behind predictions is often unknown and not transparent to the user. This has stimulated an increasing body of research for addressing the lack of interpretability in deep learning models, especially in single-cell omics data analyses, where the identification and understanding of molecular regulators are crucial for interpreting model predictions and directing downstream experimental validations. In this work, we introduce the basics of single-cell omics technologies and the concept of interpretable deep learning. This is followed by a review of the recent interpretable deep learning models applied to various single-cell omics research. Lastly, we highlight the current limitations and discuss potential future directions. We anticipate this review to bring together the single-cell and machine learning research communities to foster future development and application of interpretable deep learning in single-cell omics research. ",
        "title": "Interpretable deep learning in single-cell omics",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06824",
        "abstract_url": "http://arxiv.org/abs/2401.06824",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Tianlong"
            },
            {
                "last_name": "Zheng",
                "first_name": "Xiaoqing"
            },
            {
                "last_name": "Huang",
                "first_name": "Xuanjing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Getting large language models (LLMs) to refuse to answer hostile toxicity questions is a core issue under the theme of LLMs security. Previous approaches have used prompts engineering to jailbreak LLMs and answer some toxicity questions. These approaches can easily fail after the model manufacturer makes additional fine-tuning to the model. To promote the further understanding of model jailbreaking by researchers, we are inspired by Representation Engineering to propose a jailbreaking method that does not require elaborate construction prompts, is not affected by model fine-tuning, and can be widely applied to any open-source LLMs in a pluggable manner. We have evaluated this method on multiple mainstream LLMs on carefully supplemented toxicity datasets, and the experimental results demonstrate the significant effectiveness of our approach. After being surprised by some interesting jailbreaking cases, we did extensive in-depth research to explore the techniques behind this method. ",
        "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation  Engineering",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06825",
        "abstract_url": "http://arxiv.org/abs/2401.06825",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Jiangming"
            },
            {
                "last_name": "Yin",
                "first_name": "Xiangbo"
            },
            {
                "last_name": "Chen",
                "first_name": "Yeyun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yachao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhizhong"
            },
            {
                "last_name": "Xie",
                "first_name": "Yuan"
            },
            {
                "last_name": "Qu",
                "first_name": "Yanyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unsupervised visible-infrared person re-identification (USL-VI-ReID) is a promising yet challenging retrieval task. The key challenges in USL-VI-ReID are to effectively generate pseudo-labels and establish pseudo-label correspondences across modalities without relying on any prior annotations. Recently, clustered pseudo-label methods have gained more attention in USL-VI-ReID. However, previous methods fell short of fully exploiting the individual nuances, as they simply utilized a single memory that represented an identity to establish cross-modality correspondences, resulting in ambiguous cross-modality correspondences. To address the problem, we propose a Multi-Memory Matching (MMM) framework for USL-VI-ReID. We first design a Cross-Modality Clustering (CMC) module to generate the pseudo-labels through clustering together both two modality samples. To associate cross-modality clustered pseudo-labels, we design a Multi-Memory Learning and Matching (MMLM) module, ensuring that optimization explicitly focuses on the nuances of individual perspectives and establishes reliable cross-modality correspondences. Finally, we design a Soft Cluster-level Alignment (SCA) module to narrow the modality gap while mitigating the effect of noise pseudo-labels through a soft many-to-many alignment strategy. Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the reliability of the established cross-modality correspondences and the effectiveness of our MMM. The source codes will be released. ",
        "title": "Multi-Memory Matching for Unsupervised Visible-Infrared Person  Re-Identification",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06826",
        "abstract_url": "http://arxiv.org/abs/2401.06826",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Jialiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuo"
            },
            {
                "last_name": "Niu",
                "first_name": "Gang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Hongyuan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Joey Tianyi"
            },
            {
                "last_name": "Gong",
                "first_name": "Chen"
            },
            {
                "last_name": "Sugiyama",
                "first_name": "Masashi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Knowledge Distillation (KD) aims to learn a compact student network using knowledge from a large pre-trained teacher network, where both networks are trained on data from the same distribution. However, in practical applications, the student network may be required to perform in a new scenario (i.e., the target domain), which usually exhibits significant differences from the known scenario of the teacher network (i.e., the source domain). The traditional domain adaptation techniques can be integrated with KD in a two-stage process to bridge the domain gap, but the ultimate reliability of two-stage approaches tends to be limited due to the high computational consumption and the additional errors accumulated from both stages. To solve this problem, we propose a new one-stage method dubbed ``Direct Distillation between Different Domains\" (4Ds). We first design a learnable adapter based on the Fourier transform to separate the domain-invariant knowledge from the domain-specific knowledge. Then, we build a fusion-activation mechanism to transfer the valuable domain-invariant knowledge to the student network, while simultaneously encouraging the adapter within the teacher network to learn the domain-specific knowledge of the target data. As a result, the teacher network can effectively transfer categorical knowledge that aligns with the target domain of the student network. Intensive experiments on various benchmark datasets demonstrate that our proposed 4Ds method successfully produces reliable student networks and outperforms state-of-the-art approaches. ",
        "title": "Direct Distillation between Different Domains",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06827",
        "abstract_url": "http://arxiv.org/abs/2401.06827",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Guiming"
            },
            {
                "last_name": "Shi",
                "first_name": "Kaize"
            },
            {
                "last_name": "Fu",
                "first_name": "Hong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Huaiwen"
            },
            {
                "last_name": "Xu",
                "first_name": "Guandong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses the challenges in V-L models to promote prompt learning across both modalities, which indicates a competitive generalization performance in line with the state-of-the-art. Preeminently, APLe shows robustness and favourable performance in prompt-length experiments with an absolute advantage in adopting the V-L models. ",
        "title": "APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06829",
        "abstract_url": "http://arxiv.org/abs/2401.06829",
        "authors": [
            {
                "last_name": "Baldassini",
                "first_name": "Folco Bertini"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Chang",
                "first_name": "Ching-Chung"
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  A new approach to linguistic watermarking of language models is presented in which information is imperceptibly inserted into the output text while preserving its readability and original meaning. A cross-attention mechanism is used to embed watermarks in the text during inference. Two methods using cross-attention are presented that minimize the effect of watermarking on the performance of a pretrained model. Exploration of different training strategies for optimizing the watermarking and of the challenges and implications of applying this approach in real-world scenarios clarified the tradeoff between watermark robustness and text quality. Watermark selection substantially affects the generated output for high entropy sentences. This proactive watermarking approach has potential application in future model development. ",
        "title": "Cross-Attention Watermarking of Large Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06830",
        "abstract_url": "http://arxiv.org/abs/2401.06830",
        "authors": [
            {
                "last_name": "Manderlier",
                "first_name": "Maxime"
            },
            {
                "last_name": "Lecron",
                "first_name": "Fabian"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG"
        ],
        "abstract": "  The RecSys Challenge 2023, presented by ShareChat, consists to predict if an user will install an application on his smartphone after having seen advertising impressions in ShareChat & Moj apps. This paper presents the solution of 'Team UMONS' to this challenge, giving accurate results (our best score is 6.622686) with a relatively small model that can be easily implemented in different production configurations. Our solution scales well when increasing the dataset size and can be used with datasets containing missing values. ",
        "title": "RecSys Challenge 2023: From data preparation to prediction, a simple,  efficient, robust and scalable solution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06831",
        "abstract_url": "http://arxiv.org/abs/2401.06831",
        "authors": [
            {
                "last_name": "Shoaib",
                "first_name": "Mohamed R."
            },
            {
                "last_name": "Emara",
                "first_name": "Heba M."
            },
            {
                "last_name": "Zhao",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This survey paper explores the transformative influence of frontier AI, foundation models, and Large Language Models (LLMs) in the realm of Intelligent Transportation Systems (ITS), emphasizing their integral role in advancing transportation intelligence, optimizing traffic management, and contributing to the realization of smart cities. Frontier AI refers to the forefront of AI technology, encompassing the latest advancements, innovations, and experimental techniques in the field, especially AI foundation models and LLMs. Foundation models, like GPT-4, are large, general-purpose AI models that provide a base for a wide range of applications. They are characterized by their versatility and scalability. LLMs are obtained from finetuning foundation models with a specific focus on processing and generating natural language. They excel in tasks like language understanding, text generation, translation, and summarization. By leveraging vast textual data, including traffic reports and social media interactions, LLMs extract critical insights, fostering the evolution of ITS. The survey navigates the dynamic synergy between LLMs and ITS, delving into applications in traffic management, integration into autonomous vehicles, and their role in shaping smart cities. It provides insights into ongoing research, innovations, and emerging trends, aiming to inspire collaboration at the intersection of language, intelligence, and mobility for safer, more efficient, and sustainable transportation systems. The paper further surveys interactions between LLMs and various aspects of ITS, exploring roles in traffic management, facilitating autonomous vehicles, and contributing to smart city development, while addressing challenges brought by frontier AI and foundation models. This paper offers valuable inspiration for future research and innovation in the transformative domain of intelligent transportation. ",
        "title": "A Survey on the Applications of Frontier AI, Foundation Models, and  Large Language Models to Intelligent Transportation Systems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06832",
        "abstract_url": "http://arxiv.org/abs/2401.06832",
        "authors": [
            {
                "last_name": "Arisaputra",
                "first_name": "Panji"
            },
            {
                "last_name": "Handoyo",
                "first_name": "Alif Tri"
            },
            {
                "last_name": "Zahra",
                "first_name": "Amalia"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  This research paper focuses on the development and evaluation of Automatic Speech Recognition (ASR) technology using the XLS-R 300m model. The study aims to improve ASR performance in converting spoken language into written text, specifically for Indonesian, Javanese, and Sundanese languages. The paper discusses the testing procedures, datasets used, and methodology employed in training and evaluating the ASR systems. The results show that the XLS-R 300m model achieves competitive Word Error Rate (WER) measurements, with a slight compromise in performance for Javanese and Sundanese languages. The integration of a 5-gram KenLM language model significantly reduces WER and enhances ASR accuracy. The research contributes to the advancement of ASR technology by addressing linguistic diversity and improving performance across various languages. The findings provide insights into optimizing ASR accuracy and applicability for diverse linguistic contexts. ",
        "title": "XLS-R Deep Learning Model for Multilingual ASR on Low- Resource  Languages: Indonesian, Javanese, and Sundanese",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06833",
        "abstract_url": "http://arxiv.org/abs/2401.06833",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xue-Fang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Jingjing"
            },
            {
                "last_name": "Chen",
                "first_name": "Wen-Hua"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a comprehensive hierarchical control framework for autonomous decision-making arising in robotics and autonomous systems. In a typical hierarchical control architecture, high-level decision making is often characterised by discrete state and decision/control sets. However, a rational decision is usually affected by not only the discrete states of the autonomous system, but also the underlying continuous dynamics even the evolution of its operational environment. This paper proposes a holistic and comprehensive design process and framework for this type of challenging problems, from new modelling and design problem formulation to control design and stability analysis. It addresses the intricate interplay between traditional continuous systems dynamics utilized at the low levels for control design and discrete Markov decision processes (MDP) for facilitating high-level decision making. We model the decision making system in complex environments as a hybrid system consisting of a controlled MDP and autonomous (i.e. uncontrolled) continuous dynamics. Consequently, the new formulation is called as hybrid Markov decision process (HMDP). The design problem is formulated with a focus on ensuring both safety and optimality while taking into account the influence of both the discrete and continuous state variables of different levels. With the help of the model predictive control (MPC) concept, a decision maker design scheme is proposed for the proposed hybrid decision making model. By carefully designing key ingredients involved in this scheme, it is shown that the recursive feasibility and stability of the proposed autonomous decision making scheme are guaranteed. The proposed framework is applied to develop an autonomous lane changing system for intelligent vehicles. ",
        "title": "A hierarchical control framework for autonomous decision-making systems:  Integrating HMDP and MPC",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06834",
        "abstract_url": "http://arxiv.org/abs/2401.06834",
        "authors": [
            {
                "last_name": "Beinarovich",
                "first_name": "Andrei"
            },
            {
                "last_name": "Stepanov",
                "first_name": "Sergey"
            },
            {
                "last_name": "Zaslavsky",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  The problem is considered of optimizing discrete parameters in the presence of constraints. We use the stochastic sigmoid with temperature and put forward the new adaptive gradient method CONGA. The search for an optimal solution is carried out by a population of individuals. Each of them varies according to gradients of the 'environment' and is characterized by two temperature parameters with different annealing schedules. Unadapted individuals die, and optimal ones interbreed, the result is directed evolutionary dynamics. The proposed method is illustrated using the well-known combinatorial problem for optimal packing of a backpack (0-1 KP). ",
        "title": "Optimization of Discrete Parameters Using the Adaptive Gradient Method  and Directed Evolution",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06836",
        "abstract_url": "http://arxiv.org/abs/2401.06836",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zaijing"
            },
            {
                "last_name": "Chen",
                "first_name": "Gongwei"
            },
            {
                "last_name": "Shao",
                "first_name": "Rui"
            },
            {
                "last_name": "Jiang",
                "first_name": "Dongmei"
            },
            {
                "last_name": "Nie",
                "first_name": "Liqiang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The Emotional Generation is a subset of emotional intelligence, which aims to output an emotional response based on emotional conditions as input. Emotion generation has a wide range of applications, including emotion chat, emotional visual caption, and emotional rewriting. However, it faces challenges such as a lack of interpretability and poor evaluability. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of Large Language Models (LLMs) on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called EGS. Extensive experimental results demonstrate the effectiveness of ECoT and EGS. Further,we discuss the promise of LLMs in the field of sentiment analysis and present key insights into the LLMs with the ECoT in emotional generation tasks. ",
        "title": "Enhancing the Emotional Generation Capability of Large Language Models  via Emotional Chain-of-Thought",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06837",
        "abstract_url": "http://arxiv.org/abs/2401.06837",
        "authors": [
            {
                "last_name": "Jain",
                "first_name": "Parag"
            },
            {
                "last_name": "Marzoca",
                "first_name": "Andreea"
            },
            {
                "last_name": "Piccinno",
                "first_name": "Francesco"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of +37pp (79%) for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy. ",
        "title": "Structsum Generation for Faster Text Comprehension",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06838",
        "abstract_url": "http://arxiv.org/abs/2401.06838",
        "authors": [
            {
                "last_name": "She",
                "first_name": "Shuaijie"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            },
            {
                "last_name": "Zou",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Geng",
                "first_name": "Xiang"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in a pivot language is superior to other languages due to the imbalance of multilingual training data.To enhance reasoning abilities in non-pivot languages, we propose an alignment-as-preference optimization framework. Specifically, we adopt an open-source translation model to estimate the consistency between answers in non-pivot and pivot languages. We further adopt the answer consistency as the preference for DPO or PPO thus optimizing the lesser reasoning. Experiments show that our method significantly improves the model's multilingual reasoning, with better reasoning consistency across languages. Our framework achieved a 13.7% accuracy improvement on out-of-domain datasets MSVAMP while preserving the competitive performance on MGSM. Moreover, we find that iterative DPO is helpful for further alignment and improvement of the model's multilingual mathematical reasoning ability, further pushing the improvement to 16.7% ",
        "title": "MAPO: Advancing Multilingual Reasoning through Multilingual  Alignment-as-Preference Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06839",
        "abstract_url": "http://arxiv.org/abs/2401.06839",
        "authors": [
            {
                "last_name": "Gussman",
                "first_name": "Jude"
            },
            {
                "last_name": "Rice",
                "first_name": "Malena"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The properties of exoplanet host stars are traditionally characterized through a detailed forward-modeling analysis of high-resolution spectra. However, many exoplanet radial velocity surveys employ iodine-cell-calibrated spectrographs, such that the vast majority of spectra obtained include an imprinted forest of iodine absorption lines. For surveys that use iodine cells, iodine-free \"template\" spectra must be separately obtained for precise stellar characterization. These template spectra often require extensive additional observing time to obtain, and they are not always feasible to obtain for faint stars. In this paper, we demonstrate that machine learning methods can be applied to infer stellar parameters and chemical abundances from iodine-imprinted spectra with high accuracy and precision. The methods presented in this work are broadly applicable to any iodine-cell-calibrated spectrograph. We make publicly available our spectroscopic pipeline, the Cannon HIRES Iodine Pipeline (CHIP), which derives stellar parameters and 15 chemical abundances from iodine-imprinted spectra of FGK stars and which has been set up for ease of use with Keck/HIRES spectra. Our proof-of-concept offers an efficient new avenue to rapidly estimate a large number of stellar parameters even in the absence of an iodine-free template spectrum. ",
        "title": "Inferring Stellar Parameters from Iodine-Imprinted Keck/HIRES Spectra  with Machine Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06853",
        "abstract_url": "http://arxiv.org/abs/2401.06853",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Siheng"
            },
            {
                "last_name": "Payani",
                "first_name": "Ali"
            },
            {
                "last_name": "Kompella",
                "first_name": "Ramana"
            },
            {
                "last_name": "Fekri",
                "first_name": "Faramarz"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) learn temporal concepts from the co-occurrence of related tokens in a sequence. Compared with conventional text generation, temporal reasoning, which reaches a conclusion based on mathematical, logical and commonsense knowledge, is more challenging. In this paper, we propose TempGraph-LLM, a new paradigm towards text-based temporal reasoning. To be specific, we first teach LLMs to translate the context into a temporal graph. A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for pre-training on this task. We prove in experiments that LLMs benefit from the pre-training on other tasks. On top of that, we guide LLMs to perform symbolic reasoning with the strategies of Chain of Thoughts (CoTs) bootstrapping and special data augmentation. We observe that CoTs with symbolic reasoning bring more consistent and reliable results than those using free text. ",
        "title": "Large Language Models Can Learn Temporal Reasoning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06855",
        "abstract_url": "http://arxiv.org/abs/2401.06855",
        "authors": [
            {
                "last_name": "Mishra",
                "first_name": "Abhika"
            },
            {
                "last_name": "Asai",
                "first_name": "Akari"
            },
            {
                "last_name": "Balachandran",
                "first_name": "Vidhisha"
            },
            {
                "last_name": "Wang",
                "first_name": "Yizhong"
            },
            {
                "last_name": "Neubig",
                "first_name": "Graham"
            },
            {
                "last_name": "Tsvetkov",
                "first_name": "Yulia"
            },
            {
                "last_name": "Hajishirzi",
                "first_name": "Hannaneh"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT on fine-grained hallucination detection by a large margin though a large room for future improvement still exists. FAVA's suggested edits also improve the factuality of LM-generated text, resulting in 5-10% FActScore improvements. ",
        "title": "Fine-grained Hallucination Detection and Editing for Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06857",
        "abstract_url": "http://arxiv.org/abs/2401.06857",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Jason"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  We show that finding rank-1, rank-2, and rank-3 decompositions of a 3D tensor over a fixed finite field can be done in polynomial time. However, if some cells in the tensor are allowed to have arbitrary values, then rank-2 is NP-hard over the integers modulo 2. We also explore rank-1 decomposition of a 3D tensor and of a matrix where some cells are allowed to have arbitrary values. ",
        "title": "Low-Rank Tensor Decomposition over Finite Fields",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06859",
        "abstract_url": "http://arxiv.org/abs/2401.06859",
        "authors": [
            {
                "last_name": "Atiya",
                "first_name": "Yasseen Sadoon"
            },
            {
                "last_name": "Mobini",
                "first_name": "Zahra"
            },
            {
                "last_name": "Ngo",
                "first_name": "Hien Quoc"
            },
            {
                "last_name": "Matthaiou",
                "first_name": "Michail"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we investigate joint power control and access point (AP) selection scheme in a cell-free massive multiple-input multiple-output (CF-mMIMO) system under an active eavesdropping attack, where an eavesdropper tries to overhear the signal sent to one of the legitimate users by contaminating the uplink channel estimation. We formulate a joint optimization problem to minimize the eavesdropping spectral efficiency (SE) while guaranteeing a given SE requirement at legitimate users. The challenging formulated problem is converted into a more tractable form and an efficient low-complexity accelerated projected gradient (APG)-based approach is proposed to solve it. Our findings reveal that the proposed joint optimization approach significantly outperforms the heuristic approaches in terms of secrecy SE (SSE). For instance, the $50\\%$ likely SSE performance of the proposed approach is $265\\%$ higher than that of equal power allocation and random AP selection scheme. ",
        "title": "Joint Power Optimization and AP Selection for Secure Cell-Free Massive  MIMO",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06864",
        "abstract_url": "http://arxiv.org/abs/2401.06864",
        "authors": [
            {
                "last_name": "Balgi",
                "first_name": "Sourabh"
            },
            {
                "last_name": "Daoud",
                "first_name": "Adel"
            },
            {
                "last_name": "Pe\u00f1a",
                "first_name": "Jose M."
            },
            {
                "last_name": "Wodtke",
                "first_name": "Geoffrey T."
            },
            {
                "last_name": "Zhou",
                "first_name": "Jesse"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Social science theories often postulate causal relationships among a set of variables or events. Although directed acyclic graphs (DAGs) are increasingly used to represent these theories, their full potential has not yet been realized in practice. As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships. Nevertheless, to simplify the task of empirical evaluation, researchers tend to invoke such assumptions anyway, even though they are typically arbitrary and do not reflect any theoretical content or prior knowledge. Moreover, functional form assumptions can engender bias, whenever they fail to accurately capture the complexity of the causal system under investigation. In this article, we introduce causal-graphical normalizing flows (cGNFs), a novel approach to causal inference that leverages deep neural networks to empirically evaluate theories represented as DAGs. Unlike conventional approaches, cGNFs model the full joint distribution of the data according to a DAG supplied by the analyst, without relying on stringent assumptions about functional form. In this way, the method allows for flexible, semi-parametric estimation of any causal estimand that can be identified from the DAG, including total effects, conditional effects, direct and indirect effects, and path-specific effects. We illustrate the method with a reanalysis of Blau and Duncan's (1967) model of status attainment and Zhou's (2019) model of conditional versus controlled mobility. To facilitate adoption, we provide open-source software together with a series of online tutorials for implementing cGNFs. The article concludes with a discussion of current limitations and directions for future development. ",
        "title": "Deep Learning With DAGs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06866",
        "abstract_url": "http://arxiv.org/abs/2401.06866",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Yubin"
            },
            {
                "last_name": "Xu",
                "first_name": "Xuhai"
            },
            {
                "last_name": "McDuff",
                "first_name": "Daniel"
            },
            {
                "last_name": "Breazeal",
                "first_name": "Cynthia"
            },
            {
                "last_name": "Park",
                "first_name": "Hae Won"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is important. This paper investigates the capacity of LLMs to deliver multi-modal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps, GLOBEM, AW_FB, MIT-BIH & MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance. ",
        "title": "Health-LLM: Large Language Models for Health Prediction via Wearable  Sensor Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06868",
        "abstract_url": "http://arxiv.org/abs/2401.06868",
        "authors": [
            {
                "last_name": "Campello",
                "first_name": "Betania Silva Carneiro"
            },
            {
                "last_name": "Duarte",
                "first_name": "Leonardo Tomazeli"
            },
            {
                "last_name": "Romano",
                "first_name": "Jo\u00e3o Marcos Travassos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multicriteria decision analysis (MCDA) is a widely used tool to support decisions in which a set of alternatives should be ranked or classified based on multiple criteria. Recent studies in MCDA have shown the relevance of considering not only current evaluations of each criterion but also past data. Past-data-based approaches carry new challenges, especially in time-varying environments. This study deals with this challenge via essential tools of signal processing, such as tensorial representations and adaptive prediction. More specifically, we structure the criteria' past data as a tensor and, by applying adaptive prediction, we compose signals with these prediction values of the criteria. Besides, we transform the prediction in the time domain into a most favorable decision making domain, called the feature domain. We present a novel extension of the MCDA method PROMETHEE II, aimed at addressing the tensor in the feature domain to obtain a ranking of alternatives. Numerical experiments were performed using real-world time series, and our approach is compared with other existing strategies. The results highlight the relevance and efficiency of our proposal, especially for nonstationary time series. ",
        "title": "Multicriteria decision support employing adaptive prediction in a  tensor-based feature representation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06872",
        "abstract_url": "http://arxiv.org/abs/2401.06872",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "S."
            },
            {
                "last_name": "Magpantay",
                "first_name": "F. M. G."
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Edge-based percolation methods can be used to analyze disease transmission on complex social networks. This allows us to include complex social heterogeneity in our models while maintaining tractability. Here we review the seminal works on this field by Newman et al (2001); Newman (2002, 2003), and Miller et al (2012). We present a systematic discussion of the theoretical background behind these models, including an extensive derivation of the major results. We also connect these results relate back to the classical literature in random graph theory Molloy and Reed (1995, 1998). Finally, we also present an accompanying R package that takes epidemic and network parameters as input and generates estimates of the epidemic trajectory and final size. This manuscript and the R package was developed to help researchers easily understand and use network models to investigate the interaction between different community structures and disease transmission. ",
        "title": "Disease Transmission on Random Graphs Using Edge-Based Percolation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06874",
        "abstract_url": "http://arxiv.org/abs/2401.06874",
        "authors": [
            {
                "last_name": "Miao",
                "first_name": "Sisi"
            },
            {
                "last_name": "Mandelbaum",
                "first_name": "Jonathan"
            },
            {
                "last_name": "J\u00e4kel",
                "first_name": "Holger"
            },
            {
                "last_name": "Schmalen",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Quantum low-density parity-check (QLDPC) codes are among the most promising candidates for future quantum error correction schemes. However, a limited number of short to moderate-length QLDPC codes have been designed and their decoding performance is sub-optimal with a quaternary belief propagation (BP) decoder due to unavoidable short cycles in their Tanner graphs. In this letter, we propose a novel joint code and decoder design for QLDPC codes. The constructed codes have a minimum distance of about the square root of the block length. In addition, it is, to the best of our knowledge, the first QLDPC code family where BP decoding is not impaired by short cycles of length 4. This is achieved by using an ensemble BP decoder mitigating the influence of assembled short cycles. We outline two code construction methods based on classical quasi-cyclic codes and finite geometry codes. Numerical results demonstrate outstanding decoding performance over depolarizing channels. ",
        "title": "A Joint Code and Belief Propagation Decoder Design for Quantum LDPC  Codes",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06877",
        "abstract_url": "http://arxiv.org/abs/2401.06877",
        "authors": [
            {
                "last_name": "Mehta",
                "first_name": "Maitrey"
            },
            {
                "last_name": "Pyatkin",
                "first_name": "Valentina"
            },
            {
                "last_name": "Srikumar",
                "first_name": "Vivek"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants. ",
        "title": "Promptly Predicting Structures: The Return of Inference",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06883",
        "abstract_url": "http://arxiv.org/abs/2401.06883",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Qinyi"
            },
            {
                "last_name": "Khalil",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Shakya",
                "first_name": "Ronas"
            },
            {
                "last_name": "Jovanovic",
                "first_name": "Jelena"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Privacy poses a significant obstacle to the progress of learning analytics (LA), presenting challenges like inadequate anonymization and data misuse that current solutions struggle to address. Synthetic data emerges as a potential remedy, offering robust privacy protection. However, prior LA research on synthetic data lacks thorough evaluation, essential for assessing the delicate balance between privacy and data utility. Synthetic data must not only enhance privacy but also remain practical for data analytics. Moreover, diverse LA scenarios come with varying privacy and utility needs, making the selection of an appropriate synthetic data approach a pressing challenge. To address these gaps, we propose a comprehensive evaluation of synthetic data, which encompasses three dimensions of synthetic data quality, namely resemblance, utility, and privacy. We apply this evaluation to three distinct LA datasets, using three different synthetic data generation methods. Our results show that synthetic data can maintain similar utility (i.e., predictive performance) as real data, while preserving privacy. Furthermore, considering different privacy and data utility requirements in different LA scenarios, we make customized recommendations for synthetic data generation. This paper not only presents a comprehensive evaluation of synthetic data but also illustrates its potential in mitigating privacy concerns within the field of LA, thus contributing to a wider application of synthetic data in LA and promoting a better practice for open science. ",
        "title": "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data  Generation and Evaluation in Learning Analytics",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06885",
        "abstract_url": "http://arxiv.org/abs/2401.06885",
        "authors": [
            {
                "last_name": "Afifi",
                "first_name": "Salma"
            },
            {
                "last_name": "Sunny",
                "first_name": "Febin"
            },
            {
                "last_name": "Nikdast",
                "first_name": "Mahdi"
            },
            {
                "last_name": "Pasricha",
                "first_name": "Sudeep"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "LG"
        ],
        "abstract": "  In the rapidly evolving landscape of artificial intelligence, large language models (LLMs) and graph processing have emerged as transformative technologies for natural language processing (NLP), computer vision, and graph-structured data applications. However, the complex structures of these models pose challenges for acceleration on conventional electronic platforms. In this paper, we describe novel hardware accelerators based on silicon photonics to accelerate transformer neural networks that are used in LLMs and graph neural networks for graph data processing. Our analysis demonstrates that both hardware accelerators achieve at least 10.2x throughput improvement and 3.8x better energy efficiency over multiple state-of-the-art electronic hardware accelerators designed for LLMs and graph processing. ",
        "title": "Accelerating Neural Networks for Large Language Models and Graph  Processing with Silicon Photonics",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06889",
        "abstract_url": "http://arxiv.org/abs/2401.06889",
        "authors": [
            {
                "last_name": "Meluso",
                "first_name": "John"
            },
            {
                "last_name": "Casari",
                "first_name": "Amanda"
            },
            {
                "last_name": "McLaughlin",
                "first_name": "Katie"
            },
            {
                "last_name": "Trujillo",
                "first_name": "Milo Z."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Invisible labor is work that is not fully visible, not appropriately compensated, or both. In open source software (OSS) ecosystems, essential tasks that do not involve code (like content moderation) often become invisible to the detriment of individuals and organizations. However, invisible labor is so difficult to measure that we do not know how much of OSS activities are invisible. Our study addresses this challenge, demonstrating that roughly half of OSS work is invisible. We do this by developing a survey technique with cognitive anchoring that measures OSS developer self-assessments of labor visibility and attribution. Survey respondents (n=142) reported that their work is more likely to be nonvisible or partially visible (i.e. visible to at most 1 other person) than fully visible (i.e. visible to 2 or more people). Furthermore, cognitively anchoring participants to the idea of high work visibility increased perceptions of labor visibility and decreased visibility importance compared to anchoring to low work visibility. This suggests that advertising OSS activities as \"open\" may not make labor visible to most people, but rather lead contributors to overestimate labor visibility. We therefore add to a growing body of evidence that designing systems that recognize all kinds of labor as legitimate contributions is likely to improve fairness in software development while providing greater transparency into work designs that help organizations and communities achieve their goals. ",
        "title": "Invisible Labor in Open Source Software Ecosystems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06890",
        "abstract_url": "http://arxiv.org/abs/2401.06890",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Zhili"
            },
            {
                "last_name": "Moshkovitz",
                "first_name": "Michal"
            },
            {
                "last_name": "Di Castro",
                "first_name": "Dotan"
            },
            {
                "last_name": "Kolter",
                "first_name": "J. Zico"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Concept explanation is a popular approach for examining how human-interpretable concepts impact the predictions of a model. However, most existing methods for concept explanations are tailored to specific models. To address this issue, this paper focuses on model-agnostic measures. Specifically, we propose an approach to concept explanations that satisfy three natural axioms: linearity, recursivity, and similarity. We then establish connections with previous concept explanation methods, offering insight into their varying semantic meanings. Experimentally, we demonstrate the utility of the new method by applying it in different scenarios: for model selection, optimizer selection, and model improvement using a kind of prompt editing for zero-shot vision language models. ",
        "title": "An Axiomatic Approach to Model-Agnostic Concept Explanations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06893",
        "abstract_url": "http://arxiv.org/abs/2401.06893",
        "authors": [
            {
                "last_name": "Middleton",
                "first_name": "Jon"
            },
            {
                "last_name": "Bauer",
                "first_name": "Marko"
            },
            {
                "last_name": "Sheng",
                "first_name": "Kaining"
            },
            {
                "last_name": "Johansen",
                "first_name": "Jacob"
            },
            {
                "last_name": "Perslev",
                "first_name": "Mathias"
            },
            {
                "last_name": "Ingala",
                "first_name": "Silvia"
            },
            {
                "last_name": "Nielsen",
                "first_name": "Mads"
            },
            {
                "last_name": "Pai",
                "first_name": "Akshay"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The identification and localisation of pathological tissues in medical images continues to command much attention among deep learning practitioners. When trained on abundant datasets, deep neural networks can match or exceed human performance. However, the scarcity of annotated data complicates the training of these models. Data augmentation techniques can compensate for a lack of training samples. However, many commonly used augmentation methods can fail to provide meaningful samples during model fitting. We present local gamma augmentation, a technique for introducing new instances of intensities in pathological tissues. We leverage local gamma augmentation to compensate for a bias in intensities corresponding to ischemic stroke lesions in human brain MRIs. On three datasets, we show how local gamma augmentation can improve the image-level sensitivity of a deep neural network tasked with ischemic lesion segmentation on magnetic resonance images. ",
        "title": "Local Gamma Augmentation for Ischemic Stroke Lesion Segmentation on MRI",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06894",
        "abstract_url": "http://arxiv.org/abs/2401.06894",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Yinbin"
            },
            {
                "last_name": "Tuninetti",
                "first_name": "Daniela"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Coded caching is a technique that leverages locally cached contents at the end users to reduce the network's peak-time communication load. Coded caching has been shown to achieve significant performance gains compared to uncoded schemes and is thus considered a promising technique to boost performance in future networks by effectively trading off bandwidth for storage. The original coded caching model introduced by Maddah-Ali and Niesen does not consider the case where some users involved in the placement phase, may be offline during the delivery phase. If so, the delivery may not start or it may be wasteful to perform the delivery with fictitious demands for the offline users. In addition, the active users may require their demand to be kept private. This paper formally defines a coded caching system where some users are offline, and investigates the optimal performance with and without demand privacy against colluding users. For this novel coded caching model with offline users, achievable and converse bounds are proposed. These bounds are shown to meet under certain conditions, and otherwise to be to within a constant multiplicative gap of one another. In addition, the proposed achievable schemes have lower subpacketization and lower load compared to baseline schemes (that trivially extend known schemes so as to accommodate for privacy) in some memory regimes. ",
        "title": "On Coded Caching Systems with Offline Users, with and without Demand  Privacy against Colluding Users",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06898",
        "abstract_url": "http://arxiv.org/abs/2401.06898",
        "authors": [
            {
                "last_name": "Heddes",
                "first_name": "Mike"
            },
            {
                "last_name": "Srinivasa",
                "first_name": "Narayan"
            },
            {
                "last_name": "Givargis",
                "first_name": "Tony"
            },
            {
                "last_name": "Nicolau",
                "first_name": "Alexandru"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on the machines that can run them. Sparsification of ANNs is often motivated by time, memory and energy savings only during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients during training. We propose an efficient, always-sparse training algorithm with excellent scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during training and inference. Moreover, our guided stochastic exploration algorithm improves over the accuracy of previous sparse training methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG, and ViT models, and compare it against a range of sparsification methods. ",
        "title": "Always-Sparse Training by Growing Connections with Guided Stochastic  Exploration",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06899",
        "abstract_url": "http://arxiv.org/abs/2401.06899",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xiaofei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This article explores the critical role of statistical analysis in precision medicine. It discusses how personalized healthcare is enhanced by statistical methods that interpret complex, multidimensional datasets, focusing on predictive modeling, machine learning algorithms, and data visualization techniques. The paper addresses challenges in data integration and interpretation, particularly with diverse data sources like electronic health records (EHRs) and genomic data. It also delves into ethical considerations such as patient privacy and data security. In addition, the paper highlights the evolution of statistical analysis in medicine, core statistical methodologies in precision medicine, and future directions in the field, emphasizing the integration of artificial intelligence (AI) and machine learning (ML). ",
        "title": "Analyses and Concerns in Precision Medicine: A Statistical Perspective",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06901",
        "abstract_url": "http://arxiv.org/abs/2401.06901",
        "authors": [
            {
                "last_name": "Schneeberger",
                "first_name": "Michael"
            },
            {
                "last_name": "Mastellone",
                "first_name": "Silvia"
            },
            {
                "last_name": "D\u00f6rfler",
                "first_name": "Florian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents a novel safety filter framework based on Control Barrier Functions (CBFs) and Control Lyapunov-like Functions (CLFs). The CBF guarantees forward invariance of the safe set, constraining system trajectories within state constraints, while the CLF guides the system away from unsafe states towards a nominal region, preserving the performance of a nominal controller. The first part of this work focuses on determining compatible CBF and CLF in the presence of linear or quadratic input constraints. This is achieved by formulating the CBF and CLF conditions, along with the input constraints, as Sum of Squares (SOS) constraints using Putinar's Positivstellensatz. For solving the resulting SOS optimization problem, we employ an alternating algorithm that simultaneously searches for a feasible controller in the class of rational functions of the state. The second part of this work details the implementation of the safety filter as a Quadratically Constrained Quadratic Program (QCQP), whose constraints encode the CBF and CLF conditions as well as the input constraints. To avoid the chattering effect and guarantee the uniqueness and Lipschitz continuity of solutions, the state-dependent inequality constraints of the QCQP are selected to be sufficiently regular. Finally, we demonstrate the method on a detailed case study involving the control of a three-phase ac/dc power converter connected to an infinite bus. ",
        "title": "Advanced safety filter based on SOS Control Barrier and Lyapunov  Functions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06908",
        "abstract_url": "http://arxiv.org/abs/2401.06908",
        "authors": [
            {
                "last_name": "Mach",
                "first_name": "Pavel"
            },
            {
                "last_name": "Becvar",
                "first_name": "Zdenek"
            },
            {
                "last_name": "Nikooroo",
                "first_name": "Mohammadsaleh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  In this paper, we focus on offloading a computing task from a user equipment (UE) to a multi-access edge computing (MEC) server via multi-hop relaying. We assume a general relaying case where relays are energy-constrained devices, such as other UEs, internet of things (IoT) devices, or unmanned aerial vehicles. To this end, we formulate the problem as a minimization of the sum energy consumed by the energy-constrained devices under the constraint on the maximum requested time of the task processing. Then, we propose a multi-hop relaying combining half and full duplexes at each individual relay involved in the offloading. We proof that the proposed multi-hop relaying is convex, thus it can be optimized by conventional convex optimization methods. We show our proposal outperforms existing multi-hop relaying schemes in terms of probability that tasks are processed within required time by up to 38\\% and, at the same time, decreases energy consumption by up to 28%. ",
        "title": "Multi-hop Relaying with Mixed Half and Full Duplex Relays for Offloading  to MEC",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06910",
        "abstract_url": "http://arxiv.org/abs/2401.06910",
        "authors": [
            {
                "last_name": "Laitz",
                "first_name": "Thiago"
            },
            {
                "last_name": "Papakostas",
                "first_name": "Konstantinos"
            },
            {
                "last_name": "Lotufo",
                "first_name": "Roberto"
            },
            {
                "last_name": "Nogueira",
                "first_name": "Rodrigo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Despite multi-billion parameter neural rankers being common components of state-of-the-art information retrieval pipelines, they are rarely used in production due to the enormous amount of compute required for inference. In this work, we propose a new method for distilling large rankers into their smaller versions focusing on out-of-domain effectiveness. We introduce InRanker, a version of monoT5 distilled from monoT5-3B with increased effectiveness on out-of-domain scenarios. Our key insight is to use language models and rerankers to generate as much as possible synthetic \"in-domain\" training data, i.e., data that closely resembles the data that will be seen at retrieval time. The pipeline consists of two distillation phases that do not require additional user queries or manual annotations: (1) training on existing supervised soft teacher labels, and (2) training on teacher soft labels for synthetic queries generated using a large language model. Consequently, models like monoT5-60M and monoT5-220M improved their effectiveness by using the teacher's knowledge, despite being 50x and 13x smaller, respectively. Models and code are available at https://github.com/unicamp-dl/InRanker. ",
        "title": "InRanker: Distilled Rankers for Zero-shot Information Retrieval",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06913",
        "abstract_url": "http://arxiv.org/abs/2401.06913",
        "authors": [
            {
                "last_name": "Ryu",
                "first_name": "Myeonghoon"
            },
            {
                "last_name": "Oh",
                "first_name": "Hongseok"
            },
            {
                "last_name": "Lee",
                "first_name": "Suji"
            },
            {
                "last_name": "Park",
                "first_name": "Han"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG",
            "MM"
        ],
        "abstract": "  In this study, we introduce a new augmentation technique to enhance the resilience of sound event classification (SEC) systems against device variability through the use of CycleGAN. We also present a unique dataset to evaluate this method. As SEC systems become increasingly common, it is crucial that they work well with audio from diverse recording devices. Our method addresses limited device diversity in training data by enabling unpaired training to transform input spectrograms as if they are recorded on a different device. Our experiments show that our approach outperforms existing methods in generalization by 5.2% - 11.5% in weighted f1 score. Additionally, it surpasses the current methods in adaptability across diverse recording devices by achieving a 6.5% - 12.8% improvement in weighted f1 score. ",
        "title": "Microphone Conversion: Mitigating Device Variability in Sound Event  Classification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06915",
        "abstract_url": "http://arxiv.org/abs/2401.06915",
        "authors": [
            {
                "last_name": "Reddy",
                "first_name": "Varshini"
            },
            {
                "last_name": "Koncel-Kedziorski",
                "first_name": "Rik"
            },
            {
                "last_name": "Lai",
                "first_name": "Viet Dac"
            },
            {
                "last_name": "Tanner",
                "first_name": "Chris"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems. ",
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06916",
        "abstract_url": "http://arxiv.org/abs/2401.06916",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  While emerging adaptive cruise control (ACC) technologies are making their way into more vehicles, they also expose a vulnerability to potential malicious cyberattacks. Previous research has typically focused on constant or stochastic attacks without explicitly addressing their malicious and covert characteristics. As a result, these attacks may inadvertently benefit the compromised vehicles, inconsistent with real-world scenarios. In contrast, we establish an analytical framework to model and synthesize a range of candidate attacks, offering a physical interpretation from the attacker's standpoint. Specifically, we introduce a mathematical framework that describes mixed traffic scenarios, comprising ACC vehicles and human-driven vehicles (HDVs), grounded in car-following dynamics. Within this framework, we synthesize and integrate a class of false data injection attacks into ACC sensor measurements, influencing traffic flow dynamics. As a first-of-its-kind study, this work provides an analytical characterization of attacks, emphasizing their malicious and stealthy attributes while explicitly accounting for vehicle driving behavior, thereby yielding a set of candidate attacks with physical interpretability. To demonstrate the modeling process, we perform a series of numerical simulations to holistically assess the effects of attacks on car-following dynamics, traffic efficiency, and vehicular fuel consumption. The primary findings indicate that strategically synthesized candidate attacks can cause significant disruptions to the traffic flow while altering the driving behavior of ACC vehicles in a subtle fashion to remain stealthy, which is supported by a series of analytical results. ",
        "title": "An Analytical Framework for Modeling and Synthesizing Malicious Attacks  on ACC Vehicles",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06918",
        "abstract_url": "http://arxiv.org/abs/2401.06918",
        "authors": [
            {
                "last_name": "Brown",
                "first_name": "Ariana N."
            },
            {
                "last_name": "Landman",
                "first_name": "Malena Sabat\u00e9"
            },
            {
                "last_name": "Nagy",
                "first_name": "James G."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This study investigates the iterative regularization properties of two Krylov methods for solving large-scale ill-posed problems: the changing minimal residual Hessenberg method (CMRH) and a novel hybrid variant called the hybrid changing minimal residual Hessenberg method (H-CMRH). Both methods share the advantages of avoiding inner products, making them efficient and highly parallelizable, and particularly suited for implementations that exploit randomization and mixed precision arithmetic. Theoretical results and extensive numerical experiments suggest that H-CMRH exhibits comparable performance to the established hybrid GMRES method in terms of stabilizing semiconvergence, but H-CMRH has does not require any inner products, and requires less work and storage per iteration. ",
        "title": "H-CMRH: a novel inner product free hybrid Krylov method for large-scale  inverse problems",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06920",
        "abstract_url": "http://arxiv.org/abs/2401.06920",
        "authors": [
            {
                "last_name": "Vergho",
                "first_name": "Tyler"
            },
            {
                "last_name": "Godbout",
                "first_name": "Jean-Francois"
            },
            {
                "last_name": "Rabbany",
                "first_name": "Reihaneh"
            },
            {
                "last_name": "Pelrine",
                "first_name": "Kellin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent large language models (LLMs) have been shown to be effective for misinformation detection. However, the choice of LLMs for experiments varies widely, leading to uncertain conclusions. In particular, GPT-4 is known to be strong in this domain, but it is closed source, potentially expensive, and can show instability between different versions. Meanwhile, alternative LLMs have given mixed results. In this work, we show that Zephyr-7b presents a consistently viable alternative, overcoming key limitations of commonly used approaches like Llama-2 and GPT-3.5. This provides the research community with a solid open-source option and shows open-source models are gradually catching up on this task. We then highlight how GPT-3.5 exhibits unstable performance, such that this very widely used model could provide misleading results in misinformation detection. Finally, we validate new tools including approaches to structured output and the latest version of GPT-4 (Turbo), showing they do not compromise performance, thus unlocking them for future research and potentially enabling more complex pipelines for misinformation mitigation. ",
        "title": "Comparing GPT-4 and Open-Source Language Models in Misinformation  Mitigation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06922",
        "abstract_url": "http://arxiv.org/abs/2401.06922",
        "authors": [
            {
                "last_name": "Lotfi",
                "first_name": "Fatemeh"
            },
            {
                "last_name": "Afghah",
                "first_name": "Fatemeh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI"
        ],
        "abstract": "  With emerging applications such as autonomous driving, smart cities, and smart factories, network slicing has become an essential component of 5G and beyond networks as a means of catering to a service-aware network. However, managing different network slices while maintaining quality of services (QoS) is a challenge in a dynamic environment. To address this issue, this paper leverages the heterogeneous experiences of distributed units (DUs) in ORAN systems and introduces a novel approach to ORAN slicing xApp using distributed deep reinforcement learning (DDRL). Additionally, to enhance the decision-making performance of the RL agent, a prediction rApp based on long short-term memory (LSTM) is incorporated to provide additional information from the dynamic environment to the xApp. Simulation results demonstrate significant improvements in network performance, particularly in reducing QoS violations. This emphasizes the importance of using the prediction rApp and distributed actors' information jointly as part of a dynamic xApp. ",
        "title": "Open RAN LSTM Traffic Prediction and Slice Management using Deep  Reinforcement Learning",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06923",
        "abstract_url": "http://arxiv.org/abs/2401.06923",
        "authors": [
            {
                "last_name": "Lyu",
                "first_name": "Zimeng"
            },
            {
                "last_name": "Ororbia",
                "first_name": "Alexander"
            },
            {
                "last_name": "Li",
                "first_name": "Rui"
            },
            {
                "last_name": "Desell",
                "first_name": "Travis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Parameter prediction is essential for many applications, facilitating insightful interpretation and decision-making. However, in many real life domains, such as power systems, medicine, and engineering, it can be very expensive to acquire ground truth labels for certain datasets as they may require extensive and expensive laboratory testing. In this work, we introduce a semi-supervised learning approach based on topological projections in self-organizing maps (SOMs), which significantly reduces the required number of labeled data points to perform parameter prediction, effectively exploiting information contained in large unlabeled datasets. Our proposed method first trains SOMs on unlabeled data and then a minimal number of available labeled data points are ultimately assigned to key best matching units (BMU). The values estimated for newly-encountered data points are computed utilizing the average of the $n$ closest labeled data points in the SOM's U-matrix in tandem with a topological shortest path distance calculation scheme. Our results indicate that the proposed semi-supervised model significantly outperforms traditional regression techniques, including linear and polynomial regression, Gaussian process regression, K-nearest neighbors, as well as various deep neural network models. ",
        "title": "Minimally Supervised Learning using Topological Projections in  Self-Organizing Maps",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06925",
        "abstract_url": "http://arxiv.org/abs/2401.06925",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Leihao"
            },
            {
                "last_name": "Zoeter",
                "first_name": "Onno"
            },
            {
                "last_name": "Mooij",
                "first_name": "Joris M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection bias and how the conditioning operation helps with modeling of real-world problems. ",
        "title": "Modeling Latent Selection with Structural Causal Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06930",
        "abstract_url": "http://arxiv.org/abs/2401.06930",
        "authors": [
            {
                "last_name": "Diallo",
                "first_name": "Aissatou"
            },
            {
                "last_name": "Bikakis",
                "first_name": "Antonis"
            },
            {
                "last_name": "Dickens",
                "first_name": "Luke"
            },
            {
                "last_name": "Hunter",
                "first_name": "Anthony"
            },
            {
                "last_name": "Miller",
                "first_name": "Rob"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Decoding the core of procedural texts, exemplified by cooking recipes, is crucial for intelligent reasoning and instruction automation. Procedural texts can be comprehensively defined as a sequential chain of steps to accomplish a task employing resources. From a cooking perspective, these instructions can be interpreted as a series of modifications to a food preparation, which initially comprises a set of ingredients. These changes involve transformations of comestible resources. For a model to effectively reason about cooking recipes, it must accurately discern and understand the inputs and outputs of intermediate steps within the recipe. Aiming to address this, we present a new corpus of cooking recipes enriched with descriptions of intermediate steps of the recipes that explicate the input and output for each step. We discuss the data collection process, investigate and provide baseline models based on T5 and GPT-3.5. This work presents a challenging task and insight into commonsense reasoning and procedural text generation. ",
        "title": "PizzaCommonSense: Learning to Model Commonsense Reasoning about  Intermediate Steps in Cooking Recipes",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06935",
        "abstract_url": "http://arxiv.org/abs/2401.06935",
        "authors": [
            {
                "last_name": "Robinson",
                "first_name": "Kevin"
            },
            {
                "last_name": "Kudugunta",
                "first_name": "Sneha"
            },
            {
                "last_name": "Stella",
                "first_name": "Romina"
            },
            {
                "last_name": "Dev",
                "first_name": "Sunipa"
            },
            {
                "last_name": "Bastings",
                "first_name": "Jasmijn"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY"
        ],
        "abstract": "  Misgendering is the act of referring to someone in a way that does not reflect their gender identity. Translation systems, including foundation models capable of translation, can produce errors that result in misgendering harms. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally underpresented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both dedicated neural machine translation systems and foundation models, and show that all systems exhibit errors resulting in misgendering harms, even in high resource languages. ",
        "title": "MiTTenS: A Dataset for Evaluating Misgendering in Translation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06936",
        "abstract_url": "http://arxiv.org/abs/2401.06936",
        "authors": [
            {
                "last_name": "Hua",
                "first_name": "Xinru"
            },
            {
                "last_name": "Ahmad",
                "first_name": "Rasool"
            },
            {
                "last_name": "Blanchet",
                "first_name": "Jose"
            },
            {
                "last_name": "Cai",
                "first_name": "Wei"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In the field of computational physics and material science, the efficient sampling of rare events occurring at atomic scale is crucial. It aids in understanding mechanisms behind a wide range of important phenomena, including protein folding, conformal changes, chemical reactions and materials diffusion and deformation. Traditional simulation methods, such as Molecular Dynamics and Monte Carlo, often prove inefficient in capturing the timescale of these rare events by brute force. In this paper, we introduce a practical approach by combining the idea of importance sampling with deep neural networks (DNNs) that enhance the sampling of these rare events. In particular, we approximate the variance-free bias potential function with DNNs which is trained to maximize the probability of rare event transition under the importance potential function. This method is easily scalable to high-dimensional problems and provides robust statistical guarantees on the accuracy of the estimated probability of rare event transition. Furthermore, our algorithm can actively generate and learn from any successful samples, which is a novel improvement over existing methods. Using a 2D system as a test bed, we provide comparisons between results obtained from different training strategies, traditional Monte Carlo sampling and numerically solved optimal bias potential function under different temperatures. Our numerical results demonstrate the efficacy of the DNN-based importance sampling of rare events. ",
        "title": "Accelerated Sampling of Rare Events using a Neural Network Bias  Potential",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06945",
        "abstract_url": "http://arxiv.org/abs/2401.06945",
        "authors": [
            {
                "last_name": "Cachola",
                "first_name": "Isabel"
            },
            {
                "last_name": "Cucerzan",
                "first_name": "Silviu"
            },
            {
                "last_name": "Herring",
                "first_name": "Allen"
            },
            {
                "last_name": "Mijovic",
                "first_name": "Vuksan"
            },
            {
                "last_name": "Oveson",
                "first_name": "Erik"
            },
            {
                "last_name": "Jauhar",
                "first_name": "Sujay Kumar"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Authors seeking to communicate with broader audiences often compose their ideas about the same underlying knowledge in different documents and formats -- for example, as slide decks, newsletters, reports, brochures, etc. Prior work in document generation has generally considered the creation of each separate format to be different a task, developing independent methods for generation and evaluation. This approach is suboptimal for the advancement of AI-supported content authoring from both research and application perspectives because it leads to fragmented learning processes, redundancy in models and methods, and disjointed evaluation. Thus, in our work, we consider each of these documents to be templatic views of the same underlying knowledge, and we aim to unify the generation and evaluation of these templatic views of documents. We begin by introducing an LLM-powered method to extract the most important information from an input document and represent this information in a structured format. We show that this unified representation can be used to generate multiple templatic views with no supervision and with very little guidance, improving over strong baselines. We additionally introduce a unified evaluation method that is template agnostic, and can be adapted to building document generators for heterogeneous downstream applications. Finally, we conduct a human evaluation, which shows that humans prefer 82% of the downstream documents generated with our method. Furthermore, the newly proposed evaluation metric correlates more highly with human judgement than prior metrics, while providing a unified evaluation method. ",
        "title": "Knowledge-Centric Templatic Views of Documents",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06946",
        "abstract_url": "http://arxiv.org/abs/2401.06946",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Linlin"
            },
            {
                "last_name": "Yu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Aboah",
                "first_name": "Armstrong"
            },
            {
                "last_name": "Adu-Gyamfi",
                "first_name": "Yaw"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Traffic volume data collection is a crucial aspect of transportation engineering and urban planning, as it provides vital insights into traffic patterns, congestion, and infrastructure efficiency. Traditional manual methods of traffic data collection are both time-consuming and costly. However, the emergence of modern technologies, particularly Light Detection and Ranging (LiDAR), has revolutionized the process by enabling efficient and accurate data collection. Despite the benefits of using LiDAR for traffic data collection, previous studies have identified two major limitations that have impeded its widespread adoption. These are the need for multiple LiDAR systems to obtain complete point cloud information of objects of interest, as well as the labor-intensive process of annotating 3D bounding boxes for object detection tasks. In response to these challenges, the current study proposes an innovative framework that alleviates the need for multiple LiDAR systems and simplifies the laborious 3D annotation process. To achieve this goal, the study employed a single LiDAR system, that aims at reducing the data acquisition cost and addressed its accompanying limitation of missing point cloud information by developing a Point Cloud Completion (PCC) framework to fill in missing point cloud information using point density. Furthermore, we also used zero-shot learning techniques to detect vehicles and pedestrians, as well as proposed a unique framework for extracting low to high features from the object of interest, such as height, acceleration, and speed. Using the 2D bounding box detection and extracted height information, this study is able to generate 3D bounding boxes automatically without human intervention. ",
        "title": "3D Object Detection and High-Resolution Traffic Parameters Extraction  Using Low-Resolution LiDAR Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06947",
        "abstract_url": "http://arxiv.org/abs/2401.06947",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Tong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Caiming"
            },
            {
                "last_name": "Yavuz",
                "first_name": "Semih"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yingbo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy. ",
        "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06948",
        "abstract_url": "http://arxiv.org/abs/2401.06948",
        "authors": [
            {
                "last_name": "Picard",
                "first_name": "Cyril"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Faez"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In engineering design, navigating complex decision-making landscapes demands a thorough exploration of the design, performance, and constraint spaces, often impeded by resource-intensive simulations. Data-driven methods can mitigate this challenge by harnessing historical data to delineate feasible domains, accelerate optimization, or evaluate designs. However, the implementation of these methods usually demands machine-learning expertise and multiple trials to choose the right method and hyperparameters. This makes them less accessible for numerous engineering situations. Additionally, there is an inherent trade-off between training speed and accuracy, with faster methods sometimes compromising precision. In our paper, we demonstrate that a recently released general-purpose transformer-based classification model, TabPFN, is both fast and accurate. Notably, it requires no dataset-specific training to assess new tabular data. TabPFN is a Prior-Data Fitted Network, which undergoes a one-time offline training across a broad spectrum of synthetic datasets and performs in-context learning. We evaluated TabPFN's efficacy across eight engineering design classification problems, contrasting it with seven other algorithms, including a state-of-the-art AutoML method. For these classification challenges, TabPFN consistently outperforms in speed and accuracy. It is also the most data-efficient and provides the added advantage of being differentiable and giving uncertainty estimates. Our findings advocate for the potential of pre-trained models that learn from synthetic data and require no domain-specific tuning to make data-driven engineering design accessible to a broader community and open ways to efficient general-purpose models valid across applications. Furthermore, we share a benchmark problem set for evaluating new classification algorithms in engineering design. ",
        "title": "Fast and Accurate Zero-Training Classification for Tabular Engineering  Data",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06949",
        "abstract_url": "http://arxiv.org/abs/2401.06949",
        "authors": [
            {
                "last_name": "Darvish",
                "first_name": "Kourosh"
            },
            {
                "last_name": "Skreta",
                "first_name": "Marta"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yuchi"
            },
            {
                "last_name": "Yoshikawa",
                "first_name": "Naruki"
            },
            {
                "last_name": "Som",
                "first_name": "Sagnik"
            },
            {
                "last_name": "Bogdanovic",
                "first_name": "Miroslav"
            },
            {
                "last_name": "Cao",
                "first_name": "Yang"
            },
            {
                "last_name": "Hao",
                "first_name": "Han"
            },
            {
                "last_name": "Xu",
                "first_name": "Haoping"
            },
            {
                "last_name": "Aspuru-Guzik",
                "first_name": "Al\u00e1n"
            },
            {
                "last_name": "Garg",
                "first_name": "Animesh"
            },
            {
                "last_name": "Shkurti",
                "first_name": "Florian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Chemistry experimentation is often resource- and labor-intensive. Despite the many benefits incurred by the integration of advanced and special-purpose lab equipment, many aspects of experimentation are still manually conducted by chemists, for example, polishing an electrode in electrochemistry experiments. Traditional lab automation infrastructure faces challenges when it comes to flexibly adapting to new chemistry experiments. To address this issue, we propose a human-friendly and flexible robotic system, ORGANA, that automates a diverse set of chemistry experiments. It is capable of interacting with chemists in the lab through natural language, using Large Language Models (LLMs). ORGANA keeps scientists informed by providing timely reports that incorporate statistical analyses. Additionally, it actively engages with users when necessary for disambiguation or troubleshooting. ORGANA can reason over user input to derive experiment goals, and plan long sequences of both high-level tasks and low-level robot actions while using feedback from the visual perception of the environment. It also supports scheduling and parallel execution for experiments that require resource allocation and coordination between multiple robots and experiment stations. We show that ORGANA successfully conducts a diverse set of chemistry experiments, including solubility assessment, pH measurement, recrystallization, and electrochemistry experiments. For the latter, we show that ORGANA robustly executes a long-horizon plan, comprising 19 steps executed in parallel, to characterize the electrochemical properties of quinone derivatives, a class of molecules used in rechargeable flow batteries. Our user study indicates that ORGANA significantly improves many aspects of user experience while reducing their physical workload. More details about ORGANA can be found at https://ac-rad.github.io/organa/. ",
        "title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and  Characterization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06951",
        "abstract_url": "http://arxiv.org/abs/2401.06951",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Bai",
                "first_name": "Zhiqi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuanxing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chenchen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ge"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiakai"
            },
            {
                "last_name": "Que",
                "first_name": "Haoran"
            },
            {
                "last_name": "Chen",
                "first_name": "Yukang"
            },
            {
                "last_name": "Su",
                "first_name": "Wenbo"
            },
            {
                "last_name": "Ge",
                "first_name": "Tiezheng"
            },
            {
                "last_name": "Fu",
                "first_name": "Jie"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenhu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we introduce two different augmentation methods on the scale and position index parameters for different samples in training. It aims to make the model more robust to the different relative differences when directly interpolating the arbitrary context length at inference. Comprehensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on challenging long-context tasks. ",
        "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06952",
        "abstract_url": "http://arxiv.org/abs/2401.06952",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Peng"
            },
            {
                "last_name": "Jin",
                "first_name": "Yaochu"
            },
            {
                "last_name": "Dai",
                "first_name": "Xuewu"
            },
            {
                "last_name": "Feng",
                "first_name": "Zhenhua"
            },
            {
                "last_name": "Cui",
                "first_name": "Dongliang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Train timetable rescheduling (TTR) aims to promptly restore the original operation of trains after unexpected disturbances or disruptions. Currently, this work is still done manually by train dispatchers, which is challenging to maintain performance under various problem instances. To mitigate this issue, this study proposes a reinforcement learning-based approach to TTR, which makes the following contributions compared to existing work. First, we design a simple directed graph to represent the TTR problem, enabling the automatic extraction of informative states through graph neural networks. Second, we reformulate the construction process of TTR's solution, not only decoupling the decision model from the problem size but also ensuring the generated scheme's feasibility. Third, we design a learning curriculum for our model to handle the scenarios with different levels of delay. Finally, a simple local search method is proposed to assist the learned decision model, which can significantly improve solution quality with little additional computation cost, further enhancing the practical value of our method. Extensive experimental results demonstrate the effectiveness of our method. The learned decision model can achieve better performance for various problems with varying degrees of train delay and different scales when compared to handcrafted rules and state-of-the-art solvers. ",
        "title": "Reinforcement Learning for Scalable Train Timetable Rescheduling with  Graph Representation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06953",
        "abstract_url": "http://arxiv.org/abs/2401.06953",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Lin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Scoring the driving performance of various drivers on a unified scale, based on how safe or economical they drive on their daily trips, is essential for the driver profile task. Connected vehicles provide the opportunity to collect real-world driving data, which is advantageous for constructing scoring models. However, the lack of pre-labeled scores impede the use of supervised regression models and the data privacy issues hinder the way of traditionally data-centralized learning on the cloud side for model training. To address them, an unsupervised scoring method is presented without the need for labels while still preserving fairness and objectiveness compared to subjective scoring strategies. Subsequently, a federated learning framework based on vehicle-cloud collaboration is proposed as a privacy-friendly alternative to centralized learning. This framework includes a consistently federated version of the scoring method to reduce the performance degradation of the global scoring model caused by the statistical heterogeneous challenge of local data. Theoretical and experimental analysis demonstrate that our federated scoring model is consistent with the utility of the centrally learned counterpart and is effective in evaluating driving performance. ",
        "title": "FedDriveScore: Federated Scoring Driving Behavior with a Mixture of  Metric Distributions",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06954",
        "abstract_url": "http://arxiv.org/abs/2401.06954",
        "authors": [
            {
                "last_name": "Ke",
                "first_name": "Zixuan"
            },
            {
                "last_name": "Kong",
                "first_name": "Weize"
            },
            {
                "last_name": "Li",
                "first_name": "Cheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Mei",
                "first_name": "Qiaozhu"
            },
            {
                "last_name": "Bendersky",
                "first_name": "Michael"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, while retrieval has long been established as an effective means of obtaining task-relevant information for humans. Retrieval-augmented Generation (RAG) are known for their effectiveness in knowledge-intensive tasks by locating relevant information and placing it within the context window of the LLM. However, the relationship between retrievers and LLMs is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-friendly information and assembling a LLM-friendly context. In this work, we examine a novel bridge model, validate the ranking and selection assumptions in retrievers in the context of RAG, and propose a training framework that chains together supervised and reinforcement learning to learn a bridge model. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks. ",
        "title": "Bridging the Preference Gap between Retrievers and LLMs",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06957",
        "abstract_url": "http://arxiv.org/abs/2401.06957",
        "authors": [
            {
                "last_name": "Nadeem",
                "first_name": "Maryam"
            },
            {
                "last_name": "Imam",
                "first_name": "Raza"
            },
            {
                "last_name": "Al-Refai",
                "first_name": "Rouqaiah"
            },
            {
                "last_name": "Chkir",
                "first_name": "Meriem"
            },
            {
                "last_name": "Hoda",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Saddik",
                "first_name": "Abdulmotaleb El"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars. ",
        "title": "EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge  Distillation",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06960",
        "abstract_url": "http://arxiv.org/abs/2401.06960",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Mang"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuoyi"
            },
            {
                "last_name": "Li",
                "first_name": "Chenyue"
            },
            {
                "last_name": "Zheng",
                "first_name": "Wei-Shi"
            },
            {
                "last_name": "Crandall",
                "first_name": "David"
            },
            {
                "last_name": "Du",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Object Re-Identification (Re-ID) aims to identify and retrieve specific objects from varying viewpoints. For a prolonged period, this field has been predominantly driven by deep convolutional neural networks. In recent years, the Transformer has witnessed remarkable advancements in computer vision, prompting an increasing body of research to delve into the application of Transformer in Re-ID. This paper provides a comprehensive review and in-depth analysis of the Transformer-based Re-ID. In categorizing existing works into Image/Video-Based Re-ID, Re-ID with limited data/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly elucidate the advantages demonstrated by the Transformer in addressing a multitude of challenges across these domains. Considering the trending unsupervised Re-ID, we propose a new Transformer baseline, UntransReID, achieving state-of-the-art performance on both single-/cross modal tasks. Besides, this survey also covers a wide range of Re-ID research objects, including progress in animal Re-ID. Given the diversity of species in animal Re-ID, we devise a standardized experimental benchmark and conduct extensive experiments to explore the applicability of Transformer for this task to facilitate future research. Finally, we discuss some important yet under-investigated open issues in the big foundation model era, we believe it will serve as a new handbook for researchers in this field. ",
        "title": "Transformer for Object Re-Identification: A Survey",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06961",
        "abstract_url": "http://arxiv.org/abs/2401.06961",
        "authors": [
            {
                "last_name": "Mao",
                "first_name": "Yujun"
            },
            {
                "last_name": "Kim",
                "first_name": "Yoon"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yilun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems. And while self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting) have been shown to be helpful, whether LLMs can make use of helpful side information such as problem-specific hints has not been investigated before. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problem-specific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. We further annotate model-generated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle. The dataset and code are available on the project website. ",
        "title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'  Mathematical Reasoning Capabilities",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06962",
        "abstract_url": "http://arxiv.org/abs/2401.06962",
        "authors": [
            {
                "last_name": "Baltag",
                "first_name": "Alexandru"
            },
            {
                "last_name": "van Benthem",
                "first_name": "Johan"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We study knowable informational dependence between empirical questions, modeled as continuous functional dependence between variables in a topological setting. We also investigate epistemic independence in topological terms and show that it is compatible with functional (but non-continuous) dependence. We then proceed to study a stronger notion of knowability based on uniformly continuous dependence. On the technical logical side, we determine the complete logics of languages that combine general functional dependence, continuous dependence, and uniformly continuous dependence. ",
        "title": "Knowability as continuity: a topological account of informational  dependence",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06967",
        "abstract_url": "http://arxiv.org/abs/2401.06967",
        "authors": [
            {
                "last_name": "Katz",
                "first_name": "B. Ross"
            },
            {
                "last_name": "Khan",
                "first_name": "Abdul"
            },
            {
                "last_name": "York-Winegar",
                "first_name": "James"
            },
            {
                "last_name": "Titus",
                "first_name": "Alexander J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Summary: NHANES, the National Health and Nutrition Examination Survey, is a program of studies led by the Centers for Disease Control and Prevention (CDC) designed to assess the health and nutritional status of adults and children in the United States (U.S.). NHANES data is frequently used by biostatisticians and clinical scientists to study health trends across the U.S., but every analysis requires extensive data management and cleaning before use and this repetitive data engineering collectively costs valuable research time and decreases the reproducibility of analyses. Here, we introduce NHANES-GCP, a Cloud Development Kit for Terraform (CDKTF) Infrastructure-as-Code (IaC) and Data Build Tool (dbt) resources built on the Google Cloud Platform (GCP) that automates the data engineering and management aspects of working with NHANES data. With current GCP pricing, NHANES-GCP costs less than $2 to run and less than $15/yr of ongoing costs for hosting the NHANES data, all while providing researchers with clean data tables that can readily be integrated for large-scale analyses. We provide examples of leveraging BigQuery ML to carry out the process of selecting data, integrating data, training machine learning and statistical models, and generating results all from a single SQL-like query. NHANES-GCP is designed to enhance the reproducibility of analyses and create a well-engineered NHANES data resource for statistics, machine learning, and fine-tuning Large Language Models (LLMs).   Availability and implementation\" NHANES-GCP is available at https://github.com/In-Vivo-Group/NHANES-GCP ",
        "title": "NHANES-GCP: Leveraging the Google Cloud Platform and BigQuery ML for  reproducible machine learning with data from the National Health and  Nutrition Examination Survey",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06969",
        "abstract_url": "http://arxiv.org/abs/2401.06969",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Kai"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiaxing"
            },
            {
                "last_name": "Xie",
                "first_name": "Weiying"
            },
            {
                "last_name": "Li",
                "first_name": "Yunsong"
            },
            {
                "last_name": "Shao",
                "first_name": "Ling"
            },
            {
                "last_name": "Lu",
                "first_name": "Shijian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Large-vocabulary object detectors (LVDs) aim to detect objects of many categories, which learn super objectness features and can locate objects accurately while applied to various downstream data. However, LVDs often struggle in recognizing the located objects due to domain discrepancy in data distribution and object vocabulary. At the other end, recent vision-language foundation models such as CLIP demonstrate superior open-vocabulary recognition capability. This paper presents KGD, a Knowledge Graph Distillation technique that exploits the implicit knowledge graphs (KG) in CLIP for effectively adapting LVDs to various downstream domains. KGD consists of two consecutive stages: 1) KG extraction that employs CLIP to encode downstream domain data as nodes and their feature distances as edges, constructing KG that inherits the rich semantic relations in CLIP explicitly; and 2) KG encapsulation that transfers the extracted KG into LVDs to enable accurate cross-domain object classification. In addition, KGD can extract both visual and textual KG independently, providing complementary vision and language knowledge for object localization and object classification in detection tasks over various downstream domains. Experiments over multiple widely adopted detection benchmarks show that KGD outperforms the state-of-the-art consistently by large margins. ",
        "title": "Domain Adaptation for Large-Vocabulary Object Detectors",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06970",
        "abstract_url": "http://arxiv.org/abs/2401.06970",
        "authors": [
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Zekios",
                "first_name": "Constantinos L."
            },
            {
                "last_name": "Asadizanjani",
                "first_name": "Navid"
            },
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "HC"
        ],
        "abstract": "  Ensemble modeling has been widely used to solve complex problems as it helps to improve overall performance and generalization. In this paper, we propose a novel TemporalAugmenter approach based on ensemble modeling for augmenting the temporal information capturing for long-term and short-term dependencies in data integration of two variations of recurrent neural networks in two learning streams to obtain the maximum possible temporal extraction. Thus, the proposed model augments the extraction of temporal dependencies. In addition, the proposed approach reduces the preprocessing and prior stages of feature extraction, which reduces the required energy to process the models built upon the proposed TemporalAugmenter approach, contributing towards green AI. Moreover, the proposed model can be simply integrated into various domains including industrial, medical, and human-computer interaction applications. Our proposed approach empirically evaluated the speech emotion recognition, electrocardiogram signal, and signal quality examination tasks as three different signals with varying complexity and different temporal dependency features. ",
        "title": "TemporalAugmenter: An Ensemble Recurrent Based Deep Learning Approach  for Signal Classification",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06974",
        "abstract_url": "http://arxiv.org/abs/2401.06974",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Cain",
                "first_name": "Amelia"
            },
            {
                "last_name": "De Guzman",
                "first_name": "Erica"
            },
            {
                "last_name": "Chiu",
                "first_name": "Claudia"
            },
            {
                "last_name": "Winstein",
                "first_name": "Carolee J."
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja J."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  An over-reliance on the less-affected limb for functional tasks at the expense of the paretic limb and in spite of recovered capacity is an often-observed phenomenon in survivors of hemispheric stroke. The difference between capacity for use and actual spontaneous use is referred to as arm nonuse. Obtaining an ecologically valid evaluation of arm nonuse is challenging because it requires the observation of spontaneous arm choice for different tasks, which can easily be influenced by instructions, presumed expectations, and awareness that one is being tested. To better quantify arm nonuse, we developed the Bimanual Arm Reaching Test with a Robot (BARTR) for quantitatively assessing arm nonuse in chronic stroke survivors. The BARTR is an instrument that utilizes a robot arm as a means of remote and unbiased data collection of nuanced spatial data for clinical evaluations of arm nonuse. This approach shows promise for determining the efficacy of interventions designed to reduce paretic arm nonuse and enhance functional recovery after stroke. We show that the BARTR satisfies the criteria of an appropriate metric for neurorehabilitative contexts: it is valid, reliable, and simple to use. ",
        "title": "A metric for characterizing the arm nonuse workspace in poststroke  individuals using a robot arm",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06975",
        "abstract_url": "http://arxiv.org/abs/2401.06975",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mengtian"
            },
            {
                "last_name": "Lin",
                "first_name": "Shaohui"
            },
            {
                "last_name": "Wang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Shen",
                "first_name": "Yunhang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baochang"
            },
            {
                "last_name": "Ma",
                "first_name": "Lizhuang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semi-supervised learning (SSL), thanks to the significant reduction of data annotation costs, has been an active research topic for large-scale 3D scene understanding. However, the existing SSL-based methods suffer from severe training bias, mainly due to class imbalance and long-tail distributions of the point cloud data. As a result, they lead to a biased prediction for the tail class segmentation. In this paper, we introduce a new decoupling optimization framework, which disentangles feature representation learning and classifier in an alternative optimization manner to shift the bias decision boundary effectively. In particular, we first employ two-round pseudo-label generation to select unlabeled points across head-to-tail classes. We further introduce multi-class imbalanced focus loss to adaptively pay more attention to feature learning across head-to-tail classes. We fix the backbone parameters after feature learning and retrain the classifier using ground-truth points to update its parameters. Extensive experiments demonstrate the effectiveness of our method outperforming previous state-of-the-art methods on both indoor and outdoor 3D point cloud datasets (i.e., S3DIS, ScanNet-V2, Semantic3D, and SemanticKITTI) using 1% and 1pt evaluation. ",
        "title": "Class-Imbalanced Semi-Supervised Learning for Large-Scale Point Cloud  Semantic Segmentation via Decoupling Optimization",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06977",
        "abstract_url": "http://arxiv.org/abs/2401.06977",
        "authors": [
            {
                "last_name": "Dennler",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Nikolaidis",
                "first_name": "Stefanos"
            },
            {
                "last_name": "Matari\u0107",
                "first_name": "Maja"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  Users develop mental models of robots to conceptualize what kind of interactions they can have with those robots. The conceptualizations are often formed before interactions with the robot and are based only on observing the robot's physical design. As a result, understanding conceptualizations formed from physical design is necessary to understand how users intend to interact with the robot. We propose to use multimodal features of robot embodiments to predict what kinds of expectations users will have about a given robot's social and physical capabilities. We show that using such features provides information about general mental models of the robots that generalize across socially interactive robots. We describe how these models can be incorporated into interaction design and physical design for researchers working with socially interactive robots. ",
        "title": "Singing the Body Electric: The Impact of Robot Embodiment on User  Expectations",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06978",
        "abstract_url": "http://arxiv.org/abs/2401.06978",
        "authors": [
            {
                "last_name": "Lau",
                "first_name": "Yuen-Fui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tianjia"
            },
            {
                "last_name": "Rao",
                "first_name": "Zhefan"
            },
            {
                "last_name": "Chen",
                "first_name": "Qifeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module. ",
        "title": "ENTED: Enhanced Neural Texture Extraction and Distribution for  Reference-based Blind Face Restoration",
        "date": "2024-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06979",
        "abstract_url": "http://arxiv.org/abs/2401.06979",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yang"
            },
            {
                "last_name": "Jia",
                "first_name": "Ya-Hui"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei-Neng"
            },
            {
                "last_name": "Mei",
                "first_name": "Yi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural solvers based on attention mechanism have demonstrated remarkable effectiveness in solving vehicle routing problems. However, in the generalization process from small scale to large scale, we find a phenomenon of the dispersion of attention scores in existing neural solvers, which leads to poor performance. To address this issue, this paper proposes a distance-aware attention reshaping method, assisting neural solvers in solving large-scale vehicle routing problems. Specifically, without the need for additional training, we utilize the Euclidean distance information between current nodes to adjust attention scores. This enables a neural solver trained on small-scale instances to make rational choices when solving a large-scale problem. Experimental results show that the proposed method significantly outperforms existing state-of-the-art neural solvers on the large-scale CVRPLib dataset. ",
        "title": "Distance-aware Attention Reshaping: Enhance Generalization of Neural  Solver for Large-scale Vehicle Routing Problems",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06980",
        "abstract_url": "http://arxiv.org/abs/2401.06980",
        "authors": [
            {
                "last_name": "Saif",
                "first_name": "A F M"
            },
            {
                "last_name": "Cui",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Shen",
                "first_name": "Han"
            },
            {
                "last_name": "Lu",
                "first_name": "Songtao"
            },
            {
                "last_name": "Kingsbury",
                "first_name": "Brian"
            },
            {
                "last_name": "Chen",
                "first_name": "Tianyi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy. ",
        "title": "Joint Unsupervised and Supervised Training for Automatic Speech  Recognition via Bilevel Optimization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06981",
        "abstract_url": "http://arxiv.org/abs/2401.06981",
        "authors": [
            {
                "last_name": "Hathcock",
                "first_name": "Daniel"
            },
            {
                "last_name": "Jin",
                "first_name": "Billy"
            },
            {
                "last_name": "Patton",
                "first_name": "Kalen"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Sherry"
            },
            {
                "last_name": "Zlatin",
                "first_name": "Michael"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We study two problems in online matroid intersection. First, we consider the problem of maximizing the size of a common independent set between a general matroid and a partition matroid whose parts arrive online. This captures the classic online bipartite matching problem when both matroids are partition matroids. Our main result is a $(1 - \\frac{1}{e})$-competitive algorithm for the fractional version of this problem. This applies even for the poly-matroid setting, where the rank function of the offline matroid is replaced with a general monotone submodular function. The key new ingredient for this result is the construction of a ''water level'' vector for poly-matroids, which allows us to generalize the classic water-filling algorithm for online bipartite matching. This construction reveals connections to submodular utility allocation markets and principal partition sequences of matroids.   Our second result concerns the Online Submodular Welfare Maximization (OSWM) problem, in which items arriving online are allocated among a set of agents with the goal of maximizing their overall utility. If the utility function of each agent is a monotone, submodular function over the set of available items, then a simple greedy algorithm achieves a competitive ratio of $\\frac{1}{2}$. Kapralov, Post, and Vondr\\'ak showed that in this case, no polynomial time algorithm achieves a competitive ratio of $\\frac{1}{2} + \\varepsilon$ for any $\\varepsilon > 0$ unless NP = RP (SODA, 2013). We extend the RANKING algorithm of Karp, Vazirani, and Vazirani (STOC, 1990) to achieve an optimal $(1-\\frac{1}{e})$-competitive algorithm for OSWM in the case that the utility function of each agent is the rank function of a matroid. ",
        "title": "Online Matroid Intersection: Submodular Water-Filling and Matroidal  Welfare Maximization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06982",
        "abstract_url": "http://arxiv.org/abs/2401.06982",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Jujia"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Xu",
                "first_name": "Yiyan"
            },
            {
                "last_name": "Sun",
                "first_name": "Teng"
            },
            {
                "last_name": "Feng",
                "first_name": "Fuli"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Recommender systems often grapple with noisy implicit feedback. Most studies alleviate the noise issues from data cleaning perspective such as data resampling and reweighting, but they are constrained by heuristic assumptions. Another denoising avenue is from model perspective, which proactively injects noises into user-item interactions and enhance the intrinsic denoising ability of models. However, this kind of denoising process poses significant challenges to the recommender model's representation capacity to capture noise patterns. To address this issue, we propose Denoising Diffusion Recommender Model (DDRM), which leverages multi-step denoising process based on diffusion models to robustify user and item embeddings from any recommender models. DDRM injects controlled Gaussian noises in the forward process and iteratively removes noises in the reverse denoising process, thereby improving embedding robustness against noisy feedback. To achieve this target, the key lies in offering appropriate guidance to steer the reverse denoising process and providing a proper starting point to start the forward-reverse process during inference. In particular, we propose a dedicated denoising module that encodes collaborative information as denoising guidance. Besides, in the inference stage, DDRM utilizes the average embeddings of users' historically liked items as the starting point rather than using pure noise since pure noise lacks personalization, which increases the difficulty of the denoising process. ",
        "title": "Denoising Diffusion Recommender Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06986",
        "abstract_url": "http://arxiv.org/abs/2401.06986",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Lin"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Learning fingerprint-like driving style representations is crucial to accurately identify who is behind the wheel in open driving situations. This study explores the learning of driving styles with GPS signals that are currently available in connected vehicles for short-term driver identification. First, an input driving trajectory is windowed into subtrajectories with fixed time lengths. Then, each subtrajectory is further divided into overlapping dynamic segments. For each segment, the local features are obtained by combining statistical and state transitional patterns. Finally, the driving style embedded in each subtrajectory is learned with the proposed regularized recurrent neural network (RNN) for short-term driver identification. We evaluate the impacts of key factors and the effectiveness of the proposed approach on the identification performance of 5 and 10 drivers. The results show that our proposed neural network structure, which complements movement statistics (MS) with state transitions (ST), provides better prediction performance than existing deep learning methods. ",
        "title": "Learning driving style embedding from GPS-derived moving patterns for  driver identification",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06987",
        "abstract_url": "http://arxiv.org/abs/2401.06987",
        "authors": [
            {
                "last_name": "Loutchko",
                "first_name": "Dimitri"
            },
            {
                "last_name": "Sughiyama",
                "first_name": "Yuki"
            },
            {
                "last_name": "Kobayashi",
                "first_name": "Tetsuya J."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Chemical reaction networks (CRN) comprise an important class of models to understand biological functions such as cellular information processing, the robustness and control of metabolic pathways, circadian rhythms, and many more. However, any CRN describing a certain function does not act in isolation but is a part of a much larger network and as such is constantly subject to external changes. In [Shinar, Alon, and Feinberg. \"Sensitivity and robustness in chemical reaction networks.\" SIAM J App Math (2009): 977-998.], the responses of CRN to changes in the linear conserved quantities, called sensitivities, were studied in and the question of how to construct absolute, i.e., basis-independent, sensitivities was raised. In this article, by applying information geometric methods, such a construction is provided. The idea is to track how concentration changes in a particular chemical propagate to changes of all the other chemicals within a steady state. This is encoded in the matrix of absolute sensitivites. A linear algebraic characterization of the matrix of absolute sensitivities for quasi-thermostatic CRN is derived via a Cramer-Rao bound for CRN, which is based on the the analogy between quasi-thermostatic steady states and the exponential family of probability distributions. ",
        "title": "Cramer-Rao bound and absolute sensitivity in chemical reaction networks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06989",
        "abstract_url": "http://arxiv.org/abs/2401.06989",
        "authors": [
            {
                "last_name": "Sivasubramanian",
                "first_name": "Durga"
            },
            {
                "last_name": "Nagalapatti",
                "first_name": "Lokesh"
            },
            {
                "last_name": "Iyer",
                "first_name": "Rishabh"
            },
            {
                "last_name": "Ramakrishnan",
                "first_name": "Ganesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated Learning (FL) is used to learn machine learning models with data that is partitioned across multiple clients, including resource-constrained edge devices. It is therefore important to devise solutions that are efficient in terms of compute, communication, and energy consumption, while ensuring compliance with the FL framework's privacy requirements. Conventional approaches to these problems select a weighted subset of the training dataset, known as coreset, and learn by fitting models on it. Such coreset selection approaches are also known to be robust to data noise. However, these approaches rely on the overall statistics of the training data and are not easily extendable to the FL setup.   In this paper, we propose an algorithm called Gradient based Coreset for Robust and Efficient Federated Learning (GCFL) that selects a coreset at each client, only every $K$ communication rounds and derives updates only from it, assuming the availability of a small validation dataset at the server. We demonstrate that our coreset selection technique is highly effective in accounting for noise in clients' data. We conduct experiments using four real-world datasets and show that GCFL is (1) more compute and energy efficient than FL, (2) robust to various kinds of noise in both the feature space and labels, (3) preserves the privacy of the validation dataset, and (4) introduces a small communication overhead but achieves significant gains in performance, particularly in cases when the clients' data is noisy. ",
        "title": "Gradient Coreset for Federated Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06992",
        "abstract_url": "http://arxiv.org/abs/2401.06992",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Kaiqun"
            },
            {
                "last_name": "Jiang",
                "first_name": "Xiaoling"
            },
            {
                "last_name": "Yu",
                "first_name": "Rui"
            },
            {
                "last_name": "Luo",
                "first_name": "Yonggang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Tian"
            },
            {
                "last_name": "Wu",
                "first_name": "Xi"
            },
            {
                "last_name": "Wei",
                "first_name": "Peng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image compression has been applied in the fields of image storage and video broadcasting. However, it's formidably tough to distinguish the subtle quality differences between those distorted images generated by different algorithms. In this paper, we propose a new image quality assessment framework to decide which image is better in an image group. To capture the subtle differences, a fine-grained network is adopted to acquire multi-scale features. Subsequently, we design a cross subtract block for separating and gathering the information within positive and negative image pairs. Enabling image comparison in feature space. After that, a progressive feature fusion block is designed, which fuses multi-scale features in a novel progressive way. Hierarchical spatial 2D features can thus be processed gradually. Experimental results show that compared with the current mainstream image quality assessment methods, the proposed network can achieve more accurate image quality assessment and ranks second in the benchmark of CLIC in the image perceptual model track. ",
        "title": "Progressive Feature Fusion Network for Enhancing Image Quality  Assessment",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06994",
        "abstract_url": "http://arxiv.org/abs/2401.06994",
        "authors": [
            {
                "last_name": "Hong",
                "first_name": "Yu"
            },
            {
                "last_name": "Liu",
                "first_name": "Qian"
            },
            {
                "last_name": "Cheng",
                "first_name": "Huayuan"
            },
            {
                "last_name": "Ma",
                "first_name": "Danjiao"
            },
            {
                "last_name": "Dai",
                "first_name": "Hang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Cao",
                "first_name": "Guangzhi"
            },
            {
                "last_name": "Ding",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The past few years have witnessed the rapid development of vision-centric 3D perception in autonomous driving. Although the 3D perception models share many structural and conceptual similarities, there still exist gaps in their feature representations, data formats, and objectives, posing challenges for unified and efficient 3D perception framework design. In this paper, we present UniVision, a simple and efficient framework that unifies two major tasks in vision-centric 3D perception, \\ie, occupancy prediction and object detection. Specifically, we propose an explicit-implicit view transform module for complementary 2D-3D feature transformation. We propose a local-global feature extraction and fusion module for efficient and adaptive voxel and BEV feature extraction, enhancement, and interaction. Further, we propose a joint occupancy-detection data augmentation strategy and a progressive loss weight adjustment strategy which enables the efficiency and stability of the multi-task framework training. We conduct extensive experiments for different perception tasks on four public benchmarks, including nuScenes LiDAR segmentation, nuScenes detection, OpenOccupancy, and Occ3D. UniVision achieves state-of-the-art results with +1.5 mIoU, +1.8 NDS, +1.5 mIoU, and +1.8 mIoU gains on each benchmark, respectively. We believe that the UniVision framework can serve as a high-performance baseline for the unified vision-centric 3D perception task. The code will be available at \\url{https://github.com/Cc-Hy/UniVision}. ",
        "title": "UniVision: A Unified Framework for Vision-Centric 3D Perception",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06995",
        "abstract_url": "http://arxiv.org/abs/2401.06995",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Vishwakarma",
                "first_name": "Dinesh Kumar"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image splice manipulation presents a severe challenge in today's society. With easy access to image manipulation tools, it is easier than ever to modify images that can mislead individuals, organizations or society. In this work, a novel, \"Visually Attentive Splice Localization Network with Multi-Domain Feature Extractor and Multi-Receptive Field Upsampler\" has been proposed. It contains a unique \"visually attentive multi-domain feature extractor\" (VA-MDFE) that extracts attentional features from the RGB, edge and depth domains. Next, a \"visually attentive downsampler\" (VA-DS) is responsible for fusing and downsampling the multi-domain features. Finally, a novel \"visually attentive multi-receptive field upsampler\" (VA-MRFU) module employs multiple receptive field-based convolutions to upsample attentional features by focussing on different information scales. Experimental results conducted on the public benchmark dataset CASIA v2.0 prove the potency of the proposed model. It comfortably beats the existing state-of-the-arts by achieving an IoU score of 0.851, pixel F1 score of 0.9195 and pixel AUC score of 0.8989. ",
        "title": "A Visually Attentive Splice Localization Network with Multi-Domain  Feature Extractor and Multi-Receptive Field Upsampler",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06998",
        "abstract_url": "http://arxiv.org/abs/2401.06998",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Vishwakarma",
                "first_name": "Dinesh Kumar"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Splice detection models are the need of the hour since splice manipulations can be used to mislead, spread rumors and create disharmony in society. However, there is a severe lack of image splicing datasets, which restricts the capabilities of deep learning models to extract discriminative features without overfitting. This manuscript presents two-fold contributions toward splice detection. Firstly, a novel splice detection dataset is proposed having two variants. The two variants include spliced samples generated from code and through manual editing. Spliced images in both variants have corresponding binary masks to aid localization approaches. Secondly, a novel Spatio-Compression Lightweight Splice Detection Framework is proposed for accurate splice detection with minimum computational cost. The proposed dual-branch framework extracts discriminative spatial features from a lightweight spatial branch. It uses original resolution compression data to extract double compression artifacts from the second branch, thereby making it 'information preserving.' Several CNNs are tested in combination with the proposed framework on a composite dataset of images from the proposed dataset and the CASIA v2.0 dataset. The best model accuracy of 0.9382 is achieved and compared with similar state-of-the-art methods, demonstrating the superiority of the proposed framework. ",
        "title": "Towards Effective Image Forensics via A Novel Computationally Efficient  Framework and A New Image Splice Dataset",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.06999",
        "abstract_url": "http://arxiv.org/abs/2401.06999",
        "authors": [
            {
                "last_name": "Yadav",
                "first_name": "Ankit"
            },
            {
                "last_name": "Vishwakarma",
                "first_name": "Dinesh Kumar"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With the large chunks of social media data being created daily and the parallel rise of realistic multimedia tampering methods, detecting and localising tampering in images and videos has become essential. This survey focusses on approaches for tampering detection in multimedia data using deep learning models. Specifically, it presents a detailed analysis of benchmark datasets for malicious manipulation detection that are publicly available. It also offers a comprehensive list of tampering clues and commonly used deep learning architectures. Next, it discusses the current state-of-the-art tampering detection methods, categorizing them into meaningful types such as deepfake detection methods, splice tampering detection methods, copy-move tampering detection methods, etc. and discussing their strengths and weaknesses. Top results achieved on benchmark datasets, comparison of deep learning approaches against traditional methods and critical insights from the recent tampering detection methods are also discussed. Lastly, the research gaps, future direction and conclusion are discussed to provide an in-depth understanding of the tampering detection research arena. ",
        "title": "Datasets, Clues and State-of-the-Arts for Multimedia Forensics: An  Extensive Review",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07001",
        "abstract_url": "http://arxiv.org/abs/2401.07001",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zao"
            },
            {
                "last_name": "Xu",
                "first_name": "Lianming"
            },
            {
                "last_name": "Hou",
                "first_name": "Luyang"
            },
            {
                "last_name": "Li",
                "first_name": "Ruoguang"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  UAV-assisted integrated sensing and communication (ISAC) network is crucial for post-disaster emergency rescue. The speed of UAV deployment will directly impact rescue results. However, the ISAC UAV deployment in emergency scenarios is difficult to solve, which contradicts the rapid deployment. In this paper, we propose a two-stage deployment framework to achieve rapid ISAC UAV deployment in emergency scenarios, which consists of an offline stage and an online stage. Specifically, in the offline stage, we first formulate the ISAC UAV deployment problem and define the ISAC utility as the objective function, which integrates communication rate and localization accuracy. Secondly, we develop a dynamic particle swarm optimization (DPSO) algorithm to construct an optimized UAV deployment dataset. Finally, we train a convolutional neural network (CNN) model with this dataset, which replaces the time-consuming DPSO algorithm. In the online stage, the trained CNN model can be used to make quick decisions for the ISAC UAV deployment. The simulation results indicate that the trained CNN model achieves superior ISAC performance compared to the classic particle swarm optimization algorithm. Additionally, it significantly reduces the deployment time by more than 96%. ",
        "title": "UAV-assisted Emergency Integrated Sensing and Communication Networks: A  CNN-based Rapid Deployment Approach",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07003",
        "abstract_url": "http://arxiv.org/abs/2401.07003",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Jie"
            },
            {
                "last_name": "Xu",
                "first_name": "Yuesheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We studied the use of deep neural networks (DNNs) in the numerical solution of the oscillatory Fredholm integral equation of the second kind. It is known that the solution of the equation exhibits certain oscillatory behaviors due to the oscillation of the kernel. It was pointed out recently that standard DNNs favour low frequency functions, and as a result, they often produce poor approximation for functions containing high frequency components. We addressed this issue in this study. We first developed a numerical method for solving the equation with DNNs as an approximate solution by designing a numerical quadrature that tailors to computing oscillatory integrals involving DNNs. We proved that the error of the DNN approximate solution of the equation is bounded by the training loss and the quadrature error. We then proposed a multi-grade deep learning (MGDL) model to overcome the spectral bias issue of neural networks. Numerical experiments demonstrate that the MGDL model is effective in extracting multiscale information of the oscillatory solution and overcoming the spectral bias issue from which a standard DNN model suffers. ",
        "title": "Deep Neural Network Solutions for Oscillatory Fredholm Integral  Equations",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07004",
        "abstract_url": "http://arxiv.org/abs/2401.07004",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yikai"
            },
            {
                "last_name": "Li",
                "first_name": "Junlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Pengfei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF. ",
        "title": "Extending LLMs' Context Window with 100 Samples",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07009",
        "abstract_url": "http://arxiv.org/abs/2401.07009",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Fan"
            },
            {
                "last_name": "Qi",
                "first_name": "Quan"
            },
            {
                "last_name": "Qin",
                "first_name": "Huaibin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Medical knowledge extraction methods based on edge computing deploy deep learning models on edge devices to achieve localized entity and relation extraction. This approach avoids transferring substantial sensitive data to cloud data centers, effectively safeguarding the privacy of healthcare services. However, existing relation extraction methods mainly employ a sequential pipeline approach, which classifies relations between determined entities after entity recognition. This mode faces challenges such as error propagation between tasks, insufficient consideration of dependencies between the two subtasks, and the neglect of interrelations between different relations within a sentence. To address these challenges, a joint extraction model with parameter sharing in edge computing is proposed, named CoEx-Bert. This model leverages shared parameterization between two models to jointly extract entities and relations. Specifically, CoEx-Bert employs two models, each separately sharing hidden layer parameters, and combines these two loss functions for joint backpropagation to optimize the model parameters. Additionally, it effectively resolves the issue of entity overlapping when extracting knowledge from unstructured Uyghur medical texts by considering contextual relations. Finally, this model is deployed on edge devices for real-time extraction and inference of Uyghur medical knowledge. Experimental results demonstrate that CoEx-Bert outperforms existing state-of-the-art methods, achieving accuracy, recall, and F1 scores of 90.65\\%, 92.45\\%, and 91.54\\%, respectively, in the Uyghur traditional medical literature dataset. These improvements represent a 6.45\\% increase in accuracy, a 9.45\\% increase in recall, and a 7.95\\% increase in F1 score compared to the baseline. ",
        "title": "Joint Extraction of Uyghur Medicine Knowledge with Edge Computing",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07010",
        "abstract_url": "http://arxiv.org/abs/2401.07010",
        "authors": [
            {
                "last_name": "M",
                "first_name": "Shankar"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This review article discusses current technological advances in biomedical devices,emphasizing cardiovascular and ophthalmic application diagnostic,monitoring, and prosthetic instruments and systems. The scope encompasses various aspects, including implantable retinal prosthetic devices, portable device for carotid stiffness measurement, automatic identification algorithms for arteries, cuffless evaluation of carotid pulse pressure, wearable neural recording systems, and arterial compliance probes. Additionally, the paper explores advancements in pulse wave velocity measurement, real time heart rate estimation from wrist type signals, and the clinical significance of non invasive pulse wave velocity measurement in assessing arterial stiffness. The synthesis of these studies provides insights into the evolving landscape of biomedical devices, their validation, reproducibility, and potential clinical implications, emphasizing their role in enhancing diagnostics and therapeutic interventions in cardiovascular and ophthalmic domains. ",
        "title": "Advances in Biomedical Devices_A comprehensive Exploration of  Cardiovascular and Ophthalmic Applications",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07012",
        "abstract_url": "http://arxiv.org/abs/2401.07012",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jinli"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ye"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  High-dimensional and incomplete (HDI) matrix contains many complex interactions between numerous nodes. A stochastic gradient descent (SGD)-based latent factor analysis (LFA) model is remarkably effective in extracting valuable information from an HDI matrix. However, such a model commonly encounters the problem of slow convergence because a standard SGD algorithm only considers the current learning error to compute the stochastic gradient without considering the historical and future state of the learning error. To address this critical issue, this paper innovatively proposes an ADRC-incorporated SGD (ADS) algorithm by refining the instance learning error by considering the historical and future state by following the principle of an ADRC controller. With it, an ADS-based LFA model is further achieved for fast and accurate latent factor analysis on an HDI matrix. Empirical studies on two HDI datasets demonstrate that the proposed model outperforms the state-of-the-art LFA models in terms of computational efficiency and accuracy for predicting the missing data of an HDI matrix. ",
        "title": "An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent  Factor Analysis",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07013",
        "abstract_url": "http://arxiv.org/abs/2401.07013",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Hongzhan"
            },
            {
                "last_name": "Quan",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hehong"
            },
            {
                "last_name": "Yan",
                "first_name": "Ming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ji"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Closed-source language models such as GPT-4 have achieved remarkable performance. Many recent studies focus on enhancing the capabilities of smaller models through knowledge distillation from closed-source language models. However, due to the incapability to directly access the weights, hidden states, and output distributions of these closed-source models, the distillation can only be performed by fine-tuning smaller models with data samples generated by closed-source language models, which constrains the effectiveness of knowledge distillation. In this paper, we propose to estimate the output distributions of closed-source language models within a Bayesian estimation framework, involving both prior and posterior estimation. The prior estimation aims to derive a prior distribution by utilizing the corpus generated by closed-source language models, while the posterior estimation employs a proxy model to update the prior distribution and derive a posterior distribution. By leveraging the estimated output distribution of closed-source language models, traditional knowledge distillation can be executed. Experimental results demonstrate that our method surpasses the performance of current models directly fine-tuned on data generated by closed-source language models. ",
        "title": "Knowledge Distillation for Closed-Source Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07014",
        "abstract_url": "http://arxiv.org/abs/2401.07014",
        "authors": [
            {
                "last_name": "Hacheme",
                "first_name": "Gilles Quentin"
            },
            {
                "last_name": "Zaytar",
                "first_name": "Akram"
            },
            {
                "last_name": "Tadesse",
                "first_name": "Girmaw Abebe"
            },
            {
                "last_name": "Robinson",
                "first_name": "Caleb"
            },
            {
                "last_name": "Dodhia",
                "first_name": "Rahul"
            },
            {
                "last_name": "Ferres",
                "first_name": "Juan M. Lavista"
            },
            {
                "last_name": "Wood",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cropland mapping can play a vital role in addressing environmental, agricultural, and food security challenges. However, in the context of Africa, practical applications are often hindered by the limited availability of high-resolution cropland maps. Such maps typically require extensive human labeling, thereby creating a scalability bottleneck. To address this, we propose an approach that utilizes unsupervised object clustering to refine existing weak labels, such as those obtained from global cropland maps. The refined labels, in conjunction with sparse human annotations, serve as training data for a semantic segmentation network designed to identify cropland areas. We conduct experiments to demonstrate the benefits of the improved weak labels generated by our method. In a scenario where we train our model with only 33 human-annotated labels, the F_1 score for the cropland category increases from 0.53 to 0.84 when we add the mined negative labels. ",
        "title": "Weak Labeling for Cropland Mapping in Africa",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07017",
        "abstract_url": "http://arxiv.org/abs/2401.07017",
        "authors": [
            {
                "last_name": "Aliabadi",
                "first_name": "Zohreh"
            },
            {
                "last_name": "Kalayc\u0131",
                "first_name": "Tekg\u00fcl"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We study the asymptotic behavior of double and four circulant codes, which are quasi-cyclic codes of index two and four respectively. Exact enumeration results are derived for these families of codes with the prescribed hull dimension. These formulas, in turn, are the most used tools to prove the good behavior of double circulant and four circulant codes asymptotically. Computational results on the code families in consideration are provided as well. ",
        "title": "Asymptotic performance of double and four circulant codes with small  hull dimension",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07020",
        "abstract_url": "http://arxiv.org/abs/2401.07020",
        "authors": [
            {
                "last_name": "Mobarakeh",
                "first_name": "Sayed Amir Mousavi"
            },
            {
                "last_name": "Kazemi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Aarabi",
                "first_name": "Ardalan"
            },
            {
                "last_name": "Danyal",
                "first_name": "Habibollah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Since 2019, the global dissemination of the Coronavirus and its novel strains has resulted in a surge of new infections. The use of X-ray and computed tomography (CT) imaging techniques is critical in diagnosing and managing COVID-19. Incorporating artificial intelligence (AI) into the field of medical imaging is a powerful combination that can provide valuable support to healthcare professionals.This paper focuses on the methodological approach of using machine learning (ML) to enhance medical imaging for COVID-19 diagnosis.For example, deep learning can accurately distinguish lesions from other parts of the lung without human intervention in a matter of minutes.Moreover, ML can enhance performance efficiency by assisting radiologists in making more precise clinical decisions, such as detecting and distinguishing Covid-19 from different respiratory infections and segmenting infections in CT and X-ray images, even when the lesions have varying sizes and shapes.This article critically assesses machine learning methodologies utilized for the segmentation, classification, and detection of Covid-19 within CT and X-ray images, which are commonly employed tools in clinical and hospital settings to represent the lung in various aspects and extensive detail.There is a widespread expectation that this technology will continue to hold a central position within the healthcare sector, driving further progress in the management of the pandemic. ",
        "title": "Empowering Medical Imaging with Artificial Intelligence: A Review of  Machine Learning Approaches for the Detection, and Segmentation of COVID-19  Using Radiographic and Tomographic Images",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07022",
        "abstract_url": "http://arxiv.org/abs/2401.07022",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Fan"
            },
            {
                "last_name": "Qi",
                "first_name": "Quan"
            },
            {
                "last_name": "Qin",
                "first_name": "Huaibin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  In the rapidly advancing information era, various human behaviors are being precisely recorded in the form of data, including identity information, criminal records, and communication data. Law enforcement agencies can effectively maintain social security and precisely combat criminal activities by analyzing the aforementioned data. In comparison to traditional data analysis methods, deep learning models, relying on the robust computational power in cloud centers, exhibit higher accuracy in extracting data features and inferring data. However, within the architecture of cloud centers, the transmission of data from end devices introduces significant latency, hindering real-time inference of data. Furthermore, low-latency edge computing architectures face limitations in direct deployment due to relatively weak computing and storage capacities of nodes. To address these challenges, a lightweight distributed knowledge graph completion architecture is proposed. Firstly, we introduce a lightweight distributed knowledge graph completion architecture that utilizes knowledge graph embedding for data analysis. Subsequently, to filter out substandard data, a personnel data quality assessment method named PDQA is proposed. Lastly, we present a model pruning algorithm that significantly reduces the model size while maximizing performance, enabling lightweight deployment. In experiments, we compare the effects of 11 advanced models on completing the knowledge graph of public security personnel information. The results indicate that the RotatE model outperforms other models significantly in knowledge graph completion, with the pruned model size reduced by 70\\%, and hits@10 reaching 86.97\\%.} ",
        "title": "Edge-Enabled Anomaly Detection and Information Completion for Social  Network Knowledge Graphs",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07028",
        "abstract_url": "http://arxiv.org/abs/2401.07028",
        "authors": [
            {
                "last_name": "Bu",
                "first_name": "Tianhao"
            },
            {
                "last_name": "Lazarou",
                "first_name": "Michalis"
            },
            {
                "last_name": "Stathaki",
                "first_name": "Tania"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image classification has been a popular task due to its feasibility in real-world applications. Training neural networks by feeding them RGB images has demonstrated success over it. Nevertheless, improving the classification accuracy and computational efficiency of this process continues to present challenges that researchers are actively addressing. A widely popular embraced method to improve the classification performance of neural networks is to incorporate data augmentations during the training process. Data augmentations are simple transformations that create slightly modified versions of the training data and can be very effective in training neural networks to mitigate overfitting and improve their accuracy performance. In this study, we draw inspiration from high-boost image filtering and propose an edge enhancement-based method as means to enhance both accuracy and training speed of neural networks. Specifically, our approach involves extracting high frequency features, such as edges, from images within the available dataset and fusing them with the original images, to generate new, enriched images. Our comprehensive experiments, conducted on two distinct datasets CIFAR10 and CALTECH101, and three different network architectures ResNet-18, LeNet-5 and CNN-9 demonstrates the effectiveness of our proposed method. ",
        "title": "Image edge enhancement for effective image classification",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07031",
        "abstract_url": "http://arxiv.org/abs/2401.07031",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Nafis Tanveer"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize generating functional code without adding proper security measures, which also benefits the model by resulting in minimal loss. Therefore, in this work, for security hardening and strengthening of generated code from LLMs, we propose a reinforcement learning-based method for program-specific repair with the combination of semantic and syntactic reward mechanisms that focus heavily on adding security and functional measures in the code, respectively. ",
        "title": "Code Security Vulnerability Repair Using Reinforcement Learning with  Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07033",
        "abstract_url": "http://arxiv.org/abs/2401.07033",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lu"
            },
            {
                "last_name": "Das",
                "first_name": "Mayukh"
            },
            {
                "last_name": "Yang",
                "first_name": "Fangkai"
            },
            {
                "last_name": "Sheng",
                "first_name": "Junjie"
            },
            {
                "last_name": "Qiao",
                "first_name": "Bo"
            },
            {
                "last_name": "Dong",
                "first_name": "Hang"
            },
            {
                "last_name": "Qin",
                "first_name": "Si"
            },
            {
                "last_name": "R\u00fchle",
                "first_name": "Victor"
            },
            {
                "last_name": "Bansal",
                "first_name": "Chetan"
            },
            {
                "last_name": "Cortez",
                "first_name": "Eli"
            },
            {
                "last_name": "Goiri",
                "first_name": "\u00cd\u00f1igo"
            },
            {
                "last_name": "Rajmohan",
                "first_name": "Saravan"
            },
            {
                "last_name": "Lin",
                "first_name": "Qingwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dongmei"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Oversubscription is a prevalent practice in cloud services where the system offers more virtual resources, such as virtual cores in virtual machines, to users or applications than its available physical capacity for reducing revenue loss due to unused/redundant capacity. While oversubscription can potentially lead to significant enhancement in efficient resource utilization, the caveat is that it comes with the risks of overloading and introducing jitter at the level of physical nodes if all the co-located virtual machines have high utilization. Thus suitable oversubscription policies which maximize utilization while mitigating risks are paramount for cost-effective seamless cloud experiences. Most cloud platforms presently rely on static heuristics-driven decisions about oversubscription activation and limits, which either leads to overloading or stranded resources. Designing an intelligent oversubscription policy that can adapt to resource utilization patterns and jointly optimizes benefits and risks is, largely, an unsolved problem. We address this challenge with our proposed novel HuMan-in-the-loop Protoypical Imitation Learning (ProtoHAIL) framework that exploits approximate symmetries in utilization patterns to learn suitable policies. Also, our human-in-the-loop (knowledge-infused) training allows for learning safer policies that are robust to noise and sparsity. Our empirical investigations on real data show orders of magnitude reduction in risk and significant increase in benefits (saving stranded cores) in Microsoft cloud platform for 1st party (internal services). ",
        "title": "Risk-aware Adaptive Virtual CPU Oversubscription in Microsoft Cloud via  Prototypical Human-in-the-loop Imitation Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07035",
        "abstract_url": "http://arxiv.org/abs/2401.07035",
        "authors": [
            {
                "last_name": "Islam",
                "first_name": "Nafis Tanveer"
            },
            {
                "last_name": "Parra",
                "first_name": "Gonzalo De La Torre"
            },
            {
                "last_name": "Manual",
                "first_name": "Dylan"
            },
            {
                "last_name": "Jadliwala",
                "first_name": "Murtuza"
            },
            {
                "last_name": "Najafirad",
                "first_name": "Peyman"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Open Source Software (OSS) security and resilience are worldwide phenomena hampering economic and technological innovation. OSS vulnerabilities can cause unauthorized access, data breaches, network disruptions, and privacy violations, rendering any benefits worthless. While recent deep-learning techniques have shown great promise in identifying and localizing vulnerabilities in source code, it is unclear how effective these research techniques are from a usability perspective due to a lack of proper methodological analysis. Usually, these methods offload a developer's task of classifying and localizing vulnerable code; still, a reasonable study to measure the actual effectiveness of these systems to the end user has yet to be conducted. To address the challenge of proper developer training from the prior methods, we propose a system to link vulnerabilities to their root cause, thereby intuitively educating the developers to code more securely. Furthermore, we provide a comprehensive usability study to test the effectiveness of our system in fixing vulnerabilities and its capability to assist developers in writing more secure code. We demonstrate the effectiveness of our system by showing its efficacy in helping developers fix source code with vulnerabilities. Our study shows a 24% improvement in code repair capabilities compared to previous methods. We also show that, when trained by our system, on average, approximately 9% of the developers naturally tend to write more secure code with fewer vulnerabilities. ",
        "title": "Causative Insights into Open Source Software Security using Large  Language Code Embeddings and Semantic Vulnerability Graph",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07037",
        "abstract_url": "http://arxiv.org/abs/2401.07037",
        "authors": [
            {
                "last_name": "Chai",
                "first_name": "Linzheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Sun",
                "first_name": "Tao"
            },
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Bing"
            },
            {
                "last_name": "Liang",
                "first_name": "Xiannian"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Li",
                "first_name": "Tongliang"
            },
            {
                "last_name": "Peng",
                "first_name": "Qiyao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL)) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap. ",
        "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual  Chain-of-Thought Reasoning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07039",
        "abstract_url": "http://arxiv.org/abs/2401.07039",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Chuangtao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qinglin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper introduces the Quantum Generative Diffusion Model (QGDM), a fully quantum-mechanical model for generating quantum state ensembles, inspired by Denoising Diffusion Probabilistic Models. QGDM features a diffusion process that introduces timestep-dependent noise into quantum states, paired with a denoising mechanism trained to reverse this contamination. This model efficiently evolves a completely mixed state into a target quantum state post-training. Our comparative analysis with Quantum Generative Adversarial Networks demonstrates QGDM's superiority, with fidelity metrics exceeding 0.99 in numerical simulations involving up to 4 qubits. Additionally, we present a Resource-Efficient version of QGDM (RE-QGDM), which minimizes the need for auxiliary qubits while maintaining impressive generative capabilities for tasks involving up to 8 qubits. These results showcase the proposed models' potential for tackling challenging quantum generation problems. ",
        "title": "Quantum Generative Diffusion Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07041",
        "abstract_url": "http://arxiv.org/abs/2401.07041",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Sijie"
            },
            {
                "last_name": "Su",
                "first_name": "Ruisheng"
            },
            {
                "last_name": "Su",
                "first_name": "Jianghang"
            },
            {
                "last_name": "Xin",
                "first_name": "Jingmin"
            },
            {
                "last_name": "Wu",
                "first_name": "Jiayi"
            },
            {
                "last_name": "van Zwam",
                "first_name": "Wim"
            },
            {
                "last_name": "van Doormaal",
                "first_name": "Pieter Jan"
            },
            {
                "last_name": "van der Lugt",
                "first_name": "Aad"
            },
            {
                "last_name": "Niessen",
                "first_name": "Wiro J."
            },
            {
                "last_name": "Zheng",
                "first_name": "Nanning"
            },
            {
                "last_name": "van Walsum",
                "first_name": "Theo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate automated extraction of brain vessel centerlines from CTA images plays an important role in diagnosis and therapy of cerebrovascular diseases, such as stroke. However, this task remains challenging due to the complex cerebrovascular structure, the varying imaging quality, and vessel pathology effects. In this paper, we consider automatic lumen segmentation generation without additional annotation effort by physicians and more effective use of the generated lumen segmentation for improved centerline extraction performance. We propose an automated framework for brain vessel centerline extraction from CTA images. The framework consists of four major components: (1) pre-processing approaches that register CTA images with a CT atlas and divide these images into input patches, (2) lumen segmentation generation from annotated vessel centerlines using graph cuts and robust kernel regression, (3) a dual-branch topology-aware UNet (DTUNet) that can effectively utilize the annotated vessel centerlines and the generated lumen segmentation through a topology-aware loss (TAL) and its dual-branch design, and (4) post-processing approaches that skeletonize the predicted lumen segmentation. Extensive experiments on a multi-center dataset demonstrate that the proposed framework outperforms state-of-the-art methods in terms of average symmetric centerline distance (ASCD) and overlap (OV). Subgroup analyses further suggest that the proposed framework holds promise in clinical applications for stroke treatment. Code is publicly available at https://github.com/Liusj-gh/DTUNet. ",
        "title": "An automated framework for brain vessel centerline extraction from CTA  images",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07042",
        "abstract_url": "http://arxiv.org/abs/2401.07042",
        "authors": [
            {
                "last_name": "Barbudo",
                "first_name": "Rafael"
            },
            {
                "last_name": "Ram\u00edrez",
                "first_name": "Aurora"
            },
            {
                "last_name": "Servant",
                "first_name": "Francisco"
            },
            {
                "last_name": "Romero",
                "first_name": "Jos\u00e9 Ra\u00fal"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Design patterns (DPs) are recognised as a good practice in software development. However, the lack of appropriate documentation often hampers traceability, and their benefits are blurred among thousands of lines of code. Automatic methods for DP detection have become relevant but are usually based on the rigid analysis of either software metrics or specific properties of the source code. We propose GEML, a novel detection approach based on evolutionary machine learning using software properties of diverse nature. Firstly, GEML makes use of an evolutionary algorithm to extract those characteristics that better describe the DP, formulated in terms of human-readable rules, whose syntax is conformant with a context-free grammar. Secondly, a rule-based classifier is built to predict whether new code contains a hidden DP implementation. GEML has been validated over five DPs taken from a public repository recurrently adopted by machine learning studies. Then, we increase this number up to 15 diverse DPs, showing its effectiveness and robustness in terms of detection capability. An initial parameter study served to tune a parameter setup whose performance guarantees the general applicability of this approach without the need to adjust complex parameters to a specific pattern. Finally, a demonstration tool is also provided. ",
        "title": "GEML: A Grammar-based Evolutionary Machine Learning Approach for  Design-Pattern Detection",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07043",
        "abstract_url": "http://arxiv.org/abs/2401.07043",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Hgog",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Ritz",
                "first_name": "Fabian"
            },
            {
                "last_name": "Altmann",
                "first_name": "Philipp"
            },
            {
                "last_name": "Zorn",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Stein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Quantum computing offers efficient encapsulation of high-dimensional states. In this work, we propose a novel quantum reinforcement learning approach that combines the Advantage Actor-Critic algorithm with variational quantum circuits by substituting parts of the classical components. This approach addresses reinforcement learning's scalability concerns while maintaining high performance. We empirically test multiple quantum Advantage Actor-Critic configurations with the well known Cart Pole environment to evaluate our approach in control tasks with continuous state spaces. Our results indicate that the hybrid strategy of using either a quantum actor or quantum critic with classical post-processing yields a substantial performance increase compared to pure classical and pure quantum variants with similar parameter counts. They further reveal the limits of current quantum approaches due to the hardware constraints of noisy intermediate-scale quantum computers, suggesting further research to scale hybrid approaches for larger and more complex control tasks. ",
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07044",
        "abstract_url": "http://arxiv.org/abs/2401.07044",
        "authors": [
            {
                "last_name": "Pemberton",
                "first_name": "Joseph"
            },
            {
                "last_name": "Costa",
                "first_name": "Rui Ponte"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Training recurrent neural networks typically relies on backpropagation through time (BPTT). BPTT depends on forward and backward passes to be completed, rendering the network locked to these computations before loss gradients are available. Recently, Jaderberg et al. proposed synthetic gradients to alleviate the need for full BPTT. In their implementation synthetic gradients are learned through a mixture of backpropagated gradients and bootstrapped synthetic gradients, analogous to the temporal difference (TD) algorithm in Reinforcement Learning (RL). However, as in TD learning, heavy use of bootstrapping can result in bias which leads to poor synthetic gradient estimates. Inspired by the accumulate $\\mathrm{TD}(\\lambda)$ in RL, we propose a fully online method for learning synthetic gradients which avoids the use of BPTT altogether: accumulate $BP(\\lambda)$. As in accumulate $\\mathrm{TD}(\\lambda)$, we show analytically that accumulate $\\mathrm{BP}(\\lambda)$ can control the level of bias by using a mixture of temporal difference errors and recursively defined eligibility traces. We next demonstrate empirically that our model outperforms the original implementation for learning synthetic gradients in a variety of tasks, and is particularly suited for capturing longer timescales. Finally, building on recent work we reflect on accumulate $\\mathrm{BP}(\\lambda)$ as a principle for learning in biological circuits. In summary, inspired by RL principles we introduce an algorithm capable of bias-free online learning via synthetic gradients. ",
        "title": "BP(\\lambda): Online Learning via Synthetic Gradients",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07049",
        "abstract_url": "http://arxiv.org/abs/2401.07049",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Stenzel",
                "first_name": "Gerhard"
            },
            {
                "last_name": "Stein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Zielinski",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Ommer",
                "first_name": "Bj\u00f6rn"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation. ",
        "title": "Quantum Denoising Diffusion Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07051",
        "abstract_url": "http://arxiv.org/abs/2401.07051",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Lu"
            },
            {
                "last_name": "Das",
                "first_name": "Mayukh"
            },
            {
                "last_name": "Yang",
                "first_name": "Fangkai"
            },
            {
                "last_name": "Duo",
                "first_name": "Chao"
            },
            {
                "last_name": "Qiao",
                "first_name": "Bo"
            },
            {
                "last_name": "Dong",
                "first_name": "Hang"
            },
            {
                "last_name": "Qin",
                "first_name": "Si"
            },
            {
                "last_name": "Bansal",
                "first_name": "Chetan"
            },
            {
                "last_name": "Lin",
                "first_name": "Qingwei"
            },
            {
                "last_name": "Rajmohan",
                "first_name": "Saravan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dongmei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We address the challenge of learning safe and robust decision policies in presence of uncertainty in context of the real scientific problem of adaptive resource oversubscription to enhance resource efficiency while ensuring safety against resource congestion risk.   Traditional supervised prediction or forecasting models are ineffective in learning adaptive policies whereas standard online optimization or reinforcement learning is difficult to deploy on real systems. Offline methods such as imitation learning (IL) are ideal since we can directly leverage historical resource usage telemetry. But, the underlying aleatoric uncertainty in such telemetry is a critical bottleneck.   We solve this with our proposed novel chance-constrained imitation learning framework, which ensures implicit safety against uncertainty in a principled manner via a combination of stochastic (chance) constraints on resource congestion risk and ensemble value functions. This leads to substantial ($\\approx 3-4\\times$) improvement in resource efficiency and safety in many oversubscription scenarios, including resource management in cloud services. ",
        "title": "COIN: Chance-Constrained Imitation Learning for Uncertainty-aware  Adaptive Resource Oversubscription Policy",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07052",
        "abstract_url": "http://arxiv.org/abs/2401.07052",
        "authors": [
            {
                "last_name": "G\u00f3mez-D\u00e9niz",
                "first_name": "Emilio"
            },
            {
                "last_name": "Dorta-Gonz\u00e1lez",
                "first_name": "Pablo"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  When a graphical representation of the cumulative percentage of total citations to articles, ordered from most cited to least cited, is plotted against the cumulative percentage of articles, we obtain a Leimkuhler curve. In this study, we noticed that standard Leimkuhler functions may not be sufficient to provide accurate fits to various empirical informetrics data. Therefore, we introduce a new approach to Leimkuhler curves by fitting a known probability density function to the initial Leimkuhler curve, taking into account the presence of a heterogeneity factor. As a significant contribution to the existing literature, we introduce a pair of mixture distributions (called PG and PIG) to bibliometrics. In addition, we present closed-form expressions for Leimkuhler curves. {Some measures of citation concentration are examined empirically for the basic models (based on the Power {and Pareto distributions}) and the mixed models derived from {these}.} An application to two sources of informetric data was conducted to see how the mixing models outperform the standard basic models. The different models were fitted using non-linear least squares estimation. ",
        "title": "Modeling citation concentration through a mixture of Leimkuhler curves",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07053",
        "abstract_url": "http://arxiv.org/abs/2401.07053",
        "authors": [
            {
                "last_name": "Reimann",
                "first_name": "Lars"
            },
            {
                "last_name": "Kniesel-W\u00fcnsche",
                "first_name": "G\u00fcnter"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Third-party libraries are a cornerstone of fast application development. To enable efficient use, libraries must provide a well-designed API. An obscure API instead slows down the learning process and can lead to erroneous use.   The usual approach to improve the API of a library is to edit its code directly, either keeping the old API but deprecating it (temporarily increasing the API size) or dropping it (introducing breaking changes). If maintainers are unwilling to make such changes, others need to create a hard fork, which they can refactor. But then it is difficult to incorporate changes to the original library, such as bug fixes or performance improvements.   In this paper, we instead explore the use of the adapter pattern to provide a new API as a new library that calls the original library internally. This allows the new library to leverage all implementation changes to the original library, at no additional cost. We call this approach adaptoring. To make the approach practical, we identify API transformations for which adapter code can be generated automatically, and investigate which transformations can be inferred automatically, based on the documentation and usage patterns of the original library. For cases where automated inference is not possible, we present a tool that lets developers manually specify API transformations. Finally, we consider the issue of migrating the generated adapters if the original library introduces breaking changes. We implemented our approach for Python, demonstrating its effectiveness to quickly provide an alternative API even for large libraries. ",
        "title": "Adaptoring: Adapter Generation to Provide an Alternative API for a  Library",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07054",
        "abstract_url": "http://arxiv.org/abs/2401.07054",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Schubert",
                "first_name": "Tom"
            },
            {
                "last_name": "Altmann",
                "first_name": "Philipp"
            },
            {
                "last_name": "Zorn",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Stein",
                "first_name": "Jonas"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  With recent advancements in quantum computing technology, optimizing quantum circuits and ensuring reliable quantum state preparation have become increasingly vital. Traditional methods often demand extensive expertise and manual calculations, posing challenges as quantum circuits grow in qubit- and gate-count. Therefore, harnessing machine learning techniques to handle the growing variety of gate-to-qubit combinations is a promising approach. In this work, we introduce a comprehensive reinforcement learning environment for quantum circuit synthesis, where circuits are constructed utilizing gates from the the Clifford+T gate set to prepare specific target states. Our experiments focus on exploring the relationship between the depth of synthesized quantum circuits and the circuit depths used for target initialization, as well as qubit count. We organize the environment configurations into multiple evaluation levels and include a range of well-known quantum states for benchmarking purposes. We also lay baselines for evaluating the environment using Proximal Policy Optimization. By applying the trained agents to benchmark tests, we demonstrated their ability to reliably design minimal quantum circuits for a selection of 2-qubit Bell states. ",
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit  Synthesis",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07055",
        "abstract_url": "http://arxiv.org/abs/2401.07055",
        "authors": [
            {
                "last_name": "Bonchi",
                "first_name": "Filippo"
            },
            {
                "last_name": "Di Giorgio",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Haydon",
                "first_name": "Nathan"
            },
            {
                "last_name": "Sobocinski",
                "first_name": "Pawel"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We introduce the calculus of neo-Peircean relations, a string diagrammatic extension of the calculus of binary relations that has the same expressivity as first order logic and comes with a complete axiomatisation. The axioms are obtained by combining two well known categorical structures: cartesian and linear bicategories. ",
        "title": "Diagrammatic Algebra of First Order Logic",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07056",
        "abstract_url": "http://arxiv.org/abs/2401.07056",
        "authors": [
            {
                "last_name": "K\u00f6lle",
                "first_name": "Michael"
            },
            {
                "last_name": "Erpelding",
                "first_name": "Yannick"
            },
            {
                "last_name": "Ritz",
                "first_name": "Fabian"
            },
            {
                "last_name": "Phan",
                "first_name": "Thomy"
            },
            {
                "last_name": "Illium",
                "first_name": "Steffen"
            },
            {
                "last_name": "Linnhoff-Popien",
                "first_name": "Claudia"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Recent advances in Multi-Agent Reinforcement Learning have prompted the modeling of intricate interactions between agents in simulated environments. In particular, the predator-prey dynamics have captured substantial interest and various simulations been tailored to unique requirements. To prevent further time-intensive developments, we introduce Aquarium, a comprehensive Multi-Agent Reinforcement Learning environment for predator-prey interaction, enabling the study of emergent behavior. Aquarium is open source and offers a seamless integration of the PettingZoo framework, allowing a quick start with proven algorithm implementations. It features physics-based agent movement on a two-dimensional, edge-wrapping plane. The agent-environment interaction (observations, actions, rewards) and the environment settings (agent speed, prey reproduction, predator starvation, and others) are fully customizable. Besides a resource-efficient visualization, Aquarium supports to record video files, providing a visual comprehension of agent behavior. To demonstrate the environment's capabilities, we conduct preliminary studies which use PPO to train multiple prey agents to evade a predator. In accordance to the literature, we find Individual Learning to result in worse performance than Parameter Sharing, which significantly improves coordination and sample-efficiency. ",
        "title": "Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics  through Multi-Agent Reinforcement Learning Algorithms",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07058",
        "abstract_url": "http://arxiv.org/abs/2401.07058",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Wang",
                "first_name": "Dakuo"
            },
            {
                "last_name": "Yin",
                "first_name": "Ming"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of {second opinions} may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regardless whether the second opinion is generated by a peer or another AI model. However, if decision-makers have the control to decide when to solicit a peer's second opinion, we find that their active solicitations of second opinions have the potential to mitigate over-reliance on AI without inducing increased under-reliance in some cases. We conclude by discussing the implications of our findings for promoting effective human-AI collaborations in decision-making. ",
        "title": "Does More Advice Help? The Effects of Second Opinions in AI-Assisted  Decision Making",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07059",
        "abstract_url": "http://arxiv.org/abs/2401.07059",
        "authors": [
            {
                "last_name": "Ziegler",
                "first_name": "Christian"
            },
            {
                "last_name": "Miranda",
                "first_name": "Marcos"
            },
            {
                "last_name": "Cao",
                "first_name": "Guangye"
            },
            {
                "last_name": "Arentoft",
                "first_name": "Gustav"
            },
            {
                "last_name": "Nam",
                "first_name": "Doo Wan"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Our study demonstrates the effective use of Large Language Models (LLMs) for automating the classification of complex datasets. We specifically target proposals of Decentralized Autonomous Organizations (DAOs), as the classification of this data requires the understanding of context and, therefore, depends on human expertise, leading to high costs associated with the task. The study applies an iterative approach to specify categories and further refine them and the prompt in each iteration, which led to an accuracy rate of 95% in classifying a set of 100 proposals. With this, we demonstrate the potential of LLMs to automate data labeling tasks that depend on textual context effectively. ",
        "title": "Classifying Proposals of Decentralized Autonomous Organizations Using  Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07060",
        "abstract_url": "http://arxiv.org/abs/2401.07060",
        "authors": [
            {
                "last_name": "Carpio",
                "first_name": "Ana"
            },
            {
                "last_name": "Cebrian",
                "first_name": "Elena"
            },
            {
                "last_name": "Vidal",
                "first_name": "Perfecto"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Biofilms are bacterial aggregates encased in a self-produced polymeric matrix which attach to moist surfaces and are extremely resistant to chemicals and antibiotics. Recent experiments show that their structure is defined by the interplay of elastic deformations and liquid transport within the biofilm, in response to the cellular activity and the interaction with the surrounding environment. We propose a poroelastic model for elastic deformation and liquid transport in three dimensional biofilms spreading on agar surfaces. The motion of the boundaries can be described by the combined use of Von Karman type approximations for the agar/biofilm interface and thin film approximations for the biofilm/air interface. Bacterial activity informs the macroscopic continuous model through source terms and residual stresses, either phenomenological or derived from microscopic models. We present a procedure to estimate the structure of such residual stresses, based on a simple cellular automata description of bacterial activity. Inspired by image processing, we show that a filtering strategy effectively smooths out the rough tensors provided by the stochastic cellular automata rules, allowing us to insert them in the macroscopic model without numerical instability. ",
        "title": "Biofilms as poroelastic materials",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07061",
        "abstract_url": "http://arxiv.org/abs/2401.07061",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Hefeng"
            },
            {
                "last_name": "Ye",
                "first_name": "Guangzhi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Tian",
                "first_name": "Ling"
            },
            {
                "last_name": "Wang",
                "first_name": "Qing"
            },
            {
                "last_name": "Lin",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Learning to recognize novel concepts from just a few image samples is very challenging as the learned model is easily overfitted on the few data and results in poor generalizability. One promising but underexplored solution is to compensate the novel classes by generating plausible samples. However, most existing works of this line exploit visual information only, rendering the generated data easy to be distracted by some challenging factors contained in the few available samples. Being aware of the semantic information in the textual modality that reflects human concepts, this work proposes a novel framework that exploits semantic relations to guide dual-view data hallucination for few-shot image recognition. The proposed framework enables generating more diverse and reasonable data samples for novel classes through effective information transfer from base classes. Specifically, an instance-view data hallucination module hallucinates each sample of a novel class to generate new data by employing local semantic correlated attention and global semantic feature fusion derived from base classes. Meanwhile, a prototype-view data hallucination module exploits semantic-aware measure to estimate the prototype of a novel class and the associated distribution from the few samples, which thereby harvests the prototype as a more stable sample and enables resampling a large number of samples. We conduct extensive experiments and comparisons with state-of-the-art methods on several popular few-shot benchmarks to verify the effectiveness of the proposed framework. ",
        "title": "Dual-View Data Hallucination with Semantic Relation Guidance for  Few-Shot Image Recognition",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07062",
        "abstract_url": "http://arxiv.org/abs/2401.07062",
        "authors": [
            {
                "last_name": "Zong",
                "first_name": "Chen-Chen"
            },
            {
                "last_name": "Wang",
                "first_name": "Ye-Wen"
            },
            {
                "last_name": "Xie",
                "first_name": "Ming-Kun"
            },
            {
                "last_name": "Huang",
                "first_name": "Sheng-Jun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Learning with noisy labels can significantly hinder the generalization performance of deep neural networks (DNNs). Existing approaches address this issue through loss correction or example selection methods. However, these methods often rely on the model's predictions obtained from the softmax function, which can be over-confident and unreliable. In this study, we identify the translation invariance of the softmax function as the underlying cause of this problem and propose the \\textit{Dirichlet-based Prediction Calibration} (DPC) method as a solution. Our method introduces a calibrated softmax function that breaks the translation invariance by incorporating a suitable constant in the exponent term, enabling more reliable model predictions. To ensure stable model training, we leverage a Dirichlet distribution to assign probabilities to predicted labels and introduce a novel evidence deep learning (EDL) loss. The proposed loss function encourages positive and sufficiently large logits for the given label, while penalizing negative and small logits for other labels, leading to more distinct logits and facilitating better example selection based on a large-margin criterion. Through extensive experiments on diverse benchmark datasets, we demonstrate that DPC achieves state-of-the-art performance. The code is available at https://github.com/chenchenzong/DPC. ",
        "title": "Dirichlet-Based Prediction Calibration for Learning with Noisy Labels",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07063",
        "abstract_url": "http://arxiv.org/abs/2401.07063",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Huijia"
            },
            {
                "last_name": "Poskitt",
                "first_name": "Christopher M."
            },
            {
                "last_name": "Sun",
                "first_name": "Yang"
            },
            {
                "last_name": "Sun",
                "first_name": "Jun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuqi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The rapid progress of autonomous vehicles~(AVs) has brought the prospect of a driverless future closer than ever. Recent fatalities, however, have emphasized the importance of safety validation through large-scale testing. Multiple approaches achieve this fully automatically using high-fidelity simulators, i.e., by generating diverse driving scenarios and evaluating autonomous driving systems~(ADSs) against different test oracles. While effective at finding violations, these approaches do not identify the decisions and actions that \\emph{caused} them -- information that is critical for improving the safety of ADSs. To address this challenge, we propose ACAV, an automated framework designed to conduct causality analysis for AV accident recordings in two stages. First, we apply feature extraction schemas based on the messages exchanged between ADS modules, and use a weighted voting method to discard frames of the recording unrelated to the accident. Second, we use safety specifications to identify safety-critical frames and deduce causal events by applying CAT -- our causal analysis tool -- to a station-time graph. We evaluate ACAV on the Apollo ADS, finding that it can identify five distinct types of causal events in 93.64% of 110 accident recordings generated by an AV testing engine. We further evaluated ACAV on 1206 accident recordings collected from versions of Apollo injected with specific faults, finding that it can correctly identify causal events in 96.44% of the accidents triggered by prediction errors, and 85.73% of the accidents triggered by planning errors. ",
        "title": "ACAV: A Framework for Automatic Causality Analysis in Autonomous Vehicle  Accident Recordings",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07065",
        "abstract_url": "http://arxiv.org/abs/2401.07065",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Ling"
            },
            {
                "last_name": "Yuan",
                "first_name": "Ye"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Dynamic graphs (DG) describe dynamic interactions between entities in many practical scenarios. Most existing DG representation learning models combine graph convolutional network and sequence neural network, which model spatial-temporal dependencies through two different types of neural networks. However, this hybrid design cannot well capture the spatial-temporal continuity of a DG. In this paper, we propose a tensor graph convolutional network to learn DG representations in one convolution framework based on the tensor product with the following two-fold ideas: a) representing the information of DG by tensor form; b) adopting tensor product to design a tensor graph convolutional network modeling spatial-temporal feature simultaneously. Experiments on real-world DG datasets demonstrate that our model obtains state-of-the-art performance. ",
        "title": "Tensor Graph Convolutional Network for Dynamic Graph Representation  Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07066",
        "abstract_url": "http://arxiv.org/abs/2401.07066",
        "authors": [
            {
                "last_name": "Rauhameri",
                "first_name": "Anton"
            },
            {
                "last_name": "Robi\u00f1os",
                "first_name": "Angelo"
            },
            {
                "last_name": "Anttalainen",
                "first_name": "Osmo"
            },
            {
                "last_name": "Salpavaara",
                "first_name": "Timo"
            },
            {
                "last_name": "Rantala",
                "first_name": "Jussi"
            },
            {
                "last_name": "Surakka",
                "first_name": "Veikko"
            },
            {
                "last_name": "Kallio",
                "first_name": "Pasi"
            },
            {
                "last_name": "Vehkaoja",
                "first_name": "Antti"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "Philipp"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Background: Classification of volatile organic compounds (VOCs) is of interest in many fields. Examples include but are not limited to medicine, detection of explosives, and food quality control. Measurements collected with electronic noses can be used for classification and analysis of VOCs. One type of electronic noses that has seen considerable development in recent years is Differential Mobility Spectrometry (DMS). DMS yields measurements that are visualized as dispersion plots that contain traces, also known as alpha curves. Current methods used for analyzing DMS dispersion plots do not usually utilize the information stored in the continuity of these traces, which suggests that alternative approaches should be investigated.   Results: In this work, for the first time, dispersion plots were interpreted as a series of measurements evolving sequentially. Thus, it was hypothesized that time-series classification algorithms can be effective for classification and analysis of dispersion plots. An extensive dataset of 900 dispersion plots for five chemicals measured at five flow rates and two concentrations was collected. The data was used to analyze the classification performance of six algorithms. According to our hypothesis, the highest classification accuracy of 88\\% was achieved by a Long-Short Term Memory neural network, which supports our hypothesis.   Significance: A new concept for approaching classification tasks of dispersion plots is presented and compared with other well-known classification algorithms. This creates a new angle of view for analysis and classification of the dispersion plots. In addition, a new dataset of dispersion plots is openly shared to public. ",
        "title": "Classification of Volatile Organic Compounds by Differential Mobility  Spectrometry Based on Continuity of Alpha Curves",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07070",
        "abstract_url": "http://arxiv.org/abs/2401.07070",
        "authors": [
            {
                "last_name": "Supantha",
                "first_name": "Subhamon"
            },
            {
                "last_name": "Sharma",
                "first_name": "Naresh Kumar"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  We have used agent-based modeling as our numerical method to artificially simulate a dynamic real economy where agents are rational maximizers of an objective function of Cobb-Douglas type. The economy is characterised by heterogeneous agents, acting out of local or imperfect information, monopolistic competition, perfect product differentiation, allowance for increasing returns to scale technology and trade in disequilibrium. An algorithm for economic activity in each period is devised and a general purpose open source agent-based model is developed which allows for counterfactual inquiries, testing out treatments, analysing causality of various economic processes, outcomes and studying emergent properties. 10,000 simulations, with 10 firms and 80 consumers are run with varying parameters and the results show that from only a few initial conditions the economy reaches equilibrium while in most of the other cases it remains in perpetual disequilibrium. It also shows that from a few initial conditions the economy reaches a disaster where all the consumer wealth falls to zero or only a single producer remains. Furthermore, from some initial conditions, an ideal economy with high wage rate, high consumer utility and no unemployment is also reached. It was also observed that starting from an equal endowment of wealth in consumers and in producers, inequality emerged in the economy. In majority of the cases most of the firms(6-7) shut down because they were not profitable enough and only a few firms remained. Our results highlight that all these varying outcomes are possible for a decentralized market economy with rational optimizing agents. ",
        "title": "A Dynamic Agent Based Model of the Real Economy with Monopolistic  Competition, Perfect Product Differentiation, Heterogeneous Agents,  Increasing Returns to Scale and Trade in Disequilibrium",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07072",
        "abstract_url": "http://arxiv.org/abs/2401.07072",
        "authors": [
            {
                "last_name": "Delgado-P\u00e9rez",
                "first_name": "Pedro"
            },
            {
                "last_name": "Ram\u00edrez",
                "first_name": "Aurora"
            },
            {
                "last_name": "Valle-G\u00f3mez",
                "first_name": "Kevin J."
            },
            {
                "last_name": "Medina-Bulo",
                "first_name": "Inmaculada"
            },
            {
                "last_name": "Romero",
                "first_name": "Jos\u00e9 Ra\u00fal"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Automated test case generation has proven to be useful to reduce the usually high expenses of software testing. However, several studies have also noted the skepticism of testers regarding the comprehension of generated test suites when compared to manually designed ones. This fact suggests that involving testers in the test generation process could be helpful to increase their acceptance of automatically-produced test suites. In this paper, we propose incorporating interactive readability assessments made by a tester into EvoSuite, a widely-known evolutionary test generation tool. Our approach, InterEvo-TR, interacts with the tester at different moments during the search and shows different test cases covering the same coverage target for their subjective evaluation. The design of such an interactive approach involves a schedule of interaction, a method to diversify the selected targets, a plan to save and handle the readability values, and some mechanisms to customize the level of engagement in the revision, among other aspects. To analyze the potential and practicability of our proposal, we conduct a controlled experiment in which 39 participants, including academics, professional developers, and student collaborators, interact with InterEvo-TR. Our results show that the strategy to select and present intermediate results is effective for the purpose of readability assessment. Furthermore, the participants' actions and responses to a questionnaire allowed us to analyze the aspects influencing test code readability and the benefits and limitations of an interactive approach in the context of test case generation, paving the way for future developments based on interactivity. ",
        "title": "InterEvo-TR: Interactive Evolutionary Test Generation With Readability  Assessment",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07074",
        "abstract_url": "http://arxiv.org/abs/2401.07074",
        "authors": [
            {
                "last_name": "Hansen",
                "first_name": "Henri"
            },
            {
                "last_name": "Kanniainen",
                "first_name": "Juho"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In this paper, we introduce the Detachment Problem. It can be seen as a generalized Vaccination Problem. The aim is to optimally cut the individuals' ties to circles that connect them to others, to minimize the overall information transfer in a social network. When an individual is isolated from a particular circle, it leads to the elimination of the connections to all the members of that circle, yet the connections to other circles remain. This approach contrasts with the conventional vaccination problem, in which a subset of vertices is totally eliminated. In our case, the connections of individuals to their circles are selectively, rather than entirely, eliminated. Contextually, this article focuses on private information flows, specifically within networks formed by memberships in circles of insiders in companies. Our quasi-empirical study uses simulated information flows on an observable network, and the statistical properties of the simulated information flows are matched with real-world data. In a broader context, this paper presents the Detachment Problem as a versatile approach for optimal social distancing, applicable across various scenarios. We propose and define a concept of expected proportional outside influence, or EPOI, as measure of how widespread information leak is. We also implement a greedy algorithm for finding a set of detachments to minimize EPOI. For comparison, we devise a simple heuristic based on minimal cut, to separate the most influential circles from each other. We provide evidence that the greedy algorithm is not optimal, and it is sometimes outperformed by the simple heuristic minimum cut algorithm, However, the greedy algorithm outperforms the cut algorithm in most cases. Further avenues of research are discussed. ",
        "title": "Detachment Problem -- Application in Prevention of Information Leakage  in Stock Markets",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07078",
        "abstract_url": "http://arxiv.org/abs/2401.07078",
        "authors": [
            {
                "last_name": "Sravanthi",
                "first_name": "Settaluri Lakshmi"
            },
            {
                "last_name": "Doshi",
                "first_name": "Meet"
            },
            {
                "last_name": "Kalyan",
                "first_name": "Tankala Pavan"
            },
            {
                "last_name": "Murthy",
                "first_name": "Rudra"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Pushpak"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  LLMs have demonstrated remarkable capability for understanding semantics, but they often struggle with understanding pragmatics. To demonstrate this fact, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely, Implicature, Presupposition, Reference, and Deixis. We curated high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k of which have been created by us, and the rest are adapted from existing datasets. We evaluated nine models varying in the number of parameters and type of training. Our study indicates that fine-tuning for instruction-following and chat significantly enhances the pragmatics capabilities of smaller language models. However, for larger models, the base versions perform comparably with their chat-adapted counterparts. Additionally, there is a noticeable performance gap between human capabilities and model capabilities. Furthermore, unlike the consistent performance of humans across various tasks, the models demonstrate variability in their proficiency, with performance levels fluctuating due to different hints and the complexities of tasks within the same dataset. Overall, the benchmark aims to provide a comprehensive evaluation of LLM's ability to handle real-world language tasks that require pragmatic reasoning. ",
        "title": "PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics  Capabilities",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07080",
        "abstract_url": "http://arxiv.org/abs/2401.07080",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Haibin"
            },
            {
                "last_name": "Ye",
                "first_name": "Maoyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jing"
            },
            {
                "last_name": "Liu",
                "first_name": "Juhua"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we highlight a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching achieves impressive performance on two public benchmarks, e.g., setting a new record on the ICDAR15-video dataset, and one novel test set with arbitrary-shaped text, while saving considerable training budgets. The code will be released at https://github.com/Hxyz-123/GoMatching. ",
        "title": "GoMatching: A Simple Baseline for Video Text Spotting via Long and Short  Term Matching",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07081",
        "abstract_url": "http://arxiv.org/abs/2401.07081",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhichao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoxin"
            },
            {
                "last_name": "Cheng",
                "first_name": "Yanan"
            },
            {
                "last_name": "Li",
                "first_name": "Ning"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The discovery of active IPv6 addresses represents a pivotal challenge in IPv6 network survey, as it is a prerequisite for downstream tasks such as network topology measurements and security analysis. With the rapid spread of IPv6 networks in recent years, many researchers have focused on improving the hit rate, efficiency, and coverage of IPv6 scanning methods, resulting in considerable advancements. However, existing approaches remain heavily dependent on seed addresses, thereby limiting their effectiveness in unseeded prefixes. Consequently, this paper proposes 6Rover, a reinforcement learning-based model for active address discovery in unseeded environments. To overcome the reliance on seeded addresses, 6Rover constructs patterns with higher generality that reflects the actual address allocation strategies of network administrators, thereby avoiding biased transfers of patterns from seeded to unseeded prefixes. After that, 6Rover employs a multi-armed bandit model to optimize the probing resource allocation when applying patterns to unseeded spaces. It models the challenge of discovering optimal patterns in unseeded spaces as an exploration-exploitation dilemma, and progressively uncover the potential patterns applied in unseeded spaces, leading to the efficient discovery of active addresses without seed address as the prior knowledge. Experiments on large-scale unseeded datasets show that 6Rover has a higher hit rate than existing methods in the absence of any seed addresses as prior knowledge. In real network environments, 6Rover achieved a 5% - 8% hit rate in seedless spaces with 100 million budget scale, representing an approximate 200\\% improvement over the existing state-of-the-art methods. ",
        "title": "6Rover: Leveraging Reinforcement Learning-based Address Pattern Mining  Approach for Discovering Active Targets in IPv6 Unseeded Space",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07084",
        "abstract_url": "http://arxiv.org/abs/2401.07084",
        "authors": [
            {
                "last_name": "Veerendranath",
                "first_name": "Vishruth"
            },
            {
                "last_name": "Masti",
                "first_name": "Vibha"
            },
            {
                "last_name": "Gupta",
                "first_name": "Utkarsh"
            },
            {
                "last_name": "Chaudhuri",
                "first_name": "Hrishit"
            },
            {
                "last_name": "Srinivasa",
                "first_name": "Gowri"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM",
            "SD"
        ],
        "abstract": "  Film scores are considered an essential part of the film cinematic experience, but the process of film score generation is often expensive and infeasible for small-scale creators. Automating the process of film score composition would provide useful starting points for music in small projects. In this paper, we propose a two-stage pipeline for generating music from a movie script. The first phase is the Sentiment Analysis phase where the sentiment of a scene from the film script is encoded into the valence-arousal continuous space. The second phase is the Conditional Music Generation phase which takes as input the valence-arousal vector and conditionally generates piano MIDI music to match the sentiment. We study the efficacy of various music generation architectures by performing a qualitative user survey and propose methods to improve sentiment-conditioning in VAE architectures. ",
        "title": "ScripTONES: Sentiment-Conditioned Music Generation for Movie Scripts",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07085",
        "abstract_url": "http://arxiv.org/abs/2401.07085",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Yizhou"
            },
            {
                "last_name": "Ziyin",
                "first_name": "Liu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We identify and solve a hidden-layer model that is analytically tractable at any finite width and whose limits exhibit both the kernel phase and the feature learning phase. We analyze the phase diagram of this model in all possible limits of common hyperparameters including width, layer-wise learning rates, scale of output, and scale of initialization. We apply our result to analyze how and when feature learning happens in both infinite and finite-width models. Three prototype mechanisms of feature learning are identified: (1) learning by alignment, (2) learning by disalignment, and (3) learning by rescaling. In sharp contrast, neither of these mechanisms is present when the model is in the kernel regime. This discovery explains why large initialization often leads to worse performance. Lastly, we empirically demonstrate that discoveries we made for this analytical model also appear in nonlinear networks in real tasks. ",
        "title": "When Does Feature Learning Happen? Perspective from an Analytically  Solvable Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07087",
        "abstract_url": "http://arxiv.org/abs/2401.07087",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Junxi"
            },
            {
                "last_name": "Dong",
                "first_name": "Junhao"
            },
            {
                "last_name": "Xie",
                "first_name": "Xiaohua"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR",
            "LG"
        ],
        "abstract": "  Recently, many studies utilized adversarial examples (AEs) to raise the cost of malicious image editing and copyright violation powered by latent diffusion models (LDMs). Despite their successes, a few have studied the surrogate model they used to generate AEs. In this paper, from the perspective of adversarial transferability, we investigate how the surrogate model's property influences the performance of AEs for LDMs. Specifically, we view the time-step sampling in the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogate models. We find that the smoothness of surrogate models at different time steps differs, and we substantially improve the performance of the MC-based AEs by selecting smoother surrogate models. In the light of the theoretical framework on adversarial transferability in image classification, we also conduct a theoretical analysis to explain why smooth surrogate models can also boost AEs for LDMs. ",
        "title": "Exploring Adversarial Attacks against Latent Diffusion Model from the  Perspective of Adversarial Transferability",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07088",
        "abstract_url": "http://arxiv.org/abs/2401.07088",
        "authors": [
            {
                "last_name": "Carpio",
                "first_name": "Ana"
            },
            {
                "last_name": "Cebrian",
                "first_name": "Elena"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The dynamics of cellular aggregates is driven by the interplay of mechanochemical processes and cellular activity. Although deterministic models may capture mechanical features, local chemical fluctuations trigger random cell responses, which determine the overall evolution. Incorporating stochastic cellular behavior in macroscopic models of biological media is a challenging task. Herein, we propose hybrid models for bacterial biofilm growth, which couple a two phase solid/fluid mixture description of mechanical and chemical fields with a dynamic energy budget-based cellular automata treatment of bacterial activity. Thin film and plate approximations for the relevant interfaces allow us to obtain numerical solutions exhibiting behaviors observed in experiments, such as accelerated spread due to water intake from the environment, wrinkle formation, undulated contour development, and the appearance of inhomogeneous distributions of differentiated bacteria performing varied tasks. ",
        "title": "Incorporating Cellular Stochasticity in Solid--Fluid Mixture Biofilm  Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07091",
        "abstract_url": "http://arxiv.org/abs/2401.07091",
        "authors": [
            {
                "last_name": "Laber",
                "first_name": "Eduardo S."
            },
            {
                "last_name": "Murtinho",
                "first_name": "Lucas"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DS"
        ],
        "abstract": "  Internal measures that are used to assess the quality of a clustering usually take into account intra-group and/or inter-group criteria. There are many papers in the literature that propose algorithms with provable approximation guarantees for optimizing the former. However, the optimization of inter-group criteria is much less understood.   Here, we contribute to the state-of-the-art of this literature by devising algorithms with provable guarantees for the maximization of two natural inter-group criteria, namely the minimum spacing and the minimum spanning tree spacing. The former is the minimum distance between points in different groups while the latter captures separability through the cost of the minimum spanning tree that connects all groups. We obtain results for both the unrestricted case, in which no constraint on the clusters is imposed, and for the constrained case where each group is required to have a minimum number of points. Our constraint is motivated by the fact that the popular Single Linkage, which optimizes both criteria in the unrestricted case, produces clusterings with many tiny groups.   To complement our work, we present an empirical study with 10 real datasets, providing evidence that our methods work very well in practical settings. ",
        "title": "Optimization of Inter-group Criteria for Clustering with Minimum Size  Constraints",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07096",
        "abstract_url": "http://arxiv.org/abs/2401.07096",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Bowen"
            },
            {
                "last_name": "Shi",
                "first_name": "Bin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In the fields of statistics, machine learning, image science, and related areas, there is an increasing demand for decentralized collection or storage of large-scale datasets, as well as distributed solution methods. To tackle this challenge, the alternating direction method of multipliers (ADMM) has emerged as a widely used approach, particularly well-suited to distributed convex optimization. However, the iterative behavior of ADMM has not been well understood. In this paper, we employ dimensional analysis to derive a system of high-resolution ordinary differential equations (ODEs) for ADMM. This system captures an important characteristic of ADMM, called the $\\lambda$-correction, which causes the trajectory of ADMM to deviate from the constrained hyperplane. To explore the convergence behavior of the system of high-resolution ODEs, we utilize Lyapunov analysis and extend our findings to the discrete ADMM algorithm. Through this analysis, we identify that the numerical error resulting from the implicit scheme is a crucial factor that affects the convergence rate and monotonicity in the discrete ADMM algorithm. In addition, we further discover that if one component of the objective function is assumed to be strongly convex, the iterative average of ADMM converges strongly with a rate $O(1/N)$, where $N$ is the number of iterations. ",
        "title": "Understanding the ADMM Algorithm via High-Resolution Differential  Equations",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07098",
        "abstract_url": "http://arxiv.org/abs/2401.07098",
        "authors": [
            {
                "last_name": "Maity",
                "first_name": "Subhankar"
            },
            {
                "last_name": "Deroy",
                "first_name": "Aniket"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Sudeshna"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We introduce a multi-stage prompting approach (MSP) for the generation of multiple choice questions (MCQs), harnessing the capabilities of GPT models such as text-davinci-003 and GPT-4, renowned for their excellence across various NLP tasks. Our approach incorporates the innovative concept of chain-of-thought prompting, a progressive technique in which the GPT model is provided with a series of interconnected cues to guide the MCQ generation process. Automated evaluations consistently demonstrate the superiority of our proposed MSP method over the traditional single-stage prompting (SSP) baseline, resulting in the production of high-quality distractors. Furthermore, the one-shot MSP technique enhances automatic evaluation results, contributing to improved distractor generation in multiple languages, including English, German, Bengali, and Hindi. In human evaluations, questions generated using our approach exhibit superior levels of grammaticality, answerability, and difficulty, highlighting its efficacy in various languages. ",
        "title": "A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ  Generation using GPT",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07100",
        "abstract_url": "http://arxiv.org/abs/2401.07100",
        "authors": [
            {
                "last_name": "Javadi",
                "first_name": "Sepideh"
            },
            {
                "last_name": "Farhadi",
                "first_name": "Armin"
            },
            {
                "last_name": "Mili",
                "first_name": "Mohammad Robat"
            },
            {
                "last_name": "Jorswieck",
                "first_name": "Eduard"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) is a novel technology which enables the full-space coverage by splitting the incident signal into reflected and transmitted signals. In this letter, a multi STAR-RIS-aided system using non-orthogonal multiple access (NOMA) in an uplink transmission is considered, where the multi-order reflections among multiple STAR-RISs assist the transmission from the single-antenna users to the multi-antenna base station (BS). Specifically, the total sum rate maximization problem is solved by jointly optimizing the active beamforming, power allocation, transmission and reflection beamforming at the STAR-RIS, and user-STAR-RIS association indicator. To solve the non-convex optimization problem, a novel deep reinforcement learning algorithm is proposed which is the combination of meta-learning and deep deterministic policy gradient (DDPG), namely Meta-DDPG. Numerical results demonstrate that the proposed Meta-DDPG algorithm outperforms the conventional DDPG algorithm. ",
        "title": "Resource Allocation in Uplink Multi STAR-RIS-aided NOMA System via  Meta-Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07102",
        "abstract_url": "http://arxiv.org/abs/2401.07102",
        "authors": [
            {
                "last_name": "Hemberg",
                "first_name": "Erik"
            },
            {
                "last_name": "Moskal",
                "first_name": "Stephen"
            },
            {
                "last_name": "O'Reilly",
                "first_name": "Una-May"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM GP, a formalized LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators radically differ from GP's because they enlist an LLM, using prompting and the LLM's pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM GP and share its code. By addressing algorithms that range from the formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming. ",
        "title": "Evolving Code with A Large Language Model",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07103",
        "abstract_url": "http://arxiv.org/abs/2401.07103",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhen"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Shen",
                "first_name": "Tao"
            },
            {
                "last_name": "Xu",
                "first_name": "Can"
            },
            {
                "last_name": "Gu",
                "first_name": "Jia-Chen"
            },
            {
                "last_name": "Tao",
                "first_name": "Chongyang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This survey aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques. ",
        "title": "Leveraging Large Language Models for NLG Evaluation: A Survey",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07105",
        "abstract_url": "http://arxiv.org/abs/2401.07105",
        "authors": [
            {
                "last_name": "Plenz",
                "first_name": "Moritz"
            },
            {
                "last_name": "Frank",
                "first_name": "Anette"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  While Language Models have become workhorses for NLP, their interplay with textual knowledge graphs (KGs) - structured memories of general or domain knowledge - is actively researched. Current embedding methodologies for such graphs typically either (i) linearize graphs for embedding them using sequential Language Models (LMs), which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve graph structure, while GNNs cannot represent textual features as well as a pre-trained LM could. In this work we introduce a novel language model, the Graph Language Model (GLM), that integrates the strengths of both approaches, while mitigating their weaknesses. The GLM parameters are initialized from a pretrained LM, to facilitate nuanced understanding of individual concepts and triplets. Simultaneously, its architectural design incorporates graph biases, thereby promoting effective knowledge distribution within the graph. Empirical evaluations on relation classification tasks on ConceptNet subgraphs reveal that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot settings. ",
        "title": "Graph Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07106",
        "abstract_url": "http://arxiv.org/abs/2401.07106",
        "authors": [
            {
                "last_name": "Ganardi",
                "first_name": "Moses"
            },
            {
                "last_name": "Saglam",
                "first_name": "Irmak"
            },
            {
                "last_name": "Zetzsche",
                "first_name": "Georg"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "CL"
        ],
        "abstract": "  We study the problem of deciding whether a given language is directed. A language $L$ is \\emph{directed} if every pair of words in $L$ have a common (scattered) superword in $L$. Deciding directedness is a fundamental problem in connection with ideal decompositions of downward closed sets. Another motivation is that deciding whether two \\emph{directed} context-free languages have the same downward closures can be decided in polynomial time, whereas for general context-free languages, this problem is known to be coNEXP-complete.   We show that the directedness problem for regular languages, given as NFAs, belongs to $AC^1$, and thus polynomial time. Moreover, it is NL-complete for fixed alphabet sizes. Furthermore, we show that for context-free languages, the directedness problem is PSPACE-complete. ",
        "title": "Directed Regular and Context-Free Languages",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07108",
        "abstract_url": "http://arxiv.org/abs/2401.07108",
        "authors": [
            {
                "last_name": "Agouzal",
                "first_name": "Eki"
            },
            {
                "last_name": "Taddei",
                "first_name": "Tommaso"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present an accelerated greedy strategy for training of projection-based reduced-order models for parametric steady and unsteady partial differential equations. Our approach exploits hierarchical approximate proper orthogonal decomposition to speed up the construction of the empirical test space for least-square Petrov-Galerkin formulations, a progressive construction of the empirical quadrature rule based on a warm start of the non-negative least-square algorithm, and a two-fidelity sampling strategy to reduce the number of expensive greedy iterations. We illustrate the performance of our method for two test cases: a two-dimensional compressible inviscid flow past a LS89 blade at moderate Mach number, and a three-dimensional nonlinear mechanics problem to predict the long-time structural response of the standard section of a nuclear containment building under external loading. ",
        "title": "Accelerated construction of projection-based reduced-order models via  incremental approaches",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07114",
        "abstract_url": "http://arxiv.org/abs/2401.07114",
        "authors": [
            {
                "last_name": "Rydell",
                "first_name": "Felix"
            },
            {
                "last_name": "Torres",
                "first_name": "Ang\u00e9lica"
            },
            {
                "last_name": "Larsson",
                "first_name": "Viktor"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Many problems in computer vision can be formulated as geometric estimation problems, i.e. given a collection of measurements (e.g. point correspondences) we wish to fit a model (e.g. an essential matrix) that agrees with our observations. This necessitates some measure of how much an observation ``agrees\" with a given model. A natural choice is to consider the smallest perturbation that makes the observation exactly satisfy the constraints. However, for many problems, this metric is expensive or otherwise intractable to compute. The so-called Sampson error approximates this geometric error through a linearization scheme. For epipolar geometry, the Sampson error is a popular choice and in practice known to yield very tight approximations of the corresponding geometric residual (the reprojection error).   In this paper we revisit the Sampson approximation and provide new theoretical insights as to why and when this approximation works, as well as provide explicit bounds on the tightness under some mild assumptions. Our theoretical results are validated in several experiments on real data and in the context of different geometric estimation tasks. ",
        "title": "Revisiting Sampson Approximations for Geometric Estimation Problems",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07115",
        "abstract_url": "http://arxiv.org/abs/2401.07115",
        "authors": [
            {
                "last_name": "La Cava",
                "first_name": "Lucio"
            },
            {
                "last_name": "Costa",
                "first_name": "Davide"
            },
            {
                "last_name": "Tagarelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY",
            "HC"
        ],
        "abstract": "  The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology, leading to a proliferation of computational agents. Scholars have been studying the inherent personalities displayed by LLM agents and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by conducting a comprehensive examination of the ability of agents to emulate human personalities using Open LLMs. To achieve this, we generate a set of ten LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that: $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities; and $(iv)$ personalities typically associated with the role of teacher tend to be emulated with greater accuracy. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs. ",
        "title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human  Personalities through Open Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07118",
        "abstract_url": "http://arxiv.org/abs/2401.07118",
        "authors": [
            {
                "last_name": "Pascher",
                "first_name": "Max"
            },
            {
                "last_name": "Zinta",
                "first_name": "Kevin"
            },
            {
                "last_name": "Gerken",
                "first_name": "Jens"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  Robotic arms, integral in domestic care for individuals with motor impairments, enable them to perform Activities of Daily Living (ADLs) independently, reducing dependence on human caregivers. These collaborative robots require users to manage multiple Degrees-of-Freedom (DoFs) for tasks like grasping and manipulating objects. Conventional input devices, typically limited to two DoFs, necessitate frequent and complex mode switches to control individual DoFs. Modern adaptive controls with feed-forward multi-modal feedback reduce the overall task completion time, number of mode switches, and cognitive load. Despite the variety of input devices available, their effectiveness in adaptive settings with assistive robotics has yet to be thoroughly assessed. This study explores three different input devices by integrating them into an established XR framework for assistive robotics, evaluating them and providing empirical insights through a preliminary study for future developments. ",
        "title": "Exploring of Discrete and Continuous Input Control for AI-enhanced  Assistive Robotic Arms",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07119",
        "abstract_url": "http://arxiv.org/abs/2401.07119",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Yicheng"
            },
            {
                "last_name": "Wu",
                "first_name": "Yongji"
            },
            {
                "last_name": "Hu",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Maggs",
                "first_name": "Bruce M."
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhuo",
                "first_name": "Danyang"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "DC",
            "IR",
            "LG"
        ],
        "abstract": "  Vector databases have emerged as key enablers for bridging intelligent applications with unstructured data, providing generic search and management support for embedding vectors extracted from the raw unstructured data. As multiple data users can share the same database infrastructure, multi-tenancy support for vector databases is increasingly desirable. This hinges on an efficient filtered search operation, i.e., only querying the vectors accessible to a particular tenant. Multi-tenancy in vector databases is currently achieved by building either a single, shared index among all tenants, or a per-tenant index. The former optimizes for memory efficiency at the expense of search performance, while the latter does the opposite. Instead, this paper presents Curator, an in-memory vector index design tailored for multi-tenant queries that simultaneously achieves the two conflicting goals, low memory overhead and high performance for queries, vector insertion, and deletion. Curator indexes each tenant's vectors with a tenant-specific clustering tree and encodes these trees compactly as sub-trees of a shared clustering tree. Each tenant's clustering tree adapts dynamically to its unique vector distribution, while maintaining a low per-tenant memory footprint. Our evaluation, based on two widely used data sets, confirms that Curator delivers search performance on par with per-tenant indexing, while maintaining memory consumption at the same level as metadata filtering on a single, shared index. ",
        "title": "Curator: Efficient Indexing for Multi-Tenant Vector Databases",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07120",
        "abstract_url": "http://arxiv.org/abs/2401.07120",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Minrui"
            },
            {
                "last_name": "Niyato",
                "first_name": "Dusit"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zehui"
            },
            {
                "last_name": "Cao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Gao",
                "first_name": "Yulan"
            },
            {
                "last_name": "Ren",
                "first_name": "Chao"
            },
            {
                "last_name": "Yu",
                "first_name": "Han"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Quantum computing networks enable scalable collaboration and secure information exchange among multiple classical and quantum computing nodes while executing large-scale generative AI computation tasks and advanced quantum algorithms. Quantum computing networks overcome limitations such as the number of qubits and coherence time of entangled pairs and offer advantages for generative AI infrastructure, including enhanced noise reduction through distributed processing and improved scalability by connecting multiple quantum devices. However, efficient resource allocation in quantum computing networks is a critical challenge due to factors including qubit variability and network complexity. In this article, we propose an intelligent resource allocation framework for quantum computing networks to improve network scalability with minimized resource costs. To achieve scalability in quantum computing networks, we formulate the resource allocation problem as stochastic programming, accounting for the uncertain fidelities of qubits and entangled pairs. Furthermore, we introduce state-of-the-art reinforcement learning (RL) algorithms, from generative learning to quantum machine learning for optimal quantum resource allocation to resolve the proposed stochastic resource allocation problem efficiently. Finally, we optimize the resource allocation in heterogeneous quantum computing networks supporting quantum generative learning applications and propose a multi-agent RL-based algorithm to learn the optimal resource allocation policies without prior knowledge. ",
        "title": "Generative AI-enabled Quantum Computing Networks and Intelligent  Resource Allocation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07121",
        "abstract_url": "http://arxiv.org/abs/2401.07121",
        "authors": [
            {
                "last_name": "Parolini",
                "first_name": "Nicola"
            },
            {
                "last_name": "Poiatti",
                "first_name": "Andrea"
            },
            {
                "last_name": "Vene'",
                "first_name": "Julian"
            },
            {
                "last_name": "Verani",
                "first_name": "Marco"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we address the importance and the impact of employing structure preserving neural networks as surrogate of the analytical physics-based models typically employed to describe the rheology of non-Newtonian fluids in Stokes flows. In particular, we propose and test on real-world scenarios a novel strategy to build data-driven rheological models based on the use of Input-Output Convex Neural Networks (ICNNs), a special class of feedforward neural network scalar valued functions that are convex with respect to their inputs. Moreover, we show, through a detailed campaign of numerical experiments, that the use of ICNNs is of paramount importance to guarantee the well-posedness of the associated non-Newtonian Stokes differential problem. Finally, building upon a novel perturbation result for non-Newtonian Stokes problems, we study the impact of our data-driven ICNN based rheological model on the accuracy of the finite element approximation. ",
        "title": "Structure-preserving neural networks in data-driven rheological models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07122",
        "abstract_url": "http://arxiv.org/abs/2401.07122",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Haihui"
            },
            {
                "last_name": "Xia",
                "first_name": "Minghua"
            },
            {
                "last_name": "Wu",
                "first_name": "Peiran"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Huang",
                "first_name": "Kaibin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Federated learning (FL) enables wireless terminals to collaboratively learn a shared parameter model while keeping all the training data on devices per se. Parameter sharing consists of synchronous and asynchronous ways: the former transmits parameters as blocks or frames and waits until all transmissions finish, whereas the latter provides messages about the status of pending and failed parameter transmission requests. Whatever synchronous or asynchronous parameter sharing is applied, the learning model shall adapt to distinct network architectures as an improper learning model will deteriorate learning performance and, even worse, lead to model divergence for the asynchronous transmission in resource-limited large-scale Internet-of-Things (IoT) networks. This paper proposes a decentralized learning model and develops an asynchronous parameter-sharing algorithm for resource-limited distributed IoT networks. This decentralized learning model approaches a convex function as the number of nodes increases, and its learning process converges to a global stationary point with a higher probability than the centralized FL model. Moreover, by jointly accounting for the convergence bound of federated learning and the transmission delay of wireless communications, we develop a node scheduling and bandwidth allocation algorithm to minimize the transmission delay. Extensive simulation results corroborate the effectiveness of the distributed algorithm in terms of fast learning model convergence and low transmission delay. ",
        "title": "Decentralized Federated Learning with Asynchronous Parameter Sharing for  Large-scale IoT Networks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07123",
        "abstract_url": "http://arxiv.org/abs/2401.07123",
        "authors": [
            {
                "last_name": "Clarke",
                "first_name": "Christopher"
            },
            {
                "last_name": "Krishnamurthy",
                "first_name": "Karthik"
            },
            {
                "last_name": "Talamonti",
                "first_name": "Walter"
            },
            {
                "last_name": "Kang",
                "first_name": "Yiping"
            },
            {
                "last_name": "Tang",
                "first_name": "Lingjia"
            },
            {
                "last_name": "Mars",
                "first_name": "Jason"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CL"
        ],
        "abstract": "  Conversational agents have been gaining increasing popularity in recent years. Influenced by the widespread adoption of task-oriented agents such as Apple Siri and Amazon Alexa, these agents are being deployed into various applications to enhance user experience. Although these agents promote \"ask me anything\" functionality, they are typically built to focus on a single or finite set of expertise. Given that complex tasks often require more than one expertise, this results in the users needing to learn and adopt multiple agents. One approach to alleviate this is to abstract the orchestration of agents in the background. However, this removes the option of choice and flexibility, potentially harming the ability to complete tasks. In this paper, we explore these different interaction experiences (one agent for all) vs (user choice of agents) for conversational AI. We design prototypes for each, systematically evaluating their ability to facilitate task completion. Through a series of conducted user studies, we show that users have a significant preference for abstracting agent orchestration in both system usability and system performance. Additionally, we demonstrate that this mode of interaction is able to provide quality responses that are rated within 1% of human-selected answers. ",
        "title": "One Agent Too Many: User Perspectives on Approaches to Multi-agent  Conversational AI",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07124",
        "abstract_url": "http://arxiv.org/abs/2401.07124",
        "authors": [
            {
                "last_name": "Zadeh",
                "first_name": "Sara Shomal"
            },
            {
                "last_name": "birgani",
                "first_name": "Sina Aalipour"
            },
            {
                "last_name": "Khorshidi",
                "first_name": "Meisam"
            },
            {
                "last_name": "Kooban",
                "first_name": "Farhad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Effective crack detection is pivotal for the structural health monitoring and inspection of buildings. This task presents a formidable challenge to computer vision techniques due to the inherently subtle nature of cracks, which often exhibit low-level features that can be easily confounded with background textures, foreign objects, or irregularities in construction. Furthermore, the presence of issues like non-uniform lighting and construction irregularities poses significant hurdles for autonomous crack detection during building inspection and monitoring. Convolutional neural networks (CNNs) have emerged as a promising framework for crack detection, offering high levels of accuracy and precision. Additionally, the ability to adapt pre-trained networks through transfer learning provides a valuable tool for users, eliminating the need for an in-depth understanding of algorithm intricacies. Nevertheless, it is imperative to acknowledge the limitations and considerations when deploying CNNs, particularly in contexts where the outcomes carry immense significance, such as crack detection in buildings. In this paper, our approach to surface crack detection involves the utilization of various deep-learning models. Specifically, we employ fine-tuning techniques on pre-trained deep learning architectures: VGG19, ResNet50, Inception V3, and EfficientNetV2. These models are chosen for their established performance and versatility in image analysis tasks. We compare deep learning models using precision, recall, and F1 scores. ",
        "title": "Concrete Surface Crack Detection with Convolutional-based Deep Learning  Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07126",
        "abstract_url": "http://arxiv.org/abs/2401.07126",
        "authors": [
            {
                "last_name": "Kertes",
                "first_name": "Noga"
            },
            {
                "last_name": "Zaffrani-Reznikov",
                "first_name": "Yael"
            },
            {
                "last_name": "Afacan",
                "first_name": "Onur"
            },
            {
                "last_name": "Kurugol",
                "first_name": "Sila"
            },
            {
                "last_name": "Warfield",
                "first_name": "Simon K."
            },
            {
                "last_name": "Freiman",
                "first_name": "Moti"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Quantitative analysis of pseudo-diffusion in diffusion-weighted magnetic resonance imaging (DWI) data shows potential for assessing fetal lung maturation and generating valuable imaging biomarkers. Yet, the clinical utility of DWI data is hindered by unavoidable fetal motion during acquisition. We present IVIM-morph, a self-supervised deep neural network model for motion-corrected quantitative analysis of DWI data using the Intra-voxel Incoherent Motion (IVIM) model. IVIM-morph combines two sub-networks, a registration sub-network, and an IVIM model fitting sub-network, enabling simultaneous estimation of IVIM model parameters and motion. To promote physically plausible image registration, we introduce a biophysically informed loss function that effectively balances registration and model-fitting quality. We validated the efficacy of IVIM-morph by establishing a correlation between the predicted IVIM model parameters of the lung and gestational age (GA) using fetal DWI data of 39 subjects. IVIM-morph exhibited a notably improved correlation with gestational age (GA) when performing in-vivo quantitative analysis of fetal lung DWI data during the canalicular phase. IVIM-morph shows potential in developing valuable biomarkers for non-invasive assessment of fetal lung maturity with DWI data. Moreover, its adaptability opens the door to potential applications in other clinical contexts where motion compensation is essential for quantitative DWI analysis. The IVIM-morph code is readily available at: https://github.com/TechnionComputationalMRILab/qDWI-Morph. ",
        "title": "IVIM-Morph: Motion-compensated quantitative Intra-voxel Incoherent  Motion (IVIM) analysis for functional fetal lung maturity assessment from  diffusion-weighted MRI data",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07128",
        "abstract_url": "http://arxiv.org/abs/2401.07128",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Wenqi"
            },
            {
                "last_name": "Xu",
                "first_name": "Ran"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Yu",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jieyu"
            },
            {
                "last_name": "Wu",
                "first_name": "Hang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yuanda"
            },
            {
                "last_name": "Ho",
                "first_name": "Joyce"
            },
            {
                "last_name": "Yang",
                "first_name": "Carl"
            },
            {
                "last_name": "Wang",
                "first_name": "May D."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent1, an LLM agent empowered with a code interface, to autonomously generate and execute code for complex clinical tasks within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on two real-world EHR datasets show that EHRAgent outperforms the strongest LLM agent baseline by 36.48% and 12.41%, respectively. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations. ",
        "title": "EHRAgent: Code Empowers Large Language Models for Complex Tabular  Reasoning on Electronic Health Records",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07132",
        "abstract_url": "http://arxiv.org/abs/2401.07132",
        "authors": [
            {
                "last_name": "Boffi",
                "first_name": "Daniele"
            },
            {
                "last_name": "Khan",
                "first_name": "Arbaz"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper we discuss the optimal convergence of a standard adaptive scheme based on mixed finite element approximation to the solution of the eigenvalue problem associated with the Stokes equations. The proofs of the quasi-orthogonality and the discrete reliability are presented. Our numerical experiments confirm the efficacy of the proposed adaptive scheme. ",
        "title": "Adaptive Mixed FEM for the Stokes eigenvalue problem",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07139",
        "abstract_url": "http://arxiv.org/abs/2401.07139",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Yi"
            },
            {
                "last_name": "Yuan",
                "first_name": "Qiangqiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Liangpei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent efforts have witnessed remarkable progress in Satellite Video Super-Resolution (SVSR). However, most SVSR methods usually assume the degradation is fixed and known, e.g., bicubic downsampling, which makes them vulnerable in real-world scenes with multiple and unknown degradations. To alleviate this issue, blind SR has thus become a research hotspot. Nevertheless, existing approaches are mainly engaged in blur kernel estimation while losing sight of another critical aspect for VSR tasks: temporal compensation, especially compensating for blurry and smooth pixels with vital sharpness from severely degraded satellite videos. Therefore, this paper proposes a practical Blind SVSR algorithm (BSVSR) to explore more sharp cues by considering the pixel-wise blur levels in a coarse-to-fine manner. Specifically, we employed multi-scale deformable convolution to coarsely aggregate the temporal redundancy into adjacent frames by window-slid progressive fusion. Then the adjacent features are finely merged into mid-feature using deformable attention, which measures the blur levels of pixels and assigns more weights to the informative pixels, thus inspiring the representation of sharpness. Moreover, we devise a pyramid spatial transformation module to adjust the solution space of sharp mid-feature, resulting in flexible feature adaptation in multi-level domains. Quantitative and qualitative evaluations on both simulated and real-world satellite videos demonstrate that our BSVSR performs favorably against state-of-the-art non-blind and blind SR models. Code will be available at https://github.com/XY-boy/Blind-Satellite-VSR ",
        "title": "Deep Blind Super-Resolution for Satellite Video",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07140",
        "abstract_url": "http://arxiv.org/abs/2401.07140",
        "authors": [
            {
                "last_name": "Cuesta",
                "first_name": "Carlota M."
            },
            {
                "last_name": "de la Hoz",
                "first_name": "Francisco"
            },
            {
                "last_name": "Girona",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we develop an accurate pseudospectral method to approximate numerically the Riesz-Feller operator $D_\\gamma^\\alpha$ on $\\mathbb R$, where $\\alpha\\in(0,2)$, and $|\\gamma|\\le\\min\\{\\alpha, 2 - \\alpha\\}$. This operator can be written as a linear combination of the Weyl-Marchaud derivatives $\\mathcal{D}^{\\alpha}$ and $\\overline{\\mathcal{D}^\\alpha}$, when $\\alpha\\in(0,1)$, and of $\\partial_x\\mathcal{D}^{\\alpha-1}$ and $\\partial_x\\overline{\\mathcal{D}^{\\alpha-1}}$, when $\\alpha\\in(1,2)$.   Given the so-called Higgins functions $\\lambda_k(x) = ((ix-1)/(ix+1))^k$, where $k\\in\\mathbb Z$, we compute explicitly, using complex variable techniques, $\\mathcal{D}^{\\alpha}[\\lambda_k](x)$, $\\overline{\\mathcal{D}^\\alpha}[\\lambda_k](x)$, $\\partial_x\\mathcal{D}^{\\alpha-1}[\\lambda_k](x)$, $\\partial_x\\overline{\\mathcal{D}^{\\alpha-1}}[\\lambda_k](x)$ and $D_\\gamma^\\alpha[\\lambda_k](x)$, in terms of the Gaussian hypergeometric function ${}_2F_1$, and relate these results to previous ones for the fractional Laplacian. This enables us to approximate $\\mathcal{D}^{\\alpha}[u](x)$, $\\overline{\\mathcal{D}^\\alpha}[u](x)$, $\\partial_x\\mathcal{D}^{\\alpha-1}[u](x)$, $\\partial_x\\overline{\\mathcal{D}^{\\alpha-1}}[u](x)$ and $D_\\gamma^\\alpha[u](x)$, for bounded continuous functions $u(x)$. Finally, we simulate a nonlinear Riesz-Feller fractional diffusion equation, characterized by having front propagating solutions whose speed grows exponentially in time. ",
        "title": "Numerical Approximation of Riesz-Feller Operators on $\\mathbb R$",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07141",
        "abstract_url": "http://arxiv.org/abs/2401.07141",
        "authors": [
            {
                "last_name": "Nikkhah",
                "first_name": "Ali"
            },
            {
                "last_name": "Shoushtari",
                "first_name": "Morteza"
            },
            {
                "last_name": "Akhbari",
                "first_name": "Bahareh"
            },
            {
                "last_name": "Harrison",
                "first_name": "Willie K."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we use a linear programming (LP) optimization approach to evaluate the equivocation for a wiretap channel where the main channel is noiseless, and the wiretap channel is a binary symmetric channel (BSC). Using this technique, we present an analytical limit for the achievable secrecy rate in the finite blocklength regime that is tighter than traditional fundamental limits. We also propose a secrecy coding technique that outperforms random binning codes. When there is one overhead bit, this coding technique is optimum and achieves the analytical limit. For cases with additional bits of overhead, our coding scheme can achieve equivocation rates close to the new limit. Furthermore, we evaluate the patterns of the generator matrix and the parity-check matrix for linear codes and we present binning techniques for both linear and non-linear codes using two different approaches: recursive and non-recursive. To our knowledge, this is the first optimization solution for secrecy coding obtained through linear programming. ",
        "title": "Secrecy Coding for the Binary Symmetric Wiretap Channel via Linear  Programming",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07142",
        "abstract_url": "http://arxiv.org/abs/2401.07142",
        "authors": [
            {
                "last_name": "Aksoy",
                "first_name": "Levent"
            },
            {
                "last_name": "Yasin",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Pagliarini",
                "first_name": "Samuel"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Logic locking proposed to protect integrated circuits from serious hardware threats has been studied extensively over a decade. In these years, many efficient logic locking techniques have been proven to be broken. The state-of-the-art logic locking techniques, including the prominent corrupt and correct (CAC) technique, are resilient to satisfiability (SAT)-based and removal attacks, but vulnerable to structural analysis attacks. To overcome this drawback, this paper introduces an improved version of CAC, called CAC 2.0, which increases the search space of structural analysis attacks using obfuscation. To do so, CAC 2.0 locks the original circuit twice, one after another, on different nodes with different number of protected primary inputs using CAC, while hiding original protected primary inputs among decoy primary inputs. This paper also introduces an open source logic locking tool, called HIID, equipped with well-known techniques including CAC 2.0. Our experiments show that CAC 2.0 is resilient to existing SAT-based, removal, and structural analysis attacks. To achieve this, it increases the number of key inputs at most 4x and the gate-level area between 30.2% and 0.8% on circuits with low and high complexity with respect to CAC. ",
        "title": "CAC 2.0: A Corrupt and Correct Logic Locking Technique Resilient to  Structural Analysis Attacks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07143",
        "abstract_url": "http://arxiv.org/abs/2401.07143",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Hossam O."
            },
            {
                "last_name": "Wyatt",
                "first_name": "David"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The demand for more developed and agile urban taxi drones is increasing rapidly nowadays to sustain crowded cities and their traffic issues. The critical factor for spreading such technology could be related to the safety criteria that must be considered. One of the most critical safety aspects for such vertical and/or Short Take-Off and Landing (V/STOL) drones is related to safety during the landing stage, in which most of the recent flight accidents have occurred. This paper focused on solving this issue by proposing decentralized processing cores that could improve the landing failure rate by depending on a Fuzzy Logic System (FLS) and additional Digital Signal Processing (DSP) elements. Also, the proposed system will enhance the safety factor during the landing stages by adding a self-awareness feature in case a certain sensor malfunction occurs using the proposed Adaptive Prognostic Malfunction Unit (APMU). This proposed coarse-grained Autonomous Landing Guidance Assistance System (ALGAS4) processing architecture has been optimized using different optimization techniques. The ALGAS4 architecture has been designed completely using VHDL, and the targeted FPGA was the INTEL Cyclone V 5CGXFC9D6F27C7 chip. According to the synthesis findings of the INTEL Quartus Prime software, the maximum working frequency of the ALGAS4 system is 278.24 MHz. In addition, the proposed ALGAS4 system could maintain a maximum computing performance of approximately 74.85 GOPS while using just 166.56 mW for dynamic and I/O power dissipation. ",
        "title": "Adaptive Prognostic Malfunction Based Processor for Autonomous Landing  Guidance Assistance System Using FPGA",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07145",
        "abstract_url": "http://arxiv.org/abs/2401.07145",
        "authors": [
            {
                "last_name": "Ahmed",
                "first_name": "Soyed Tuhin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Neural networks (NNs) can achieved high performance in various fields such as computer vision, and natural language processing. However, deploying NNs in resource-constrained safety-critical systems has challenges due to uncertainty in the prediction caused by out-of-distribution data, and hardware non-idealities. To address the challenges of deploying NNs in resource-constrained safety-critical systems, this paper summarizes the (4th year) PhD thesis work that explores scalable and efficient methods for uncertainty estimation and reduction in deep learning, with a focus on Computation-in-Memory (CIM) using emerging resistive non-volatile memories. We tackle the inherent uncertainties arising from out-of-distribution inputs and hardware non-idealities, crucial in maintaining functional safety in automated decision-making systems. Our approach encompasses problem-aware training algorithms, novel NN topologies, and hardware co-design solutions, including dropout-based \\emph{binary} Bayesian Neural Networks leveraging spintronic devices and variational inference techniques. These innovations significantly enhance OOD data detection, inference accuracy, and energy efficiency, thereby contributing to the reliability and robustness of NN implementations. ",
        "title": "Scalable and Efficient Methods for Uncertainty Estimation and Reduction  in Deep Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07147",
        "abstract_url": "http://arxiv.org/abs/2401.07147",
        "authors": [
            {
                "last_name": "Pago",
                "first_name": "Benedikt"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  The search for a logic capturing PTIME is a long standing open problem in finite model theory. One of the most promising candidate logics for this is Choiceless Polynomial Time with counting (CPT). Abstractly speaking, CPT is an isomorphism-invariant computation model working with hereditarily finite sets as data structures. While it is easy to check that the evaluation of CPT-sentences is possible in polynomial time, the converse has been open for more than 20 years: Can every PTIME-decidable property of finite structures be expressed in CPT? We attempt to make progress towards a negative answer and show that Choiceless Polynomial Time cannot compute a preorder with colour classes of logarithmic size in every hypercube. The reason is that such preorders have super-polynomially many automorphic images, which makes it impossible for CPT to define them. While the computation of such a preorder is not a decision problem that would immediately separate P and CPT, it is significant for the following reason: The so-called Cai-F\\\"urer-Immerman (CFI) problem is one of the standard benchmarks for logics and maybe best known for separating fixed-point logic with counting (FPC) from P. Hence, it is natural to consider this also a potential candidate for the separation of CPT and P. The strongest known positive result in this regard says that CPT is able to solve CFI if a preorder with logarithmically sized colour classes is present in the input structure. Our result implies that this approach cannot be generalised to unordered inputs. In other words, CFI on unordered hypercubes is a PTIME-problem which provably cannot be tackled with the state-of-the-art choiceless algorithmic techniques. ",
        "title": "Choiceless Computation and Symmetry: Limitations of Definability",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07148",
        "abstract_url": "http://arxiv.org/abs/2401.07148",
        "authors": [
            {
                "last_name": "Vaidya",
                "first_name": "Ruturaj K."
            },
            {
                "last_name": "Kulkarni",
                "first_name": "Prasad A."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  Memory corruption is an important class of vulnerability that can be leveraged to craft control flow hijacking attacks. Control Flow Integrity (CFI) provides protection against such attacks. Application of type-based CFI policies requires information regarding the number and type of function arguments. Binary-level type recovery is inherently speculative, which motivates the need for an evaluation framework to assess the effectiveness of binary-level CFI techniques compared with their source-level counterparts, where such type information is fully and accurately accessible. In this work, we develop a novel, generalized and extensible framework to assess how the program analysis information we get from state-of-the-art binary analysis tools affects the efficacy of type-based CFI techniques. We introduce new and insightful metrics to quantitatively compare source independent CFI policies with their ground truth source aware counterparts. We leverage our framework to evaluate binary-level CFI policies implemented using program analysis information extracted from the IDA Pro binary analyzer and compared with the ground truth information obtained from the LLVM compiler, and present our observations. ",
        "title": "Assessing the Effectiveness of Binary-Level CFI Techniques",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07154",
        "abstract_url": "http://arxiv.org/abs/2401.07154",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Cheng"
            },
            {
                "last_name": "Kakkar",
                "first_name": "Akshay"
            },
            {
                "last_name": "Redino",
                "first_name": "Christopher"
            },
            {
                "last_name": "Rahman",
                "first_name": "Abdul"
            },
            {
                "last_name": "S",
                "first_name": "Ajinsyam"
            },
            {
                "last_name": "Clark",
                "first_name": "Ryan"
            },
            {
                "last_name": "Radke",
                "first_name": "Daniel"
            },
            {
                "last_name": "Cody",
                "first_name": "Tyler"
            },
            {
                "last_name": "Huang",
                "first_name": "Lanxiao"
            },
            {
                "last_name": "Bowen",
                "first_name": "Edward"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Command and control (C2) paths for issuing commands to malware are sometimes the only indicators of its existence within networks. Identifying potential C2 channels is often a manually driven process that involves a deep understanding of cyber tradecraft. Efforts to improve discovery of these channels through using a reinforcement learning (RL) based approach that learns to automatically carry out C2 attack campaigns on large networks, where multiple defense layers are in place serves to drive efficiency for network operators. In this paper, we model C2 traffic flow as a three-stage process and formulate it as a Markov decision process (MDP) with the objective to maximize the number of valuable hosts whose data is exfiltrated. The approach also specifically models payload and defense mechanisms such as firewalls which is a novel contribution. The attack paths learned by the RL agent can in turn help the blue team identify high-priority vulnerabilities and develop improved defense strategies. The method is evaluated on a large network with more than a thousand hosts and the results demonstrate that the agent can effectively learn attack paths while avoiding firewalls. ",
        "title": "Discovering Command and Control Channels Using Reinforcement Learning",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07157",
        "abstract_url": "http://arxiv.org/abs/2401.07157",
        "authors": [
            {
                "last_name": "Vafiadis",
                "first_name": "Dimitris"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The problem of decoupling a nonsquare state space system by state feedback with singular input transformation is considered. The problem is solved by conducting a finite search for decouplable square systems, appropriately derived from the original. Decoupling feedback on any of these systems defines the decoupling feedback for the original. The issue of fixed poles is also considered and the possibility of selecting the uncontrollable poles is investigated. ",
        "title": "A matrix pencil approach to the Morgan's problem",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07159",
        "abstract_url": "http://arxiv.org/abs/2401.07159",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhengxin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Dan"
            },
            {
                "last_name": "Miao",
                "first_name": "Xupeng"
            },
            {
                "last_name": "Oliaro",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yong"
            },
            {
                "last_name": "Jia",
                "first_name": "Zhihao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory and none can simultaneously mitigate memory footprint for all three sources. In this paper, we present Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM's model weights into 4-bit to reduce the memory footprint of the LLM's original weights; QST also introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing backpropagation through the LLM, thus reducing the memory requirement of the intermediate activations. Furthermore, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3 $\\times$ and speed up the finetuning process by up to 3 $\\times$ while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7 $\\times$. ",
        "title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized  Large Language Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07162",
        "abstract_url": "http://arxiv.org/abs/2401.07162",
        "authors": [
            {
                "last_name": "Karihaloo",
                "first_name": "Vivek"
            },
            {
                "last_name": "Shah",
                "first_name": "Ruchi"
            },
            {
                "last_name": "Wu",
                "first_name": "Panruo"
            },
            {
                "last_name": "Laszka",
                "first_name": "Aron"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "DC"
        ],
        "abstract": "  Fueled by the increasing popularity of proof-of-stake blockchains, there has been increasing interest and progress in permissioned consensus protocols, which could provide a simpler alternative to existing protocols, such as Paxos and PBFT. In particular, the recently proposed Streamlet protocol provides a surprisingly simple and streamlined consensus protocol, which crystallizes years of research in simplifying and improving classic consensus protocols. While the simplicity of Streamlet is a major accomplishment, the protocol lacks certain practical features, such as working without synchronized clocks and supporting a stable block proposer, which limits its applicability. In this paper, we strive to approach the simplicity of Streamlet and the performance of PaLa, by introducing Pipelet, a streamlined version of the PaLa protocol. We formally prove the consistency and liveness of the Pipelet protocol in a partially synchronous communication model. ",
        "title": "Pipelet: Practical Streamlined Blockchain Protocol",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07163",
        "abstract_url": "http://arxiv.org/abs/2401.07163",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zihao"
            },
            {
                "last_name": "Hou",
                "first_name": "Yu"
            },
            {
                "last_name": "Soibelman",
                "first_name": "Lucio"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The potential energy loss of aging buildings traps building owners in a cycle of underfunding operations and overpaying maintenance costs. Energy auditors intending to generate an energy model of a target building for performance assessment may struggle to obtain accurate results as the spatial distribution of temperatures is not considered when calculating the U-value of the building envelope. This paper proposes a pixel-level method based on infrared thermography (IRT) that considers two-dimensional (2D) spatial temperature distributions of the outdoor and indoor surfaces of the target wall to generate a 2D U-value map of the wall. The result supports that the proposed method can better reflect the actual thermal insulation performance of the target wall compared to the current IRT-based methods that use a single-point room temperature as input. ",
        "title": "A New Method of Pixel-level In-situ U-value Measurement for Building  Envelopes Based on Infrared Thermography",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07164",
        "abstract_url": "http://arxiv.org/abs/2401.07164",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Shuo"
            },
            {
                "last_name": "Mielle",
                "first_name": "Malcolm"
            },
            {
                "last_name": "Lilienthal",
                "first_name": "Achim J."
            },
            {
                "last_name": "Magnusson",
                "first_name": "Martin"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Neural implicit surface representations are currently receiving a lot of interest as a means to achieve high-fidelity surface reconstruction at a low memory cost, compared to traditional explicit representations.However, state-of-the-art methods still struggle with excessive memory usage and non-smooth surfaces. This is particularly problematic in large-scale applications with sparse inputs, as is common in robotics use cases. To address these issues, we first introduce a sparse structure, \\emph{tri-quadtrees}, which represents the environment using learnable features stored in three planar quadtree projections. Secondly, we concatenate the learnable features with a Fourier feature positional encoding. The combined features are then decoded into signed distance values through a small multi-layer perceptron. We demonstrate that this approach facilitates smoother reconstruction with a higher completion ratio with fewer holes. Compared to two recent baselines, one implicit and one explicit, our approach requires only 10\\%--50\\% as much memory, while achieving competitive quality. ",
        "title": "3QFP: Efficient neural implicit surface reconstruction using  Tri-Quadtrees and Fourier feature Positional encoding",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07167",
        "abstract_url": "http://arxiv.org/abs/2401.07167",
        "authors": [
            {
                "last_name": "Mandal",
                "first_name": "Avijit"
            },
            {
                "last_name": "Brandsen",
                "first_name": "S."
            },
            {
                "last_name": "Pfister",
                "first_name": "Henry D."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper considers the design and decoding of polar codes for general classical-quantum (CQ) channels. It focuses on decoding via belief-propagation with quantum messages (BPQM) and, in particular, the idea of paired-measurement BPQM (PM-BPQM) decoding. Since the PM-BPQM decoder admits a classical density evolution (DE) analysis, one can use DE to design a polar code for any CQ channel and then efficiently compute the trade-off between code rate and error probability. We have also implemented and tested a classical simulation of our PM-BPQM decoder for polar codes. While the decoder can be implemented efficiently on a quantum computer, simulating the decoder on a classical computer actually has exponential complexity. Thus, simulation results for the decoder are somewhat limited and are included primarily to validate our theoretical results. ",
        "title": "Polar Codes for CQ Channels: Decoding via Belief-Propagation with  Quantum Messages",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07174",
        "abstract_url": "http://arxiv.org/abs/2401.07174",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Shizhou"
            },
            {
                "last_name": "Strohmer",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  We study the compatibility between the optimal statistical parity solutions and individual fairness. While individual fairness seeks to treat similar individuals similarly, optimal statistical parity aims to provide similar treatment to individuals who share relative similarity within their respective sensitive groups. The two fairness perspectives, while both desirable from a fairness perspective, often come into conflict in applications. Our goal in this work is to analyze the existence of this conflict and its potential solution. In particular, we establish sufficient (sharp) conditions for the compatibility between the optimal (post-processing) statistical parity $L^2$ learning and the ($K$-Lipschitz or $(\\epsilon,\\delta)$) individual fairness requirements. Furthermore, when there exists a conflict between the two, we first relax the former to the Pareto frontier (or equivalently the optimal trade-off) between $L^2$ error and statistical disparity, and then analyze the compatibility between the frontier and the individual fairness requirements. Our analysis identifies regions along the Pareto frontier that satisfy individual fairness requirements. (Lastly, we provide individual fairness guarantees for the composition of a trained model and the optimal post-processing step so that one can determine the compatibility of the post-processed model.) This provides practitioners with a valuable approach to attain Pareto optimality for statistical parity while adhering to the constraints of individual fairness. ",
        "title": "On the (In)Compatibility between Group Fairness and Individual Fairness",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07175",
        "abstract_url": "http://arxiv.org/abs/2401.07175",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Somya"
            },
            {
                "last_name": "Sharma",
                "first_name": "Swati"
            },
            {
                "last_name": "Padilha",
                "first_name": "Rafael"
            },
            {
                "last_name": "Kiciman",
                "first_name": "Emre"
            },
            {
                "last_name": "Chandra",
                "first_name": "Ranveer"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Monitoring organic matter is pivotal for maintaining soil health and can help inform sustainable soil management practices. While sensor-based soil information offers higher-fidelity and reliable insights into organic matter changes, sampling and measuring sensor data is cost-prohibitive. We propose a multi-modal, scalable framework that can estimate organic matter from remote sensing data, a more readily available data source while leveraging sparse soil information for improving generalization. Using the sensor data, we preserve underlying causal relations among sensor attributes and organic matter. Simultaneously we leverage inherent structure in the data and train the model to discriminate among domains using contrastive learning. This causal and contrastive constraint minimization ensures improved generalization and adaptation to other domains. We also shed light on the interpretability of the framework by identifying attributes that are important for improving generalization. Identifying these key soil attributes that affect organic matter will aid in efforts to standardize data collection efforts. ",
        "title": "Domain Adaptation for Sustainable Soil Management using Causal and  Contrastive Constraint Minimization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07179",
        "abstract_url": "http://arxiv.org/abs/2401.07179",
        "authors": [
            {
                "last_name": "Barbaglia",
                "first_name": "Luca"
            },
            {
                "last_name": "Consoli",
                "first_name": "Sergio"
            },
            {
                "last_name": "Manzan",
                "first_name": "Sebastiano"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "CL"
        ],
        "abstract": "  We evaluate the informational content of news-based sentiment indicators for forecasting Gross Domestic Product (GDP) and other macroeconomic variables of the five major European economies. Our data set includes over 27 million articles for 26 major newspapers in 5 different languages. The evidence indicates that these sentiment indicators are significant predictors to forecast macroeconomic variables and their predictive content is robust to controlling for other indicators available to forecasters in real-time. ",
        "title": "Forecasting GDP in Europe with Textual Data",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07181",
        "abstract_url": "http://arxiv.org/abs/2401.07181",
        "authors": [
            {
                "last_name": "Barj",
                "first_name": "Houda Nait El"
            },
            {
                "last_name": "Sautory",
                "first_name": "Theophile"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We introduce a method to address goal misgeneralization in reinforcement learning (RL), leveraging Large Language Model (LLM) feedback during training. Goal misgeneralization, a type of robustness failure in RL occurs when an agent retains its capabilities out-of-distribution yet pursues a proxy rather than the intended one. Our approach utilizes LLMs to analyze an RL agent's policies during training and identify potential failure scenarios. The RL agent is then deployed in these scenarios, and a reward model is learnt through the LLM preferences and feedback. This LLM-informed reward model is used to further train the RL agent on the original dataset. We apply our method to a maze navigation task, and show marked improvements in goal generalization, especially in cases where true and proxy goals are somewhat distinguishable and behavioral biases are pronounced. This study demonstrates how the LLM, despite its lack of task proficiency, can efficiently supervise RL agents, providing scalable oversight and valuable insights for enhancing goal-directed learning in RL through the use of LLMs. ",
        "title": "Reinforcement Learning from LLM Feedback to Counteract Goal  Misgeneralization",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07183",
        "abstract_url": "http://arxiv.org/abs/2401.07183",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Huisheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "H. Vicky"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we study the optimal investment problem involving two agents, where the decision of one agent is influenced by the other. To measure the distance between two agents' decisions, we introduce the average deviation. We formulate the stochastic optimal control problem considering herd behavior and derive the analytical solution through the variational method. We theoretically analyze the impact of users' herd behavior on the optimal decision by decomposing it into their rational decisions, which is called the rational decision decomposition. Furthermore, to quantify the preference for their rational decision over that of the other agent, we introduce the agent's investment opinion. Our study is validated through simulations on real stock data. ",
        "title": "Herd Behavior in Optimal Investment: A Dual-Agent Approach with  Investment Opinion and Rational Decision Decomposition",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07187",
        "abstract_url": "http://arxiv.org/abs/2401.07187",
        "authors": [
            {
                "last_name": "Suh",
                "first_name": "Namjoon"
            },
            {
                "last_name": "Cheng",
                "first_name": "Guang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. In the last part, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs). The former two models are known to be the main pillars of the modern generative AI era, while ICL is a strong capability of LLMs in learning from a few examples in the context. Finally, we conclude the paper by suggesting several promising directions for deep learning theory. ",
        "title": "A Survey on Statistical Theory of Deep Learning: Approximation, Training  Dynamics, and Generative Models",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07188",
        "abstract_url": "http://arxiv.org/abs/2401.07188",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Hui",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Lu",
                "first_name": "Beijia"
            },
            {
                "last_name": "Lilith",
                "first_name": "Nimrod"
            },
            {
                "last_name": "Liu",
                "first_name": "Jun"
            },
            {
                "last_name": "Alam",
                "first_name": "Sameer"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Stereo matching neural networks often involve a Siamese structure to extract intermediate features from left and right images. The similarity between these intermediate left-right features significantly impacts the accuracy of disparity estimation. In this paper, we introduce a novel adversarial attack approach that generates perturbation noise specifically designed to maximize the discrepancy between left and right image features. Extensive experiments demonstrate the superior capability of our method to induce larger prediction errors in stereo neural networks, e.g. outperforming existing state-of-the-art attack methods by 219% MAE on the KITTI dataset and 85% MAE on the Scene Flow dataset. Additionally, we extend our approach to include a proxy network black-box attack method, eliminating the need for access to stereo neural network. This method leverages an arbitrary network from a different vision task as a proxy to generate adversarial noise, effectively causing the stereo network to produce erroneous predictions. Our findings highlight a notable sensitivity of stereo networks to discrepancies in shallow layer features, offering valuable insights that could guide future research in enhancing the robustness of stereo vision systems. ",
        "title": "Left-right Discrepancy for Adversarial Attack on Stereo Networks",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07190",
        "abstract_url": "http://arxiv.org/abs/2401.07190",
        "authors": [
            {
                "last_name": "Vente",
                "first_name": "Blake"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This work finds limited evidence supporting the theory that using multiple tasks with sequence-to-sequence transformer language models can improve performance on some metrics. In particular, the multi-task generalist t5-small outperforms the specialist t5-small with a $F_1$ of $0.771$ up from $0.692$, which may point to underlying cross-task knowledge generalization. This further suggests that even with the same network, \"re-using\" the same data in a different way may lead to higher performance in some metrics. However, the inverse task alone is likely only an optimization strategy, since it does not yield a significant general improvement at the model sizes explored in this work. Also, adding $\\approx 4500$ LLM annotated records (interlaced with the $12800$ WebNLG training records) does not substantially change automatic metric performance compared to the same t5-small model without the synthetic data. This may be due to a learning capacity bottleneck on account of model size, and decreases observed may be due to distributional differences in the corpora. Future research using larger models or human evaluation is required to more fully explain the mechanisms contributing to performance on these tasks. ",
        "title": "Inroads to a Structured Data Natural Language Bijection and the role of  LLM annotation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07193",
        "abstract_url": "http://arxiv.org/abs/2401.07193",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Hongxia"
            },
            {
                "last_name": "Hu",
                "first_name": "Guanghui"
            },
            {
                "last_name": "Ma",
                "first_name": "Guanqiu"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper addresses a factorization method for imaging the support of a wave-number-dependent source function from multi-frequency data measured at a finite pair of symmetric receivers in opposite directions. The source function is given by the inverse Fourier transform of a compactly supported time-dependent source whose initial moment or terminal moment for radiating is unknown. Using the multi-frequency far-field data at two opposite observation directions, we provide a computational criterion for characterizing the smallest strip containing the support and perpendicular to the directions. A new parameter is incorporated into the design of test functions for indicating the unknown moment. The data from a finite pair of opposite directions can be used to recover the $\\Theta$-convex polygon of the support. Uniqueness in recovering the convex hull of the support is obtained as a by-product of our analysis using all observation directions. Similar results are also discussed with the multi-frequency near-field data from a finite pair of observation positions in three dimensions. We further comment on possible extensions to source functions with two disconnected supports. Extensive numerical tests in both two and three dimensions are implemented to show effectiveness and feasibility of the approach. The theoretical framework explored here should be seen as the frequency-domain analysis for inverse source problems in the time domain. ",
        "title": "Inverse wave-number-dependent source problems for the Helmholtz equation  with partial information on radiating period",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07194",
        "abstract_url": "http://arxiv.org/abs/2401.07194",
        "authors": [
            {
                "last_name": "Hussain",
                "first_name": "Razin Farhan"
            },
            {
                "last_name": "Salehi",
                "first_name": "Mohsen Amini"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The Industry 4.0 revolution has been made possible via AI-based applications (e.g., for automation and maintenance) deployed on the serverless edge (aka fog) computing platforms at the industrial sites -- where the data is generated. Nevertheless, fulfilling the fault-intolerant and real-time constraints of Industry 4.0 applications on resource-limited fog systems in remote industrial sites (e.g., offshore oil fields) that are uncertain, disaster-prone, and have no cloud access is challenging. It is this challenge that our research aims at addressing. We consider the inelastic nature of the fog systems, software architecture of the industrial applications (micro-service-based versus monolithic), and scarcity of human experts in remote sites. To enable cloud-like elasticity, our approach is to dynamically and seamlessly (i.e., without human intervention) federate nearby fog systems. Then, we develop serverless resource allocation solutions that are cognizant of the applications' software architecture, their latency requirements, and distributed nature of the underlying infrastructure. We propose methods to seamlessly and optimally partition micro-service-based application across the federated fog. Our experimental evaluation express that not only the elasticity is overcome in a serverless manner, but also our developed application partitioning method can serve around 20% more tasks on-time than the existing methods in the literature. ",
        "title": "Resource Allocation of Industry 4.0 Micro-Service Applications across  Serverless Fog Federation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07196",
        "abstract_url": "http://arxiv.org/abs/2401.07196",
        "authors": [
            {
                "last_name": "Goda",
                "first_name": "Takashi"
            },
            {
                "last_name": "Kazashi",
                "first_name": "Yoshihito"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Ken'ichiro"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Numerical integration over the real line for analytic functions is studied. Our main focus is on the sharpness of the error bounds. We first derive two general lower estimates for the worst-case integration error, and then apply these to establish lower bounds for various quadrature rules. These bounds turn out to be either novel or improve upon existing results, leading to lower bounds that closely match upper bounds for various formulas. Specifically, for the suitably truncated trapezoidal rule, we improve upon general lower bounds on the worst-case error obtained by Sugihara [\\textit{Numer. Math.}, 75 (1997), pp.~379--395] and provide exceptionally sharp lower bounds apart from a polynomial factor, in particular show that the worst-case error for the trapezoidal rule by Sugihara is not improvable more than a polynomial factor. Additionally, our research reveals a discrepancy between the error decay of the trapezoidal rule and Sugihara's lower bound for general numerical integration rules, introducing a new open problem. Moreover, Gauss--Hermite quadrature is proven sub-optimal under the decay conditions on integrands we consider, a result not deducible from upper-bound arguments alone. Furthermore, to establish the near-optimality of the suitably scaled Gauss--Legendre and Clenshaw--Curtis quadratures, we generalize a recent result of Trefethen [\\textit{SIAM Rev.}, 64 (2022), pp.~132--150] for the upper error bounds in terms of the decay conditions. ",
        "title": "How sharp are error bounds? --lower bounds on quadrature worst-case  errors for analytic functions",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07200",
        "abstract_url": "http://arxiv.org/abs/2401.07200",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Chen-Hsiu"
            },
            {
                "last_name": "Wu",
                "first_name": "Ja-Ling"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We propose an end-to-end learned image compression codec wherein the analysis transform is jointly trained with an object classification task. This study affirms that the compressed latent representation can predict human perceptual distance judgments with an accuracy comparable to a custom-tailored DNN-based quality metric. We further investigate various neural encoders and demonstrate the effectiveness of employing the analysis transform as a perceptual loss network for image tasks beyond quality judgments. Our experiments show that the off-the-shelf neural encoder proves proficient in perceptual modeling without needing an additional VGG network. We expect this research to serve as a valuable reference developing of a semantic-aware and coding-efficient neural encoder. ",
        "title": "Exploring Compressed Image Representation as a Perceptual Proxy: A Study",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07201",
        "abstract_url": "http://arxiv.org/abs/2401.07201",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jingyi"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Bimanual manipulation needs robots to be sensitive on the grasp force which is hard to be accurately detected. This paper proposes RL framework for enhancing the grasp quality during the bimanual manipulation. This framework is based on finger configurations and its feedback. After that, the grasp quality is evaluated by the reward mechanism for the hands to determine strategies. There are 2 strategies, simultaneous and interleaved strategies, which will be determined in this framework to manipulate objects. In this paper, the contour and centroid of objects to the robot are unknown. Through the RL framework, robots can perceive hand-object relation and then optimize fingers configurations. The simulations and experiments showed that this framework can improve the success rates and finger motion accuracy. ",
        "title": "The Multi-fingered Kinematic Model for Dual-arm Manipulation",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07205",
        "abstract_url": "http://arxiv.org/abs/2401.07205",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shiming"
            },
            {
                "last_name": "Ji",
                "first_name": "Zhe"
            },
            {
                "last_name": "Xiang",
                "first_name": "Liyao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinbing"
            },
            {
                "last_name": "Zhou",
                "first_name": "Chenghu"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CV",
            "LG"
        ],
        "abstract": "  With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods. ",
        "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft  on Deep Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07206",
        "abstract_url": "http://arxiv.org/abs/2401.07206",
        "authors": [
            {
                "last_name": "Mo",
                "first_name": "Yanfang"
            },
            {
                "last_name": "Qin",
                "first_name": "S. Joe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we propose a probabilistic reduced-dimensional vector autoregressive (PredVAR) model to extract low-dimensional dynamics from high-dimensional noisy data. The model utilizes an oblique projection to partition the measurement space into a subspace that accommodates the reduced-dimensional dynamics and a complementary static subspace. An optimal oblique decomposition is derived for the best predictability regarding prediction error covariance. Building on this, we develop an iterative PredVAR algorithm using maximum likelihood and the expectation-maximization (EM) framework. This algorithm alternately updates the estimates of the latent dynamics and optimal oblique projection, yielding dynamic latent variables with rank-ordered predictability and an explicit latent VAR model that is consistent with the outer projection model. The superior performance and efficiency of the proposed approach are demonstrated using data sets from a synthesized Lorenz system and an industrial process from Eastman Chemical. ",
        "title": "Probabilistic Reduced-Dimensional Vector Autoregressive Modeling with  Oblique Projections",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07207",
        "abstract_url": "http://arxiv.org/abs/2401.07207",
        "authors": [
            {
                "last_name": "Rostami",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A major technique for tackling unsupervised domain adaptation involves mapping data points from both the source and target domains into a shared embedding space. The mapping encoder to the embedding space is trained such that the embedding space becomes domain agnostic, allowing a classifier trained on the source domain to generalize well on the target domain. To further enhance the performance of unsupervised domain adaptation (UDA), we develop an additional technique which makes the internal distribution of the source domain more compact, thereby improving the model's ability to generalize in the target domain.We demonstrate that by increasing the margins between data representations for different classes in the embedding space, we can improve the model performance for UDA. To make the internal representation more compact, we estimate the internally learned multi-modal distribution of the source domain as Gaussian mixture model (GMM). Utilizing the estimated GMM, we enhance the separation between different classes in the source domain, thereby mitigating the effects of domain shift. We offer theoretical analysis to support outperofrmance of our method. To evaluate the effectiveness of our approach, we conduct experiments on widely used UDA benchmark UDA datasets. The results indicate that our method enhances model generalizability and outperforms existing techniques. ",
        "title": "Unsupervised Domain Adaptation Using Compact Internal Representations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07208",
        "abstract_url": "http://arxiv.org/abs/2401.07208",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Mingli"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zihao"
            },
            {
                "last_name": "Chen",
                "first_name": "Sihong"
            },
            {
                "last_name": "Chen",
                "first_name": "Chen"
            },
            {
                "last_name": "Wu",
                "first_name": "Baoyuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Few-shot class-incremental learning (FSCIL) aims to continually fit new classes with limited training data, while maintaining the performance of previously learned classes. The main challenges are overfitting the rare new training samples and forgetting old classes. While catastrophic forgetting has been extensively studied, the overfitting problem has attracted less attention in FSCIL. To tackle overfitting challenge, we design a new ensemble model framework cooperated with data augmentation to boost generalization. In this way, the enhanced model works as a library storing abundant features to guarantee fast adaptation to downstream tasks. Specifically, the multi-input multi-output ensemble structure is applied with a spatial-aware data augmentation strategy, aiming at diversifying the feature extractor and alleviating overfitting in incremental sessions. Moreover, self-supervised learning is also integrated to further improve the model generalization. Comprehensive experimental results show that the proposed method can indeed mitigate the overfitting problem in FSCIL, and outperform the state-of-the-art methods. ",
        "title": "Enhanced Few-Shot Class-Incremental Learning via Ensemble Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07209",
        "abstract_url": "http://arxiv.org/abs/2401.07209",
        "authors": [
            {
                "last_name": "Raza",
                "first_name": "Ali"
            },
            {
                "last_name": "Penuel",
                "first_name": "William R."
            },
            {
                "last_name": "Sumner",
                "first_name": "Tamara"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Supporting equitable instruction is an important issue for teachers attending diverse STEM classrooms. Visual learning analytics along with effective student survey measures can support providing on time feedback to teachers in making instruction more culturally relevant to all students. We adopted a user-centered approach, where we engaged seven middle school science teachers in iterative testing of thirty data visualizations disaggregated over markers such as gender and race for implementation of selected displays in a visual learning analytics tool- Student Electronic Exit Ticket (SEET). This process helped us gather insights into teachers' sensemaking in identifying patterns of student data related to gender and race, selecting and improving the design of the feedback displays for the SEET [10]. ",
        "title": "Designing Visual Learning Analytics for Supporting Equity in STEM  Classrooms",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07211",
        "abstract_url": "http://arxiv.org/abs/2401.07211",
        "authors": [
            {
                "last_name": "Adenekan",
                "first_name": "Rachel A. G."
            },
            {
                "last_name": "Reyes",
                "first_name": "Alejandrina Gonzalez"
            },
            {
                "last_name": "Yoshida",
                "first_name": "Kyle T."
            },
            {
                "last_name": "Kodali",
                "first_name": "Sreela"
            },
            {
                "last_name": "Okamura",
                "first_name": "Allison M."
            },
            {
                "last_name": "Nunez",
                "first_name": "Cara M."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Tactile perception plays an important role in activities of daily living, and it can be impaired in individuals with medical conditions. The most common tools used to assess tactile sensation, the Semmes-Weinstein monofilaments and the 128 Hz tuning fork, have poor repeatability and resolution. Long term, we aim to provide a repeatable, high-resolution testing platform that can be used to assess vibrotactile perception through smartphones without the need for an experimenter to be present to conduct the test. We present a smartphone-based vibration perception measurement platform and compare its performance to measurements from standard monofilament and tuning fork tests. We conducted a user study with 36 healthy adults in which we tested each tool on the hand, wrist, and foot, to assess how well our smartphone-based vibration perception thresholds (VPTs) detect known trends obtained from standard tests. The smartphone platform detected statistically significant changes in VPT between the index finger and the foot and also between the feet of younger adults and older adults. Our smartphone-based VPT had a moderate correlation to tuning fork-based VPT. A long-term objective of this work is to develop an accessible smartphone-based platform that can be used to measure disease progression and regression. ",
        "title": "A Comparative Analysis of Smartphone and Standard Tools for Touch  Perception Assessment Across Multiple Body Sites",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07212",
        "abstract_url": "http://arxiv.org/abs/2401.07212",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Zexuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiahong"
            },
            {
                "last_name": "Chen",
                "first_name": "Yankai"
            },
            {
                "last_name": "King",
                "first_name": "Irwin"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Existing unsupervised deep product quantization methods primarily aim for the increased similarity between different views of the identical image, whereas the delicate multi-level semantic similarities preserved between images are overlooked. Moreover, these methods predominantly focus on the Euclidean space for computational convenience, compromising their ability to map the multi-level semantic relationships between images effectively. To mitigate these shortcomings, we propose a novel unsupervised product quantization method dubbed \\textbf{Hi}erarchical \\textbf{H}yperbolic \\textbf{P}roduct \\textbf{Q}uantization (HiHPQ), which learns quantized representations by incorporating hierarchical semantic similarity within hyperbolic geometry. Specifically, we propose a hyperbolic product quantizer, where the hyperbolic codebook attention mechanism and the quantized contrastive learning on the hyperbolic product manifold are introduced to expedite quantization. Furthermore, we propose a hierarchical semantics learning module, designed to enhance the distinction between similar and non-matching images for a query by utilizing the extracted hierarchical semantics as an additional training supervision. Experiments on benchmarks show that our proposed method outperforms state-of-the-art baselines. ",
        "title": "HiHPQ: Hierarchical Hyperbolic Product Quantization for Unsupervised  Image Retrieval",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07213",
        "abstract_url": "http://arxiv.org/abs/2401.07213",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Honglei"
            },
            {
                "last_name": "Shu",
                "first_name": "Yan"
            },
            {
                "last_name": "Liu",
                "first_name": "Shaohui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single image dehazing is a challenging ill-posed problem. Existing datasets for training deep learning-based methods can be generated by hand-crafted or synthetic schemes. However, the former often suffers from small scales, while the latter forces models to learn scene depth instead of haze distribution, decreasing their dehazing ability. To overcome the problem, we propose a simple yet novel synthetic method to decouple the relationship between haze density and scene depth, by which a depth-agnostic dataset (DA-HAZE) is generated. Meanwhile, a Global Shuffle Strategy (GSS) is proposed for generating differently scaled datasets, thereby enhancing the generalization ability of the model. Extensive experiments indicate that models trained on DA-HAZE achieve significant improvements on real-world benchmarks, with less discrepancy between SOTS and DA-SOTS (the test set of DA-HAZE). Additionally, Depth-agnostic dehazing is a more complicated task because of the lack of depth prior. Therefore, an efficient architecture with stronger feature modeling ability and fewer computational costs is necessary. We revisit the U-Net-based architectures for dehazing, in which dedicatedly designed blocks are incorporated. However, the performances of blocks are constrained by limited feature fusion methods. To this end, we propose a Convolutional Skip Connection (CSC) module, allowing vanilla feature fusion methods to achieve promising results with minimal costs. Extensive experimental results demonstrate that current state-of-the-art methods. equipped with CSC can achieve better performance and reasonable computational expense, whether the haze distribution is relevant to the scene depth. ",
        "title": "Depth-agnostic Single Image Dehazing",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07216",
        "abstract_url": "http://arxiv.org/abs/2401.07216",
        "authors": [
            {
                "last_name": "Cherumanal",
                "first_name": "Sachin Pathiyan"
            },
            {
                "last_name": "Tian",
                "first_name": "Lin"
            },
            {
                "last_name": "Abushaqra",
                "first_name": "Futoon M."
            },
            {
                "last_name": "de Paula",
                "first_name": "Angel Felipe Magnossao"
            },
            {
                "last_name": "Ji",
                "first_name": "Kaixin"
            },
            {
                "last_name": "Hettiachchi",
                "first_name": "Danula"
            },
            {
                "last_name": "Trippas",
                "first_name": "Johanne R."
            },
            {
                "last_name": "Ali",
                "first_name": "Halil"
            },
            {
                "last_name": "Scholer",
                "first_name": "Falk"
            },
            {
                "last_name": "Spina",
                "first_name": "Damiano"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Creating and deploying customized applications is crucial for operational success and enriching user experiences in the rapidly evolving modern business world. A prominent facet of modern user experiences is the integration of chatbots or voice assistants. The rapid evolution of Large Language Models (LLMs) has provided a powerful tool to build conversational applications. We present Walert, a customized LLM-based conversational agent able to answer frequently asked questions about computer science degrees and programs at RMIT University. Our demo aims to showcase how conversational information-seeking researchers can effectively communicate the benefits of using best practices to stakeholders interested in developing and deploying LLM-based chatbots. These practices are well-known in our community but often overlooked by practitioners who may not have access to this knowledge. The methodology and resources used in this demo serve as a bridge to facilitate knowledge transfer from experts, address industry professionals' practical needs, and foster a collaborative environment. The data and code of the demo are available at https://github.com/rmit-ir/walert. ",
        "title": "Walert: Putting Conversational Search Knowledge into Action by Building  and Evaluating a Large Language Model-Powered Chatbot",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07218",
        "abstract_url": "http://arxiv.org/abs/2401.07218",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Junyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Lina"
            },
            {
                "last_name": "Jiang",
                "first_name": "Bofeng"
            },
            {
                "last_name": "Wen",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Li",
                "first_name": "Wanlong"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  An event camera is a novel vision sensor that can capture per-pixel brightness changes and output a stream of asynchronous ``events''. It has advantages over conventional cameras in those scenes with high-speed motions and challenging lighting conditions because of the high temporal resolution, high dynamic range, low bandwidth, low power consumption, and no motion blur. Therefore, several supervised monocular depth estimation from events is proposed to address scenes difficult for conventional cameras. However, depth annotation is costly and time-consuming. In this paper, to lower the annotation cost, we propose a self-supervised event-based monocular depth estimation framework named EMoDepth. EMoDepth constrains the training process using the cross-modal consistency from intensity frames that are aligned with events in the pixel coordinate. Moreover, in inference, only events are used for monocular depth prediction. Additionally, we design a multi-scale skip-connection architecture to effectively fuse features for depth estimation while maintaining high inference speed. Experiments on MVSEC and DSEC datasets demonstrate that our contributions are effective and that the accuracy can outperform existing supervised event-based and unsupervised frame-based methods. ",
        "title": "Self-supervised Event-based Monocular Depth Estimation using Cross-modal  Consistency",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07220",
        "abstract_url": "http://arxiv.org/abs/2401.07220",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Linlin"
            },
            {
                "last_name": "Yu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Daud",
                "first_name": "Abdulateef"
            },
            {
                "last_name": "Mussah",
                "first_name": "Abdul Rashid"
            },
            {
                "last_name": "Adu-Gyamfi",
                "first_name": "Yaw"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Traffic cameras remain the primary source data for surveillance activities such as congestion and incident monitoring. To date, State agencies continue to rely on manual effort to extract data from networked cameras due to limitations of the current automatic vision systems including requirements for complex camera calibration and inability to generate high resolution data. This study implements a three-stage video analytics framework for extracting high-resolution traffic data such vehicle counts, speed, and acceleration from infrastructure-mounted CCTV cameras. The key components of the framework include object recognition, perspective transformation, and vehicle trajectory reconstruction for traffic data collection. First, a state-of-the-art vehicle recognition model is implemented to detect and classify vehicles. Next, to correct for camera distortion and reduce partial occlusion, an algorithm inspired by two-point linear perspective is utilized to extracts the region of interest (ROI) automatically, while a 2D homography technique transforms the CCTV view to bird's-eye view (BEV). Cameras are calibrated with a two-layer matrix system to enable the extraction of speed and acceleration by converting image coordinates to real-world measurements. Individual vehicle trajectories are constructed and compared in BEV using two time-space-feature-based object trackers, namely Motpy and BYTETrack. The results of the current study showed about +/- 4.5% error rate for directional traffic counts, less than 10% MSE for speed bias between camera estimates in comparison to estimates from probe data sources. Extracting high-resolution data from traffic cameras has several implications, ranging from improvements in traffic management and identify dangerous driving behavior, high-risk areas for accidents, and other safety concerns, enabling proactive measures to reduce accidents and fatalities. ",
        "title": "Application of 2D Homography for High Resolution Traffic Data Collection  using CCTV Cameras",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07222",
        "abstract_url": "http://arxiv.org/abs/2401.07222",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Kaijian"
            },
            {
                "last_name": "Liu",
                "first_name": "Tao"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents a new robust data-driven predictive control scheme for unknown linear time-invariant systems by using input-state-output or input-output data based on whether the state is measurable. To remove the need for the persistently exciting (PE) condition of a sufficiently high order on pre-collected data, a set containing all systems capable of generating such data is constructed. Then, at each time step, an upper bound of a given objective function is derived for all systems in the set, and a feedback controller is designed to minimize this bound. The optimal control gain at each time step is determined by solving a set of linear matrix inequalities. We prove that if the synthesis problem is feasible at the initial time step, it remains feasible for all future time steps. Unlike current data-driven predictive control schemes based on behavioral system theory, our approach requires less stringent conditions for the pre-collected data, facilitating easier implementation. Further, the proposed predictive control scheme features an infinite prediction horizon, potentially resulting in superior overall control performance compared to existing methods with finite prediction horizons. The effectiveness of our proposed methods is demonstrated through application to an unknown and unstable batch reactor. ",
        "title": "Robust Data-Driven Predictive Control for Unknown Linear Time-Invariant  Systems",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07224",
        "abstract_url": "http://arxiv.org/abs/2401.07224",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Qiong"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaobo"
            },
            {
                "last_name": "Fan",
                "first_name": "Pingyi"
            },
            {
                "last_name": "Fan",
                "first_name": "Qiang"
            },
            {
                "last_name": "Zhu",
                "first_name": "Huiling"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiangzhou"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Federated learning (FL) is a promising technology for vehicular networks to protect vehicles' privacy in Internet of Vehicles (IoV). Vehicles with limited computation capacity may face a large computational burden associated with FL. Federated edge learning (FEEL) systems are introduced to solve such a problem. In FEEL systems, vehicles adopt the cellular-vehicle to everything (C-V2X) mode 4 to upload encrypted data to road side units' (RSUs)' cache queue. Then RSUs train the data transmitted by vehicles, update the locally model hyperparameters and send back results to vehicles, thus vehicles' computational burden can be released. However, each RSU has limited cache queue. To maintain the stability of cache queue and maximize the accuracy of model, it is essential to select appropriate vehicles to upload data. The vehicle selection method for FEEL systems faces challenges due to the random departure of data from the cache queue caused by the stochastic channel and the different system status of vehicles, such as remaining data amount, transmission delay, packet collision probability and survival ability. This paper proposes a vehicle selection method for FEEL systems that aims to maximize the accuracy of model while keeping the cache queue stable. Extensive simulation experiments demonstrate that our proposed method outperforms other baseline selection methods. ",
        "title": "Vehicle Selection for C-V2X Mode 4 Based Federated Edge Learning Systems",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07228",
        "abstract_url": "http://arxiv.org/abs/2401.07228",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Bo"
            },
            {
                "last_name": "Ma",
                "first_name": "Ying"
            },
            {
                "last_name": "Wang",
                "first_name": "Chushan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose a Lawson-time-splitting extended Fourier pseudospectral (LTSeFP) method for the numerical integration of the Gross-Pitaevskii equation with time-dependent potential that is of low regularity in space. For the spatial discretization of low regularity potential, we use an extended Fourier pseudospectral (eFP) method, i.e., we compute the discrete Fourier transform of the low regularity potential in an extended window. For the temporal discretization, to efficiently implement the eFP method for time-dependent low regularity potential, we combine the standard time-splitting method with a Lawson-type exponential integrator to integrate potential and nonlinearity differently. The LTSeFP method is both accurate and efficient: it achieves first-order convergence in time and optimal-order convergence in space in $L^2$-norm under low regularity potential, while the computational cost is comparable to the standard time-splitting Fourier pseudospectral method. Theoretically, we also prove such convergence orders for a large class of spatially low regularity time-dependent potential. Extensive numerical results are reported to confirm the error estimates and to demonstrate the superiority of our method. ",
        "title": "A Lawson-time-splitting extended Fourier pseudospectral method for the  Gross-Pitaevskii equation with time-dependent low regularity potential",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07230",
        "abstract_url": "http://arxiv.org/abs/2401.07230",
        "authors": [
            {
                "last_name": "Deng",
                "first_name": "Yue"
            },
            {
                "last_name": "He",
                "first_name": "Changyang"
            },
            {
                "last_name": "Li",
                "first_name": "Bo"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "SI"
        ],
        "abstract": "  Quarantine is a widely-adopted measure during health crises caused by highly-contagious diseases like COVID-19, yet it poses critical challenges to public mental health. Given this context, emotional disclosure on social media in the form of keeping a diary emerges as a popular way for individuals to express emotions and record their mental health status. However, the exploration of emotional disclosure via diary-keeping on social media during quarantine is underexplored, understanding which could be beneficial to facilitate emotional connections and enlighten health intervention measures. Focusing on this particular form of self-disclosure, this work proposes a quantitative approach to figure out the prevalence and changing patterns of emotional disclosure during quarantine, and the possible factors contributing to the negative emotions. We collected 58, 796 posts with the \"Quarantine Diary\" keyword on Weibo, a popular social media website in China. Through text classification, we capture diverse emotion categories that characterize public emotion disclosure during quarantine, such as annoyed, anxious, boring, happy, hopeful and appreciative. Based on temporal analysis, we uncover the changing patterns of emotional disclosure from long-term perspectives and period-based perspectives (e.g., the gradual decline of all negative emotions and the upsurge of the annoyed emotion near the end of quarantine). Leveraging topic modeling, we also encapsulate the possible influencing factors of negative emotions, such as freedom restriction and solitude, and uncertainty of infection and supply. We reflect on how our findings could deepen the understanding of mental health on social media and further provide practical and design implications to mitigate mental health issues during quarantine. ",
        "title": "Understanding Emotional Disclosure via Diary-keeping in Quarantine on  Social Media",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07231",
        "abstract_url": "http://arxiv.org/abs/2401.07231",
        "authors": [
            {
                "last_name": "Maeda",
                "first_name": "Takashi Nicholas"
            },
            {
                "last_name": "Shohei",
                "first_name": "Shimizu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method by using simulated data to demonstrate that the accuracy of causal discovery increases as more prior knowledge is accumulated. Additionally, we test the second proposed method by comparing it with existing time series causal discovery methods, using both simulated data and real-world data. ",
        "title": "Use of Prior Knowledge to Discover Causal Additive Models with  Unobserved Variables and its Application to Time Series Data",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07232",
        "abstract_url": "http://arxiv.org/abs/2401.07232",
        "authors": [
            {
                "last_name": "Sedov",
                "first_name": "Evgeny"
            },
            {
                "last_name": "Kavokin",
                "first_name": "Alexey"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  We introduce a novel neuromorphic network architecture based on a lattice of exciton-polariton condensates, intricately interconnected and energized through non-resonant optical pumping. The network employs a binary framework, where each neuron, facilitated by the spatial coherence of pairwise coupled condensates, performs binary operations. This coherence, emerging from the ballistic propagation of polaritons, ensures efficient, network-wide communication. The binary neuron switching mechanism, driven by the nonlinear repulsion through the excitonic component of polaritons, offers computational efficiency and scalability advantages over continuous weight neural networks. Our network enables parallel processing, enhancing computational speed compared to sequential or pulse-coded binary systems. The system's performance was evaluated using the MNIST dataset for handwritten digit recognition, showcasing the potential to outperform existing polaritonic neuromorphic systems, as demonstrated by its impressive predicted classification accuracy of up to 97.5%. ",
        "title": "Polariton lattices as binarized neuromorphic networks",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07234",
        "abstract_url": "http://arxiv.org/abs/2401.07234",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Shuyao"
            },
            {
                "last_name": "Tay",
                "first_name": "Jordan"
            },
            {
                "last_name": "Baiz",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Credit risk forecasting plays a crucial role for commercial banks and other financial institutions in granting loans to customers and minimise the potential loss. However, traditional machine learning methods require the sharing of sensitive client information with an external server to build a global model, potentially posing a risk of security threats and privacy leakage. A newly developed privacy-preserving distributed machine learning technique known as Federated Learning (FL) allows the training of a global model without the necessity of accessing private local data directly. This investigation examined the feasibility of federated learning in credit risk assessment and showed the effects of data imbalance on model performance. Two neural network architectures, Multilayer Perceptron (MLP) and Long Short-Term Memory (LSTM), and one tree ensemble architecture, Extreme Gradient Boosting (XGBoost), were explored across three different datasets under various scenarios involving different numbers of clients and data distribution configurations. We demonstrate that federated models consistently outperform local models on non-dominant clients with smaller datasets. This trend is especially pronounced in highly imbalanced data scenarios, yielding a remarkable average improvement of 17.92% in model performance. However, for dominant clients (clients with more data), federated models may not exhibit superior performance, suggesting the need for special incentives for this type of clients to encourage their participation. ",
        "title": "The Effects of Data Imbalance Under a Federated Learning Approach for  Credit Risk Forecasting",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07237",
        "abstract_url": "http://arxiv.org/abs/2401.07237",
        "authors": [
            {
                "last_name": "Wadhwa",
                "first_name": "Somin"
            },
            {
                "last_name": "Hassanzadeh",
                "first_name": "Oktie"
            },
            {
                "last_name": "Bhattacharjya",
                "first_name": "Debarun"
            },
            {
                "last_name": "Barker",
                "first_name": "Ken"
            },
            {
                "last_name": "Ni",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex structured knowledge from pattern mining and probabilistic event models. We release our sequence generation code and evaluation framework, as well as corpus of event sequence data. ",
        "title": "Distilling Event Sequence Knowledge From Large Language Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07240",
        "abstract_url": "http://arxiv.org/abs/2401.07240",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Shuai"
            },
            {
                "last_name": "Li",
                "first_name": "Boyang"
            },
            {
                "last_name": "Fang",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Huang",
                "first_name": "Kai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, significant progress has been made in the research of 3D object detection. However, most prior studies have focused on the utilization of center-based or anchor-based label assignment schemes. Alternative label assignment strategies remain unexplored in 3D object detection. We find that the center-based label assignment often fails to generate sufficient positive samples for training, while the anchor-based label assignment tends to encounter an imbalanced issue when handling objects of varying scales. To solve these issues, we introduce a dynamic cross label assignment (DCLA) scheme, which dynamically assigns positive samples for each object from a cross-shaped region, thus providing sufficient and balanced positive samples for training. Furthermore, to address the challenge of accurately regressing objects with varying scales, we put forth a rotation-weighted Intersection over Union (RWIoU) metric to replace the widely used L1 metric in regression loss. Extensive experiments demonstrate the generality and effectiveness of our DCLA and RWIoU-based regression loss. The Code will be available at https://github.com/Say2L/DCDet.git. ",
        "title": "DCDet: Dynamic Cross-based 3D Object Detector",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07242",
        "abstract_url": "http://arxiv.org/abs/2401.07242",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "Nadimpalli",
                "first_name": "Shivam"
            },
            {
                "last_name": "Randolph",
                "first_name": "Tim"
            },
            {
                "last_name": "Servedio",
                "first_name": "Rocco A."
            },
            {
                "last_name": "Zamir",
                "first_name": "Or"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  A subset $S$ of the Boolean hypercube $\\mathbb{F}_2^n$ is a *sumset* if $S = \\{a + b : a, b\\in A\\}$ for some $A \\subseteq \\mathbb{F}_2^n$. Sumsets are central objects of study in additive combinatorics, featuring in several influential results. We prove a lower bound of $\\Omega(2^{n/2})$ for the number of queries needed to test whether a Boolean function $f:\\mathbb{F}_2^n \\to \\{0,1\\}$ is the indicator function of a sumset. Our lower bound for testing sumsets follows from sharp bounds on the related problem of *shift testing*, which may be of independent interest. We also give a near-optimal {$2^{O(n/2)} \\cdot \\mathrm{poly}(n)$}-query algorithm for a smoothed analysis formulation of the sumset *refutation* problem. ",
        "title": "Testing Sumsets is Hard",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07245",
        "abstract_url": "http://arxiv.org/abs/2401.07245",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Guo",
                "first_name": "Xiaobao"
            },
            {
                "last_name": "Peng",
                "first_name": "Xiaojiang"
            },
            {
                "last_name": "Kot",
                "first_name": "Alex"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cutting-edge research in facial expression recognition (FER) currently favors the utilization of convolutional neural networks (CNNs) backbone which is supervisedly pre-trained on face recognition datasets for feature extraction. However, due to the vast scale of face recognition datasets and the high cost associated with collecting facial labels, this pre-training paradigm incurs significant expenses. Towards this end, we propose to pre-train vision Transformers (ViTs) through a self-supervised approach on a mid-scale general image dataset. In addition, when compared with the domain disparity existing between face datasets and FER datasets, the divergence between general datasets and FER datasets is more pronounced. Therefore, we propose a contrastive fine-tuning approach to effectively mitigate this domain disparity. Specifically, we introduce a novel FER training paradigm named Mask Image pre-training with MIx Contrastive fine-tuning (MIMIC). In the initial phase, we pre-train the ViT via masked image reconstruction on general images. Subsequently, in the fine-tuning stage, we introduce a mix-supervised contrastive learning process, which enhances the model with a more extensive range of positive samples by the mixing strategy. Through extensive experiments conducted on three benchmark datasets, we demonstrate that our MIMIC outperforms the previous training paradigm, showing its capability to learn better representations. Remarkably, the results indicate that the vanilla ViT can achieve impressive performance without the need for intricate, auxiliary-designed modules. Moreover, when scaling up the model size, MIMIC exhibits no performance saturation and is superior to the current state-of-the-art methods. ",
        "title": "MIMIC: Mask Image Pre-training with Mix Contrastive Fine-tuning for  Facial Expression Recognition",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07246",
        "abstract_url": "http://arxiv.org/abs/2401.07246",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Fridman",
                "first_name": "Emilia"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Recently, a constructive method was suggested for finite-dimensional observer-based control of 1D linear heat equation, which is robust to input/output delays. In this paper, we aim to extend this method to the 2D case with general time-varying input/output delays (known output delay and unknown input delay) or sawtooth delays (that correspond to network-based control). We use the modal decomposition approach and consider boundary or non-local sensing together with non-local actuation, or Neumann actuation with non-local sensing. To compensate the output delay that appears in the infinite-dimensional part of the closed-loop system, for the first time for delayed PDEs we suggest a vector Lyapunov functional combined with the recently introduced vector Halanay inequality. We provide linear matrix inequality (LMI) conditions for finding the observer dimension and upper bounds on delays that preserve the exponential stability. We prove that the LMIs are always feasible for large enough observer dimension and small enough upper bounds on delays. A numerical example demonstrates the efficiency of our method and shows that the employment of vector Halanay's inequality allows for larger delays than the classical scalar Halanay inequality for comparatively large observer dimension. ",
        "title": "Delayed finite-dimensional observer-based control of 2D linear parabolic  PDEs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07249",
        "abstract_url": "http://arxiv.org/abs/2401.07249",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Chu",
                "first_name": "Xu"
            },
            {
                "last_name": "Ma",
                "first_name": "Liantao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yasha"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wenwu"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Irregularly sampled time series are ubiquitous, presenting significant challenges for analysis due to missing values. Despite existing methods address imputation, they predominantly focus on leveraging intra-series information, neglecting the potential benefits that inter-series information could provide, such as reducing uncertainty and memorization effect. To bridge this gap, we propose PRIME, a Prototype Recurrent Imputation ModEl, which integrates both intra-series and inter-series information for imputing missing values in irregularly sampled time series. Our framework comprises a prototype memory module for learning inter-series information, a bidirectional gated recurrent unit utilizing prototype information for imputation, and an attentive prototypical refinement module for adjusting imputations. We conducted extensive experiments on three datasets, and the results underscore PRIME's superiority over the state-of-the-art models by up to 26% relative improvement on mean square error. ",
        "title": "Imputation with Inter-Series Information from Prototypes for Irregular  Sampled Time Series",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07250",
        "abstract_url": "http://arxiv.org/abs/2401.07250",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Chengli"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiangshe"
            },
            {
                "last_name": "Liu",
                "first_name": "Junmin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yicheng"
            },
            {
                "last_name": "Hao",
                "first_name": "Yunda"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, sharpness-aware minimization (SAM) has attracted a lot of attention because of its surprising effectiveness in improving generalization performance.However, training neural networks with SAM can be highly unstable since the loss does not decrease along the direction of the exact gradient at the current point, but instead follows the direction of a surrogate gradient evaluated at another point nearby. To address this issue, we propose a simple renormalization strategy, dubbed StableSAM, so that the norm of the surrogate gradient maintains the same as that of the exact gradient. Our strategy is easy to implement and flexible enough to integrate with SAM and its variants, almost at no computational cost. With elementary tools from convex optimization and learning theory, we also conduct a theoretical analysis of sharpness-aware training, revealing that compared to stochastic gradient descent (SGD), the effectiveness of SAM is only assured in a limited regime of learning rate. In contrast, we show how StableSAM extends this regime of learning rate and when it can consistently perform better than SAM with minor modification. Finally, we demonstrate the improved performance of StableSAM on several representative data sets and tasks. ",
        "title": "Stabilizing Sharpness-aware Minimization Through A Simple  Renormalization Strategy",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07251",
        "abstract_url": "http://arxiv.org/abs/2401.07251",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Fan"
            },
            {
                "last_name": "Mao",
                "first_name": "Shuyi"
            },
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Peng",
                "first_name": "Xiaojiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  3D landmark detection plays a pivotal role in various applications such as 3D registration, pose estimation, and virtual try-on. While considerable success has been achieved in 2D human landmark detection or pose estimation, there is a notable scarcity of reported works on landmark detection in unordered 3D point clouds. This paper introduces a novel challenge, namely 3D landmark detection on human point clouds, presenting two primary contributions. Firstly, we establish a comprehensive human point cloud dataset, named HPoint103, designed to support the 3D landmark detection community. This dataset comprises 103 human point clouds created with commercial software and actors, each manually annotated with 11 stable landmarks. Secondly, we propose a Dual Cascade Point Transformer (D-CPT) model for precise point-based landmark detection. D-CPT gradually refines the landmarks through cascade Transformer decoder layers across the entire point cloud stream, simultaneously enhancing landmark coordinates with a RefineNet over local regions. Comparative evaluations with popular point-based methods on HPoint103 and the public dataset DHP19 demonstrate the dramatic outperformance of our D-CPT. Additionally, the integration of our RefineNet into existing methods consistently improves performance. ",
        "title": "3D Landmark Detection on Human Point Clouds: A Benchmark and A Dual  Cascade Point Transformer Framework",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07255",
        "abstract_url": "http://arxiv.org/abs/2401.07255",
        "authors": [
            {
                "last_name": "Tariverdi",
                "first_name": "Abbas"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "MA"
        ],
        "abstract": "  The paper begins by exploring the rationality of ethical trust as a foundational concept. This involves distinguishing between trust and trustworthiness and delving into scenarios where trust is both rational and moral. It lays the groundwork for understanding the complexities of trust dynamics in decision-making scenarios. Following this theoretical groundwork, we introduce an agent-based simulation framework that investigates these dynamics of ethical trust, specifically in the context of a disaster response scenario. These agents, utilizing emotional models like Plutchik's Wheel of Emotions and memory learning mechanisms, are tasked with allocating limited resources in disaster-affected areas. The model, which embodies the principles discussed in the first section, integrates cognitive load management, Big Five personality traits, and structured interactions within networked or hierarchical settings. It also includes feedback loops and simulates external events to evaluate their impact on the formation and evolution of trust among agents. Through our simulations, we demonstrate the intricate interplay of cognitive, emotional, and social factors in ethical decision-making. These insights shed light on the behaviors and resilience of trust networks in crisis situations, emphasizing the role of rational and moral considerations in the development of trust among autonomous agents. This study contributes to the field by offering an understanding of trust dynamics in socio-technical systems and by providing a robust, adaptable framework capable of addressing ethical dilemmas in disaster response and beyond. The implementation of the algorithms presented in this paper is available at this GitHub repository: \\url{https://github.com/abbas-tari/ethical-trust-cognitive-modeling}. ",
        "title": "Trust from Ethical Point of View: Exploring Dynamics Through  Multiagent-Driven Cognitive Modeling",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07256",
        "abstract_url": "http://arxiv.org/abs/2401.07256",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Zhihao"
            },
            {
                "last_name": "He",
                "first_name": "Jiafan"
            },
            {
                "last_name": "Hou",
                "first_name": "Luyang"
            },
            {
                "last_name": "Xu",
                "first_name": "Lianming"
            },
            {
                "last_name": "Zhu",
                "first_name": "Wendi"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  In emergency search and rescue scenarios, the quick location of trapped people is essential. However, disasters can render the Global Positioning System (GPS) unusable. Unmanned aerial vehicles (UAVs) with localization devices can serve as mobile anchors due to their agility and high line-of-sight (LoS) probability. Nonetheless, the number of available UAVs during the initial stages of disaster relief is limited, and innovative methods are needed to quickly plan UAV trajectories to locate non-uniformly distributed dynamic targets while ensuring localization accuracy. To address this challenge, we design a single UAV localization method without hovering, use the maximum likelihood estimation (MLE) method to estimate the location of mobile users and define the upper bound of the localization error by considering users' movement.Combining this localization method and localization error-index, we utilize the enhanced particle swarm optimization (EPSO) algorithm and edge access strategy to develop a low complexity localization-oriented adaptive trajectory planning algorithm. Simulation results demonstrate that our method outperforms other baseline algorithms, enabling faster localization without compromising localization accuracy. ",
        "title": "Emergency Localization for Mobile Ground Users: An Adaptive UAV  Trajectory Planning Method",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07257",
        "abstract_url": "http://arxiv.org/abs/2401.07257",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Hengchang"
            },
            {
                "last_name": "Liu",
                "first_name": "Qijiong"
            },
            {
                "last_name": "Li",
                "first_name": "Chuang"
            },
            {
                "last_name": "Kan",
                "first_name": "Min-Yen"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In Sequential Recommenders (SR), encoding and utilizing modalities in an end-to-end manner is costly in terms of modality encoder sizes. Two-stage approaches can mitigate such concerns, but they suffer from poor performance due to modality forgetting, where the sequential objective overshadows modality representation. We propose a lightweight knowledge distillation solution that preserves both merits: retaining modality information and maintaining high efficiency. Specifically, we introduce a novel method that enhances the learning of embeddings in SR through the supervision of modality correlations. The supervision signals are distilled from the original modality representations, including both (1) holistic correlations, which quantify their overall associations, and (2) dissected correlation types, which refine their relationship facets (honing in on specific aspects like color or shape consistency). To further address the issue of modality forgetting, we propose an asynchronous learning step, allowing the original information to be retained longer for training the representation learning module. Our approach is compatible with various backbone architectures and outperforms the top baselines by 6.8% on average. We empirically demonstrate that preserving original feature associations from modality encoders significantly boosts task-specific recommendation adaptation. Additionally, we find that larger modality encoders (e.g., Large Language Models) contain richer feature sets which necessitate more fine-grained modeling to reach their full performance potential. ",
        "title": "Lightweight Modality Adaptation to Sequential Recommendation via  Correlation Supervision",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07261",
        "abstract_url": "http://arxiv.org/abs/2401.07261",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Shoupeng"
            },
            {
                "last_name": "Tu",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Jian"
            },
            {
                "last_name": "Wu",
                "first_name": "Di"
            },
            {
                "last_name": "Ren",
                "first_name": "Kui"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  DeFi incidents stemming from various smart contract vulnerabilities have culminated in financial damages exceeding 3 billion USD. The attacks causing such incidents commonly commence with the deployment of adversarial contracts, subsequently leveraging these contracts to execute adversarial transactions that exploit vulnerabilities in victim contracts. Existing defense mechanisms leverage heuristic or machine learning algorithms to detect adversarial transactions, but they face significant challenges in detecting private adversarial transactions. Namely, attackers can send adversarial transactions directly to miners, evading visibility within the blockchain network and effectively bypassing the detection. In this paper, we propose a new direction for detecting DeFi attacks, i.e., detecting adversarial contracts instead of adversarial transactions, allowing us to proactively identify potential attack intentions, even if they employ private adversarial transactions. Specifically, we observe that most adversarial contracts follow a similar pattern, e.g., anonymous fund source, closed-source, frequent token-related function calls. Based on this observation, we build a machine learning classifier that can effectively distinguish adversarial contracts from benign ones. We build a dataset consists of features extracted from 304 adversarial contracts and 13,000 benign contracts. Based on this dataset, we evaluate different classifiers, the results of which show that our method for identifying DeFi adversarial contracts performs exceptionally well. For example, the F1-Score for LightGBM-based classifier is 0.9434, with a remarkably low false positive rate of only 0.12%. ",
        "title": "LookAhead: Preventing DeFi Attacks via Unveiling Adversarial Contracts",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07263",
        "abstract_url": "http://arxiv.org/abs/2401.07263",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Jie"
            },
            {
                "last_name": "Chen",
                "first_name": "Wubing"
            },
            {
                "last_name": "Tan",
                "first_name": "Mao"
            },
            {
                "last_name": "Su",
                "first_name": "Yongxing"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Despite the impressive capabilities of Deep Reinforcement Learning (DRL) agents in many challenging scenarios, their black-box decision-making process significantly limits their deployment in safety-sensitive domains. Several previous self-interpretable works focus on revealing the critical states of the agent's decision. However, they cannot pinpoint the error-prone states. To address this issue, we propose a novel self-interpretable structure, named Backbone Extract Tree (BET), to better explain the agent's behavior by identify the error-prone states. At a high level, BET hypothesizes that states in which the agent consistently executes uniform decisions exhibit a reduced propensity for errors. To effectively model this phenomenon, BET expresses these states within neighborhoods, each defined by a curated set of representative states. Therefore, states positioned at a greater distance from these representative benchmarks are more prone to error. We evaluate BET in various popular RL environments and show its superiority over existing self-interpretable models in terms of explanation fidelity. Furthermore, we demonstrate a use case for providing explanations for the agents in StarCraft II, a sophisticated multi-agent cooperative game. To the best of our knowledge, we are the first to explain such a complex scenarios using a fully transparent structure. ",
        "title": "BET: Explaining Deep Reinforcement Learning through The Error-Prone  Decisions",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07271",
        "abstract_url": "http://arxiv.org/abs/2401.07271",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Sheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Minheng"
            },
            {
                "last_name": "Wu",
                "first_name": "Junxian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziyue"
            },
            {
                "last_name": "Li",
                "first_name": "Tonglong"
            },
            {
                "last_name": "Xue",
                "first_name": "Cheng"
            },
            {
                "last_name": "Kong",
                "first_name": "Youyong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vertebrae identification in arbitrary fields-of-view plays a crucial role in diagnosing spine disease. Most spine CT contain only local regions, such as the neck, chest, and abdomen. Therefore, identification should not depend on specific vertebrae or a particular number of vertebrae being visible. Existing methods at the spine-level are unable to meet this challenge. In this paper, we propose a three-stage method to address the challenges in 3D CT vertebrae identification at vertebrae-level. By sequentially performing the tasks of vertebrae localization, segmentation, and identification, the anatomical prior information of the vertebrae is effectively utilized throughout the process. Specifically, we introduce a dual-factor density clustering algorithm to acquire localization information for individual vertebra, thereby facilitating subsequent segmentation and identification processes. In addition, to tackle the issue of interclass similarity and intra-class variability, we pre-train our identification network by using a supervised contrastive learning method. To further optimize the identification results, we estimated the uncertainty of the classification network and utilized the message fusion module to combine the uncertainty scores, while aggregating global information about the spine. Our method achieves state-of-the-art results on the VerSe19 and VerSe20 challenge benchmarks. Additionally, our approach demonstrates outstanding generalization performance on an collected dataset containing a wide range of abnormal cases. ",
        "title": "SpineCLUE: Automatic Vertebrae Identification Using Contrastive Learning  and Uncertainty Estimation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07272",
        "abstract_url": "http://arxiv.org/abs/2401.07272",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Zhengyang"
            },
            {
                "last_name": "Wang",
                "first_name": "Feng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Super-resolution techniques are crucial in improving image granularity, particularly in complex urban scenes, where preserving geometric structures is vital for data-informed cultural heritage applications. In this paper, we propose a city scene super-resolution method via geometric error minimization. The geometric-consistent mechanism leverages the Hough Transform to extract regular geometric features in city scenes, enabling the computation of geometric errors between low-resolution and high-resolution images. By minimizing mixed mean square error and geometric align error during the super-resolution process, the proposed method efficiently restores details and geometric regularities. Extensive validations on the SET14, BSD300, Cityscapes and GSV-Cities datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, especially in urban scenes. ",
        "title": "City Scene Super-Resolution via Geometric Error Minimization",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07278",
        "abstract_url": "http://arxiv.org/abs/2401.07278",
        "authors": [
            {
                "last_name": "Luu",
                "first_name": "Vinh Quoc"
            },
            {
                "last_name": "Le",
                "first_name": "Duy Khanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy Thanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Minh Thanh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Thinh Tien"
            },
            {
                "last_name": "Dinh",
                "first_name": "Vinh Quang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Artificial Intelligence (AI) in healthcare, especially in white blood cell cancer diagnosis, is hindered by two primary challenges: the lack of large-scale labeled datasets for white blood cell (WBC) segmentation and outdated segmentation methods. To address the first challenge, a semi-supervised learning framework should be brought to efficiently annotate the large dataset. In this work, we address this issue by proposing a novel self-training pipeline with the incorporation of FixMatch. We discover that by incorporating FixMatch in the self-training pipeline, the performance improves in the majority of cases. Our performance achieved the best performance with the self-training scheme with consistency on DeepLab-V3 architecture and ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC datasets, respectively. ",
        "title": "Semi-supervised Semantic Segmentation using Redesigned Self-Training for  White Blood Cel",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07282",
        "abstract_url": "http://arxiv.org/abs/2401.07282",
        "authors": [
            {
                "last_name": "Kamber",
                "first_name": "Anil"
            },
            {
                "last_name": "Yilmaz",
                "first_name": "H. Birkan"
            },
            {
                "last_name": "Pusane",
                "first_name": "Ali Emre"
            },
            {
                "last_name": "Tugcu",
                "first_name": "Tuna"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "ET"
        ],
        "abstract": "  Molecular communications is a technique emulated by researchers, which has already been used by the nature for millions of years. In Molecular Communications via Diffusion (MCvD), messenger molecules are emitted by a transmitter and propagate in the fluidic environment in a random manner. In biological systems, the environment can be considered a bounded space, surrounded by different structures, such as tissues and organs. The propagation of molecules is affected by these structures in the environment, which reflect the molecules upon collision. Hence, understanding the behavior of MCvD systems near reflecting surfaces is important for modeling molecular communication systems analytically. However, deriving the channel response of MCvD systems with an absorbing spherical receiver requires solving the diffusion equation in 3-D space in the presence of a reflecting boundary, which is extremely challenging. Therefore, derivation of the channel response in a bounded environment has remained one of the unanswered questions in the literature. In this paper, a method to model molecular communication systems near reflecting surfaces is proposed, and an analytical closed-form solution for the channel response is derived. ",
        "title": "Half-Space Modeling with Reflecting Surface in Molecular Communication",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07283",
        "abstract_url": "http://arxiv.org/abs/2401.07283",
        "authors": [
            {
                "last_name": "Miandji",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Tongbuasirilai",
                "first_name": "Tanaboon"
            },
            {
                "last_name": "Hajisharif",
                "first_name": "Saghi"
            },
            {
                "last_name": "Kavoosighafi",
                "first_name": "Behnaz"
            },
            {
                "last_name": "Unger",
                "first_name": "Jonas"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CV"
        ],
        "abstract": "  Efficient and accurate BRDF acquisition of real world materials is a challenging research problem that requires sampling millions of incident light and viewing directions. To accelerate the acquisition process, one needs to find a minimal set of sampling directions such that the recovery of the full BRDF is accurate and robust given such samples. In this paper, we formulate BRDF acquisition as a compressed sensing problem, where the sensing operator is one that performs sub-sampling of the BRDF signal according to a set of optimal sample directions. To solve this problem, we propose the Fast and Robust Optimal Sampling Technique (FROST) for designing a provably optimal sub-sampling operator that places light-view samples such that the recovery error is minimized. FROST casts the problem of designing an optimal sub-sampling operator for compressed sensing into a sparse representation formulation under the Multiple Measurement Vector (MMV) signal model. The proposed reformulation is exact, i.e. without any approximations, hence it converts an intractable combinatorial problem into one that can be solved with standard optimization techniques. As a result, FROST is accompanied by strong theoretical guarantees from the field of compressed sensing. We perform a thorough analysis of FROST-BRDF using a 10-fold cross-validation with publicly available BRDF datasets and show significant advantages compared to the state-of-the-art with respect to reconstruction quality. Finally, FROST is simple, both conceptually and in terms of implementation, it produces consistent results at each run, and it is at least two orders of magnitude faster than the prior art. ",
        "title": "FROST-BRDF: A Fast and Robust Optimal Sampling Technique for BRDF  Acquisition",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07284",
        "abstract_url": "http://arxiv.org/abs/2401.07284",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Ting"
            },
            {
                "last_name": "Huang",
                "first_name": "Shaohan"
            },
            {
                "last_name": "Luo",
                "first_name": "Shengyue"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Huang",
                "first_name": "Haizhen"
            },
            {
                "last_name": "Wei",
                "first_name": "Furu"
            },
            {
                "last_name": "Deng",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Sun",
                "first_name": "Feng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qi"
            },
            {
                "last_name": "Wang",
                "first_name": "Deqing"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Fuzhen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement exceeding 5% in domain-specific tasks. Our code will available at https://github.com/microsoft/LMOps. ",
        "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07286",
        "abstract_url": "http://arxiv.org/abs/2401.07286",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Weiqi"
            },
            {
                "last_name": "Fang",
                "first_name": "Tianqing"
            },
            {
                "last_name": "Li",
                "first_name": "Chunyang"
            },
            {
                "last_name": "Shi",
                "first_name": "Haochen"
            },
            {
                "last_name": "Ding",
                "first_name": "Wenxuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Baixuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaowei"
            },
            {
                "last_name": "Bai",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Xin"
            },
            {
                "last_name": "Cheng",
                "first_name": "Jiayang"
            },
            {
                "last_name": "Chan",
                "first_name": "Chunkit"
            },
            {
                "last_name": "Song",
                "first_name": "Yangqiu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across four downstream tasks. Our code, data, and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE. ",
        "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from  Large Language Models for Commonsense Reasoning",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07288",
        "abstract_url": "http://arxiv.org/abs/2401.07288",
        "authors": [
            {
                "last_name": "Sheshjavani",
                "first_name": "Abdollah Ghaffari"
            },
            {
                "last_name": "Khonsari",
                "first_name": "Ahmad"
            },
            {
                "last_name": "Moradian",
                "first_name": "Masoumeh"
            },
            {
                "last_name": "Shariatpanahi",
                "first_name": "Seyed Pooya"
            },
            {
                "last_name": "Hassanpour",
                "first_name": "Seyedeh Bahereh"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "NI"
        ],
        "abstract": "  To address the massive growth of data traffic over cellular networks, increasing spatial reuse of the frequency spectrum by the deployment of small base stations (SBSs) has been considered. For rapid deployment of SBSs in the networks, caching popular content along with new coded caching schemes are proposed. To maximize the cellular network's capacity, densifying it with small base stations is inevitable. In ultra-dense cellular networks, coverage of SBSs may overlap. To this aim, the multi-access caching system, where users potentially can access multiple cache nodes simultaneously, has attracted more attention in recent years. Most previous works on multi-access coded caching, only consider specific conditions such as cyclic wrap-around network topologies. In this paper, we investigate caching in ultra-dense cellular networks, where different users can access different numbers of caches under non-uniform content popularity distribution, and propose Multi-Access Hybrid coded-uncoded Caching (MAHC). We formulate the optimization problem of the proposed scheme for general network topologies and evaluate it for 2-SBS network scenarios. The numerical and simulation results show that the proposed MAHC scheme outperforms optimal conventional uncoded and previous multi-access coded caching (MACC) schemes. ",
        "title": "Hybrid Coded-Uncoded Caching in Multi-Access Networks with Non-uniform  Demands",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07290",
        "abstract_url": "http://arxiv.org/abs/2401.07290",
        "authors": [
            {
                "last_name": "Mahadevan",
                "first_name": "Ananth"
            },
            {
                "last_name": "Mathioudakis",
                "first_name": "Michael"
            },
            {
                "last_name": "M\u00e4kel\u00e4",
                "first_name": "Eetu"
            },
            {
                "last_name": "Tolonen",
                "first_name": "Mikko"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  Text reuse is a methodological element of fundamental importance in humanities research: pieces of text that re-appear across different documents, verbatim or paraphrased, provide invaluable information about the historical spread and evolution of ideas. Large modern digitized corpora enable the joint analysis of text collections that span entire centuries and the detection of large-scale patterns, impossible to detect with traditional small-scale analysis. For this opportunity to materialize, it is necessary to develop efficient data science systems that perform the corresponding analysis tasks.   In this paper, we share insights from ReceptionReader, a system for analyzing text reuse in large historical corpora. The system is built upon billions of instances of text reuses from large digitized corpora of 18th-century texts. Its main functionality is to perform downstream text reuse analysis tasks, such as finding reuses that stem from a given article or identifying the most reused quotes from a set of documents, with each task expressed as a database query. For the purposes of the paper, we discuss the related design choices including various database normalization levels and query execution frameworks, such as distributed data processing (Apache Spark), indexed row store engine (MariaDB Aria), and compressed column store engine (MariaDB Columnstore). Moreover, we present an extensive evaluation with various metrics of interest (latency, storage size, and computing costs) for varying workloads, and we offer insights from the trade-offs we observed and the choices that emerged as optimal in our setting. In summary, our results show that (1) for the workloads that are most relevant to text-reuse analysis, the MariaDB Aria framework emerges as the overall optimal choice, (2) big data processing (Apache Spark) is irreplaceable for all processing stages of the system's pipeline. ",
        "title": "Optimizing a Data Science System for Text Reuse Analysis",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07291",
        "abstract_url": "http://arxiv.org/abs/2401.07291",
        "authors": [
            {
                "last_name": "Buckwar",
                "first_name": "Evelyn"
            },
            {
                "last_name": "Djurdjevac",
                "first_name": "Ana"
            },
            {
                "last_name": "Eisenmann",
                "first_name": "Monika"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In recent years, SPDEs have become a well-studied field in mathematics. With their increase in popularity, it becomes important to efficiently approximate their solutions. Thus, our goal is a contribution towards the development of efficient and practical time-stepping methods for SPDEs.   Operator splitting schemes are a powerful tool for deterministic and stochastic differential equations. An example is given by domain decomposition schemes, where we split the domain into sub-domains. Instead of solving one expensive problem on the entire domain, we deal with cheaper problems on the sub-domains. This is particularly useful in modern computer architectures, as the sub-problems may often be solved in parallel. While splitting methods have already been used to study domain decomposition methods for deterministic PDEs, this is a new approach for SPDEs.   We provide an abstract convergence analysis of a splitting scheme for stochastic evolution equations and state a domain decomposition scheme as an application of the setting. The theoretical results are verified through numerical experiments. ",
        "title": "A domain decomposition method for stochastic evolution equations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07298",
        "abstract_url": "http://arxiv.org/abs/2401.07298",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Yue"
            },
            {
                "last_name": "Hsieh",
                "first_name": "Cho-Jui"
            },
            {
                "last_name": "Lee",
                "first_name": "Thomas C. M."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In the stochastic contextual low-rank matrix bandit problem, the expected reward of an action is given by the inner product between the action's feature matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\\Theta^*$ with rank $r \\ll \\{d_1, d_2\\}$, and an agent sequentially takes actions based on past experience to maximize the cumulative reward. In this paper, we study the generalized low-rank matrix bandit problem, which has been recently proposed in \\cite{lu2021low} under the Generalized Linear Model (GLM) framework. To overcome the computational infeasibility and theoretical restrain of existing algorithms on this problem, we first propose the G-ESTT framework that modifies the idea from \\cite{jun2019bilinear} by using Stein's method on the subspace estimation and then leverage the estimated subspaces via a regularization idea. Furthermore, we remarkably improve the efficiency of G-ESTT by using a novel exclusion idea on the estimated subspace instead, and propose the G-ESTS framework. We also show that G-ESTT can achieve the $\\tilde{O}(\\sqrt{(d_1+d_2)MrT})$ bound of regret while G-ESTS can achineve the $\\tilde{O}(\\sqrt{(d_1+d_2)^{3/2}Mr^{3/2}T})$ bound of regret under mild assumption up to logarithm terms, where $M$ is some problem dependent value. Under a reasonable assumption that $M = O((d_1+d_2)^2)$ in our problem setting, the regret of G-ESTT is consistent with the current best regret of $\\tilde{O}((d_1+d_2)^{3/2} \\sqrt{rT}/D_{rr})$~\\citep{lu2021low} ($D_{rr}$ will be defined later). For completeness, we conduct experiments to illustrate that our proposed algorithms, especially G-ESTS, are also computationally tractable and consistently outperform other state-of-the-art (generalized) linear matrix bandit methods based on a suite of simulations. ",
        "title": "Efficient Frameworks for Generalized Low-Rank Matrix Bandit Problems",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07300",
        "abstract_url": "http://arxiv.org/abs/2401.07300",
        "authors": [
            {
                "last_name": "Riva",
                "first_name": "Stefano"
            },
            {
                "last_name": "Introini",
                "first_name": "Carolina"
            },
            {
                "last_name": "Cammi",
                "first_name": "Antonio"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Nowadays, interest in combining mathematical knowledge about phenomena and data from the physical system is growing. Past research was devoted to developing so-called high-fidelity models, intending to make them able to catch most of the physical phenomena occurring in the system. Nevertheless, models will always be affected by uncertainties related, for example, to the parameters and inevitably limited by the underlying simplifying hypotheses on, for example, geometry and mathematical equations; thus, in a way, there exists an upper threshold of model performance. Now, research in many engineering sectors also focuses on the so-called data-driven modelling, which aims at extracting information from available data to combine it with the mathematical model. Focusing on the nuclear field, interest in this approach is also related to the Multi-Physics modelling of nuclear reactors. Due to the multiple physics involved and their mutual and complex interactions, developing accurate and stable models both from the physical and numerical point of view remains a challenging task despite the advancements in computational hardware and software, and combining the available mathematical model with data can further improve the performance and the accuracy of the former.   This work investigates this aspect by applying two Data-Driven Reduced Order Modelling (DDROM) techniques, the Generalised Empirical Interpolation Method and the Parametrised-Background Data-Weak formulation, to literature benchmark nuclear case studies. The main goal of this work is to assess the possibility of using data to perform model bias correction, that is, verifying the reliability of DDROM approaches in improving the model performance and accuracy through the information provided by the data. The obtained numerical results are promising, foreseeing further investigation of the DDROM approach to nuclear industrial cases. ",
        "title": "Multi-Physics Model Bias Correction with Data-Driven Reduced Order  Modelling Techniques: Application to Nuclear Case Studies",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07301",
        "abstract_url": "http://arxiv.org/abs/2401.07301",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Haixia"
            },
            {
                "last_name": "Liang",
                "first_name": "Jiaqing"
            },
            {
                "last_name": "Shi",
                "first_name": "Jie"
            },
            {
                "last_name": "He",
                "first_name": "Qianyu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yanghua"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Generative Language Models (LMs) such as ChatGPT have exhibited remarkable performance across various downstream tasks. Nevertheless, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. Previous studies have devised sophisticated pipelines and prompts to induce large LMs to exhibit the capability for self-correction. However, large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans. Moreover, these complex prompts are extremely challenging for small LMs to follow. In this paper, we introduce the \\underline{I}ntrinsic \\underline{S}elf-\\underline{C}orrection (ISC) in generative language models, aiming to correct the initial output of LMs in a self-triggered manner, even for those small LMs with 6 billion parameters. Specifically, we devise a pipeline for constructing self-correction data and propose Partial Answer Masking (PAM), aiming to endow the model with the capability for intrinsic self-correction through fine-tuning. We conduct experiments using LMs with parameters sizes ranging from 6 billion to 13 billion in two tasks, including commonsense reasoning and factual knowledge reasoning. Our experiments demonstrate that the outputs generated using ISC outperform those generated without self-correction. We believe that the output quality of even small LMs can be further improved by empowering them with the ability to intrinsic self-correct. ",
        "title": "Small Language Model Can Self-correct",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07305",
        "abstract_url": "http://arxiv.org/abs/2401.07305",
        "authors": [
            {
                "last_name": "Kuang",
                "first_name": "Xu"
            },
            {
                "last_name": "Mendelson",
                "first_name": "Gal"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Being able to detect service slowdowns is crucial to many operational problems. We study how to use observational congestion data to detect service slowdown in a multi-server system, and in particular, the statistical implications of running adaptive congestion control mechanisms in such settings. We show that a commonly used summary statistic that relies on the marginal congestion measured at individual servers can be highly inaccurate the presence of adaptive congestion control. We propose a new statistic based on potential routing actions, and show it provides a much more robust signal for server slowdown in these settings. Unlike the marginal statistic, potential action aims to detect changes in the {routing actions}, and is able to uncover slowdowns even when they do not reflect in marginal congestion. Our work highlights the complexity in performing observational statistical analysis for service systems in the presence of adaptive congestion control. Our results also suggest that practitioners may want to combine multiple, orthogonal statistics to achieve reliable slowdown detection. ",
        "title": "Detecting Service Slowdown using Observational Data",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07308",
        "abstract_url": "http://arxiv.org/abs/2401.07308",
        "authors": [
            {
                "last_name": "Alahmadi",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Alharbi",
                "first_name": "Salma"
            },
            {
                "last_name": "Alharbi",
                "first_name": "Talal"
            },
            {
                "last_name": "Almutairi",
                "first_name": "Nadiyah"
            },
            {
                "last_name": "Alshammari",
                "first_name": "Tuwailaa"
            },
            {
                "last_name": "Bhattacharyya",
                "first_name": "Anirban"
            },
            {
                "last_name": "Koutny",
                "first_name": "Maciej"
            },
            {
                "last_name": "Li",
                "first_name": "Bowen"
            },
            {
                "last_name": "Randell",
                "first_name": "Brian"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The concept of structured occurrence nets is an extension of that of occurrence nets which are directed acyclic graphs that represent causality and concurrency information concerning a single execution of a distributed system. The formalism of structured occurrence nets has been introduced to facilitate the portrayal and analysis of the behaviours, and in particular failures, of complex evolving systems. Such systems are composed of a large number of sub-systems which may proceed concurrently and interact with each other and with the external environment while their behaviour is subject to modification by other systems. The purpose of this paper is to provide an extension of structured occurrence nets to include models built up of acyclic nets rather than occurrence nets. ",
        "title": "Structured Acyclic Nets",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07310",
        "abstract_url": "http://arxiv.org/abs/2401.07310",
        "authors": [
            {
                "last_name": "Chowdhury",
                "first_name": "Ahmadul Karim"
            },
            {
                "last_name": "Sujon",
                "first_name": "Md. Saidur Rahman"
            },
            {
                "last_name": "Shafi",
                "first_name": "Md. Shirajus Salekin"
            },
            {
                "last_name": "Ahmmad",
                "first_name": "Tasin"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Sifat"
            },
            {
                "last_name": "Hasib",
                "first_name": "Khan Md"
            },
            {
                "last_name": "Shah",
                "first_name": "Faisal Muhammad"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In an era where the silent struggle of underdiagnosed depression pervades globally, our research delves into the crucial link between mental health and social media. This work focuses on early detection of depression, particularly in extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our proposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning models(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT, SahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into \"Depressive\" and \"Non-Depressive\" segments, translated into Bengali by native speakers with expertise in mental health, resulting in the creation of the Bengali Social Media Depressive Dataset (BSMDD). Our work provides full architecture details for each model and a methodical way to assess their performance in Bengali depressive text categorization using zero-shot and few-shot learning techniques. Our work demonstrates the superiority of SahajBERT and Bi-LSTM with FastText embeddings in their respective domains also tackles explainability issues with transformer models and emphasizes the effectiveness of LLMs, especially DepGPT, demonstrating flexibility and competence in a range of learning contexts. According to the experiment results, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in zero-shot and few-shot scenarios but also every other model, achieving a near-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and exceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B show relatively poorer effectiveness in zero-shot and few-shot situations. The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models. ",
        "title": "Harnessing Large Language Models Over Transformer Models for Detecting  Bengali Depressive Social Media Text: A Comprehensive Study",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07312",
        "abstract_url": "http://arxiv.org/abs/2401.07312",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "JiayiZhou. Renzhong"
            },
            {
                "last_name": "Tang",
                "first_name": "Junxiu"
            },
            {
                "last_name": "Tang",
                "first_name": "Tan"
            },
            {
                "last_name": "Li",
                "first_name": "Haotian"
            },
            {
                "last_name": "Cui",
                "first_name": "Weiwei"
            },
            {
                "last_name": "Wu",
                "first_name": "Yingcaui"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Creative design is a nonlinear process where designers generate diverse ideas in the pursuit of an open-ended goal and converge towards consensus through iterative remixing. In contrast, AI-powered design tools often employ a linear sequence of incremental and precise instructions to approximate design objectives. Such operations violate customary creative design practices and thus hinder AI agents' ability to complete creative design tasks. To explore better human-AI co-design tools, we first summarize human designers' practices through a formative study with 12 design experts. Taking graphic design as a representative scenario, we formulate a nonlinear human-AI co-design framework and develop a proof-of-concept prototype, OptiMuse. We evaluate OptiMuse and validate the nonlinear framework through a comparative study. We notice a subconscious change in people's attitudes towards AI agents, shifting from perceiving them as mere executors to regarding them as opinionated colleagues. This shift effectively fostered the exploration and reflection processes of individual designers. ",
        "title": "Understanding Nonlinear Collaboration between Human and AI Agents: A  Co-design Framework for Creative Design",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07314",
        "abstract_url": "http://arxiv.org/abs/2401.07314",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiaqi"
            },
            {
                "last_name": "Lin",
                "first_name": "Bingqian"
            },
            {
                "last_name": "Xu",
                "first_name": "Ran"
            },
            {
                "last_name": "Chai",
                "first_name": "Zhenhua"
            },
            {
                "last_name": "Liang",
                "first_name": "Xiaodan"
            },
            {
                "last_name": "Wong",
                "first_name": "Kwan-Yee K."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Embodied agents equipped with GPT as their brain have exhibited extraordinary thinking and decision-making abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT to handle excessive environmental information and select potential locations within localized environments, without constructing an effective ''global-view'' (e.g., a commonly-used map) for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based path-planning agent, dubbed MapGPT, for the zero-shot VLN task. Specifically, we convert a topological map constructed online into prompts to encourage map-guided global exploration, and require the agent to explicitly output and update multi-step path planning to avoid getting stuck in local exploration. Extensive experiments demonstrate that our MapGPT is effective, achieving impressive performance on both the R2R and REVERIE datasets (38.8% and 28.4% success rate, respectively) and showcasing the newly emerged global thinking and path planning capabilities of the GPT model. Unlike previous VLN agents, which require separate parameters fine-tuning or specific prompt design to accommodate various instruction styles across different datasets, our MapGPT is more unified as it can adapt to different instruction styles seamlessly, which is the first of its kind in this field. ",
        "title": "MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07316",
        "abstract_url": "http://arxiv.org/abs/2401.07316",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Feiyang"
            },
            {
                "last_name": "\u00d8stvold",
                "first_name": "Bjarte M."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CR"
        ],
        "abstract": "  Privacy code review is a critical process that enables developers and legal experts to ensure compliance with data protection regulations. However, the task is challenging due to resource constraints. To address this, we introduce the concept of privacy-relevant methods - specific methods in code that are directly involved in the processing of personal data. We then present an automated approach to assist in code review by identifying and categorizing these privacy-relevant methods in source code.   Using static analysis, we identify a set of methods based on their occurrences in 50 commonly used libraries. We then rank these methods according to their frequency of invocation with actual personal data in the top 30 GitHub applications. The highest-ranked methods are the ones we designate as privacy-relevant in practice. For our evaluation, we examined 100 open-source applications and found that our approach identifies fewer than 5% of the methods as privacy-relevant for personal data processing. This reduces the time required for code reviews. Case studies on Signal Desktop and Cal.com further validate the effectiveness of our approach in aiding code reviewers to produce enhanced reports that facilitate compliance with privacy regulations. ",
        "title": "Finding Privacy-relevant Source Code",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07319",
        "abstract_url": "http://arxiv.org/abs/2401.07319",
        "authors": [
            {
                "last_name": "Friedlander",
                "first_name": "Izzy"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The MacWilliams Identity is a well established theorem relating the weight enumerator of a code to the weight enumerator of its dual. The ability to use a known weight enumerator to generate the weight enumerator of another through a simple transform proved highly effective and efficient. An equivalent relation was also developed by Delsarte which linked the eigenvalues of any association scheme to the eigenvalues of it's dual association scheme but this was less practical to use in reality. A functional transform was developed for some specific association schemes including those based on the rank metric, the skew rank metric and Hermitian matrices. In this paper those results are unified into a single consistent theory applied to these \"Krawtchouk association schemes\" using a $b$-algebra. The derivatives formed using the $b$-algebra have also been applied to derive the moments of the weight distribution for any code within these association schemes. ",
        "title": "The MacWilliams Identity for Krawtchouk Association Schemes",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07322",
        "abstract_url": "http://arxiv.org/abs/2401.07322",
        "authors": [
            {
                "last_name": "Zunair",
                "first_name": "Hasib"
            },
            {
                "last_name": "Khan",
                "first_name": "Shakib"
            },
            {
                "last_name": "Hamza",
                "first_name": "A. Ben"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Road scene understanding is crucial in autonomous driving, enabling machines to perceive the visual environment. However, recent object detectors tailored for learning on datasets collected from certain geographical locations struggle to generalize across different locations. In this paper, we present RSUD20K, a new dataset for road scene understanding, comprised of over 20K high-resolution images from the driving perspective on Bangladesh roads, and includes 130K bounding box annotations for 13 objects. This challenging dataset encompasses diverse road scenes, narrow streets and highways, featuring objects from different viewpoints and scenes from crowded environments with densely cluttered objects and various weather conditions. Our work significantly improves upon previous efforts, providing detailed annotations and increased object complexity. We thoroughly examine the dataset, benchmarking various state-of-the-art object detectors and exploring large vision models as image annotators. ",
        "title": "RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07323",
        "abstract_url": "http://arxiv.org/abs/2401.07323",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Toyota"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  High-Definition (HD) maps are pivotal to autopilot navigation. Integrating the capability of lightweight HD map construction at runtime into a self-driving system recently emerges as a promising direction. In this surge, vision-only perception stands out, as a camera rig can still perceive the stereo information, let alone its appealing signature of portability and economy. The latest MapTR architecture solves the online HD map construction task in an end-to-end fashion but its potential is yet to be explored. In this work, we present a full-scale upgrade of MapTR and propose MapNeXt, the next generation of HD map learning architecture, delivering major contributions from the model training and scaling perspectives. After shedding light on the training dynamics of MapTR and exploiting the supervision from map elements thoroughly, MapNeXt-Tiny raises the mAP of MapTR-Tiny from 49.0% to 54.8%, without any architectural modifications. Enjoying the fruit of map segmentation pre-training, MapNeXt-Base further lifts the mAP up to 63.9% that has already outperformed the prior art, a multi-modality MapTR, by 1.4% while being $\\sim1.8\\times$ faster. Towards pushing the performance frontier to the next level, we draw two conclusions on practical model scaling: increased query favors a larger decoder network for adequate digestion; a large backbone steadily promotes the final accuracy without bells and whistles. Building upon these two rules of thumb, MapNeXt-Huge achieves state-of-the-art performance on the challenging nuScenes benchmark. Specifically, we push the mapless vision-only single-model performance to be over 78% for the first time, exceeding the best model from existing methods by 16%. ",
        "title": "MapNeXt: Revisiting Training and Scaling Practices for Online Vectorized  HD Map Construction",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07324",
        "abstract_url": "http://arxiv.org/abs/2401.07324",
        "authors": [
            {
                "last_name": "Shen",
                "first_name": "Weizhou"
            },
            {
                "last_name": "Li",
                "first_name": "Chenliang"
            },
            {
                "last_name": "Chen",
                "first_name": "Hongzhan"
            },
            {
                "last_name": "Yan",
                "first_name": "Ming"
            },
            {
                "last_name": "Quan",
                "first_name": "Xiaojun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hehong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ji"
            },
            {
                "last_name": "Huang",
                "first_name": "Fei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. ",
        "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07326",
        "abstract_url": "http://arxiv.org/abs/2401.07326",
        "authors": [
            {
                "last_name": "Chung",
                "first_name": "Dat T."
            },
            {
                "last_name": "Dang",
                "first_name": "Minh-Anh"
            },
            {
                "last_name": "Vu",
                "first_name": "Mai-Anh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Minh T."
            },
            {
                "last_name": "Nguyen",
                "first_name": "Thanh-Huy"
            },
            {
                "last_name": "Dinh",
                "first_name": "Vinh Q."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Breast Ultrasound plays a vital role in cancer diagnosis as a non-invasive approach with cost-effective. In recent years, with the development of deep learning, many CNN-based approaches have been widely researched in both tumor localization and cancer classification tasks. Even though previous single models achieved great performance in both tasks, these methods have some limitations in inference time, GPU requirement, and separate fine-tuning for each model. In this study, we aim to redesign and build end-to-end multi-task architecture to conduct both segmentation and classification. With our proposed approach, we achieved outstanding performance and time efficiency, with 79.8% and 86.4% in DeepLabV3+ architecture in the segmentation task. ",
        "title": "Beyond Traditional Approaches: Multi-Task Network for Breast Ultrasound  Diagnosis",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07329",
        "abstract_url": "http://arxiv.org/abs/2401.07329",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Tong",
                "first_name": "Haonan"
            },
            {
                "last_name": "Yang",
                "first_name": "Nuocheng"
            },
            {
                "last_name": "Yin",
                "first_name": "Changchuan"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  This paper studies the problem of the lightweight image semantic communication system that is deployed on Internet of Things (IoT) devices. In the considered system model, devices must use semantic communication techniques to support user behavior recognition in ultimate video service with high data transmission efficiency. However, it is computationally expensive for IoT devices to deploy semantic codecs due to the complex calculation processes of deep learning (DL) based codec training and inference. To make it affordable for IoT devices to deploy semantic communication systems, we propose an attention-based UNet enabled lightweight image semantic communication (LSSC) system, which achieves low computational complexity and small model size. In particular, we first let the LSSC system train the codec at the edge server to reduce the training computation load on IoT devices. Then, we introduce the convolutional block attention module (CBAM) to extract the image semantic features and decrease the number of downsampling layers thus reducing the floating-point operations (FLOPs). Finally, we experimentally adjust the structure of the codec and find out the optimal number of downsampling layers. Simulation results show that the proposed LSSC system can reduce the semantic codec FLOPs by 14%, and reduce the model size by 55%, with a sacrifice of 3% accuracy, compared to the baseline. Moreover, the proposed scheme can achieve a higher transmission accuracy than the traditional communication scheme in the low channel signal-to-noise (SNR) region. ",
        "title": "Attention-based UNet enabled Lightweight Image Semantic Communication  System over Internet of Things",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07331",
        "abstract_url": "http://arxiv.org/abs/2401.07331",
        "authors": [
            {
                "last_name": "Naghavi",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Wang",
                "first_name": "Haifeng"
            },
            {
                "last_name": "Fan",
                "first_name": "Lei"
            },
            {
                "last_name": "Choy",
                "first_name": "Jenny S."
            },
            {
                "last_name": "Kassab",
                "first_name": "Ghassan"
            },
            {
                "last_name": "Baek",
                "first_name": "Seungik"
            },
            {
                "last_name": "Lee",
                "first_name": "Lik-Chuan"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Physics-based computer models based on numerical solution of the governing equations generally cannot make rapid predictions, which in turn, limits their applications in the clinic. To address this issue, we developed a physics-informed neural network (PINN) model that encodes the physics of a closed-loop blood circulation system embedding a left ventricle (LV). The PINN model is trained to satisfy a system of ordinary differential equations (ODEs) associated with a lumped parameter description of the circulatory system. The model predictions have a maximum error of less than 5% when compared to those obtained by solving the ODEs numerically. An inverse modeling approach using the PINN model is also developed to rapidly estimate model parameters (in $\\sim$ 3 mins) from single-beat LV pressure and volume waveforms. Using synthetic LV pressure and volume waveforms generated by the PINN model with different model parameter values, we show that the inverse modeling approach can recover the corresponding ground truth values, which suggests that the model parameters are unique. The PINN inverse modeling approach is then applied to estimate LV contractility indexed by the end-systolic elastance $E_{es}$ using waveforms acquired from 11 swine models, including waveforms acquired before and after administration of dobutamine (an inotropic agent) in 3 animals. The estimated $E_{es}$ is about 58% to 284% higher for the data associated with dobutamine compared to those without, which implies that this approach can be used to estimate LV contractility using single-beat measurements. The PINN inverse modeling can potentially be used in the clinic to simultaneously estimate LV contractility and other physiological parameters from single-beat measurements. ",
        "title": "Rapid Estimation of Left Ventricular Contractility with a  Physics-Informed Neural Network Inverse Modeling Approach",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07333",
        "abstract_url": "http://arxiv.org/abs/2401.07333",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Yakun"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaofei"
            },
            {
                "last_name": "Ma",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Xie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/. ",
        "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided  Sequence Reordering",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07336",
        "abstract_url": "http://arxiv.org/abs/2401.07336",
        "authors": [
            {
                "last_name": "Ting",
                "first_name": "Zhu"
            },
            {
                "last_name": "Liangqi",
                "first_name": "Li"
            },
            {
                "last_name": "Shufei",
                "first_name": "Duan"
            },
            {
                "last_name": "Xueying",
                "first_name": "Zhang"
            },
            {
                "last_name": "Zhongzhe",
                "first_name": "Xiao"
            },
            {
                "last_name": "Hairng",
                "first_name": "Jia"
            },
            {
                "last_name": "Huizhi",
                "first_name": "Liang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  A multi-modal emotional speech Mandarin database including articulatory kinematics, acoustics, glottal and facial micro-expressions is designed and established, which is described in detail from the aspects of corpus design, subject selection, recording details and data processing. Where signals are labeled with discrete emotion labels (neutral, happy, pleasant, indifferent, angry, sad, grief) and dimensional emotion labels (pleasure, arousal, dominance). In this paper, the validity of dimension annotation is verified by statistical analysis of dimension annotation data. The SCL-90 scale data of annotators are verified and combined with PAD annotation data for analysis, so as to explore the internal relationship between the outlier phenomenon in annotation and the psychological state of annotators. In order to verify the speech quality and emotion discrimination of the database, this paper uses 3 basic models of SVM, CNN and DNN to calculate the recognition rate of these seven emotions. The results show that the average recognition rate of seven emotions is about 82% when using acoustic data alone. When using glottal data alone, the average recognition rate is about 72%. Using kinematics data alone, the average recognition rate also reaches 55.7%. Therefore, the database is of high quality and can be used as an important source for speech analysis research, especially for the task of multimodal emotional speech analysis. ",
        "title": "Construction and Evaluation of Mandarin Multimodal Emotional Speech  Database",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07337",
        "abstract_url": "http://arxiv.org/abs/2401.07337",
        "authors": [
            {
                "last_name": "Echenique",
                "first_name": "Federico"
            },
            {
                "last_name": "Pourbabaee",
                "first_name": "Farzad"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We provide a quantitative assessment of welfare in the classical model of risk-sharing and exchange under uncertainty. We prove three kinds of results. First, that in an equilibrium allocation, the scope for improving individual welfare by a given margin (an $\\ve$-improvement) vanishes as the number of states increases. Second, that the scope for a change in aggregate resources that may be distributed to enhance individual welfare by a given margin also vanishes. Equivalently: in an inefficient allocation, for a given level of resource sub-optimality (as measured by the coefficient of resource under-utilization), the possibilities for enhancing welfare by perturbing aggregate resources decrease exponentially to zero with the number of states. Finally, we consider efficient risk-sharing in standard models of uncertainty aversion with multiple priors, and show that, in an inefficient allocation, certain sets of priors shrink with the size of the state space. ",
        "title": "Individual and Collective Welfare in Risk Sharing with Many States",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07339",
        "abstract_url": "http://arxiv.org/abs/2401.07339",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Kechi"
            },
            {
                "last_name": "Li",
                "first_name": "Jia"
            },
            {
                "last_name": "Li",
                "first_name": "Ge"
            },
            {
                "last_name": "Shi",
                "first_name": "Xianjie"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\\% to 250\\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges. ",
        "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems  for Real-World Repo-level Coding Challenges",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07340",
        "abstract_url": "http://arxiv.org/abs/2401.07340",
        "authors": [
            {
                "last_name": "Antoniak",
                "first_name": "Maria"
            },
            {
                "last_name": "Mimno",
                "first_name": "David"
            },
            {
                "last_name": "Thalken",
                "first_name": "Rosamond"
            },
            {
                "last_name": "Walsh",
                "first_name": "Melanie"
            },
            {
                "last_name": "Wilkens",
                "first_name": "Matthew"
            },
            {
                "last_name": "Yauney",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The growth of social reading platforms such as Goodreads and LibraryThing enables us to analyze reading activity at very large scale and in remarkable detail. But twenty-first century systems give us a perspective only on contemporary readers. Meanwhile, the digitization of the lending library records of Shakespeare and Company provides a window into the reading activity of an earlier, smaller community in interwar Paris. In this article, we explore the extent to which we can make comparisons between the Shakespeare and Company and Goodreads communities. By quantifying similarities and differences, we can identify patterns in how works have risen or fallen in popularity across these datasets. We can also measure differences in how works are received by measuring similarities and differences in co-reading patterns. Finally, by examining the complete networks of co-readership, we can observe changes in the overall structures of literary reception. ",
        "title": "The Afterlives of Shakespeare and Company in Online Social Readership",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07341",
        "abstract_url": "http://arxiv.org/abs/2401.07341",
        "authors": [
            {
                "last_name": "Hochbaum",
                "first_name": "Dorit S."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We address here spanning tree problems on a graph with binary edge weights. For a general weighted graph the minimum spanning tree is solved in super-linear running time, even when the edges of the graph are pre-sorted. A related problem, of finding a spanning tree with a pre-specified sum of weights, is NP-hard. In contrast, for a graph with binary weights associated with the edges, it is shown that the minimum spanning tree and finding a spanning tree with a given total sum, are solvable in linear time with simple algorithms. ",
        "title": "Binary weights spanning trees and the $k$-red spanning tree problem in  linear time",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07342",
        "abstract_url": "http://arxiv.org/abs/2401.07342",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Anchen"
            },
            {
                "last_name": "Londono",
                "first_name": "Juan J"
            },
            {
                "last_name": "Elbaum",
                "first_name": "Batya"
            },
            {
                "last_name": "Estrada",
                "first_name": "Luis"
            },
            {
                "last_name": "Lazo",
                "first_name": "Roberto Jose"
            },
            {
                "last_name": "Vitale",
                "first_name": "Laura"
            },
            {
                "last_name": "Villasanti",
                "first_name": "Hugo Gonzalez"
            },
            {
                "last_name": "Fusaroli",
                "first_name": "Riccardo"
            },
            {
                "last_name": "Perry",
                "first_name": "Lynn K"
            },
            {
                "last_name": "Messinger",
                "first_name": "Daniel S"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2 teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was .76, with an error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for both teacher and child transcriptions was .15, meaning that 15% of words would need to be deleted, added, or changed to equate the Whisper and expert transcriptions. Moreover, speech features such as the mean length of utterances in words, the proportion of teacher and child utterances that were questions, and the proportion of utterances that were responded to within 2.5 seconds were similar when calculated separately from expert and automated transcriptions. The results suggest substantial progress in analyzing classroom speech that may support children's language development. Future research using natural language processing is underway to improve speaker classification and to analyze results from the application of the automated it framework to a larger dataset containing classroom recordings from 13 children and 4 teachers observed on 17 occasions over one year. ",
        "title": "Who Said What? An Automated Approach to Analyzing Speech in Preschool  Classrooms",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07343",
        "abstract_url": "http://arxiv.org/abs/2401.07343",
        "authors": [
            {
                "last_name": "Ahsan",
                "first_name": "Shakil Ibne"
            },
            {
                "last_name": "Legg",
                "first_name": "Phil"
            },
            {
                "last_name": "Alam",
                "first_name": "S M Iftekharul"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The absence of robust security protocols renders the VANET (Vehicle ad-hoc Networks) network open to cyber threats by compromising passengers and road safety. Intrusion Detection Systems (IDS) are widely employed to detect network security threats. With vehicles' high mobility on the road and diverse environments, VANETs devise ever-changing network topologies, lack privacy and security, and have limited bandwidth efficiency. The absence of privacy precautions, End-to-End Encryption methods, and Local Data Processing systems in VANET also present many privacy and security difficulties. So, assessing whether a novel real-time processing IDS approach can be utilized for this emerging technology is crucial. The present study introduces a novel approach for intrusion detection using Federated Learning (FL) capabilities in conjunction with the BERT model for sequence classification (FL-BERT). The significance of data privacy is duly recognized. According to FL methodology, each client has its own local model and dataset. They train their models locally and then send the model's weights to the server. After aggregation, the server aggregates the weights from all clients to update a global model. After aggregation, the global model's weights are shared with the clients. This practice guarantees the secure storage of sensitive raw data on individual clients' devices, effectively protecting privacy. After conducting the federated learning procedure, we assessed our models' performance using a separate test dataset. The FL-BERT technique has yielded promising results, opening avenues for further investigation in this particular area of research. We reached the result of our approaches by comparing existing research works and found that FL-BERT is more effective for privacy and security concerns. Our results suggest that FL-BERT is a promising technique for enhancing attack detection. ",
        "title": "Privacy-Preserving Intrusion Detection in Software-defined VANET using  Federated Learning with BERT",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07348",
        "abstract_url": "http://arxiv.org/abs/2401.07348",
        "authors": [
            {
                "last_name": "Novelli",
                "first_name": "Claudio"
            },
            {
                "last_name": "Casolari",
                "first_name": "Federico"
            },
            {
                "last_name": "Hacker",
                "first_name": "Philipp"
            },
            {
                "last_name": "Spedicato",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Floridi",
                "first_name": "Luciano"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models, ensuring they align with the EU's evolving digital landscape and legal standards. ",
        "title": "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and  Cybersecurity",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07353",
        "abstract_url": "http://arxiv.org/abs/2401.07353",
        "authors": [
            {
                "last_name": "Gohar",
                "first_name": "Usman"
            },
            {
                "last_name": "Hunter",
                "first_name": "Michael C."
            },
            {
                "last_name": "Marczak-Czajka",
                "first_name": "Agnieszka"
            },
            {
                "last_name": "Lutz",
                "first_name": "Robyn R."
            },
            {
                "last_name": "Cohen",
                "first_name": "Myra B."
            },
            {
                "last_name": "Cleland-Huang",
                "first_name": "Jane"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversion to any AI-supported automation, highlighting the need for full transparency in automated decision-making. Results provide a societal perspective on the challenges of automating UTM flight authorization decisions and help frame the ongoing design of a solution acceptable to the broader sUAS community. ",
        "title": "Towards Engineering Fair and Equitable Software Systems for Managing  Low-Altitude Airspace Authorizations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07356",
        "abstract_url": "http://arxiv.org/abs/2401.07356",
        "authors": [
            {
                "last_name": "Pramod",
                "first_name": "K. D."
            },
            {
                "last_name": "De Silva",
                "first_name": "W. T. N."
            },
            {
                "last_name": "Thabrew",
                "first_name": "W. U. K."
            },
            {
                "last_name": "Shariffdeen",
                "first_name": "Ridwan"
            },
            {
                "last_name": "Wickramanayake",
                "first_name": "Sandareka"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Automated Program Repair (APR) improves developer productivity by saving debugging and bug-fixing time. While APR has been extensively explored for C/C++ and Java programs, there is little research on bugs in PHP programs due to the lack of a benchmark PHP bug dataset. This is surprising given that PHP has been one of the most widely used server-side languages for over two decades, being used in a variety of contexts such as e-commerce, social networking, and content management. This paper presents a benchmark dataset of PHP bugs on real-world applications called BUGSPHP, which can enable research on analysis, testing, and repair for PHP programs. The dataset consists of training and test datasets, separately curated from GitHub and processed locally. The training dataset includes more than 600,000 bug-fixing commits. The test dataset contains 513 manually validated bug-fixing commits equipped with developer-provided test cases to assess patch correctness. ",
        "title": "BUGSPHP: A dataset for Automated Program Repair in PHP",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07358",
        "abstract_url": "http://arxiv.org/abs/2401.07358",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yuyang"
            },
            {
                "last_name": "Hao",
                "first_name": "Yizhi"
            },
            {
                "last_name": "Cong",
                "first_name": "Amando Xu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the realm of digital media, the advent of AI-generated synthetic images has introduced significant challenges in distinguishing between real and fabricated visual content. These images, often indistinguishable from authentic ones, pose a threat to the credibility of digital media, with potential implications for disinformation and fraud. Our research addresses this challenge by employing machine learning techniques to discern between AI-generated and genuine images. Central to our approach is the CIFAKE dataset, a comprehensive collection of images labeled as \"Real\" and \"Fake\". We refine and adapt advanced deep learning architectures like ResNet, VGGNet, and DenseNet, utilizing transfer learning to enhance their precision in identifying synthetic images. We also compare these with a baseline model comprising a vanilla Support Vector Machine (SVM) and a custom Convolutional Neural Network (CNN). The experimental results were significant, demonstrating that our optimized deep learning models outperform traditional methods, with DenseNet achieving an accuracy of 97.74%. Our application study contributes by applying and optimizing these advanced models for synthetic image detection, conducting a comparative analysis using various metrics, and demonstrating their superior capability in identifying AI-generated images over traditional machine learning techniques. This research not only advances the field of digital media integrity but also sets a foundation for future explorations into the ethical and technical dimensions of AI-generated content in digital media. ",
        "title": "Harnessing Machine Learning for Discerning AI-Generated Synthetic Images",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07359",
        "abstract_url": "http://arxiv.org/abs/2401.07359",
        "authors": [
            {
                "last_name": "Scorzato",
                "first_name": "Luigi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown to be language-independent. It is argued that the high epistemic complexity of DNN models hinders the estimate of their reliability and also their prospect of long-term progress. Some potential ways forward are suggested. Thirdly, this article identifies the close relation between a model's epistemic complexity and its interpretability, as introduced in the context of responsible AI. This clarifies in which sense, and to what extent, the lack of understanding of a model (black-box problem) impacts its interpretability in a way that is independent of individual skills. It also clarifies how interpretability is a precondition for assessing the reliability of any model, which cannot be based on statistical analysis alone. This article focuses on the comparison between traditional scientific models and DNN models. But, Random Forest and Logistic Regression models are also briefly considered. ",
        "title": "Reliability and Interpretability in Science and Deep Learning",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07360",
        "abstract_url": "http://arxiv.org/abs/2401.07360",
        "authors": [
            {
                "last_name": "Duarte-Torres",
                "first_name": "Sergio"
            },
            {
                "last_name": "Sen",
                "first_name": "Arunasish"
            },
            {
                "last_name": "Rana",
                "first_name": "Aman"
            },
            {
                "last_name": "Drude",
                "first_name": "Lukas"
            },
            {
                "last_name": "Gomez-Alanis",
                "first_name": "Alejandro"
            },
            {
                "last_name": "Schwarz",
                "first_name": "Andreas"
            },
            {
                "last_name": "R\u00e4del",
                "first_name": "Leif"
            },
            {
                "last_name": "Leutnant",
                "first_name": "Volker"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  Context cues carry information which can improve multi-turn interactions in automatic speech recognition (ASR) systems. In this paper, we introduce a novel mechanism inspired by hyper-prompting to fuse textual context with acoustic representations in the attention mechanism. Results on a test set with multi-turn interactions show that our method achieves 5.9% relative word error rate reduction (rWERR) over a strong baseline. We show that our method does not degrade in the absence of context and leads to improvements even if the model is trained without context. We further show that leveraging a pre-trained sentence-piece model for context embedding generation can outperform an external BERT model. ",
        "title": "Promptformer: Prompted Conformer Transducer for ASR",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07361",
        "abstract_url": "http://arxiv.org/abs/2401.07361",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Anthony"
            },
            {
                "last_name": "Jablonowski",
                "first_name": "Christiane"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Fast summation refers to a family of techniques for approximating $O(N^2)$ sums in $O(N\\log{N})$ or $O(N)$ time. These techniques have traditionally found wide use in astrophysics and electrostatics in calculating the forces in a $N$-body problem. In this work, we present a spherical tree code, and apply it to the problem of efficiently solving the barotropic vorticity equation. ",
        "title": "Fast Summation on the Sphere with Applications to the Barotropic  Vorticity Equation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07363",
        "abstract_url": "http://arxiv.org/abs/2401.07363",
        "authors": [
            {
                "last_name": "Lotfi",
                "first_name": "Ehsan"
            },
            {
                "last_name": "De Bruyn",
                "first_name": "Maxime"
            },
            {
                "last_name": "Buhmann",
                "first_name": "Jeska"
            },
            {
                "last_name": "Daelemans",
                "first_name": "Walter"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The new wave of Large Language Models (LLM) has offered an efficient tool to curate sizeable conversational datasets. So far studies have mainly focused on task-oriented or generic open-domain dialogs, and have not fully explored the ability of LLMs in following complicated prompts. In this work, we focus on personalization, and employ LLMs to curate a dataset which is difficult and costly to crowd-source: PersonalityChat is a synthetic conversational dataset based upon the popular PersonaChat dataset, but conditioned on both personas and (Big-5) personality traits. Evaluating models fine-tuned on this dataset, we show that the personality trait labels can be used for trait-based personalization of generative dialogue models. We also perform a head-to-head comparison between PersonalityChat and PersonaChat, and show that training on the distilled dataset results in more fluent and coherent dialog agents in the small-model regime. ",
        "title": "PersonalityChat: Conversation Distillation for Personalized Dialog  Modeling with Facts and Traits",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07364",
        "abstract_url": "http://arxiv.org/abs/2401.07364",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Liu"
            },
            {
                "last_name": "Osher",
                "first_name": "Stanley J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) [1] represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated in [1]. In this paper, we explore the second question by investigating the generalization capabilities of ICON for conservation laws, a family of PDEs with temporal evolution. We show the positive answer to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. We also show how to broaden the range of problems that ICON can address, by transforming functions and equations to ICON's capability scope. We believe that the progress in this paper is a significant step towards the goal of training a foundation model for PDE-related tasks under the in-context operator learning framework. ",
        "title": "PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar  Nonlinear Conservation Laws",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07367",
        "abstract_url": "http://arxiv.org/abs/2401.07367",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xuesong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Human annotation of training samples is expensive, laborious, and sometimes challenging, especially for Natural Language Processing (NLP) tasks. To reduce the labeling cost and enhance the sample efficiency, Active Learning (AL) technique can be used to label as few samples as possible to reach a reasonable or similar results. To reduce even more costs and with the significant advances of Large Language Models (LLMs), LLMs can be a good candidate to annotate samples. This work investigates the accuracy and cost of using LLMs (GPT-3.5 and GPT-4) to label samples on 3 different datasets. A consistency-based strategy is proposed to select samples that are potentially incorrectly labeled so that human annotations can be used for those samples in AL settings, and we call it mixed annotation strategy. Then we test performance of AL under two different settings: (1) using human annotations only; (2) using the proposed mixed annotation strategy. The accuracy of AL models under 3 AL query strategies are reported on 3 text classification datasets, i.e., AG's News, TREC-6, and Rotten Tomatoes. On AG's News and Rotten Tomatoes, the models trained with the mixed annotation strategy achieves similar or better results compared to that with human annotations. The method reveals great potentials of LLMs as annotators in terms of accuracy and cost efficiency in active learning settings. ",
        "title": "Active Learning for NLP with Large Language Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07368",
        "abstract_url": "http://arxiv.org/abs/2401.07368",
        "authors": [
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Bay",
                "first_name": "Sajjad"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The integration of Internet of Things (IoT) devices in healthcare applications has revolutionized patient care, monitoring, and data management. The Global IoT in Healthcare Market value is $252.2 Billion in 2023. However, the rapid involvement of these devices brings information security concerns that pose critical threats to patient privacy and the integrity of healthcare data. This paper introduces a novel machine learning (ML) based architecture explicitly designed to address and mitigate security vulnerabilities in IoT devices within healthcare applications. By leveraging advanced convolution ML architecture, the proposed architecture aims to proactively monitor and detect potential threats, ensuring the confidentiality and integrity of sensitive healthcare information while minimizing the cost and increasing the portability specialized for healthcare and emergency environments. The experimental results underscore the accuracy of up to 93.6% for predicting various attacks based on the results demonstrate a zero-day detection accuracy simulated using the CICIoT2023 dataset and reduces the cost by a factor of x10. The significance of our approach is in fortifying the security posture of IoT devices and maintaining a robust implementation of trustful healthcare systems. ",
        "title": "A Novel Zero-Trust Machine Learning Green Architecture for Healthcare  IoT Cybersecurity: Review, Analysis, and Implementation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07369",
        "abstract_url": "http://arxiv.org/abs/2401.07369",
        "authors": [
            {
                "last_name": "Yi",
                "first_name": "Zeji"
            },
            {
                "last_name": "Pan",
                "first_name": "Chaoyi"
            },
            {
                "last_name": "He",
                "first_name": "Guanqi"
            },
            {
                "last_name": "Qu",
                "first_name": "Guannan"
            },
            {
                "last_name": "Shi",
                "first_name": "Guanya"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  Sampling-based Model Predictive Control (MPC) has been a practical and effective approach in many domains, notably model-based reinforcement learning, thanks to its flexibility and parallelizability. Despite its appealing empirical performance, the theoretical understanding, particularly in terms of convergence analysis and hyperparameter tuning, remains absent. In this paper, we characterize the convergence property of a widely used sampling-based MPC method, Model Predictive Path Integral Control (MPPI). We show that MPPI enjoys at least linear convergence rates when the optimization is quadratic, which covers time-varying LQR systems. We then extend to more general nonlinear systems. Our theoretical analysis directly leads to a novel sampling-based MPC algorithm, CoVariance-Optimal MPC (CoVo-MPC) that optimally schedules the sampling covariance to optimize the convergence rate. Empirically, CoVo-MPC significantly outperforms standard MPPI by 43-54% in both simulations and real-world quadrotor agile control tasks. Videos and Appendices are available at \\url{https://lecar-lab.github.io/CoVO-MPC/}. ",
        "title": "CoVO-MPC: Theoretical Analysis of Sampling-based MPC and Optimal  Covariance Design",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07370",
        "abstract_url": "http://arxiv.org/abs/2401.07370",
        "authors": [
            {
                "last_name": "Seib",
                "first_name": "Viktor"
            },
            {
                "last_name": "Roosen",
                "first_name": "Malte"
            },
            {
                "last_name": "Germann",
                "first_name": "Ida"
            },
            {
                "last_name": "Wirtz",
                "first_name": "Stefan"
            },
            {
                "last_name": "Paulus",
                "first_name": "Dietrich"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Creating annotated datasets demands a substantial amount of manual effort. In this proof-of-concept work, we address this issue by proposing a novel image generation pipeline. The pipeline consists of three distinct generative adversarial networks (previously published), combined in a novel way to augment a dataset for pedestrian detection. Despite the fact that the generated images are not always visually pleasant to the human eye, our detection benchmark reveals that the results substantially surpass the baseline. The presented proof-of-concept work was done in 2020 and is now published as a technical report after a three years retention period. ",
        "title": "Generation of Synthetic Images for Pedestrian Detection Using a Sequence  of GANs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07371",
        "abstract_url": "http://arxiv.org/abs/2401.07371",
        "authors": [
            {
                "last_name": "Kays",
                "first_name": "H M Imran"
            },
            {
                "last_name": "Momin",
                "first_name": "Khondhaker Al"
            },
            {
                "last_name": "Muraleetharan",
                "first_name": "K. K. \"Muralee\""
            },
            {
                "last_name": "Sadri",
                "first_name": "Arif Mohaimin"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  Roadway reconfiguration is a crucial aspect of transportation planning, aiming to enhance traffic flow, reduce congestion, and improve overall road network performance with existing infrastructure and resources. This paper presents a novel roadway reconfiguration technique by integrating optimization based Brute Force search approach and decision support framework to rank various roadway configurations for better performance. The proposed framework incorporates a multi-criteria decision analysis (MCDA) approach, combining input from generated scenarios during the optimization process. By utilizing data from optimization, the model identifies total betweenness centrality (TBC), system travel time (STT), and total link traffic flow (TLTF) as the most influential decision variables. The developed framework leverages graph theory to model the transportation network topology and apply network science metrics as well as stochastic user equilibrium traffic assignment to assess the impact of each roadway configuration on the overall network performance. To rank the roadway configurations, the framework employs machine learning algorithms, such as ridge regression, to determine the optimal weights for each criterion (i.e., TBC, STT, TLTF). Moreover, the network-based analysis ensures that the selected configurations not only optimize individual roadway segments but also enhance system-level efficiency, which is particularly helpful as the increasing frequency and intensity of natural disasters and other disruptive events underscore the critical need for resilient transportation networks. By integrating multi-criteria decision analysis, machine learning, and network science metrics, the proposed framework would enable transportation planners to make informed and data-driven decisions, leading to more sustainable, efficient, and resilient roadway configurations. ",
        "title": "A Data-driven Resilience Framework of Directionality Configuration based  on Topological Credentials in Road Networks",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07378",
        "abstract_url": "http://arxiv.org/abs/2401.07378",
        "authors": [
            {
                "last_name": "Meng",
                "first_name": "Guangyu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ruyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Liu"
            },
            {
                "last_name": "Liang",
                "first_name": "Peixian"
            },
            {
                "last_name": "Liu",
                "first_name": "Fang"
            },
            {
                "last_name": "Chen",
                "first_name": "Danny"
            },
            {
                "last_name": "Niemier",
                "first_name": "Michael"
            },
            {
                "last_name": "Hu",
                "first_name": "X. Sharon"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods. ",
        "title": "Efficient approximation of Earth Mover's Distance Based on Nearest  Neighbor Search",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07379",
        "abstract_url": "http://arxiv.org/abs/2401.07379",
        "authors": [
            {
                "last_name": "Mircea",
                "first_name": "Maria"
            },
            {
                "last_name": "Garlaschelli",
                "first_name": "Diego"
            },
            {
                "last_name": "Semrau",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  One of the main goals of developmental biology is to reveal the gene regulatory networks (GRNs) underlying the robust differentiation of multipotent progenitors into precisely specified cell types. Most existing methods to infer GRNs from experimental data have limited predictive power as the inferred GRNs merely reflect gene expression similarity or correlation. Here, we demonstrate, how physics-informed neural networks (PINNs) can be used to infer the parameters of predictive, dynamical GRNs that provide mechanistic understanding of biological processes. Specifically we study GRNs that exhibit bifurcation behavior and can therefore model cell differentiation. We show that PINNs outperform regular feed-forward neural networks on the parameter inference task and analyze two relevant experimental scenarios: 1. a system with cell communication for which gene expression trajectories are available and 2. snapshot measurements of a cell population in which cell communication is absent. Our analysis will inform the design of future experiments to be analyzed with PINNs and provides a starting point to explore this powerful class of neural network models further. ",
        "title": "Inference of dynamical gene regulatory networks from single-cell data  with physics informed neural networks",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07381",
        "abstract_url": "http://arxiv.org/abs/2401.07381",
        "authors": [
            {
                "last_name": "Borriello",
                "first_name": "Enrico"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In network theory, a triad census is a method designed to categorize and enumerate the various types of subgraphs with three nodes and their connecting edges within a network. Triads serve as fundamental building blocks for comprehending the structure and dynamics of networks, and the triad census offers a systematic approach to their classification. Typically, triad counts are obtained numerically, but lesser-known methods have been developed to precisely evaluate them without the need for sampling. In our study, we build upon Moody's matrix approach, presenting general diagrammatic rules that systematically and intuitively generate closed formulas for the occurrence numbers of triads in a network. ",
        "title": "Diagrammatic Rules for Triad Census",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07382",
        "abstract_url": "http://arxiv.org/abs/2401.07382",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Meng"
            },
            {
                "last_name": "Shu",
                "first_name": "Lei"
            },
            {
                "last_name": "Yu",
                "first_name": "Lei"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yun"
            },
            {
                "last_name": "Wichers",
                "first_name": "Nevan"
            },
            {
                "last_name": "Liu",
                "first_name": "Yinxiao"
            },
            {
                "last_name": "Meng",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only one reward for the entire generation. This sparsity of rewards can lead to inefficient and unstable learning. In this paper, we introduce a novel framework leveraging the critique ability of LLMs to produce dense rewards throughout the learning process. Our approach incorporates a critic language model alongside the policy model. This critic is prompted with the task description, question, policy model's output, and environment's reward signal as input, and provides token or span-level dense rewards that reflect the quality of each segment of the output. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial dense rewards in training yields consistent performance gains over the PPO baseline with holistic rewards. Furthermore, in a setting where the same model serves as both policy and critic, we demonstrate that \"self-critique\" rewards also boost learning efficiency. ",
        "title": "DRLC: Reinforcement Learning with Dense Rewards from LLM Critic",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07383",
        "abstract_url": "http://arxiv.org/abs/2401.07383",
        "authors": [
            {
                "last_name": "Faulstich",
                "first_name": "Fabian M."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This article presents an in-depth educational overview of the latest mathematical developments in coupled cluster (CC) theory, beginning with Schneider's seminal work from 2009 that introduced the first local analysis of CC theory. We offer a tutorial review of second quantization and the CC ansatz, laying the groundwork for understanding the mathematical basis of the theory. This is followed by a detailed exploration of the most recent mathematical advancements in CC theory.Our review starts with an in-depth look at the local analysis pioneered by Schneider which has since been applied to analyze various CC methods. We then move on to discuss the graph-based framework for CC methods developed by Csirik and Laestadius. This framework provides a comprehensive platform for comparing different CC methods, including multireference approaches. Next, we delve into the latest numerical analysis results analyzing the single reference CC method developed by Hassan, Maday, and Wang. This very general approach is based on the invertibility of the CC function's Fr\\'echet derivative. We conclude the article with a discussion on the recent incorporation of algebraic geometry into CC theory, highlighting how this novel and fundamentally different mathematical perspective has furthered our understanding and provides exciting pathways to new computational approaches. ",
        "title": "Recent mathematical advances in coupled cluster theory",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07386",
        "abstract_url": "http://arxiv.org/abs/2401.07386",
        "authors": [
            {
                "last_name": "Queiroz",
                "first_name": "Rubens Lacerda"
            },
            {
                "last_name": "Lima",
                "first_name": "Cabral"
            },
            {
                "last_name": "Sampaio",
                "first_name": "Fabio Ferrentini"
            },
            {
                "last_name": "Lima",
                "first_name": "Priscila Machado Vieira"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  This paper evaluates AIcon2abs (Queiroz et al., 2021), a recently proposed method that enables awareness among the general public on machine learning. Such is possible due to the use of WiSARD, an easily understandable machine learning mechanism, thus requiring little effort and no technical background from the target users. WiSARD is adherent to digital computing; training consists of writing to RAM-type memories, and classification consists of reading from these memories. The model enables easy visualization and understanding of training and classification tasks' internal realization through ludic activities. Furthermore, the WiSARD model does not require an Internet connection for training and classification, and it can learn from a few or one example. This feature makes it easier to observe the machine, increasing its accuracy on a particular task with each new example used. WiSARD can also create \"mental images\" of what it has learned so far, evidencing key features pertaining to a given class. The assessment of the AIcon2abs method's effectiveness was conducted through the evaluation of a remote course with a workload of approximately 6 hours. It was completed by thirty-four Brazilian subjects: 5 children between 8 and 11 years old; 5 adolescents between 12 and 17 years old; and 24 adults between 21 and 72 years old. Data analysis adopted a hybrid approach. AIcon2abs was well-rated by almost 100% of the research subjects, and the data collected revealed quite satisfactory results concerning the intended outcomes. This research has been approved by the CEP/HUCFF/FM/UFRJ Human Research Ethics Committee. ",
        "title": "How do machines learn? Evaluating the AIcon2abs method",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07387",
        "abstract_url": "http://arxiv.org/abs/2401.07387",
        "authors": [
            {
                "last_name": "Manneschi",
                "first_name": "Luca"
            },
            {
                "last_name": "Vidamour",
                "first_name": "Ian T."
            },
            {
                "last_name": "Stenning",
                "first_name": "Kilian D."
            },
            {
                "last_name": "Gartside",
                "first_name": "Jack C."
            },
            {
                "last_name": "Swindells",
                "first_name": "Charles"
            },
            {
                "last_name": "Venkat",
                "first_name": "Guru"
            },
            {
                "last_name": "Griffin",
                "first_name": "David"
            },
            {
                "last_name": "Stepney",
                "first_name": "Susan"
            },
            {
                "last_name": "Branford",
                "first_name": "Will R."
            },
            {
                "last_name": "Hayward",
                "first_name": "Thomas"
            },
            {
                "last_name": "Ellis",
                "first_name": "Matt O"
            },
            {
                "last_name": "Vasilaki",
                "first_name": "Eleni"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "ET",
            "NE"
        ],
        "abstract": "  Physically implemented neural networks hold the potential to achieve the performance of deep learning models by exploiting the innate physical properties of devices as computational tools. This exploration of physical processes for computation requires to also consider their intrinsic dynamics, which can serve as valuable resources to process information. However, existing computational methods are unable to extend the success of deep learning techniques to parameters influencing device dynamics, which often lack a precise mathematical description. In this work, we formulate a universal framework to optimise interactions with dynamic physical systems in a fully data-driven fashion. The framework adopts neural stochastic differential equations as differentiable digital twins, effectively capturing both deterministic and stochastic behaviours of devices. Employing differentiation through the trained models provides the essential mathematical estimates for optimizing a physical neural network, harnessing the intrinsic temporal computation abilities of its physical nodes. To accurately model real devices' behaviours, we formulated neural-SDE variants that can operate under a variety of experimental settings. Our work demonstrates the framework's applicability through simulations and physical implementations of interacting dynamic devices, while highlighting the importance of accurately capturing system stochasticity for the successful deployment of a physically defined neural network. ",
        "title": "Optimising network interactions through device agnostic models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07389",
        "abstract_url": "http://arxiv.org/abs/2401.07389",
        "authors": [
            {
                "last_name": "Yin",
                "first_name": "Hui"
            },
            {
                "last_name": "Aryani",
                "first_name": "Amir"
            },
            {
                "last_name": "Petrie",
                "first_name": "Stephen"
            },
            {
                "last_name": "Nambissan",
                "first_name": "Aishwarya"
            },
            {
                "last_name": "Astudillo",
                "first_name": "Aland"
            },
            {
                "last_name": "Cao",
                "first_name": "Shengyuan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Clustering algorithms aim to organize data into groups or clusters based on the inherent patterns and similarities within the data. They play an important role in today's life, such as in marketing and e-commerce, healthcare, data organization and analysis, and social media. Numerous clustering algorithms exist, with ongoing developments introducing new ones. Each algorithm possesses its own set of strengths and weaknesses, and as of now, there is no universally applicable algorithm for all tasks. In this work, we analyzed existing clustering algorithms and classify mainstream algorithms across five different dimensions: underlying principles and characteristics, data point assignment to clusters, dataset capacity, predefined cluster numbers and application area. This classification facilitates researchers in understanding clustering algorithms from various perspectives and helps them identify algorithms suitable for solving specific tasks. Finally, we discussed the current trends and potential future directions in clustering algorithms. We also identified and discussed open challenges and unresolved issues in the field. ",
        "title": "A Rapid Review of Clustering Algorithms",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07390",
        "abstract_url": "http://arxiv.org/abs/2401.07390",
        "authors": [
            {
                "last_name": "Wendt",
                "first_name": "Veronica"
            },
            {
                "last_name": "Yu",
                "first_name": "Byunggu"
            },
            {
                "last_name": "Kelly",
                "first_name": "Caleb"
            },
            {
                "last_name": "Kim",
                "first_name": "Junwhan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Self-attention transformers have demonstrated accuracy for image classification with smaller data sets. However, a limitation is that tests to-date are based upon single class image detection with known representation of image populations. For instances where the input image classes may be greater than one and test sets that lack full information on representation of image populations, accuracy calculations must adapt. The Receiver Operating Characteristic (ROC) accuracy thresh-old can address the instances of multi-class input images. However, this approach is unsuitable in instances where image population representation is unknown. We consider calculating accuracy using the knee method to determine threshold values on an ad-hoc basis. Results of ROC curve and knee thresholds for a multi-class data set, created from CIFAR-10 images, are discussed for multi-class image detection. ",
        "title": "Knee or ROC",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07392",
        "abstract_url": "http://arxiv.org/abs/2401.07392",
        "authors": [
            {
                "last_name": "Scilipoti",
                "first_name": "Marco"
            },
            {
                "last_name": "Fuster",
                "first_name": "Marina"
            },
            {
                "last_name": "Ramele",
                "first_name": "Rodrigo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning networks have become the de-facto standard in Computer Vision for industry and research. However, recent developments in their cousin, Natural Language Processing (NLP), have shown that there are areas where parameter-less models with strong inductive biases can serve as computationally cheaper and simpler alternatives. We propose such a model for binary image classification: a nearest neighbor classifier combined with a general purpose compressor like Gzip. We test and compare it against popular deep learning networks like Resnet, EfficientNet and Mobilenet and show that it achieves better accuracy and utilizes significantly less space, more than two order of magnitude, within a few-shot setting. As a result, we believe that this underlines the untapped potential of models with stronger inductive biases in few-shot scenarios. ",
        "title": "A Strong Inductive Bias: Gzip for binary image classification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07393",
        "abstract_url": "http://arxiv.org/abs/2401.07393",
        "authors": [
            {
                "last_name": "Aviles",
                "first_name": "Robert S."
            },
            {
                "last_name": "Beerel",
                "first_name": "Peter A."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Adiabatic Quantum-Flux-Parametron (AQFP) logic is a promising emerging device technology that promises six orders of magnitude lower power than CMOS. However, AQFP is challenged by operation at only ultra-low temperatures, has high latency and area, and requires a complex clocking scheme. In particular, every logic gate, buffer, and splitter must be clocked and each pair of connected clocked gates requires overlapping alternating current (AC) clock signals. In particular, clocked buffers need to be used to balance re-convergent logic paths, a problem that is exacerbated by every multi-node fanout needing a tree of clocked splitters. To reduce circuit area many works have proposed buffer and splitter insertion optimization algorithms and recent works have demonstrated a phase-skipping clocking scheme that reduces latency and area. This paper proposes the first algorithm to optimize buffer and splitter insertion for circuits that adopt phase-skipping and demonstrate the resulting performance improvements for a suite of AQFP benchmark circuits. ",
        "title": "A Novel Optimization Algorithm for Buffer and Splitter Minimization in  Phase-Skipping Adiabatic Quantum-Flux-Parametron Circuits",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07395",
        "abstract_url": "http://arxiv.org/abs/2401.07395",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Wei"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Ngoc Dang"
            },
            {
                "last_name": "Du",
                "first_name": "Lan"
            },
            {
                "last_name": "Buntine",
                "first_name": "Wray"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Within the scope of natural language processing, the domain of multi-label text classification is uniquely challenging due to its expansive and uneven label distribution. The complexity deepens due to the demand for an extensive set of annotated data for training an advanced deep learning model, especially in specialized fields where the labeling task can be labor-intensive and often requires domain-specific knowledge. Addressing these challenges, our study introduces a novel deep active learning strategy, capitalizing on the Beta family of proper scoring rules within the Expected Loss Reduction framework. It computes the expected increase in scores using the Beta Scoring Rules, which are then transformed into sample vector representations. These vector representations guide the diverse selection of informative samples, directly linking this process to the model's expected proper score. Comprehensive evaluations across both synthetic and real datasets reveal our method's capability to often outperform established acquisition techniques in multi-label text classification, presenting encouraging outcomes across various architectural and dataset scenarios. ",
        "title": "Harnessing the Power of Beta Scoring in Deep Active Learning for  Multi-Label Text Classification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07398",
        "abstract_url": "http://arxiv.org/abs/2401.07398",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yiqun"
            },
            {
                "last_name": "Huang",
                "first_name": "Hui"
            },
            {
                "last_name": "State",
                "first_name": "Radu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Driven by abundant satellite imagery, machine learning-based approaches have recently been promoted to generate high-resolution crop cultivation maps to support many agricultural applications. One of the major challenges faced by these approaches is the limited availability of ground truth labels. In the absence of ground truth, existing work usually adopts the \"direct transfer strategy\" that trains a classifier using historical labels collected from other regions and then applies the trained model to the target region. Unfortunately, the spectral features of crops exhibit inter-region and inter-annual variability due to changes in soil composition, climate conditions, and crop progress, the resultant models perform poorly on new and unseen regions or years. This paper presents the Crop Generative Adversarial Network (CropGAN) to address the above cross-domain issue. Our approach does not need labels from the target domain. Instead, it learns a mapping function to transform the spectral features of the target domain to the source domain (with labels) while preserving their local structure. The classifier trained by the source domain data can be directly applied to the transformed data to produce high-accuracy early crop maps of the target domain. Comprehensive experiments across various regions and years demonstrate the benefits and effectiveness of the proposed approach. Compared with the widely adopted direct transfer strategy, the F1 score after applying the proposed CropGAN is improved by 13.13% - 50.98% ",
        "title": "Cross Domain Early Crop Mapping using CropGAN and CNN Classifier",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07402",
        "abstract_url": "http://arxiv.org/abs/2401.07402",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Kexuan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Gu",
                "first_name": "Shuhang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Implicit Neural Representation (INR) as a mighty representation paradigm has achieved success in various computer vision tasks recently. Due to the low-frequency bias issue of vanilla multi-layer perceptron (MLP), existing methods have investigated advanced techniques, such as positional encoding and periodic activation function, to improve the accuracy of INR. In this paper, we connect the network training bias with the reparameterization technique and theoretically prove that weight reparameterization could provide us a chance to alleviate the spectral bias of MLP. Based on our theoretical analysis, we propose a Fourier reparameterization method which learns coefficient matrix of fixed Fourier bases to compose the weights of MLP. We evaluate the proposed Fourier reparameterization method on different INR tasks with various MLP architectures, including vanilla MLP, MLP with positional encoding and MLP with advanced activation function, etc. The superiority approximation results on different MLP architectures clearly validate the advantage of our proposed method. Armed with our Fourier reparameterization method, better INR with more textures and less artifacts can be learned from the training data. ",
        "title": "Improved Implicity Neural Representation with Fourier Bases  Reparameterized Training",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07404",
        "abstract_url": "http://arxiv.org/abs/2401.07404",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Rahul K."
            },
            {
                "last_name": "Buason",
                "first_name": "Paprapee"
            },
            {
                "last_name": "Molzahn",
                "first_name": "Daniel K."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a framework for fairly curtailing photovoltaic (PV) plants in response to the over-voltage problem in PV-rich distribution networks. The framework imposes PV generation limits to avoid overvoltages. These limits are computed a day ahead of real-time operations by solving an offline stochastic optimization problem using forecasted scenarios for PV generation and load demand. The framework minimizes the overall curtailment while considering fairness by reducing disparities in curtailments among different PV owners. We model the distribution grid constraints using a conservative linear approximation (CLA) of the AC power flow equations which is computed using a set of sampled power injections from the day-ahead predicted scenarios. The proposed framework is numerically validated on a CIGRE benchmark network interfaced with a large number of PV plants. We compare the performance of the proposed framework versus an alternative formulation that does not incorporate fairness considerations. To this end, we assess tradeoffs between fairness, as quantified with the Jain Fairness Index (JFI), and the total curtailed energy. ",
        "title": "Fairness-aware Photovoltaic Generation Limits for Voltage Regulation in  Power Distribution Networks using Conservative Linear Approximations",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07408",
        "abstract_url": "http://arxiv.org/abs/2401.07408",
        "authors": [
            {
                "last_name": "Ock",
                "first_name": "Janghoon"
            },
            {
                "last_name": "Magar",
                "first_name": "Rishikesh"
            },
            {
                "last_name": "Antony",
                "first_name": "Akshay"
            },
            {
                "last_name": "Farimani",
                "first_name": "Amir Barati"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Adsorption energy, a reactivity descriptor, should be accurately assessed for efficient catalyst screening. This evaluation requires determining the lowest energy across various adsorption configurations on the catalytic surface. While graph neural networks (GNNs) have gained popularity as a machine learning approach for computing the energy of catalyst systems, they rely heavily on atomic spatial coordinates and often lack clarity in their interpretations. Recent advancements in language models have broadened their applicability to predicting catalytic properties, allowing us to bypass the complexities of graph representation. These models are adept at handling textual data, making it possible to incorporate observable features in a human-readable format. However, language models encounter challenges in accurately predicting the energy of adsorption configurations, typically showing a high mean absolute error (MAE) of about 0.71 eV. Our study addresses this limitation by introducing a self-supervised multi-modal learning approach, termed graph-assisted pretraining. This method significantly reduces the MAE to 0.35 eV through a combination of data augmentation, achieving comparable accuracy with DimeNet++ while using 0.4% of its training data size. Furthermore, the Transformer encoder at the core of the language model can provide insights into the feature focus through its attention scores. This analysis shows that our multimodal training effectively redirects the model's attention toward relevant adsorption configurations from adsorbate-related features, enhancing prediction accuracy and interpretability. ",
        "title": "Multimodal Language and Graph Learning of Adsorption Configuration in  Catalysis",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07410",
        "abstract_url": "http://arxiv.org/abs/2401.07410",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Meng"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "CR"
        ],
        "abstract": "  DNS is an essential Internet infrastructure to support network applications and services, but is also a significant tool exploited by various cyberattacks. Existing DNS security analysis techniques mostly focus on one specific task associated with one single entity (e.g., domain) via conventional feature engineering. They rely heavily on the labor-intensive feature selection and largely ignore the intrinsic correlations among the heterogeneous DNS entities (e.g., domain and IP). In this paper, I explore the potential of heterogeneous graph embedding to automatically learn the behavior features of multiple DNS entities, and to simultaneously support more than one security tasks. Considering the joint optimization of malicious domain detection and IP reputation evaluation as an example, I propose a novel joint DNS embedding (JDE) model to formulate the DNS query behavior via a similarity-enhanced graph with heterogeneous entities. The random walk technique is applied to the heterogeneous graph to comprehensively explore the hidden homogeneous and heterogeneous high-order proximities among domains and IPs. Extensive experiments on real DNS traffic demonstrate that the joint optimization of multiple tasks with the latent high-order proximities can lead to better security analysis performance for all the tasks than respectively optimizing each single task with the observable low-order proximity. ",
        "title": "Multi-Task DNS Security Analysis via High-Order Heterogeneous Graph  Embedding",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07411",
        "abstract_url": "http://arxiv.org/abs/2401.07411",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Li",
                "first_name": "Chunxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yongxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Baoxian"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Short video applications have attracted billions of users on the Internet and can satisfy diverse users' fragmented spare time with content-rich and duration-short videos. To achieve fast playback at user side, existing short video systems typically enforce burst transmission of initial segment of each video when being requested for improved quality of user experiences. However, such a way of burst transmissions can cause unexpected large startup delays at user side. This is because users may frequently switch videos when sequentially watching a list of short videos recommended by the server side, which can cause excessive burst transmissions of initial segments of different short videos and thus quickly deplete the network transmission capacity. In this paper, we adopt token bucket to characterize the video transmission path between video server and each user, and accordingly study how to effectively reduce the startup delay of short videos by effectively arranging the viewing order of a video list at the server side. We formulate the optimal video ordering problem for minimizing the maximum video startup delay as a combinatorial optimization problem and prove its NP-hardness. We accordingly propose a Partially Shared Actor Critic reinforcement learning algorithm (PSAC) to learn optimized video ordering strategy. Numerical results based on a real dataset provided by a large-scale short video service provider demonstrate that the proposed PSAC algorithm can significantly reduce the video startup delay compared to baseline algorithms. ",
        "title": "Startup Delay Aware Short Video Ordering: Problem, Model, and A  Reinforcement Learning based Algorithm",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07414",
        "abstract_url": "http://arxiv.org/abs/2401.07414",
        "authors": [
            {
                "last_name": "Meque",
                "first_name": "Abdul Gafar Manuel"
            },
            {
                "last_name": "Angel",
                "first_name": "Jason"
            },
            {
                "last_name": "Sidorov",
                "first_name": "Grigori"
            },
            {
                "last_name": "Gelbukh",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent years, language models and deep learning techniques have revolutionized natural language processing tasks, including emotion detection. However, the specific emotion of guilt has received limited attention in this field. In this research, we explore the applicability of three transformer-based language models for detecting guilt in text and compare their performance for general emotion detection and guilt detection. Our proposed model outformed BERT and RoBERTa models by two and one points respectively. Additionally, we analyze the challenges in developing accurate guilt-detection models and evaluate our model's effectiveness in detecting related emotions like \"shame\" through qualitative analysis of results. ",
        "title": "Leveraging the power of transformers for guilt detection in text",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07417",
        "abstract_url": "http://arxiv.org/abs/2401.07417",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Lipeng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Decentralized applications (DApps), which are innovative blockchain-powered software systems designed to serve as the fundamental building blocks for the next generation of Internet services, have witnessed exponential growth in recent years. This paper thoroughly compares and analyzes two blockchain-based decentralized storage networks (DSNs), which are crucial foundations for DApp and blockchain ecosystems. The study examines their respective mechanisms for data persistence, strategies for enforcing data retention, and token economics. In addition to delving into technical details, the suitability of each storage solution for decentralized application development is assessed, taking into consideration network performance, storage costs, and existing use cases. By evaluating these factors, the paper aims to provide insights into the effectiveness of these technologies in supporting the desirable properties of truly decentralized blockchain applications. In conclusion, the findings of this research are discussed and synthesized, offering valuable perspectives on the capabilities of these technologies. It sheds light on their potential to facilitate the development of DApps and provides an understanding of the ongoing trends in blockchain development. ",
        "title": "A Comparative Examination of Network and Contract-Based Blockchain  Storage Solutions for Decentralized Applications",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07425",
        "abstract_url": "http://arxiv.org/abs/2401.07425",
        "authors": [
            {
                "last_name": "Carnerero",
                "first_name": "A. Daniel"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Taichi"
            },
            {
                "last_name": "Li",
                "first_name": "Mengmou"
            },
            {
                "last_name": "Hatanaka",
                "first_name": "Takeshi"
            },
            {
                "last_name": "Wasa",
                "first_name": "Yasuaki"
            },
            {
                "last_name": "Hirata",
                "first_name": "Kenji"
            },
            {
                "last_name": "Ushifusa",
                "first_name": "Yoshiaki"
            },
            {
                "last_name": "Ida",
                "first_name": "Takanori"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we explore the concept of net-Zero Energy Houses (ZEH) - houses designed to have an annual net energy consumption around zero. To achieve this, we present a constrained optimization problem whose objective is finding the optimal sizing of photovoltaic panels and a battery system to be integrated at home. The original optimization problem is nonlinear with nonconvex constraints. Nevertheless, by applying a series of transformations, it is possible to find an equivalent Linear Programming (LP) problem which is computationally tractable. The attainment of ZEH can be tackled by introducing a single constraint in the optimization problem. Additionally, we propose a sharing economy approach to the investment problem, a strategy that carries the potential to reduce the cost of the investment and facilitate the attainment of ZEH in a more efficient manner. Finally, we apply the proposed frameworks to a neighborhood in Japan as a case study, demonstrating the potential for long-term ZEH attainment. The results show the importance of choosing an appropriate incentive to motivate residents towards achieving ZEH. ",
        "title": "ZEH-oriented Linear Programming for the Sizing Problem of Photovoltaic  Panels and Batteries",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07426",
        "abstract_url": "http://arxiv.org/abs/2401.07426",
        "authors": [
            {
                "last_name": "Lei",
                "first_name": "Chao"
            },
            {
                "last_name": "Lipovetzky",
                "first_name": "Nir"
            },
            {
                "last_name": "Ehinger",
                "first_name": "Krista A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Abstraction and Reasoning Corpus (ARC) is a general artificial intelligence benchmark that poses difficulties for pure machine learning methods due to its requirement for fluid intelligence with a focus on reasoning and abstraction. In this work, we introduce an ARC solver, Generalized Planning for Abstract Reasoning (GPAR). It casts an ARC problem as a generalized planning (GP) problem, where a solution is formalized as a planning program with pointers. We express each ARC problem using the standard Planning Domain Definition Language (PDDL) coupled with external functions representing object-centric abstractions. We show how to scale up GP solvers via domain knowledge specific to ARC in the form of restrictions over the actions model, predicates, arguments and valid structure of planning programs. Our experiments demonstrate that GPAR outperforms the state-of-the-art solvers on the object-centric tasks of the ARC, showing the effectiveness of GP and the expressiveness of PDDL to model ARC problems. The challenges provided by the ARC benchmark motivate research to advance existing GP solvers and understand new relations with other planning computational models. Code is available at github.com/you68681/GPAR. ",
        "title": "Generalized Planning for the Abstraction and Reasoning Corpus",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07427",
        "abstract_url": "http://arxiv.org/abs/2401.07427",
        "authors": [
            {
                "last_name": "Emre",
                "first_name": "Sariyildiz"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper extends the result of my previous papers on the analysis and synthesis of disturbance observer based robust control systems in state space. ",
        "title": "AMC'24 \"Analysis and Synthesis of the Disturbance Observer-based Robust  Force Control Systems in State Space\"",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07429",
        "abstract_url": "http://arxiv.org/abs/2401.07429",
        "authors": [
            {
                "last_name": "Govindasamy",
                "first_name": "Hariprasadh"
            },
            {
                "last_name": "Esfandiari",
                "first_name": "Babak"
            },
            {
                "last_name": "Garcia",
                "first_name": "Paulo"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  We present a hardware-accelerated SAT solver targeting processor/Field Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint Propagation (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art solutions, our solver eliminates costly clause look-up operations by assigning clauses directly to clause processors on the FPGA and dividing large formulas into smaller partitions manageable by FPGA. Partitions are hot-swapped during runtime as required and the supported formula size is limited only by available external memory, not on-chip FPGA memory. We evaluate our solver on a Xilinx Zynq platform with results showing quicker execution time across various formula sizes, subject to formula partitioning strategy. Compared to prior state-of-the-art, we achieve 1.7x and 1.1x speed up on BCP for 2 representative benchmarks and up to 6x total speedup over software-only implementation. ",
        "title": "Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on  FPGAs",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07430",
        "abstract_url": "http://arxiv.org/abs/2401.07430",
        "authors": [
            {
                "last_name": "Emre",
                "first_name": "Sariyildiz"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper presents a new stiffness modulation mechanism that enables infinite-range stiffness modulation in a fast manner. The proposed stiffness modulation mechanism can help improve many robot environment interaction applications such as human-robot collaboration and robotic rehabilitation. ",
        "title": "AMC'24 \"A Novel Stiffness Modulation Mechanism for Energy Efficient  Variable Stiffness Actuators\"",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07433",
        "abstract_url": "http://arxiv.org/abs/2401.07433",
        "authors": [
            {
                "last_name": "Farsiabi",
                "first_name": "Ali"
            },
            {
                "last_name": "Ebrahimzad",
                "first_name": "Hamid"
            },
            {
                "last_name": "Ardakani",
                "first_name": "Masoud"
            },
            {
                "last_name": "Li",
                "first_name": "Chuandong"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Non-binary polar codes (NBPCs) decoded by successive cancellation (SC) algorithm have remarkable bit-error-rate performance compared to the binary polar codes (BPCs). Due to the serial nature, SC decoding suffers from large latency. The latency issue in BPCs has been the topic of extensive research and it has been notably resolved by the introduction of fast SC-based decoders. However, the vast majority of research on NBPCs is devoted to issues concerning design and efficient implementation. In this paper, we propose fast SC decoding for NBPCs constructed based on 2 x 2 kernels. In particular, we identify various non-binary special nodes in the SC decoding tree of NBPCs and propose their fast decoding. This way, we avoid traversing the full decoding tree and significantly reduce the decoding delay compared to symbol-by-symbol SC decoding. We also propose a simplified NBPC structure that facilitates the procedure of non-binary fast SC decoding. Using our proposed fast non-binary decoder, we observed an improvement of up to 95% in latency concerning the original SC decoding. This is while our proposed fast SC decoder for NBPCs incurs no error-rate loss. ",
        "title": "Fast Successive-Cancellation Decoding of 2 x 2 Kernel Non-Binary Polar  Codes: Identification, Decoding and Simplification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07435",
        "abstract_url": "http://arxiv.org/abs/2401.07435",
        "authors": [
            {
                "last_name": "Knill",
                "first_name": "Oliver"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  If f maps a discrete d-manifold G onto a (k+1)-partite complex P then H(G,f,P),the set of simplices x in G such that f(x) contains at least one facet in P defines a (d-k)-manifold. ",
        "title": "Manifolds from Partitions",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07437",
        "abstract_url": "http://arxiv.org/abs/2401.07437",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Dong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Kwang-Ting"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Nuclei segmentation is a fundamental prerequisite in the digital pathology workflow. The development of automated methods for nuclei segmentation enables quantitative analysis of the wide existence and large variances in nuclei morphometry in histopathology images. However, manual annotation of tens of thousands of nuclei is tedious and time-consuming, which requires significant amount of human effort and domain-specific expertise. To alleviate this problem, in this paper, we propose a weakly-supervised nuclei segmentation method that only requires partial point labels of nuclei. Specifically, we propose a novel boundary mining framework for nuclei segmentation, named BoNuS, which simultaneously learns nuclei interior and boundary information from the point labels. To achieve this goal, we propose a novel boundary mining loss, which guides the model to learn the boundary information by exploring the pairwise pixel affinity in a multiple-instance learning manner. Then, we consider a more challenging problem, i.e., partial point label, where we propose a nuclei detection module with curriculum learning to detect the missing nuclei with prior morphological knowledge. The proposed method is validated on three public datasets, MoNuSeg, CPM, and CoNIC datasets. Experimental results demonstrate the superior performance of our method to the state-of-the-art weakly-supervised nuclei segmentation methods. Code: https://github.com/hust-linyi/bonus. ",
        "title": "BoNuS: Boundary Mining for Nuclei Segmentation with Partial Point Labels",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07439",
        "abstract_url": "http://arxiv.org/abs/2401.07439",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Tingxuan"
            },
            {
                "last_name": "Miao",
                "first_name": "Jiacheng"
            },
            {
                "last_name": "Deng",
                "first_name": "Shizhuo"
            },
            {
                "last_name": "Tong",
                "first_name": ""
            },
            {
                "last_name": "Chen",
                "first_name": "Dongyue"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Depth completion is a critical task for handling depth images with missing pixels, which can negatively impact further applications. Recent approaches have utilized Convolutional Neural Networks (CNNs) to reconstruct depth images with the assistance of color images. However, vanilla convolution has non-negligible drawbacks in handling missing pixels. To solve this problem, we propose a new model for depth completion based on an encoder-decoder structure. Our model introduces two key components: the Mask-adaptive Gated Convolution (MagaConv) architecture and the Bi-directional Progressive Fusion (BP-Fusion) module. The MagaConv architecture is designed to acquire precise depth features by modulating convolution operations with iteratively updated masks, while the BP-Fusion module progressively integrates depth and color features, utilizing consecutive bi-directional fusion structures in a global perspective. Extensive experiments on popular benchmarks, including NYU-Depth V2, DIML, and SUN RGB-D, demonstrate the superiority of our model over state-of-the-art methods. We achieved remarkable performance in completing depth maps and outperformed existing approaches in terms of accuracy and reliability. ",
        "title": "Mask-adaptive Gated Convolution and Bi-directional Progressive Fusion  Network for Depth Completion",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07440",
        "abstract_url": "http://arxiv.org/abs/2401.07440",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Jia-Wei"
            },
            {
                "last_name": "Amenta",
                "first_name": "Nina"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We explore the fairness of a redistricting game introduced by Mixon and Villar, which provides a two-party protocol for dividing a state into electoral districts, without the participation of an impartial independent authority. We analyze the game in an abstract setting that ignores the geographic distribution of voters and assumes that voter preferences are fixed and known. We first show that the minority player can always win at least $p-1$ districts, where $p$ is proportional to the percentage of minority voters, and that when the minority is large they can win more than $p$ districts. We also show that a \"cracking\" strategy by the majority party limits the number of districts the minority player can win as a function of the size of the minority. ",
        "title": "The Fairness of Redistricting Ghost",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07441",
        "abstract_url": "http://arxiv.org/abs/2401.07441",
        "authors": [
            {
                "last_name": "Ouyang",
                "first_name": "Tinghui"
            },
            {
                "last_name": "MaungMaung",
                "first_name": "AprilPyone"
            },
            {
                "last_name": "Konishi",
                "first_name": "Koichi"
            },
            {
                "last_name": "Seo",
                "first_name": "Yoshiki"
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the era of large AI models, the complex architecture and vast parameters present substantial challenges for effective AI quality management (AIQM), e.g. large language model (LLM). This paper focuses on investigating the quality assurance of a specific LLM-based AI product--a ChatGPT-based sentiment analysis system. The study delves into stability issues related to both the operation and robustness of the expansive AI model on which ChatGPT is based. Experimental analysis is conducted using benchmark datasets for sentiment analysis. The results reveal that the constructed ChatGPT-based sentiment analysis system exhibits uncertainty, which is attributed to various operational factors. It demonstrated that the system also exhibits stability issues in handling conventional small text attacks involving robustness. ",
        "title": "Stability Analysis of ChatGPT-based Sentiment Analysis in AI Quality  Assurance",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07444",
        "abstract_url": "http://arxiv.org/abs/2401.07444",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Vint"
            },
            {
                "last_name": "Roy",
                "first_name": "Sohom"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  For small-scale liquid rockets, pressure-fed systems are commonly favoured due to their simplicity and low weight. In such systems, accurate regulation of both tank and injector pressures over a wide range of upstream pressures is critical $-$ more accurate regulation allows for higher engine efficiency and minimal tank mass, thus improving flight performance. However, existing methods such as dome-loaded pressure regulators are inflexible, or require extensive characterization to function accurately. These methods also suffer from limited orifice size, droop, and slow reaction times, making them unsuitable for throttling by adjusting pressures in flight, which are increasingly important as propulsively landing rockets become more common. To overcome these challenges, we designed an electronic pressure regulator (eReg), a multi-input multi-output system utilising closed loop feedback to accurately control downstream pressures. Our design is simple, low-cost and robust: with a single ball valve actuated by a motor, we regulate both gaseous pressurant and cryogenic liquid propellant at high flow rates (1.14 kg/s of liquid; 0.39 kg/s of gas) and upstream pressures (310 bar). Using 2 eRegs to regulate propellant tank pressures, and 2 eRegs for regulating propellant flow to the engine, we demonstrated our system's ability, in a static fire test, to regulate pressures accurately (within 0.2 bar) while simultaneously throttling our engine. To the best of our knowledge, this is the first time any undergraduate team has successfully throttled a liquid bipropellant engine. ",
        "title": "Low-cost, Lightweight Electronic Flow Regulators for Throttling Liquid  Rocket Engines",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07445",
        "abstract_url": "http://arxiv.org/abs/2401.07445",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haowen"
            },
            {
                "last_name": "Du",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Jin",
                "first_name": "Congyun"
            },
            {
                "last_name": "Li",
                "first_name": "Yujiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yingbo"
            },
            {
                "last_name": "Sun",
                "first_name": "Tao"
            },
            {
                "last_name": "Qin",
                "first_name": "Piqi"
            },
            {
                "last_name": "Fan",
                "first_name": "Cong"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG"
        ],
        "abstract": "  Predicting click-through rate (CTR) is the core task of many ads online recommendation systems, which helps improve user experience and increase platform revenue. In this type of recommendation system, we often encounter two main problems: the joint usage of multi-page historical advertising data and the cold start of new ads. In this paper, we proposed GACE, a graph-based cross-page ads embedding generation method. It can warm up and generate the representation embedding of cold-start and existing ads across various pages. Specifically, we carefully build linkages and a weighted undirected graph model considering semantic and page-type attributes to guide the direction of feature fusion and generation. We designed a variational auto-encoding task as pre-training module and generated embedding representations for new and old ads based on this task. The results evaluated in the public dataset AliEC from RecBole and the real-world industry dataset from Alipay show that our GACE method is significantly superior to the SOTA method. In the online A/B test, the click-through rate on three real-world pages from Alipay has increased by 3.6%, 2.13%, and 3.02%, respectively. Especially in the cold-start task, the CTR increased by 9.96%, 7.51%, and 8.97%, respectively. ",
        "title": "GACE: Learning Graph-Based Cross-Page Ads Embedding For Click-Through  Rate Prediction",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07446",
        "abstract_url": "http://arxiv.org/abs/2401.07446",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Ruizhe"
            },
            {
                "last_name": "Ren",
                "first_name": "Hong"
            },
            {
                "last_name": "Pan",
                "first_name": "Cunhua"
            },
            {
                "last_name": "Jin",
                "first_name": "Shi"
            },
            {
                "last_name": "Popovski",
                "first_name": "Petar"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiangzhou"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we investigate a cascaded channel estimation method for a millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) system aided by a reconfigurable intelligent surface (RIS) with the BS equipped with low-resolution analog-to-digital converters (ADCs), where the BS and the RIS are both equipped with a uniform planar array (UPA). Due to the sparse property of mmWave channel, the channel estimation can be solved as a compressed sensing (CS) problem. However, the low-resolution quantization cause severe information loss of signals, and traditional CS algorithms are unable to work well. To recovery the signal and the sparse angular domain channel from quantization, we introduce Bayesian inference and efficient vector approximate message passing (VAMP) algorithm to solve the quantize output CS problem. To further improve the efficiency of the VAMP algorithm, a Fast Fourier Transform (FFT) based fast computation method is derived. Simulation results demonstrate the effectiveness and the accuracy of the proposed cascaded channel estimation method for the RIS-aided mmWave massive MIMO system with few-bit ADCs. Furthermore, the proposed channel estimation method can reach an acceptable performance gap between the low-resolution ADCs and the infinite ADCs for the low signal-to-noise ratio (SNR), which implies the applicability of few-bit ADCs in practice. ",
        "title": "Quantized RIS-aided mmWave Massive MIMO Channel Estimation with Uniform  Planar Arrays",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07447",
        "abstract_url": "http://arxiv.org/abs/2401.07447",
        "authors": [
            {
                "last_name": "N\u00e9dellec",
                "first_name": "Claire"
            },
            {
                "last_name": "Sauvion",
                "first_name": "Clara"
            },
            {
                "last_name": "Bossy",
                "first_name": "Robert"
            },
            {
                "last_name": "Borovikova",
                "first_name": "Mariya"
            },
            {
                "last_name": "Del\u00e9ger",
                "first_name": "Louise"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Wheat varieties show a large diversity of traits and phenotypes. Linking them to genetic variability is essential for shorter and more efficient wheat breeding programs. Newly desirable wheat variety traits include disease resistance to reduce pesticide use, adaptation to climate change, resistance to heat and drought stresses, or low gluten content of grains. Wheat breeding experiments are documented by a large body of scientific literature and observational data obtained in-field and under controlled conditions. The cross-referencing of complementary information from the literature and observational data is essential to the study of the genotype-phenotype relationship and to the improvement of wheat selection. The scientific literature on genetic marker-assisted selection describes much information about the genotype-phenotype relationship. However, the variety of expressions used to refer to traits and phenotype values in scientific articles is a hinder to finding information and cross-referencing it. When trained adequately by annotated examples, recent text mining methods perform highly in named entity recognition and linking in the scientific domain. While several corpora contain annotations of human and animal phenotypes, currently, no corpus is available for training and evaluating named entity recognition and entity-linking methods in plant phenotype literature. The Triticum aestivum trait Corpus is a new gold standard for traits and phenotypes of wheat. It consists of 540 PubMed references fully annotated for trait, phenotype, and species named entities using the Wheat Trait and Phenotype Ontology and the species taxonomy of the National Center for Biotechnology Information. A study of the performance of tools trained on the Triticum aestivum trait Corpus shows that the corpus is suitable for the training and evaluation of named entity recognition and linking. ",
        "title": "Taec: a Manually annotated text dataset for trait and phenotype  extraction and entity linking in wheat breeding literature",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07448",
        "abstract_url": "http://arxiv.org/abs/2401.07448",
        "authors": [
            {
                "last_name": "An",
                "first_name": "Ziyan"
            },
            {
                "last_name": "Johnson",
                "first_name": "Taylor T."
            },
            {
                "last_name": "Ma",
                "first_name": "Meiyi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters and develop a partitioning algorithm to effectively group clients based on the alignment of their temporal reasoning properties. We evaluate the proposed method on two tasks: a real-world traffic volume prediction task consisting of sensory data from fifteen states and a smart city multi-task prediction utilizing synthetic data. The evaluation results exhibit clear improvements, with performance accuracy improved by up to 54% across all sequential prediction models. ",
        "title": "Formal Logic Enabled Personalized Federated Learning Through Property  Inference",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07450",
        "abstract_url": "http://arxiv.org/abs/2401.07450",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhifeng"
            },
            {
                "last_name": "li",
                "first_name": "Hao"
            },
            {
                "last_name": "Ding",
                "first_name": "Huiming"
            },
            {
                "last_name": "Li",
                "first_name": "Mengtian"
            },
            {
                "last_name": "Cao",
                "first_name": "Ying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \\eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed them in different time step to the diffusion model according to the criteria of professional clothing designers.HieraFashDiff allows designers to add low-level attributes after high-level prompts for interactive editing incrementally.In addition, we design a differentiable loss function in the sampling process with a mask to keep non-edit areas.Comprehensive experiments performed on our newly conducted Hierarchical fashion dataset,demonstrate that our proposed method outperforms other state-of-the-art competitors. ",
        "title": "Hierarchical Fashion Design with Multi-stage Diffusion Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07451",
        "abstract_url": "http://arxiv.org/abs/2401.07451",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Alkhateeb",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Massive MIMO basestations, operating with frequency-division duplexing (FDD), require the users to feedback their channel state information (CSI) in order to design the precoding matrices. Given the powerful capabilities of deep neural networks in learning quantization codebooks, utilizing these networks in compressing the channels and reducing the massive MIMO CSI feedback overhead has recently gained increased interest. Learning one model, however, for the full cell or sector may not be optimal as the channel distribution could change significantly from one \\textit{zone} (an area or region) to another. In this letter, we introduce the concept of \\textit{zone-specific} CSI feedback. By partitioning the site space into multiple channel zones, the underlying channel distribution can be efficiently leveraged to reduce the CSI feedback. This concept leverages the implicit or explicit user position information to select the right zone-specific model and its parameters. To facilitate the evaluation of associated overhead, we introduce two novel metrics named \\textit{model parameters transmission rate} (MPTR) and \\textit{model parameters update rate} (MPUR). They jointly provide important insights and guidance for the system design and deployment. Simulation results show that significant gains could be achieved by the proposed framework. For example, using the large-scale Boston downtown scenario of DeepMIMO, the proposed zone-specific CSI feedback approach can on average achieve around 6dB NMSE gain compared to the other solutions, while keeping the same model complexity. ",
        "title": "Zone-Specific CSI Feedback for Massive MIMO: A Situation-Aware Deep  Learning Approach",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07453",
        "abstract_url": "http://arxiv.org/abs/2401.07453",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Akshat"
            },
            {
                "last_name": "Rao",
                "first_name": "Anurag"
            },
            {
                "last_name": "Anumanchipalli",
                "first_name": "Gopala"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgetting phase. Both gradual and catastrophic forgetting limit the usefulness of model editing methods at scale -- the former making model editing less effective as multiple edits are made to the model while the latter caps the scalability of such model editing methods. Our analysis also highlights other key limitations of ROME and MEMIT at scale. With our work, we push for the development and evaluation of model editing methods keeping scalability in mind. ",
        "title": "Model Editing at Scale leads to Gradual and Catastrophic Forgetting",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07454",
        "abstract_url": "http://arxiv.org/abs/2401.07454",
        "authors": [
            {
                "last_name": "Do",
                "first_name": "Anh Viet"
            },
            {
                "last_name": "Guo",
                "first_name": "Mingyu"
            },
            {
                "last_name": "Neumann",
                "first_name": "Aneta"
            },
            {
                "last_name": "Neumann",
                "first_name": "Frank"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Creating diverse sets of high quality solutions has become an important problem in recent years. Previous works on diverse solutions problems consider solutions' objective quality and diversity where one is regarded as the optimization goal and the other as the constraint. In this paper, we treat this problem as a bi-objective optimization problem, which is to obtain a range of quality-diversity trade-offs. To address this problem, we frame the evolutionary process as evolving a population of populations, and present a suitable general implementation scheme that is compatible with existing evolutionary multi-objective search methods. We realize the scheme in NSGA-II and SPEA2, and test the methods on various instances of maximum coverage, maximum cut and minimum vertex cover problems. The resulting non-dominated populations exhibit rich qualitative features, giving insights into the optimization instances and the quality-diversity trade-offs they induce. ",
        "title": "Evolutionary Multi-Objective Diversity Optimization",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07455",
        "abstract_url": "http://arxiv.org/abs/2401.07455",
        "authors": [
            {
                "last_name": "Satsukawa",
                "first_name": "Koki"
            },
            {
                "last_name": "Wada",
                "first_name": "Kentaro"
            },
            {
                "last_name": "Iryo",
                "first_name": "Takamasa"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  In this study, we analyse the global stability of the equilibrium in a departure time choice problem using a game-theoretic approach that deals with atomic users. We first formulate the departure time choice problem as a strategic game in which atomic users select departure times to minimise their trip cost; we call this game the 'departure time choice game'. The concept of the epsilon-Nash equilibrium is introduced to ensure the existence of pure-strategy equilibrium corresponding to the departure time choice equilibrium in conventional fluid models. Then, we prove that the departure time choice game is a weakly acyclic game. By analysing the convergent better responses, we clarify the mechanisms of global convergence to equilibrium. This means that the epsilon-Nash equilibrium is achieved by sequential better responses of users, which are departure time changes to improve their own utility, in an appropriate order. Specifically, the following behavioural rules are important to ensure global convergence: (i) the adjustment of the departure time of the first user departing from the origin to the corresponding equilibrium departure time and (ii) the fixation of users to their equilibrium departure times in order (starting with the earliest). Using convergence mechanisms, we construct evolutionary dynamics under which global stability is guaranteed. We also investigate the stable and unstable dynamics studied in the literature based on convergence mechanisms, and gain insight into the factors influencing the different stability results. Finally, numerical experiments are conducted to demonstrate the theoretical results. ",
        "title": "Stability analysis of a departure time choice problem with atomic  vehicle models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07456",
        "abstract_url": "http://arxiv.org/abs/2401.07456",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Yun-Wei"
            },
            {
                "last_name": "Han",
                "first_name": "Dong-Jun"
            },
            {
                "last_name": "Brinton",
                "first_name": "Christopher G."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Federated learning (FL) is a promising approach for solving multilingual tasks, potentially enabling clients with their own language-specific data to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. In this paper, we propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget. ",
        "title": "Only Send What You Need: Learning to Communicate Efficiently in  Federated Multilingual Machine Translation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07457",
        "abstract_url": "http://arxiv.org/abs/2401.07457",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ce"
            },
            {
                "last_name": "Yu",
                "first_name": "Ke"
            },
            {
                "last_name": "Tang",
                "first_name": "Yushun"
            },
            {
                "last_name": "He",
                "first_name": "Zhihai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Contrastive Language-Image Pretraining (CLIP) model has exhibited remarkable efficacy in establishing cross-modal connections between texts and images, yielding impressive performance across a broad spectrum of downstream applications through fine-tuning. However, for generalization tasks, the current fine-tuning methods for CLIP, such as CoOp and CoCoOp, demonstrate relatively low performance on some fine-grained datasets. We recognize the underlying reason is that these previous methods only projected global features into the prompt, neglecting the various visual concepts, such as colors, shapes, and sizes, which are naturally transferable across domains and play a crucial role in generalization tasks. To address this issue, in this work, we propose Concept-Guided Prompt Learning (CPL) for vision-language models. Specifically, we leverage the well-learned knowledge of CLIP to create a visual concept cache to enable concept-guided prompting. In order to refine the text features, we further develop a projector that transforms multi-level visual features into text features. We observe that this concept-guided prompt learning approach is able to achieve enhanced consistency between visual and linguistic modalities. Extensive experimental results demonstrate that our CPL method significantly improves generalization capabilities compared to the current state-of-the-art methods. ",
        "title": "Concept-Guided Prompt Learning for Generalization in Vision-Language  Models",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07459",
        "abstract_url": "http://arxiv.org/abs/2401.07459",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xin"
            },
            {
                "last_name": "Yan",
                "first_name": "Wending"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yuan"
            },
            {
                "last_name": "Mi",
                "first_name": "Michael Bi"
            },
            {
                "last_name": "Tan",
                "first_name": "Robby T."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semantic segmentation's performance is often compromised when applied to unlabeled adverse weather conditions. Unsupervised domain adaptation is a potential approach to enhancing the model's adaptability and robustness to adverse weather. However, existing methods encounter difficulties when sequentially adapting the model to multiple unlabeled adverse weather conditions. They struggle to acquire new knowledge while also retaining previously learned knowledge.To address these problems, we propose a semantic segmentation method for multiple adverse weather conditions that incorporates adaptive knowledge acquisition, pseudolabel blending, and weather composition replay. Our adaptive knowledge acquisition enables the model to avoid learning from extreme images that could potentially cause the model to forget. In our approach of blending pseudo-labels, we not only utilize the current model but also integrate the previously learned model into the ongoing learning process. This collaboration between the current teacher and the previous model enhances the robustness of the pseudo-labels for the current target. Our weather composition replay mechanism allows the model to continuously refine its previously learned weather information while simultaneously learning from the new target domain. Our method consistently outperforms the stateof-the-art methods, and obtains the best performance with averaged mIoU (%) of 65.7 and the lowest forgetting (%) of 3.6 against 60.1 and 11.3, on the ACDC datasets for a four-target continual multi-target domain adaptation. ",
        "title": "Semantic Segmentation in Multiple Adverse Weather Conditions with Domain  Knowledge Retention",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07463",
        "abstract_url": "http://arxiv.org/abs/2401.07463",
        "authors": [
            {
                "last_name": "Calder",
                "first_name": "Jeff"
            },
            {
                "last_name": "Drenska",
                "first_name": "Nadejda"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper we give a broad overview of the intersection of partial differential equations (PDEs) and graph-based semi-supervised learning. The overview is focused on a large body of recent work on PDE continuum limits of graph-based learning, which have been used to prove well-posedness of semi-supervised learning algorithms in the large data limit. We highlight some interesting research directions revolving around consistency of graph-based semi-supervised learning, and present some new results on the consistency of p-Laplacian semi-supervised learning using the stochastic tug-of-war game interpretation of the p-Laplacian. We also present the results of some numerical experiments that illustrate our results and suggest directions for future work. ",
        "title": "Consistency of semi-supervised learning, stochastic tug-of-war games,  and the p-Laplacian",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07464",
        "abstract_url": "http://arxiv.org/abs/2401.07464",
        "authors": [
            {
                "last_name": "Watkins",
                "first_name": "William"
            },
            {
                "last_name": "Wang",
                "first_name": "Heehwan"
            },
            {
                "last_name": "Bae",
                "first_name": "Sangyoon"
            },
            {
                "last_name": "Tseng",
                "first_name": "Huan-Hsin"
            },
            {
                "last_name": "Cha",
                "first_name": "Jiook"
            },
            {
                "last_name": "Chen",
                "first_name": "Samuel Yen-Chi"
            },
            {
                "last_name": "Yoo",
                "first_name": "Shinjae"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  The utility of machine learning has rapidly expanded in the last two decades and presents an ethical challenge. Papernot et. al. developed a technique, known as Private Aggregation of Teacher Ensembles (PATE) to enable federated learning in which multiple teacher models are trained on disjoint datasets. This study is the first to apply PATE to an ensemble of quantum neural networks (QNN) to pave a new way of ensuring privacy in quantum machine learning (QML) models. ",
        "title": "Quantum Privacy Aggregation of Teacher Ensembles (QPATE) for  Privacy-preserving Quantum Machine Learning",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07465",
        "abstract_url": "http://arxiv.org/abs/2401.07465",
        "authors": [
            {
                "last_name": "Tiwari",
                "first_name": "Deepak"
            },
            {
                "last_name": "Zideh",
                "first_name": "Mehdi Jabbari"
            },
            {
                "last_name": "Talreja",
                "first_name": "Veeru"
            },
            {
                "last_name": "Verma",
                "first_name": "Vishal"
            },
            {
                "last_name": "Solanki",
                "first_name": "Sarika K."
            },
            {
                "last_name": "Solanki",
                "first_name": "Jignesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Most power systems' approaches are currently tending towards stochastic and probabilistic methods due to the high variability of renewable sources and the stochastic nature of loads. Conventional power flow (PF) approaches such as forward-backward sweep (FBS) and Newton-Raphson require a high number of iterations to solve non-linear PF equations making them computationally very intensive. PF is the most important study performed by utility, required in all stages of the power system, especially in operations and planning. This paper discusses the applications of deep learning (DL) to predict PF solutions for three-phase unbalanced power distribution grids. Three deep neural networks (DNNs); Radial Basis Function Network (RBFnet), Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN), are proposed in this paper to predict PF solutions. The PF problem is formulated as a multi-output regression model where two or more output values are predicted based on the inputs. The training and testing data are generated through the OpenDSS-MATLAB COM interface. These methods are completely data-driven where the training relies on reducing the mismatch at each node without the need for the knowledge of the system. The novelty of the proposed methodology is that the models can accurately predict the PF solutions for the unbalanced distribution grids with mutual coupling and are robust to different R/X ratios, topology changes as well as generation and load variability introduced by the integration of distributed energy resources (DERs) and electric vehicles (EVs). To test the efficacy of the DNN models, they are applied to IEEE 4-node and 123-node test cases, and the American Electric Power (AEP) feeder model. The PF results for RBFnet, MLP, and CNN models are discussed in this paper demonstrating that all three DNN models provide highly accurate results in predicting PF solutions. ",
        "title": "Power Flow Analysis Using Deep Neural Networks in Three-Phase Unbalanced  Smart Distribution Grids",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07466",
        "abstract_url": "http://arxiv.org/abs/2401.07466",
        "authors": [
            {
                "last_name": "Yusuf",
                "first_name": "Imam Nur Bani"
            },
            {
                "last_name": "Jiang",
                "first_name": "Lingxiao"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software, while beneficial, poses potential cybersecurity risks due to inherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep learning has shown promise as an effective tool for this task due to its ability to perform well without extensive feature engineering. However, a challenge in deploying deep learning for vulnerability detection is the limited availability of training data. Recent research highlights the deep learning efficacy in diverse tasks. This success is attributed to instruction fine-tuning, a technique that remains under-explored in the context of vulnerability detection. This paper investigates the capability of models, specifically a recent language model, to generalize beyond the programming languages used in their training data. It also examines the role of natural language instructions in enhancing this generalization. Our study evaluates the model performance on a real-world dataset to predict vulnerable code. We present key insights and lessons learned, contributing to understanding the deep learning application in software vulnerability detection. ",
        "title": "Your Instructions Are Not Always Helpful: Assessing the Efficacy of  Instruction Fine-tuning for Software Vulnerability Detection",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07467",
        "abstract_url": "http://arxiv.org/abs/2401.07467",
        "authors": [
            {
                "last_name": "Wynn",
                "first_name": "Scott"
            },
            {
                "last_name": "Kyritsis",
                "first_name": "Alec"
            },
            {
                "last_name": "Alberi",
                "first_name": "Stephora"
            },
            {
                "last_name": "Lu",
                "first_name": "Enyue"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Sequential algorithms for the Stable Matching Problem are often too slow in the context of some large scale applications like switch scheduling. Parallel architectures can offer a notable decrease in runtime complexity. We propose a stable matching algorithm using n^2 processors that converges in O(nlog(n)) average runtime. The algorithm is structurally based on the Parallel Iterative Improvement (PII) algorithm, which successfully finds a stable matching in approximately 90% of cases. We suggest alternative selection methods for pairs in the PII algorithm, called Right-Minimum and Dynamic Selection, resulting in full convergence over 3.3 million trials and generally much faster termination. ",
        "title": "Selection Improvements for the Parallel Iterative Algorithm for Stable  Matching",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07468",
        "abstract_url": "http://arxiv.org/abs/2401.07468",
        "authors": [
            {
                "last_name": "Or",
                "first_name": "Barak"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this study, a novel deep neural network (DNN) architecture, CarSpeedNet, is introduced to estimate car speed using three-axis accelerometer data from smartphones. Utilizing 13 hours of data collected from smartphones mounted in vehicles navigating through various regions in Israel, the CarSpeedNet effectively learns the relationship between measured smartphone acceleration and car speed. Ground truth speed data was obtained at 1[Hz] from the GPS receiver in the smartphones. The proposed model enables high-frequency speed estimation, incorporating historical inputs. Our trained model demonstrates exceptional accuracy in car speed estimation, achieving a precision of less than 0.72[m/s] during an extended driving test, solely relying on smartphone accelerometer data without any connectivity to the car. ",
        "title": "CarSpeedNet: A Deep Neural Network-based Car Speed Estimation from  Smartphone Accelerometer",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07469",
        "abstract_url": "http://arxiv.org/abs/2401.07469",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Yihu"
            },
            {
                "last_name": "Liu",
                "first_name": "Shuaishi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Most existing methods tackle the problem of occluded person re-identification (ReID) by utilizing auxiliary models, resulting in a complicated and inefficient ReID framework that is unacceptable for real-time applications. In this work, a speed-up person ReID framework named SUReID is proposed to mitigate occlusion interference while speeding up inference. The SUReID consists of three key components: hierarchical token sparsification (HTS) strategy, non-parametric feature alignment knowledge distillation (NPKD), and noise occlusion data augmentation (NODA). The HTS strategy works by pruning the redundant tokens in the vision transformer to achieve highly effective self-attention computation and eliminate interference from occlusions or background noise. However, the pruned tokens may contain human part features that contaminate the feature representation and degrade the performance. To solve this problem, the NPKD is employed to supervise the HTS strategy, retaining more discriminative tokens and discarding meaningless ones. Furthermore, the NODA is designed to introduce more noisy samples, which further trains the ability of the HTS to disentangle different tokens. Experimental results show that the SUReID achieves superior performance with surprisingly fast inference. ",
        "title": "A Deep Hierarchical Feature Sparse Framework for Occluded Person  Re-Identification",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07470",
        "abstract_url": "http://arxiv.org/abs/2401.07470",
        "authors": [
            {
                "last_name": "Ahani",
                "first_name": "Zahra"
            },
            {
                "last_name": "Tash",
                "first_name": "Moein Shahiki"
            },
            {
                "last_name": "Mezquita",
                "first_name": "Yoel Ledo"
            },
            {
                "last_name": "Angel",
                "first_name": "Jason"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper provides an extensive examination of a sizable dataset of English tweets focusing on nine widely recognized cryptocurrencies, specifically Cardano, Binance, Bitcoin, Dogecoin, Ethereum, Fantom, Matic, Shiba, and Ripple. Our primary objective was to conduct a psycholinguistic and emotion analysis of social media content associated with these cryptocurrencies. To enable investigators to make more informed decisions. The study involved comparing linguistic characteristics across the diverse digital coins, shedding light on the distinctive linguistic patterns that emerge within each coin's community. To achieve this, we utilized advanced text analysis techniques. Additionally, our work unveiled an intriguing Understanding of the interplay between these digital assets within the cryptocurrency community. By examining which coin pairs are mentioned together most frequently in the dataset, we established correlations between different cryptocurrencies. To ensure the reliability of our findings, we initially gathered a total of 832,559 tweets from Twitter. These tweets underwent a rigorous preprocessing stage, resulting in a refined dataset of 115,899 tweets that were used for our analysis. Overall, our research offers valuable Perception into the linguistic nuances of various digital coins' online communities and provides a deeper understanding of their interactions in the cryptocurrency space. ",
        "title": "Utilizing deep learning models for the identification of enhancers and  super-enhancers based on genomic and epigenomic features",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07472",
        "abstract_url": "http://arxiv.org/abs/2401.07472",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Donggil"
            },
            {
                "last_name": "Kim",
                "first_name": "Taekyoo"
            },
            {
                "last_name": "Lee",
                "first_name": "Seungjoon"
            },
            {
                "last_name": "Shim",
                "first_name": "Hyungbo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we propose a distributed scheme for estimating the network size, which refers to the total number of agents in a network. By leveraging a synchronization technique for multi-agent systems, we devise an agent dynamics that ensures convergence to an equilibrium point located near the network size regardless of its initial condition. Our approach is based on an assumption that each agent has a unique identifier, and an estimation algorithm for obtaining the largest identifier value. By adopting this approach, we successfully implement the agent dynamics in a fully decentralized manner, ensuring accurate network size estimation even when some agents join or leave the network. ",
        "title": "Fully Decentralized Design of Initialization-free Distributed Network  Size Estimation",
        "date": "2024-01-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07475",
        "abstract_url": "http://arxiv.org/abs/2401.07475",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Chengwei"
            },
            {
                "last_name": "Pang",
                "first_name": "Runqi"
            },
            {
                "last_name": "Kuo",
                "first_name": "C. -C. Jay"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As a fundamental tool for natural language processing (NLP), the part-of-speech (POS) tagger assigns the POS label to each word in a sentence. A novel lightweight POS tagger based on word embeddings is proposed and named GWPT (green word-embedding-based POS tagger) in this work. Following the green learning (GL) methodology, GWPT contains three modules in cascade: 1) representation learning, 2) feature learning, and 3) decision learning modules. The main novelty of GWPT lies in representation learning. It uses non-contextual or contextual word embeddings, partitions embedding dimension indices into low-, medium-, and high-frequency sets, and represents them with different N-grams. It is shown by experimental results that GWPT offers state-of-the-art accuracies with fewer model parameters and significantly lower computational complexity in both training and inference as compared with deep-learning-based methods. ",
        "title": "GWPT: A Green Word-Embedding-based POS Tagger",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07477",
        "abstract_url": "http://arxiv.org/abs/2401.07477",
        "authors": [
            {
                "last_name": "Liang",
                "first_name": "Yingping"
            },
            {
                "last_name": "Fu",
                "first_name": "Ying"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Anchor-free object detectors are highly efficient in performing point-based prediction without the need for extra post-processing of anchors. However, different from the 2D grids, the 3D points used in these detectors are often far from the ground truth center, making it challenging to accurately regress the bounding boxes. To address this issue, we propose a Cascade Voting (CascadeV) strategy that provides high-quality 3D object detection with point-based prediction. Specifically, CascadeV performs cascade detection using a novel Cascade Voting decoder that combines two new components: Instance Aware Voting (IA-Voting) and a Cascade Point Assignment (CPA) module. The IA-Voting module updates the object features of updated proposal points within the bounding box using conditional inverse distance weighting. This approach prevents features from being aggregated outside the instance and helps improve the accuracy of object detection. Additionally, since model training can suffer from a lack of proposal points with high centerness, we have developed the CPA module to narrow down the positive assignment threshold with cascade stages. This approach relaxes the dependence on proposal centerness in the early stages while ensuring an ample quantity of positives with high centerness in the later stages. Experiments show that FCAF3D with our CascadeV achieves state-of-the-art 3D object detection results with 70.4\\% mAP@0.25 and 51.6\\% mAP@0.5 on SUN RGB-D and competitive results on ScanNet. Code will be released at https://github.com/Sharpiless/CascadeV-Det ",
        "title": "CascadeV-Det: Cascade Point Voting for 3D Object Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07479",
        "abstract_url": "http://arxiv.org/abs/2401.07479",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Alkhateeb",
                "first_name": "Ahmed"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Beam codebooks are integral components of the future millimeter wave (mmWave) multiple input multiple output (MIMO) system to relax the reliance on the instantaneous channel state information (CSI). The design of these codebooks, therefore, becomes one of the fundamental problems for these systems, and the well-designed codebooks play key roles in enabling efficient and reliable communications. Prior work has primarily focused on the codebook learning problem within a single cell/network and under stationary interference. In this work, we generalize the interference-aware codebook learning problem to networks with multiple cells/basestations. One of the key differences compared to the single-cell codebook learning problem is that the underlying environment becomes non-stationary, as the behavior of one base station will influence the learning of the others. Moreover, to encompass some of the challenging scenarios, information exchange between the different learning nodes is not allowed, which leads to a fully decentralized system with significantly increased learning difficulties. To tackle the non-stationarity, the averaging of the measurements is used to estimate the interference nulling performance of a particular beam, based on which a decision rule is provided. Furthermore, we theoretically justify the adoption of such estimator and prove that it is a sufficient statistic for the underlying quantity of interest in an asymptotic sense. Finally, a novel reward function based on averaging is proposed to fully decouple the learning of the multiple agents running at different nodes. Simulation results show that the developed solution is capable of learning well-shaped codebook patterns for different networks that significantly suppress the interference without information exchange, highlighting ... ",
        "title": "Decentralized Interference-Aware Codebook Learning in Millimeter Wave  MIMO Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07482",
        "abstract_url": "http://arxiv.org/abs/2401.07482",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Chunxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Feature selection is an important process in machine learning and knowledge discovery. By selecting the most informative features and eliminating irrelevant ones, the performance of learning algorithms can be improved and the extraction of meaningful patterns and insights from data can be facilitated. However, most existing feature selection methods, when applied to large datasets, encountered the bottleneck of high computation costs. To address this problem, we propose a novel filter feature selection method, ContrastFS, which selects discriminative features based on the discrepancies features shown between different classes. We introduce a dimensionless quantity as a surrogate representation to summarize the distributional individuality of certain classes, based on this quantity we evaluate features and study the correlation among them. We validate effectiveness and efficiency of our approach on several widely studied benchmark datasets, results show that the new method performs favorably with negligible computation in comparison with other state-of-the-art feature selection methods. ",
        "title": "A Contrast Based Feature Selection Algorithm for High-dimensional Data  set in Machine Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07484",
        "abstract_url": "http://arxiv.org/abs/2401.07484",
        "authors": [
            {
                "last_name": "Gurvich",
                "first_name": "Vladimir"
            },
            {
                "last_name": "Krnc",
                "first_name": "Matja\u017e"
            },
            {
                "last_name": "Vyalyi",
                "first_name": "Mikhail"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  An amoeba is a tree with a number assigned to each vertex. We describe a natural process of growing trees from a given amoeba and discuss conditions for such a process to be finite. ",
        "title": "Growing Trees and Amoebas' Replications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07487",
        "abstract_url": "http://arxiv.org/abs/2401.07487",
        "authors": [
            {
                "last_name": "Ju",
                "first_name": "Yuanchen"
            },
            {
                "last_name": "Hu",
                "first_name": "Kaizhe"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guowei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Gu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Mingrun"
            },
            {
                "last_name": "Xu",
                "first_name": "Huazhe"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Enabling robotic manipulation that generalizes to out-of-distribution scenes is a crucial step toward open-world embodied intelligence. For human beings, this ability is rooted in the understanding of semantic correspondence among objects, which naturally transfers the interaction experience of familiar objects to novel ones. Although robots lack such a reservoir of interaction experience, the vast availability of human videos on the Internet may serve as a valuable resource, from which we extract an affordance memory including the contact points. Inspired by the natural way humans think, we propose Robo-ABC: when confronted with unfamiliar objects that require generalization, the robot can acquire affordance by retrieving objects that share visual or semantic similarities from the affordance memory. The next step is to map the contact points of the retrieved objects to the new object. While establishing this correspondence may present formidable challenges at first glance, recent research finds it naturally arises from pre-trained diffusion models, enabling affordance mapping even across disparate object categories. Through the Robo-ABC framework, robots may generalize to manipulate out-of-category objects in a zero-shot manner without any manual annotation, additional training, part segmentation, pre-coded knowledge, or viewpoint restrictions. Quantitatively, Robo-ABC significantly enhances the accuracy of visual affordance retrieval by a large margin of 31.6% compared to state-of-the-art (SOTA) end-to-end affordance models. We also conduct real-world experiments of cross-category object-grasping tasks. Robo-ABC achieved a success rate of 85.7%, proving its capacity for real-world tasks. ",
        "title": "Robo-ABC: Affordance Generalization Beyond Categories via Semantic  Correspondence for Robot Manipulation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07488",
        "abstract_url": "http://arxiv.org/abs/2401.07488",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Chunxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  For many data-intensive tasks, feature selection is an important preprocessing step. However, most existing methods do not directly and intuitively explore the intrinsic discriminative information of features. We propose a novel feature selection framework based on the distance between class conditional distributions, measured by integral probability metrics (IPMs). Our framework directly explores the discriminative information of features in the sense of distributions for supervised classification. We analyze the theoretical and practical aspects of IPMs for feature selection, construct criteria based on IPMs. We propose several variant feature selection methods of our framework based on the 1-Wasserstein distance and implement them on real datasets from different domains. Experimental results show that our framework can outperform state-of-the-art methods in terms of classification accuracy and robustness to perturbations. ",
        "title": "Feature Selection via Maximizing Distances between Class Conditional  Distributions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07489",
        "abstract_url": "http://arxiv.org/abs/2401.07489",
        "authors": [
            {
                "last_name": "Alhussein",
                "first_name": "Hussam"
            },
            {
                "last_name": "Daqaq",
                "first_name": "Mohammed"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Recent advances in the application of physics-informed learning into the field of fluid mechanics have been predominantly grounded in the Newtonian framework, primarly leveraging Navier-Stokes Equation or one of its various derivative to train a neural network. Here, we propose an alternative approach based on variational methods. The proposed approach uses the principle of minimum pressure gradient combined with the continuity constraint to train a neural network and predict the flow field in incompressible fluids. We describe the underlying principles of the proposed approach, then use a demonstrative example to illustrate its implementation and show that it reduces the computational time per training epoch when compared to the conventional approach. ",
        "title": "The Principle of Minimum Pressure Gradient: An Alternative Basis for  Physics-Informed Learning of Incompressible Fluid Mechanics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07490",
        "abstract_url": "http://arxiv.org/abs/2401.07490",
        "authors": [
            {
                "last_name": "Hsu",
                "first_name": "Kevin"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  Maximin share (MMS) allocations are a popular relaxation of envy-free allocations that have received wide attention in the context of the fair division of indivisible items. Although MMS allocations can fail to exist [1], previous work has found conditions under which they exist. Specifically, MMS allocations exist whenever $m \\leq n+5$ in the context of goods allocation, and this bound is tight in the sense that MMS allocations can fail to exist when $m = n+6$ [2]. Unfortunately, the technique used to establish this result does not generalize readily to the chores and mixed manna settings. This paper generalizes this result to the chores setting and provides a partial solution for the mixed manna setting. Our results depend on the presence of certain types of agents. Specifically, an agent $i$ is a goods agent (resp. chores agent) if every item is a good (resp. chore) to $i$, and a non-negative mixed agent if $i$ is neither a goods nor a chores agent and the MMS guarantee of $i$ is non-negative. In this paper, we prove that an MMS allocation exists if $m \\leq n+5$ and there exists a goods agent, a non-negative mixed agent, or only chores agents.   [1] David Kurokawa, Ariel D Procaccia, and Junxing Wang. When can the maximin share guarantee be guaranteed? In Thirtieth AAAI Conference on Artificial Intelligence, 2016.   [2] Uriel Feige, Ariel Sapir, and Laliv Tauber. A tight negative example for mms fair allocations. In International Conference on Web and Internet Economics, pages 355-372. Springer, 2021. ",
        "title": "Existence of MMS Allocations with Mixed Manna",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07493",
        "abstract_url": "http://arxiv.org/abs/2401.07493",
        "authors": [
            {
                "last_name": "Karmakar",
                "first_name": "T. K."
            },
            {
                "last_name": "Dalal",
                "first_name": "D. C."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  There is a class of problems that exhibit smooth behavior on macroscopic scales, where only a microscopic evolution law is known. Patch dynamics scheme of `equation-free multiscale modelling' is one of the techniques, which aims to extract the macroscopic information using such known time-dependent microscopic model simulation in patches (which is a fraction of the space-time domain) that reduces the computational complexity. Here, extrapolation time step has an important role to reduce the error at macroscopic level. In this study, a generalized patch dynamics (GPD) scheme is proposed by distributing the gap-tooth timesteppers (GTTs) within each long (macroscopic) time step. This distribution is done in two ways, namely, GPD schemes of type-I and type-II. The proposed GPD scheme is based on three different time scales namely, micro, meso and macro to predict the system level behaviours. The GPD scheme of both types are capable of providing better accuracy with less computation time compared to the usual patch dynamics (UPD) scheme. The physical behaviours of the problems can be more appropriately addressed by the GPD scheme as one may use a non-uniform (variable) distribution of gap-tooth timesteppers (GTTs), as well as the extrapolation times based on the physics of the problem. Where the UPD scheme fails to converge for a long extrapolation time, both types of GPD schemes can be successfully applied. The whole method has been analyzed successfully for the one-dimensional reaction-diffusion problem. ",
        "title": "Generalized Patch Dynamics Scheme in Equation-free Multiscale Modelling",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07494",
        "abstract_url": "http://arxiv.org/abs/2401.07494",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zihao"
            },
            {
                "last_name": "Pravin",
                "first_name": "P S"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Network. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor. ",
        "title": "Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering  Tasks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07496",
        "abstract_url": "http://arxiv.org/abs/2401.07496",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Mingzhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Dongzhu"
            },
            {
                "last_name": "Simeone",
                "first_name": "Osvaldo"
            },
            {
                "last_name": "Wen",
                "first_name": "Dingzhu"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "LG"
        ],
        "abstract": "  This paper presents a novel approach to enhance the communication efficiency of federated learning (FL) in multiple input and multiple output (MIMO) wireless systems. The proposed method centers on a low-rank matrix factorization strategy for local gradient compression based on alternating least squares, along with over-the-air computation and error feedback. The proposed protocol, termed over-the-air low-rank compression (Ota-LC), is demonstrated to have lower computation cost and lower communication overhead as compared to existing benchmarks while guaranteeing the same inference performance. As an example, when targeting a test accuracy of 80% on the Cifar-10 dataset, Ota-LC achieves a reduction in total communication costs of at least 30% when contrasted with benchmark schemes, while also reducing the computational complexity order by a factor equal to the sum of the dimension of the gradients. ",
        "title": "Low-Rank Gradient Compression with Error Feedback for MIMO Wireless  Federated Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07500",
        "abstract_url": "http://arxiv.org/abs/2401.07500",
        "authors": [
            {
                "last_name": "Otal",
                "first_name": "Hakan T."
            },
            {
                "last_name": "Zavar",
                "first_name": "Elyse"
            },
            {
                "last_name": "Binder",
                "first_name": "Sherri B."
            },
            {
                "last_name": "Greer",
                "first_name": "Alex"
            },
            {
                "last_name": "Canbaz",
                "first_name": "M. Abdullah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CY"
        ],
        "abstract": "  Environmental disasters such as floods, hurricanes, and wildfires have increasingly threatened communities worldwide, prompting various mitigation strategies. Among these, property buyouts have emerged as a prominent approach to reducing vulnerability to future disasters. This strategy involves governments purchasing at-risk properties from willing sellers and converting the land into open space, ostensibly reducing future disaster risk and impact. However, the aftermath of these buyouts, particularly concerning land-use patterns and community impacts, remains under-explored. This research aims to fill this gap by employing innovative techniques like satellite imagery analysis and deep learning to study these patterns. To achieve this goal, we employed FEMA's Hazard Mitigation Grant Program (HMGP) buyout dataset, encompassing over 41,004 addresses of these buyout properties from 1989 to 2017. Leveraging Google's Maps Static API, we gathered 40,053 satellite images corresponding to these buyout lands. Subsequently, we implemented five cutting-edge machine learning models to evaluate their performance in classifying land cover types. Notably, this task involved multi-class classification, and our model achieved an outstanding ROC-AUC score of 98.86% ",
        "title": "Harnessing Deep Learning and Satellite Imagery for Post-Buyout Land  Cover Mapping",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07502",
        "abstract_url": "http://arxiv.org/abs/2401.07502",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Wenhui"
            },
            {
                "last_name": "Wong",
                "first_name": "Man Sing"
            },
            {
                "last_name": "Yu",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Shi",
                "first_name": "Guoqiang"
            },
            {
                "last_name": "Kwok",
                "first_name": "Coco Yin Tung"
            },
            {
                "last_name": "Zou",
                "first_name": "Kang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Semantic segmentation-based methods have attracted extensive attention in oil spill detection from SAR images. However, the existing approaches require a large number of finely annotated segmentation samples in the training stage. To alleviate this issue, we propose a composite oil spill detection framework, SAM-OIL, comprising an object detector (e.g., YOLOv8), an adapted Segment Anything Model (SAM), and an Ordered Mask Fusion (OMF) module. SAM-OIL is the first application of the powerful SAM in oil spill detection. Specifically, the SAM-OIL strategy uses YOLOv8 to obtain the categories and bounding boxes of oil spill-related objects, then inputs bounding boxes into the adapted SAM to retrieve category-agnostic masks, and finally adopts the Ordered Mask Fusion (OMF) module to fuse the masks and categories. The adapted SAM, combining a frozen SAM with a learnable Adapter module, can enhance SAM's ability to segment ambiguous objects. The OMF module, a parameter-free method, can effectively resolve pixel category conflicts within SAM. Experimental results demonstrate that SAM-OIL surpasses existing semantic segmentation-based oil spill detection methods, achieving mIoU of 69.52%. The results also indicated that both OMF and Adapter modules can effectively improve the accuracy in SAM-OIL. ",
        "title": "Compositional Oil Spill Detection Based on Object Detector and Adapted  Segment Anything Model from SAR Images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07503",
        "abstract_url": "http://arxiv.org/abs/2401.07503",
        "authors": [
            {
                "last_name": "Kato",
                "first_name": "Shunya"
            },
            {
                "last_name": "Saito",
                "first_name": "Masaki"
            },
            {
                "last_name": "Ishiguro",
                "first_name": "Katsuhiko"
            },
            {
                "last_name": "Cummings",
                "first_name": "Sol"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Despeckling is a crucial noise reduction task in improving the quality of synthetic aperture radar (SAR) images. Directly obtaining noise-free SAR images is a challenging task that has hindered the development of accurate despeckling algorithms. The advent of deep learning has facilitated the study of denoising models that learn from only noisy SAR images. However, existing methods deal solely with single-polarization images and cannot handle the multi-polarization images captured by modern satellites. In this work, we present an extension of the existing model for generating single-polarization SAR images to handle multi-polarization SAR images. Specifically, we propose a novel self-supervised despeckling approach called channel masking, which exploits the relationship between polarizations. Additionally, we utilize a spatial masking method that addresses pixel-to-pixel correlations to further enhance the performance of our approach. By effectively incorporating multiple polarization information, our method surpasses current state-of-the-art methods in quantitative evaluation in both synthetic and real-world scenarios. ",
        "title": "PolMERLIN: Self-Supervised Polarimetric Complex SAR Image Despeckling  with Masked Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07506",
        "abstract_url": "http://arxiv.org/abs/2401.07506",
        "authors": [
            {
                "last_name": "Sasindran",
                "first_name": "Zitha"
            },
            {
                "last_name": "Yelchuri",
                "first_name": "Harsha"
            },
            {
                "last_name": "Prabhakar",
                "first_name": "T. V."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SD"
        ],
        "abstract": "  In this study, we present SeMaScore, generated using a segment-wise mapping and scoring algorithm that serves as an evaluation metric for automatic speech recognition tasks. SeMaScore leverages both the error rate and a more robust similarity score. We show that our algorithm's score generation improves upon the state-of-the-art BERTscore. Our experimental results show that SeMaScore corresponds well with expert human assessments, signal-to-noise ratio levels, and other natural language metrics. We outperform BERTscore by 41x in metric computation speed. Overall, we demonstrate that SeMaScore serves as a more dependable evaluation metric, particularly in real-world situations involving atypical speech patterns. ",
        "title": "SeMaScore : a new evaluation metric for automatic speech recognition  tasks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07510",
        "abstract_url": "http://arxiv.org/abs/2401.07510",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Qing"
            },
            {
                "last_name": "Li",
                "first_name": "Lei"
            },
            {
                "last_name": "Li",
                "first_name": "Yu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks such as unimodal-related question answering, reading comprehension, reasoning, diagnosis, relation extraction, probability modeling, and others, as well as multimodal-related tasks like vision question answering, image caption, cross-modal retrieval, report summarization, and generation, are discussed in detail. Each section delves into the intricate specifics of the respective method under consideration. This paper highlights the structures and advancements of medical domain explorations against general domain methods, emphasizing their applications across different tasks and datasets. It also outlines current challenges and opportunities for future medical domain research, paving the way for continued innovation and application in this rapidly evolving field. ",
        "title": "Developing ChatGPT for Biology and Medicine: A Complete Review of  Biomedical Question Answering",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07511",
        "abstract_url": "http://arxiv.org/abs/2401.07511",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xiangtong"
            },
            {
                "last_name": "Han",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Yang",
                "first_name": "Menglong"
            },
            {
                "last_name": "Han",
                "first_name": "Songchen"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  This paper presents SNK, a novel simulation platform designed to evaluate the network performance of constellation systems for global Internet services. SNK offers realtime communication visualization and supports the simulation of routing between edge node of network. The platform enables the evaluation of routing and network performance metrics such as latency, stretch, network capacity, and throughput under different network structures and density. The effectiveness of SNK is demonstrated through various simulation cases, including the routing between fixed edge stations or mobile edge stations and analysis of space network structures. ",
        "title": "Space Networking Kit: A Novel Simulation Platform for Emerging LEO  Mega-constellations",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07515",
        "abstract_url": "http://arxiv.org/abs/2401.07515",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Hao"
            },
            {
                "last_name": "Liang",
                "first_name": "Le"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  To enhance the performance of massive multi-input multi-output (MIMO) detection using deep learning, prior research primarily adopts a model-driven methodology, integrating deep neural networks (DNNs) with traditional iterative detectors. Despite these efforts, achieving a purely data-driven detector has remained elusive, primarily due to the inherent complexities arising from the problem's high dimensionality. This paper introduces ChannelNet, a simple yet effective purely data-driven massive MIMO detector. ChannelNet embeds the channel matrix into the network as linear layers rather than viewing it as input, enabling scalability to massive MIMO scenarios. ChannelNet is computationally efficient and has a computational complexity of $\\mathcal{O}(N_t N_r)$, where $N_t$ and $N_r$ represent the numbers of transmit and receive antennas, respectively. Despite the low computation complexity, ChannelNet demonstrates robust empirical performance, matching or surpassing state-of-the-art detectors in various scenarios. In addition, theoretical insights establish ChannelNet as a universal approximator in probability for any continuous permutation-equivariant functions. ChannelNet demonstrates that designing deep learning based massive MIMO detectors can be purely data-driven and free from the constraints posed by the conventional iterative frameworks as well as the channel and noise distribution models. ",
        "title": "On Purely Data-Driven Massive MIMO Detectors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07516",
        "abstract_url": "http://arxiv.org/abs/2401.07516",
        "authors": [
            {
                "last_name": "Fard",
                "first_name": "Sanaz Hasanzadeh"
            },
            {
                "last_name": "Ghassemi",
                "first_name": "Mohammad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Graphs are a powerful representation tool in machine learning applications, with link prediction being a key task in graph learning. Temporal link prediction in dynamic networks is of particular interest due to its potential for solving complex scientific and real-world problems. Traditional approaches to temporal link prediction have focused on finding the aggregation of dynamics of the network as a unified output. In this study, we propose a novel perspective on temporal link prediction by defining nodes as Newtonian objects and incorporating the concept of velocity to predict network dynamics. By computing more specific dynamics of each node, rather than overall dynamics, we improve both accuracy and explainability in predicting future connections. We demonstrate the effectiveness of our approach using two datasets, including 17 years of co-authorship data from PubMed. Experimental results show that our temporal graph embedding dynamics approach improves downstream classification models' ability to predict future collaboration efficacy in co-authorship networks by 17.34% (AUROC improvement relative to the baseline model). Furthermore, our approach offers an interpretable layer over traditional approaches to address the temporal link prediction problem. ",
        "title": "Temporal Link Prediction Using Graph Embedding Dynamics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07517",
        "abstract_url": "http://arxiv.org/abs/2401.07517",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xiangtong"
            },
            {
                "last_name": "Yang",
                "first_name": "Menglong"
            },
            {
                "last_name": "Han",
                "first_name": "Songchen"
            },
            {
                "last_name": "Li",
                "first_name": "Wei"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The structure and routing architecture design is critical for achieving low latency and high capacity in future LEO space networks (SNs). Existing studies mainly focus on topologies of space networks, but there is a lack of analysis on constellation structures, which can greatly affect network performance. In addition, some routing architectures are designed for networks with a small number of network nodes such as Iridium while they introduce significant network overhead for high-density networks (i.e., mega-constellation networks containing thousands of satellites). In this paper, we conduct the quantitatively study on the design of network structure and routing architecture in space. The high density, high dynamics, and large scale nature of emerging Space Networks (SNs) pose significant challenges, such as unstable routing paths, low network reachability, high latency, and large jitter. To alleviate the above challenges, we design the structure of space network to maximum the connectivity through wisely adjusting the inter-plane inter satellite link. We further propose Multi-Protocol Location Forwarding (MPLF), a distributed routing architecture, targeting at minimizing the propagation latency with a distributed, convergence-free routing paradigm, while keeping routing stable and maximum the path diversity. Comprehensive experiments are conducted on a customized platform \\textit{Space Networking Kits} (SNK) which demonstrate that our solution can outperform existing related schemes by about 14\\% reduction of propagation latency and 66\\% reduction of hops-count on average, while sustaining a high path diversity with only $O(1)$ time complexity. ",
        "title": "Multi-Protocol Location Forwarding (MPLF) for Space Routing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07518",
        "abstract_url": "http://arxiv.org/abs/2401.07518",
        "authors": [
            {
                "last_name": "Lan",
                "first_name": "Yunshi"
            },
            {
                "last_name": "Li",
                "first_name": "Xinyuan"
            },
            {
                "last_name": "Du",
                "first_name": "Hanyue"
            },
            {
                "last_name": "Lu",
                "first_name": "Xuesong"
            },
            {
                "last_name": "Gao",
                "first_name": "Ming"
            },
            {
                "last_name": "Qian",
                "first_name": "Weining"
            },
            {
                "last_name": "Zhou",
                "first_name": "Aoying"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions. ",
        "title": "Survey of Natural Language Processing for Education: Taxonomy,  Systematic Review, and Future Trends",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07519",
        "abstract_url": "http://arxiv.org/abs/2401.07519",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qixun"
            },
            {
                "last_name": "Bai",
                "first_name": "Xu"
            },
            {
                "last_name": "Wang",
                "first_name": "Haofan"
            },
            {
                "last_name": "Qin",
                "first_name": "Zekui"
            },
            {
                "last_name": "Chen",
                "first_name": "Anthony"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at https://github.com/InstantID/InstantID. ",
        "title": "InstantID: Zero-shot Identity-Preserving Generation in Seconds",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07521",
        "abstract_url": "http://arxiv.org/abs/2401.07521",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Jie"
            },
            {
                "last_name": "Ding",
                "first_name": "Zhaoying"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaoshuang"
            },
            {
                "last_name": "Chen",
                "first_name": "Qi"
            },
            {
                "last_name": "Wang",
                "first_name": "Yincheng"
            },
            {
                "last_name": "Zhan",
                "first_name": "Kaiqiao"
            },
            {
                "last_name": "Wang",
                "first_name": "Ben"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The watch time is a significant indicator of user satisfaction in video recommender systems. However, the prediction of watch time as a target variable is often hindered by its highly imbalanced distribution with a scarcity of observations for larger target values and over-populated samples for small values. State-of-the-art watch time prediction models discretize the continuous watch time into a set of buckets in order to consider the distribution of watch time. However, it is highly uninvestigated how these discrete buckets should be created from the continuous watch time distribution, and existing discretization approaches suffer from either a large learning error or a large restoration error. To address this challenge, we propose a Classification-Restoration framework with Error-Adaptive-Discretization (CREAD) to accurately predict the watch time. The proposed framework contains a discretization module, a classification module, and a restoration module. It predicts the watch time through multiple classification problems. The discretization process is a key contribution of the CREAD framework. We theoretically analyze the impacts of the discretization on the learning error and the restoration error, and then propose the error-adaptive discretization (EAD) technique to better balance the two errors, which achieves better performance over traditional discretization approaches. We conduct detailed offline evaluations on a public dataset and an industrial dataset, both showing performance gains through the proposed approach. Moreover, We have fully launched our framework to Kwai App, an online video platform, which resulted in a significant increase in users' video watch time by 0.29% through A/B testing. These results highlight the effectiveness of the CREAD framework in watch time prediction in video recommender systems. ",
        "title": "CREAD: A Classification-Restoration Framework with Error Adaptive  Discretization for Watch Time Prediction in Video Recommender Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07525",
        "abstract_url": "http://arxiv.org/abs/2401.07525",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Yihan"
            },
            {
                "last_name": "Chen",
                "first_name": "Xu"
            },
            {
                "last_name": "Du",
                "first_name": "Lun"
            },
            {
                "last_name": "Chen",
                "first_name": "Hao"
            },
            {
                "last_name": "Fu",
                "first_name": "Qiang"
            },
            {
                "last_name": "Han",
                "first_name": "Shi"
            },
            {
                "last_name": "Du",
                "first_name": "Yushu"
            },
            {
                "last_name": "Kang",
                "first_name": "Yanbin"
            },
            {
                "last_name": "Lu",
                "first_name": "Guangming"
            },
            {
                "last_name": "Li",
                "first_name": "Zi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Person-job fit is an essential part of online recruitment platforms in serving various downstream applications like Job Search and Candidate Recommendation. Recently, pretrained large language models have further enhanced the effectiveness by leveraging richer textual information in user profiles and job descriptions apart from user behavior features and job metadata. However, the general domain-oriented design struggles to capture the unique structural information within user profiles and job descriptions, leading to a loss of latent semantic correlations. We propose TAROT, a hierarchical multitask co-pretraining framework, to better utilize structural and semantic information for informative text embeddings. TAROT targets semi-structured text in profiles and jobs, and it is co-pretained with multi-grained pretraining tasks to constrain the acquired semantic information at each level. Experiments on a real-world LinkedIn dataset show significant performance improvements, proving its effectiveness in person-job fit tasks. ",
        "title": "TAROT: A Hierarchical Framework with Multitask Co-Pretraining on  Semi-Structured Data towards Effective Person-Job Fit",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07526",
        "abstract_url": "http://arxiv.org/abs/2401.07526",
        "authors": [
            {
                "last_name": "Feigenbaum",
                "first_name": "Itai"
            },
            {
                "last_name": "Arpit",
                "first_name": "Devansh"
            },
            {
                "last_name": "Wang",
                "first_name": "Huan"
            },
            {
                "last_name": "Heinecke",
                "first_name": "Shelby"
            },
            {
                "last_name": "Niebles",
                "first_name": "Juan Carlos"
            },
            {
                "last_name": "Yao",
                "first_name": "Weiran"
            },
            {
                "last_name": "Xiong",
                "first_name": "Caiming"
            },
            {
                "last_name": "Savarese",
                "first_name": "Silvio"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large Language Model (LLM) editing modifies factual information in LLMs. Locate-and-Edit (L\\&E) methods accomplish this by finding where relevant information is stored within the neural network, and editing the weights at that location. The goal of editing is to modify the response of an LLM to a proposition independently of its phrasing, while not modifying its response to other related propositions. Existing methods are limited to binary propositions, which represent straightforward binary relations between a subject and an object. Furthermore, existing methods rely on semantic subject labels, which may not be available or even be well-defined in practice. In this paper, we show that both of these issues can be effectively skirted with a simple and fast localization method called Gradient Tracing (GT). This localization method allows editing arbitrary propositions instead of just binary ones, and does so without the need for subject labels. As propositions always have a truth value, our experiments prompt an LLM as a boolean classifier, and edit its T/F response to propositions. Our method applies GT for location tracing, and then edit the model at that location using a mild variant of Rank-One Model Editing (ROME). On datasets of binary propositions derived from the CounterFact dataset, we show that our method -- without access to subject labels -- performs close to state-of-the-art L\\&E methods which has access subject labels. We then introduce a new dataset, Factual Accuracy Classification Test (FACT), which includes non-binary propositions and for which subject labels are not generally applicable, and therefore is beyond the scope of existing L\\&E methods. Nevertheless, we show that with our method editing is possible on FACT. ",
        "title": "Editing Arbitrary Propositions in LLMs without Subject Labels",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07527",
        "abstract_url": "http://arxiv.org/abs/2401.07527",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Zhitong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Fahong"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiao Xiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Foundation models characterized by extensive parameters and trained on large-scale datasets have demonstrated remarkable efficacy across various downstream tasks for remote sensing data. Current remote sensing foundation models typically specialize in a single modality or a specific spatial resolution range, limiting their versatility for downstream datasets. While there have been attempts to develop multi-modal remote sensing foundation models, they typically employ separate vision encoders for each modality or spatial resolution, necessitating a switch in backbones contingent upon the input data. To address this issue, we introduce a simple yet effective method, termed OFA-Net (One-For-All Network): employing a single, shared Transformer backbone for multiple data modalities with different spatial resolutions. Using the masked image modeling mechanism, we pre-train a single Transformer backbone on a curated multi-modal dataset with this simple design. Then the backbone model can be used in different downstream tasks, thus forging a path towards a unified foundation backbone model in Earth vision. The proposed method is evaluated on 12 distinct downstream tasks and demonstrates promising performance. ",
        "title": "One for All: Toward Unified Foundation Models for Earth Vision",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07528",
        "abstract_url": "http://arxiv.org/abs/2401.07528",
        "authors": [
            {
                "last_name": "Prieur",
                "first_name": "Nils C."
            },
            {
                "last_name": "Amaro",
                "first_name": "Brian"
            },
            {
                "last_name": "Gonzalez",
                "first_name": "Emiliano"
            },
            {
                "last_name": "Kerner",
                "first_name": "Hannah"
            },
            {
                "last_name": "Medvedev",
                "first_name": "Sergei"
            },
            {
                "last_name": "Rubanenko",
                "first_name": "Lior"
            },
            {
                "last_name": "Werner",
                "first_name": "Stephanie C."
            },
            {
                "last_name": "Xiao8",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Zastrozhnov",
                "first_name": "Dmitry"
            },
            {
                "last_name": "Lap\u00f4tre",
                "first_name": "Mathieu G. A."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Boulders form from a variety of geological processes, which their size, shape, and orientation may help us better understand. Furthermore, they represent potential hazards to spacecraft landing that need to be characterized. However, mapping individual boulders across vast areas is extremely labor-intensive, often limiting the extent over which they are characterized and the statistical robustness of obtained boulder morphometrics. To automate boulder characterization, we use an instance segmentation neural network, Mask R-CNN, to detect and outline boulders in high-resolution satellite images. Our neural network, BoulderNet, was trained from a dataset of > 33,000 boulders in > 750 image tiles from Earth, the Moon, and Mars. BoulderNet not only correctly detects the majority of boulders in images, but it identifies the outline of boulders with high fidelity, achieving average precision and recall values of 72% and 64% relative to manually digitized boulders from the test dataset, when only detections with intersection-over-union ratios > 50% are considered valid. These values are similar to those obtained by human mappers. On Earth, equivalent boulder diameters, aspect ratios, and orientations extracted from predictions were benchmarked against ground measurements and yield values within 15%, 0.20, and 20 degrees of their ground-truth values, respectively. BoulderNet achieves better boulder detection and characterization performance relative to existing methods, providing a versatile open-source tool to characterize entire boulder fields on planetary surfaces. ",
        "title": "Automatic characterization of boulders on planetary surfaces from  high-resolution satellite images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07529",
        "abstract_url": "http://arxiv.org/abs/2401.07529",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yuhao"
            },
            {
                "last_name": "Liao",
                "first_name": "Yusheng"
            },
            {
                "last_name": "Liu",
                "first_name": "Heyang"
            },
            {
                "last_name": "Liu",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Yu"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanfeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Multimodal Large Language Models (MLLMs) have shown their remarkable abilities in visual perception and understanding recently. However, how to comprehensively evaluate the capabilities of MLLMs remains a challenge. Most of the existing benchmarks predominantly focus on assessing perception, cognition, and reasoning, neglecting the abilities of self-awareness, referring to the model's recognition of its own capability boundary. In our study, we focus on self-awareness in image perception and introduce the knowledge quadrant for MLLMs, which clearly defines the knowns and unknowns in perception. Based on this, we propose a novel benchmark specifically designed to evaluate the Self-Aware capabilities in Perception for MLLMs(MM-SAP). MM-SAP encompasses three distinct sub-datasets, each focusing on different aspects of self-awareness. We evaluated eight well-known MLLMs using MM-SAP, analyzing their self-awareness and providing detailed insights. Code and data are available at https://github.com/YHWmz/MM-SAP ",
        "title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of  Multimodal Large Language Models in Perception",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07532",
        "abstract_url": "http://arxiv.org/abs/2401.07532",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Chen",
                "first_name": "Jun"
            },
            {
                "last_name": "Tang",
                "first_name": "Boshi"
            },
            {
                "last_name": "Sha",
                "first_name": "Binzhu"
            },
            {
                "last_name": "Yang",
                "first_name": "Jing"
            },
            {
                "last_name": "Ju",
                "first_name": "Yaolong"
            },
            {
                "last_name": "Fan",
                "first_name": "Fan"
            },
            {
                "last_name": "Kang",
                "first_name": "Shiyin"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Meng",
                "first_name": "Helen"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music. ",
        "title": "Multi-view MidiVAE: Fusing Track- and Bar-view Representations for Long  Multi-track Symbolic Music Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07533",
        "abstract_url": "http://arxiv.org/abs/2401.07533",
        "authors": [
            {
                "last_name": "Bornes",
                "first_name": "Laetitia"
            },
            {
                "last_name": "Letondal",
                "first_name": "Catherine"
            },
            {
                "last_name": "Vingerhoeds",
                "first_name": "Rob"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Until recently, research into the sustainable design of interactive systems has primarily focused on the direct material impact of a system, through improving its energy efficiency and optimizing its lifecycle. Yet the way a system is designed and marketed often has wider repercussions, such as rebound effects, and systemic change in practices. These effects are harder to assess (and to anticipate) than the direct physical impact of the construction and use of the system itself. Current tools are unable to account for the complexity of these effects: the underlying causal mechanisms, their multi-level nature, their different temporalities, and the variety of their consequences (environmental and societal). This is why we are seeking to develop a specific methodology and tool, inspired by systemic design and system dynamics. These are intended for decision-makers and designers of interactive systems within systems of systems (for example, in the fields of agricultural robotics or public transportation). In this paper, we present this modeling approach and our prototype tool through the example of a second-hand clothing sales platform. ",
        "title": "Understanding the Indirect Effects of Interactive Systems Within Systems  of Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07534",
        "abstract_url": "http://arxiv.org/abs/2401.07534",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jialong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Mingyue"
            },
            {
                "last_name": "Li",
                "first_name": "Nianyu"
            },
            {
                "last_name": "Weyns",
                "first_name": "Danny"
            },
            {
                "last_name": "Jin",
                "first_name": "Zhi"
            },
            {
                "last_name": "Tei",
                "first_name": "Kenji"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Large Language Models (LLMs), with their abilities in knowledge acquisition and reasoning, can potentially enhance the various aspects of Self-adaptive Systems (SAS). Yet, the potential of LLMs in SAS remains largely unexplored and ambiguous, due to the lack of literature from flagship conferences or journals in the field, such as SEAMS and TAAS. The interdisciplinary nature of SAS suggests that drawing and integrating ideas from related fields, such as software engineering and autonomous agents, could unveil innovative research directions for LLMs within SAS. To this end, this paper reports the results of a literature review of studies in relevant fields, summarizes and classifies the studies relevant to SAS, and outlines their potential to specific aspects of SAS. ",
        "title": "Exploring the Potential of Large Language Models in Self-adaptive  Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07538",
        "abstract_url": "http://arxiv.org/abs/2401.07538",
        "authors": [
            {
                "last_name": "Gosti",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Succi",
                "first_name": "Sauro"
            },
            {
                "last_name": "Ruocco",
                "first_name": "Giancarlo"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  It is shown that a Hopfield recurrent neural network, informed by experimentally derived brain topology, recovers the scaling picture recently introduced by Deco et al., according to which the process of information transfer within the human brain shows spatially correlated patterns qualitatively similar to those displayed by turbulent flows. Although both models employ a coupling strength which decays exponentially with the euclidean distance between the nodes, their mathematical nature is widely different, Hopf oscillators versus Hopfield neural network. Hence, their convergence suggests a remarkable robustness of the aforementioned scaling picture. Furthermore, the present analysis shows that the Hopfield model brain remains functional by removing links above about five decay lengths, corresponding to about one sixth of the size of the global brain. This suggests that, in terms of connectivity decay length, the Hopfield brain functions in a sort of intermediate \"turbulent liquid\"-like state, whose essential connections are the intermediate ones between the connectivity decay length and the global brain size. This \"turbulent-like liquid\" appears to be more spiky than actual turbulent fluids, with a scaling exponent around $2/5$ instead of $2/3$. ",
        "title": "Evidence of Scaling Regimes in the Hopfield Dynamics of Whole Brain  Model",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07539",
        "abstract_url": "http://arxiv.org/abs/2401.07539",
        "authors": [
            {
                "last_name": "Aqasizade",
                "first_name": "Hossein"
            },
            {
                "last_name": "Ataie",
                "first_name": "Ehsan"
            },
            {
                "last_name": "Bastam",
                "first_name": "Mostafa"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "AR",
            "PF"
        ],
        "abstract": "  Over the past two decades, the cloud computing paradigm has gradually attracted more popularity due to its efficient resource usage and simple service access model. Virtualization technology is the fundamental element of cloud computing that brings several benefits to cloud users and providers, such as workload isolation, energy efficiency, server consolidation, and cost reduction. This paper examines the combination of operating system-level virtualization (containers) and hardware-level virtualization (virtual machines). To this end, the performance of containers running on top of virtual machines is experimentally compared with standalone virtual machines and containers based on different hardware resources, including the processor, main memory, disk, and network in a real testbed by running the most commonly used benchmarks. Paravirtualization and full virtualization as well as type 1 and type 2 hypervisors are covered in this study. In addition, three prevalent containerization platforms are examined. ",
        "title": "Experimental Assessment of Containers Running on Top of Virtual Machines",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07540",
        "abstract_url": "http://arxiv.org/abs/2401.07540",
        "authors": [
            {
                "last_name": "Cao",
                "first_name": "Chunxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qiang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we present a novel framework for data redundancy measurement based on probabilistic modeling of datasets, and a new criterion for redundancy detection that is resilient to noise. We also develop new methods for data redundancy reduction using both deterministic and stochastic optimization techniques. Our framework is flexible and can handle different types of features, and our experiments on benchmark datasets demonstrate the effectiveness of our methods. We provide a new perspective on feature selection, and propose effective and robust approaches for both supervised and unsupervised learning problems. ",
        "title": "Study Features via Exploring Distribution Structure",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07541",
        "abstract_url": "http://arxiv.org/abs/2401.07541",
        "authors": [
            {
                "last_name": "Habibiroudkenar",
                "first_name": "Pejman"
            },
            {
                "last_name": "Ojala",
                "first_name": "Risto"
            },
            {
                "last_name": "Tammi",
                "first_name": "Kari"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In the field of indoor robotics, accurately navigating and mapping in dynamic environments using point clouds can be a challenging task due to the presence of dynamic points. These dynamic points are often represented by people in indoor environments, but in industrial settings with moving machinery, there can be various types of dynamic points. This study introduces DynaHull, a novel technique designed to enhance indoor mapping accuracy by effectively removing dynamic points from point clouds. DynaHull works by leveraging the observation that, over multiple scans, stationary points have a higher density compared to dynamic ones. Furthermore, DynaHull addresses mapping challenges related to unevenly distributed points by clustering the map into smaller sections. In each section, the density factor of each point is determined by dividing the number of neighbors by the volume these neighboring points occupy using a convex hull method. The algorithm removes the dynamic points using an adaptive threshold based on the point count of each cluster, thus reducing the false positives. The performance of DynaHull was compared to state-of-the-art techniques, such as ERASOR, Removert, OctoMap, and a baseline statistical outlier removal from Open3D, by comparing each method to the ground truth map created during a low activity period in which only a few dynamic points were present. The results indicated that DynaHull outperformed these techniques in various metrics, noticeably in the Earth Mover's Distance. This research contributes to indoor robotics by providing efficient methods for dynamic point removal, essential for accurate mapping and localization in dynamic environments. ",
        "title": "DynaHull: Density-centric Dynamic Point Filtering in Point Clouds",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07542",
        "abstract_url": "http://arxiv.org/abs/2401.07542",
        "authors": [
            {
                "last_name": "Keuth",
                "first_name": "Ron"
            },
            {
                "last_name": "Heinrich",
                "first_name": "Mattias"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  When solving a segmentation task, shaped-base methods can be beneficial compared to pixelwise classification due to geometric understanding of the target object as shape, preventing the generation of anatomical implausible predictions in particular for corrupted data. In this work, we propose a novel hybrid method that combines a lightweight CNN backbone with a geometric neural network (Point Transformer) for shape regression. Using the same CNN encoder, the Point Transformer reaches segmentation quality on per with current state-of-the-art convolutional decoders ($4\\pm1.9$ vs $3.9\\pm2.9$ error in mm and $85\\pm13$ vs $88\\pm10$ Dice), but crucially, is more stable w.r.t image distortion, starting to outperform them at a corruption level of 30%. Furthermore, we include the nnU-Net as an upper baseline, which has $3.7\\times$ more trainable parameters than our proposed method. ",
        "title": "Combining Image- and Geometric-based Deep Learning for Shape Regression:  A Comparison to Pixel-level Methods for Segmentation in Chest X-Ray",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07543",
        "abstract_url": "http://arxiv.org/abs/2401.07543",
        "authors": [
            {
                "last_name": "Zang",
                "first_name": "Zelin"
            },
            {
                "last_name": "Li",
                "first_name": "Liangyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Yongjie"
            },
            {
                "last_name": "Duan",
                "first_name": "Chenrui"
            },
            {
                "last_name": "Wang",
                "first_name": "Kai"
            },
            {
                "last_name": "You",
                "first_name": "Yang"
            },
            {
                "last_name": "Sun",
                "first_name": "Yi"
            },
            {
                "last_name": "Li",
                "first_name": "Stan Z."
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Spatial transcriptomics (ST) technologies have revolutionized the study of gene expression patterns in tissues by providing multimodality data in transcriptomic, spatial, and morphological, offering opportunities for understanding tissue biology beyond transcriptomics. However, we identify the modality bias phenomenon in ST data species, i.e., the inconsistent contribution of different modalities to the labels leads to a tendency for the analysis methods to retain the information of the dominant modality. How to mitigate the adverse effects of modality bias to satisfy various downstream tasks remains a fundamental challenge. This paper introduces Multiple-modality Structure Transformation, named MuST, a novel methodology to tackle the challenge. MuST integrates the multi-modality information contained in the ST data effectively into a uniform latent space to provide a foundation for all the downstream tasks. It learns intrinsic local structures by topology discovery strategy and topology fusion loss function to solve the inconsistencies among different modalities. Thus, these topology-based and deep learning techniques provide a solid foundation for a variety of analytical tasks while coordinating different modalities. The effectiveness of MuST is assessed by performance metrics and biological significance. The results show that it outperforms existing state-of-the-art methods with clear advantages in the precision of identifying and preserving structures of tissues and biomarkers. MuST offers a versatile toolkit for the intricate analysis of complex biological systems. ",
        "title": "Must: Maximizing Latent Capacity of Spatial Transcriptomics Data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07544",
        "abstract_url": "http://arxiv.org/abs/2401.07544",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Youcheng"
            },
            {
                "last_name": "Lei",
                "first_name": "Wenqiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zheng"
            },
            {
                "last_name": "Lv",
                "first_name": "Jiancheng"
            },
            {
                "last_name": "Yan",
                "first_name": "Shuicheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Knowledge-editing updates knowledge of large language models (LLMs) and contributes to the interpretability and application of LLMs. However, knowledge applying is context-consistent: LLMs can recall the same knowledge in different contexts. Existing works ignore this property and the editing lacks generalization. In this paper, we empirically find that the effects of different contexts upon LLMs in recalling the same knowledge follow a Gaussian-like distribution. We then sample Gaussian noises to simulate the effects of different contexts when updating LLMs. By such, we can make LLMs see the unseen contexts where the edited knowledge will be applied, therefore improving the editing generalization. Experimental results on three LLMs demonstrate the effectiveness of our methods and also distinguish our methods from the others of fine-tuning LLMs by noises. ",
        "title": "See the Unseen: Better Context-Consistent Knowledge-Editing by Noises",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07548",
        "abstract_url": "http://arxiv.org/abs/2401.07548",
        "authors": [
            {
                "last_name": "Majumdar",
                "first_name": "Rupak"
            },
            {
                "last_name": "Saglam",
                "first_name": "Irmak"
            },
            {
                "last_name": "Thejaswini",
                "first_name": "K. S."
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "DS",
            "FL"
        ],
        "abstract": "  We provide an algorithm to solve Rabin and Streett games over graphs with $n$ vertices, $m$ edges, and $k$ colours that runs in $\\tilde{O}\\left(mn(k!)^{1+o(1)} \\right)$ time and $O(nk\\log k \\log n)$ space, where $\\tilde{O}$ hides poly-logarithmic factors. Our algorithm is an improvement by a super quadratic dependence on $k!$ from the currently best known run time of $O\\left(mn^2(k!)^{2+o(1)}\\right)$, obtained by converting a Rabin game into a parity game, while simultaneously improving its exponential space requirement.   Our main technical ingredient is a characterisation of progress measures for Rabin games using \\emph{colourful trees} and a combinatorial construction of succinctly-represented, universal colourful trees. Colourful universal trees are generalisations of universal trees used by Jurdzi\\'{n}ski and Lazi\\'{c} (2017) to solve parity games, as well as of Rabin progress measures of Klarlund and Kozen (1991). Our algorithm for Rabin games is a progress measure lifting algorithm where the lifting is performed on succinct, colourful, universal trees. ",
        "title": "Rabin Games and Colourful Universal Trees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07549",
        "abstract_url": "http://arxiv.org/abs/2401.07549",
        "authors": [
            {
                "last_name": "Callard",
                "first_name": "Antonin"
            },
            {
                "last_name": "Salomon",
                "first_name": "L\u00e9o Paviet"
            },
            {
                "last_name": "Vanier",
                "first_name": "Pascal"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "CC"
        ],
        "abstract": "  Subshifts are colorings of $\\mathbb{Z}^d$ defined by families of forbidden patterns. Given a subshift and a finite pattern, its extender set is the set of admissible completions of this pattern. It has been conjectured that the behavior of extender sets, and in particular their growth called extender entropy (arXiv:1711.07515), could provide a way to separate the classes of sofic and effective subshifts. We prove here that both classes have the same possible extender entropies: exactly the $\\Pi_3$ real numbers of $[0,+\\infty)$. We also consider computational properties of extender entropies for subshifts with some language or dynamical properties: computable language, minimal and some mixing properties. ",
        "title": "Computability of extender sets in multidimensional subshifts",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07551",
        "abstract_url": "http://arxiv.org/abs/2401.07551",
        "authors": [
            {
                "last_name": "Xi",
                "first_name": "Wenjuan"
            },
            {
                "last_name": "Song",
                "first_name": "Xin"
            },
            {
                "last_name": "Guo",
                "first_name": "Weili"
            },
            {
                "last_name": "Yang",
                "first_name": "Yang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Existing semi-supervised learning (SSL) methods assume that labeled and unlabeled data share the same class space. However, in real-world applications, unlabeled data always contain classes not present in the labeled set, which may cause classification performance degradation of known classes. Therefore, open-world SSL approaches are researched to handle the presence of multiple unknown classes in the unlabeled data, which aims to accurately classify known classes while fine-grained distinguishing different unknown classes. To address this challenge, in this paper, we propose an open-world SSL method for Self-learning Open-world Classes (SSOC), which can explicitly self-learn multiple unknown classes. Specifically, SSOC first defines class center tokens for both known and unknown classes and autonomously learns token representations according to all samples with the cross-attention mechanism. To effectively discover novel classes, SSOC further designs a pairwise similarity loss in addition to the entropy loss, which can wisely exploit the information available in unlabeled data from instances' predictions and relationships. Extensive experiments demonstrate that SSOC outperforms the state-of-the-art baselines on multiple popular classification benchmarks. Specifically, on the ImageNet-100 dataset with a novel ratio of 90%, SSOC achieves a remarkable 22% improvement. ",
        "title": "Robust Semi-Supervised Learning for Self-learning Open-World Classes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07553",
        "abstract_url": "http://arxiv.org/abs/2401.07553",
        "authors": [
            {
                "last_name": "Lou",
                "first_name": "Xingzhou"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junge"
            },
            {
                "last_name": "Wang",
                "first_name": "Ziyan"
            },
            {
                "last_name": "Huang",
                "first_name": "Kaiqi"
            },
            {
                "last_name": "Du",
                "first_name": "Yali"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Safe reinforcement learning (RL) agents accomplish given tasks while adhering to specific constraints. Employing constraints expressed via easily-understandable human language offers considerable potential for real-world applications due to its accessibility and non-reliance on domain expertise. Previous safe RL methods with natural language constraints typically adopt a recurrent neural network, which leads to limited capabilities when dealing with various forms of human language input. Furthermore, these methods often require a ground-truth cost function, necessitating domain expertise for the conversion of language constraints into a well-defined cost function that determines constraint violation. To address these issues, we proposes to use pre-trained language models (LM) to facilitate RL agents' comprehension of natural language constraints and allow them to infer costs for safe policy learning. Through the use of pre-trained LMs and the elimination of the need for a ground-truth cost, our method enhances safe policy learning under a diverse set of human-derived free-form natural language constraints. Experiments on grid-world navigation and robot control show that the proposed method can achieve strong performance while adhering to given constraints. The usage of pre-trained LMs allows our method to comprehend complicated constraints and learn safe policies without the need for ground-truth cost at any stage of training or evaluation. Extensive ablation studies are conducted to demonstrate the efficacy of each part of our method. ",
        "title": "Safe Reinforcement Learning with Free-form Natural Language Constraints  and Pre-Trained Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07558",
        "abstract_url": "http://arxiv.org/abs/2401.07558",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Biwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongliang"
            },
            {
                "last_name": "Xu",
                "first_name": "Minghui"
            },
            {
                "last_name": "Yu",
                "first_name": "Dongxiao"
            },
            {
                "last_name": "Cheng",
                "first_name": "Xiuzhen"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  Federated learning is a powerful technique that enables collaborative learning among different clients. Prototype-based federated learning is a specific approach that improves the performance of local models under non-IID (non-Independently and Identically Distributed) settings by integrating class prototypes. However, prototype-based federated learning faces several challenges, such as prototype redundancy and prototype failure, which limit its accuracy. It is also susceptible to poisoning attacks and server malfunctions, which can degrade the prototype quality. To address these issues, we propose FedRFQ, a prototype-based federated learning approach that aims to reduce redundancy, minimize failures, and improve \\underline{q}uality. FedRFQ leverages a SoftPool mechanism, which effectively mitigates prototype redundancy and prototype failure on non-IID data. Furthermore, we introduce the BFT-detect, a BFT (Byzantine Fault Tolerance) detectable aggregation algorithm, to ensure the security of FedRFQ against poisoning attacks and server malfunctions. Finally, we conduct experiments on three different datasets, namely MNIST, FEMNIST, and CIFAR-10, and the results demonstrate that FedRFQ outperforms existing baselines in terms of accuracy when handling non-IID data. ",
        "title": "FedRFQ: Prototype-Based Federated Learning with Reduced Redundancy,  Minimal Failure, and Enhanced Quality",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07559",
        "abstract_url": "http://arxiv.org/abs/2401.07559",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "J\u00f8rgensen",
                "first_name": "Bo N\u00f8rregaard"
            },
            {
                "last_name": "Ma",
                "first_name": "Zheng Grace"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The transportation industry remains a significant contributor to greenhouse gas emissions, highlighting the requirement for intelligent systems to enhance vehicle energy efficiency. The intellectual property rights of developed systems should be protected by patents. However, there is no patent overview of eco-driving intelligent systems. Unlike a scientific article, a patent documentation indicates both novelty and commercialization potential of an inventor. To address this research gap, this paper provides a patent overview of eco-driving intelligent systems and algorithms. 424 patents in the Google Patent database are analyzed. The patent analysis results show that the top three Cooperative Patent Classifications are: Y02T - climate change mitigation technologies related to transportation (50.7%), B60W - Conjoint control of vehicle subunits of different types or different functions (34.4%) and B60L - Propulsion of electrically-propelled vehicles (20.2%). 219 patents were filed after 2016 when deep learning became popular and can be categorized into five groups: vehicle energy management, smart driving, ecological and sustainable driving, fuel consumption reduction, and driving behavior optimization. Furthermore, all 219 patents involve the physical components of the intelligent system and/or novel machine learning/deep learning algorithms. Moreover, over 70% of them are granted by the China National Intellectual Property Administration. ",
        "title": "Eco-driving Intelligent Systems and Algorithms: A Patent Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07562",
        "abstract_url": "http://arxiv.org/abs/2401.07562",
        "authors": [
            {
                "last_name": "Oates",
                "first_name": "Chris. J."
            },
            {
                "last_name": "Karvonen",
                "first_name": "Toni"
            },
            {
                "last_name": "Teckentrup",
                "first_name": "Aretha L."
            },
            {
                "last_name": "Strocchi",
                "first_name": "Marina"
            },
            {
                "last_name": "Niederer",
                "first_name": "Steven A."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  For over a century, extrapolation methods have provided a powerful tool to improve the convergence order of a numerical method. However, these tools are not well-suited to modern computer codes, where multiple continua are discretised and convergence orders are not easily analysed. To address this challenge we present a probabilistic perspective on Richardson extrapolation, a point of view that unifies classical extrapolation methods with modern multi-fidelity modelling, and handles uncertain convergence orders by allowing these to be statistically estimated. The approach is developed using Gaussian processes, leading to Gauss-Richardson Extrapolation (GRE). Conditions are established under which extrapolation using the conditional mean achieves a polynomial (or even an exponential) speed-up compared to the original numerical method. Further, the probabilistic formulation unlocks the possibility of experimental design, casting the selection of fidelities as a continuous optimisation problem which can then be (approximately) solved. A case-study involving a computational cardiac model demonstrates that practical gains in accuracy can be achieved using the GRE method. ",
        "title": "Probabilistic Richardson Extrapolation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07565",
        "abstract_url": "http://arxiv.org/abs/2401.07565",
        "authors": [
            {
                "last_name": "Pettersen",
                "first_name": "H\u00e5vard"
            },
            {
                "last_name": "Morrison",
                "first_name": "Donn"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  This study addresses the challenge of reverse engineering binaries from unknown instruction set architectures, a complex task with potential implications for software maintenance and cyber-security. We focus on the tasks of detecting candidate call and return opcodes for automatic extraction of call graphs in order to simplify the reverse engineering process. Empirical testing on a small dataset of binary files from different architectures demonstrates that the approach can accurately detect specific opcodes under conditions of noisy data. The method lays the groundwork for a valuable tool for reverse engineering where the reverse engineer has minimal a priori knowledge of the underlying instruction set architecture. ",
        "title": "Call graph discovery in binary programs from unknown instruction set  architectures",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07567",
        "abstract_url": "http://arxiv.org/abs/2401.07567",
        "authors": [
            {
                "last_name": "Qi",
                "first_name": "Zhaobo"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yibo"
            },
            {
                "last_name": "Ruan",
                "first_name": "Xiaowen"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuhui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weigang"
            },
            {
                "last_name": "Huang",
                "first_name": "Qingming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias strategy (BSSARD), which dynamically generates bias-conflict samples by explicitly leveraging potentially spurious correlations between single-modality features and the temporal position of the target moments. Through adversarial training, its bias generators continuously introduce biases and generate bias-conflict samples to deceive its grounding model. Meanwhile, the grounding model continuously eliminates the introduced biases, which requires it to model multi-modality alignment information. BSSARD will cover most kinds of coupling relationships and disrupt language and visual biases simultaneously. Extensive experiments on Charades-CD and ActivityNet-CD demonstrate the promising debiasing capability of BSSARD. Source codes are available at https://github.com/qzhb/BSSARD. ",
        "title": "Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy  for Temporal Sentence Grounding in Video",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07571",
        "abstract_url": "http://arxiv.org/abs/2401.07571",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Shi",
                "first_name": "Sheng"
            },
            {
                "last_name": "An",
                "first_name": "Shan"
            },
            {
                "last_name": "Fan",
                "first_name": "Fengmei"
            },
            {
                "last_name": "Ge",
                "first_name": "Wenshu"
            },
            {
                "last_name": "Wang",
                "first_name": "Qi"
            },
            {
                "last_name": "Yu",
                "first_name": "Feng"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhiren"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous research on the diagnosis of Bipolar disorder has mainly focused on resting-state functional magnetic resonance imaging. However, their accuracy can not meet the requirements of clinical diagnosis. Efficient multimodal fusion strategies have great potential for applications in multimodal data and can further improve the performance of medical diagnosis models. In this work, we utilize both sMRI and fMRI data and propose a novel multimodal diagnosis model for bipolar disorder. The proposed Patch Pyramid Feature Extraction Module extracts sMRI features, and the spatio-temporal pyramid structure extracts the fMRI features. Finally, they are fused by a fusion module to output diagnosis results with a classifier. Extensive experiments show that our proposed method outperforms others in balanced accuracy from 0.657 to 0.732 on the OpenfMRI dataset, and achieves the state of the art. ",
        "title": "A Bi-Pyramid Multimodal Fusion Method for the Diagnosis of Bipolar  Disorders",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07572",
        "abstract_url": "http://arxiv.org/abs/2401.07572",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Qi"
            },
            {
                "last_name": "Cui",
                "first_name": "Xiao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Wengang"
            },
            {
                "last_name": "Li",
                "first_name": "Houqiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  In this study, we tackle the challenge of classifying the object category in point clouds, which previous works like PointCLIP struggle to address due to the inherent limitations of the CLIP architecture. Our approach leverages GPT-4 Vision (GPT-4V) to overcome these challenges by employing its advanced generative abilities, enabling a more adaptive and robust classification process. We adapt the application of GPT-4V to process complex 3D data, enabling it to achieve zero-shot recognition capabilities without altering the underlying model architecture. Our methodology also includes a systematic strategy for point cloud image visualization, mitigating domain gap and enhancing GPT-4V's efficiency. Experimental validation demonstrates our approach's superiority in diverse scenarios, setting a new benchmark in zero-shot point cloud classification. ",
        "title": "Exploiting GPT-4 Vision for Zero-shot Point Cloud Understanding",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07575",
        "abstract_url": "http://arxiv.org/abs/2401.07575",
        "authors": [
            {
                "last_name": "Ristea",
                "first_name": "Nicolae-Catalin"
            },
            {
                "last_name": "Anghel",
                "first_name": "Andrei"
            },
            {
                "last_name": "Ionescu",
                "first_name": "Radu Tudor"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  Speech classification tasks often require powerful language understanding models to grasp useful features, which becomes problematic when limited training data is available. To attain superior classification performance, we propose to harness the inherent value of multimodal representations by transcribing speech using automatic speech recognition (ASR) models and translating the transcripts into different languages via pretrained translation models. We thus obtain an audio-textual (multimodal) representation for each data sample. Subsequently, we combine language-specific Bidirectional Encoder Representations from Transformers (BERT) with Wav2Vec2.0 audio features via a novel cascaded cross-modal transformer (CCMT). Our model is based on two cascaded transformer blocks. The first one combines text-specific features from distinct languages, while the second one combines acoustic features with multilingual features previously learned by the first transformer block. We employed our system in the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge. CCMT was declared the winning solution, obtaining an unweighted average recall (UAR) of 65.41% and 85.87% for complaint and request detection, respectively. Moreover, we applied our framework on the Speech Commands v2 and HarperValleyBank dialog data sets, surpassing previous studies reporting results on these benchmarks. Our code is freely available for download at: https://github.com/ristea/ccmt. ",
        "title": "Cascaded Cross-Modal Transformer for Audio-Textual Classification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07576",
        "abstract_url": "http://arxiv.org/abs/2401.07576",
        "authors": [
            {
                "last_name": "Takerngsaksiri",
                "first_name": "Wannita"
            },
            {
                "last_name": "Charakorn",
                "first_name": "Rujikorn"
            },
            {
                "last_name": "Tantithamthavorn",
                "first_name": "Chakkrit"
            },
            {
                "last_name": "Li",
                "first_name": "Yuan-Fang"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Test-driven development (TDD) is a widely-employed software development practice that mandates writing test cases based on requirements before writing the actual code. While writing test cases is the centerpiece of TDD, it is time-consuming, expensive, and often shunned by developers. To address these issues associated with TDD, automated test case generation approaches have recently been investigated. Such approaches take source code as input, but not the requirements. Therefore, existing work does not fully support true TDD, as actual code is required to generate test cases. In addition, current deep learning-based test case generation approaches are trained with one learning objective, i.e., to generate test cases that are exactly matched with the ground-truth test cases. However, such approaches may limit the model's ability to generate different yet correct test cases. In this paper, we introduce PyTester, a Text-to-Testcase generation approach that can automatically generate syntactically correct, executable, complete, and effective test cases while being aligned with a given natural language requirement. We evaluate PyTester on the public APPS benchmark dataset, and the results show that our Deep RL approach enables PyTester, a small language model, to outperform much larger language models like GPT3.5, StarCoder, and InCoder. Our findings suggest that future research could consider improving small over large LMs for better resource efficiency by integrating the SE domain knowledge into the design of reinforcement learning architecture. ",
        "title": "TDD Without Tears: Towards Test Case Generation from Requirements  through Deep Reinforcement Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07577",
        "abstract_url": "http://arxiv.org/abs/2401.07577",
        "authors": [
            {
                "last_name": "Garc\u00eda-D\u00edaz",
                "first_name": "Jes\u00fas"
            },
            {
                "last_name": "Cornejo-Acosta",
                "first_name": "Jos\u00e9 Alejandro"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  Given a graph $G$, the optimization version of the graph burning problem seeks for a sequence of vertices, $(u_1,u_2,...,u_k) \\in V(G)^k$, with minimum $k$ and such that every $v \\in V(G)$ has distance at most $k-i$ to some vertex $u_i$. The length $k$ of the optimal solution is known as the burning number and is denoted by $b(G)$, an invariant that helps quantify the graph's vulnerability to contagion. This paper explores the advantages and limitations of an $\\mathcal{O}(mn + kn^2)$ deterministic greedy heuristic for this problem, where $n$ is the graph's order, $m$ is the graph's size, and $k$ is a guess on $b(G)$. This heuristic is based on the relationship between the graph burning problem and the clustered maximum coverage problem, and despite having limitations on paths and cycles, it found most of the optimal and best-known solutions of benchmark and synthetic graphs with up to 102400 vertices. ",
        "title": "A greedy heuristic for graph burning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07578",
        "abstract_url": "http://arxiv.org/abs/2401.07578",
        "authors": [
            {
                "last_name": "Jamshidi",
                "first_name": "Fateme"
            },
            {
                "last_name": "Etesami",
                "first_name": "Jalal"
            },
            {
                "last_name": "Kiyavash",
                "first_name": "Negar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study the problem of learning 'good' interventions in a stochastic environment modeled by its underlying causal graph. Good interventions refer to interventions that maximize rewards. Specifically, we consider the setting of a pre-specified budget constraint, where interventions can have non-uniform costs. We show that this problem can be formulated as maximizing the expected reward for a stochastic multi-armed bandit with side information. We propose an algorithm to minimize the cumulative regret in general causal graphs. This algorithm trades off observations and interventions based on their costs to achieve the optimal reward. This algorithm generalizes the state-of-the-art methods by allowing non-uniform costs and hidden confounders in the causal graph. Furthermore, we develop an algorithm to minimize the simple regret in the budgeted setting with non-uniform costs and also general causal graphs. We provide theoretical guarantees, including both upper and lower bounds, as well as empirical evaluations of our algorithms. Our empirical results showcase that our algorithms outperform the state of the art. ",
        "title": "Confounded Budgeted Causal Bandits",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07579",
        "abstract_url": "http://arxiv.org/abs/2401.07579",
        "authors": [
            {
                "last_name": "Zhong",
                "first_name": "Jiahui"
            },
            {
                "last_name": "Tian",
                "first_name": "Wenhong"
            },
            {
                "last_name": "Xie",
                "first_name": "Yuanlun"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhijia"
            },
            {
                "last_name": "Ou",
                "first_name": "Jie"
            },
            {
                "last_name": "Tian",
                "first_name": "Taoran"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Current state-of-the-art medical image segmentation methods prioritize accuracy but often at the expense of increased computational demands and larger model sizes. Applying these large-scale models to the relatively limited scale of medical image datasets tends to induce redundant computation, complicating the process without the necessary benefits. This approach not only adds complexity but also presents challenges for the integration and deployment of lightweight models on edge devices. For instance, recent transformer-based models have excelled in 2D and 3D medical image segmentation due to their extensive receptive fields and high parameter count. However, their effectiveness comes with a risk of overfitting when applied to small datasets and often neglects the vital inductive biases of Convolutional Neural Networks (CNNs), essential for local feature representation. In this work, we propose PMFSNet, a novel medical imaging segmentation model that effectively balances global and local feature processing while avoiding the computational redundancy typical in larger models. PMFSNet streamlines the UNet-based hierarchical structure and simplifies the self-attention mechanism's computational complexity, making it suitable for lightweight applications. It incorporates a plug-and-play PMFS block, a multi-scale feature enhancement module based on attention mechanisms, to capture long-term dependencies. Extensive comprehensive results demonstrate that even with a model (less than 1 million parameters), our method achieves superior performance in various segmentation tasks across different data scales. It achieves (IoU) metrics of 84.68%, 82.02%, and 78.82% on public datasets of teeth CT (CBCT), ovarian tumors ultrasound(MMOTU), and skin lesions dermoscopy images (ISIC 2018), respectively. The source code is available at https://github.com/yykzjh/PMFSNet. ",
        "title": "PMFSNet: Polarized Multi-scale Feature Self-attention Network For  Lightweight Medical Image Segmentation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07582",
        "abstract_url": "http://arxiv.org/abs/2401.07582",
        "authors": [
            {
                "last_name": "Shami",
                "first_name": "Mamoona Birkhez"
            },
            {
                "last_name": "Kiss",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Haakonsen",
                "first_name": "Trond Arve"
            },
            {
                "last_name": "Lindseth",
                "first_name": "Frank"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Geolocation is integral to the seamless functioning of autonomous vehicles and advanced traffic monitoring infrastructures. This paper introduces a methodology to geolocate road objects using a monocular camera, leveraging the NVIDIA DriveWorks platform. We use the Centimeter Positioning Service (CPOS) and the inverse Haversine formula to geo-locate road objects accurately. The real-time algorithm processing capability of the NVIDIA DriveWorks platform enables instantaneous object recognition and spatial localization for Advanced Driver Assistance Systems (ADAS) and autonomous driving platforms. We present a measurement pipeline suitable for autonomous driving (AD) platforms and provide detailed guidelines for calibrating cameras using NVIDIA DriveWorks. Experiments were carried out to validate the accuracy of the proposed method for geolocating targets in both controlled and dynamic settings. We show that our approach can locate targets with less than 1m error when the AD platform is stationary and less than 4m error at higher speeds (i.e. up to 60km/h) within a 15m radius. ",
        "title": "Geo-locating Road Objects using Inverse Haversine Formula with NVIDIA  Driveworks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07583",
        "abstract_url": "http://arxiv.org/abs/2401.07583",
        "authors": [
            {
                "last_name": "Koukoulekidis",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "\u0160imkovic IV",
                "first_name": "Fedor"
            },
            {
                "last_name": "Leib",
                "first_name": "Martin"
            },
            {
                "last_name": "Pereira",
                "first_name": "Francisco Revson Fernandes"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Quantum error correction is rapidly seeing first experimental implementations, but there is a significant gap between asymptotically optimal error-correcting codes and codes that are experimentally feasible. Quantum LDPC codes range from the surface code, which has a vanishing encoding rate, to very promising codes with constant encoding rate and linear distance. In this work, motivated by current small-scale experimental quantum processing units, we devise small quantum codes that are inspired by a subset of quantum LDPC codes, known as generalized bicycle (GB) codes. We introduce a code construction based on algebraic manipulation of the parity-check matrix of GB codes, rather than manipulation of Tanner graphs. Our construction leads to families of quantum LDPC codes of small size, and we demonstrate numerically that their performance scales comparably to the performance of surface codes for similar sizes under a phenomenological noise model. The advantage of our code family is that they encode many logical qubits in one code, at the expense of non-local connectivity. We then explore three variants of the code construction focusing on reducing the long-range connectivity by bringing it closer to the current experimental capabilities of short-range connectivity devices. ",
        "title": "Small Quantum Codes from Algebraic Extensions of Generalized Bicycle  Codes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07584",
        "abstract_url": "http://arxiv.org/abs/2401.07584",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Jie"
            },
            {
                "last_name": "Wan",
                "first_name": "Zhifan"
            },
            {
                "last_name": "Hu",
                "first_name": "Lanqing"
            },
            {
                "last_name": "Lin",
                "first_name": "Stephen"
            },
            {
                "last_name": "Wu",
                "first_name": "Shuzhe"
            },
            {
                "last_name": "Shan",
                "first_name": "Shiguang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Considering the close connection between action recognition and human pose estimation, we design a Collaboratively Self-supervised Video Representation (CSVR) learning framework specific to action recognition by jointly considering generative pose prediction and discriminative context matching as pretext tasks. Specifically, our CSVR consists of three branches: a generative pose prediction branch, a discriminative context matching branch, and a video generating branch. Among them, the first one encodes dynamic motion feature by utilizing Conditional-GAN to predict the human poses of future frames, and the second branch extracts static context features by pulling the representations of clips and compressed key frames from the same video together while pushing apart the pairs from different videos. The third branch is designed to recover the current video frames and predict the future ones, for the purpose of collaboratively improving dynamic motion features and static context features. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the UCF101 and HMDB51 datasets. ",
        "title": "Collaboratively Self-supervised Video Representation Learning for Action  Recognition",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07586",
        "abstract_url": "http://arxiv.org/abs/2401.07586",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Muhammad Asif"
            },
            {
                "last_name": "Menouar",
                "first_name": "Hamid"
            },
            {
                "last_name": "Hamila",
                "first_name": "Ridha"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advances in deep learning techniques have achieved remarkable performance in several computer vision problems. A notably intuitive technique called Curriculum Learning (CL) has been introduced recently for training deep learning models. Surprisingly, curriculum learning achieves significantly improved results in some tasks but marginal or no improvement in others. Hence, there is still a debate about its adoption as a standard method to train supervised learning models. In this work, we investigate the impact of curriculum learning in crowd counting using the density estimation method. We performed detailed investigations by conducting 112 experiments using six different CL settings using eight different crowd models. Our experiments show that curriculum learning improves the model learning performance and shortens the convergence time. ",
        "title": "Curriculum for Crowd Counting -- Is it Worthy?",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07589",
        "abstract_url": "http://arxiv.org/abs/2401.07589",
        "authors": [
            {
                "last_name": "Hurtado",
                "first_name": "Juana Valeria"
            },
            {
                "last_name": "Valada",
                "first_name": "Abhinav"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Comprehensive scene understanding is a critical enabler of robot autonomy. Semantic segmentation is one of the key scene understanding tasks which is pivotal for several robotics applications including autonomous driving, domestic service robotics, last mile delivery, amongst many others. Semantic segmentation is a dense prediction task that aims to provide a scene representation in which each pixel of an image is assigned a semantic class label. Therefore, semantic segmentation considers the full scene context, incorporating the object category, location, and shape of all the scene elements, including the background. Numerous algorithms have been proposed for semantic segmentation over the years. However, the recent advances in deep learning combined with the boost in the computational capacity and the availability of large-scale labeled datasets have led to significant advances in semantic segmentation. In this chapter, we introduce the task of semantic segmentation and present the deep learning techniques that have been proposed to address this task over the years. We first define the task of semantic segmentation and contrast it with other closely related scene understanding problems. We detail different algorithms and architectures for semantic segmentation and the commonly employed loss functions. Furthermore, we present an overview of datasets, benchmarks, and metrics that are used in semantic segmentation. We conclude the chapter with a discussion of challenges and opportunities for further research in this area. ",
        "title": "Semantic Scene Segmentation for Robotics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07591",
        "abstract_url": "http://arxiv.org/abs/2401.07591",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Muhammad Asif"
            },
            {
                "last_name": "Menouar",
                "first_name": "Hamid"
            },
            {
                "last_name": "Hamila",
                "first_name": "Ridha"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Most state-of-the-art crowd counting methods use color (RGB) images to learn the density map of the crowd. However, these methods often struggle to achieve higher accuracy in densely crowded scenes with poor illumination. Recently, some studies have reported improvement in the accuracy of crowd counting models using a combination of RGB and thermal images. Although multimodal data can lead to better predictions, multimodal data might not be always available beforehand. In this paper, we propose the use of generative adversarial networks (GANs) to automatically generate thermal infrared (TIR) images from color (RGB) images and use both to train crowd counting models to achieve higher accuracy. We use a Pix2Pix GAN network first to translate RGB images to TIR images. Our experiments on several state-of-the-art crowd counting models and benchmark crowd datasets report significant improvement in accuracy. ",
        "title": "Multimodal Crowd Counting with Pix2Pix GANs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07595",
        "abstract_url": "http://arxiv.org/abs/2401.07595",
        "authors": [
            {
                "last_name": "Unke",
                "first_name": "Oliver T."
            },
            {
                "last_name": "Maennel",
                "first_name": "Hartmut"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x. ",
        "title": "E3x: $\\mathrm{E}(3)$-Equivariant Deep Learning Made Easy",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07598",
        "abstract_url": "http://arxiv.org/abs/2401.07598",
        "authors": [
            {
                "last_name": "Aggarwal",
                "first_name": "Divyanshu"
            },
            {
                "last_name": "Sathe",
                "first_name": "Ashutosh"
            },
            {
                "last_name": "Sitaram",
                "first_name": "Sunayana"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Parameter efficient finetuning has emerged as a viable solution for improving the performance of Large Language Models without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the LLaMA-7B and Mistral-7B models on synthetic multilingual instruction tuning data to determine its effect on model performance on five downstream tasks covering twenty three languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and higher quantisation values benefit low-resource languages. We find that parameter efficient finetuning of smaller open source models sometimes bridges the gap between the performance of these models and the larger ones, however, English performance can take a hit. We also find that finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages. ",
        "title": "MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of  Large Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07599",
        "abstract_url": "http://arxiv.org/abs/2401.07599",
        "authors": [
            {
                "last_name": "Mekacher",
                "first_name": "Amin"
            },
            {
                "last_name": "Falkenberg",
                "first_name": "Max"
            },
            {
                "last_name": "Baronchelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Increasingly, alternative platforms are playing a key role in the social media ecosystem. Koo, a microblogging platform based in India, has emerged as a major new social network hosting high profile politicians from several countries (India, Brazil, Nigeria) and many internationally renowned celebrities. This paper presents the largest publicly available Koo dataset, spanning from the platform's founding in early 2020 to September 2023, providing detailed metadata for 72M posts, 75M comments, 40M shares, 284M likes and 1.4M user profiles. Along with the release of the dataset, we provide an overview of the platform including a discussion of the news ecosystem on the platform, hashtag usage and user engagement. Our results highlight the pivotal role that new platforms play in shaping online communities in emerging economies and the Global South, connecting local politicians and public figures with their followers. With Koo's ambition to become the town hall for diverse non-English speaking communities, our dataset offers new opportunities for studying social media beyond a Western context. ",
        "title": "The Koo Dataset: An Indian Microblogging Platform With Global Ambitions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07602",
        "abstract_url": "http://arxiv.org/abs/2401.07602",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Jing"
            },
            {
                "last_name": "Du",
                "first_name": "Lei"
            },
            {
                "last_name": "Sogabe",
                "first_name": "Tomohiro"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shao-Liang"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  It is well-known that a multilinear system with a nonsingular M-tensor and a positive right-hand side has a unique positive solution. Tensor splitting methods generalizing the classical iterative methods for linear systems have been proposed for finding the unique positive solution. The Alternating Anderson-Richardson (AAR) method is an effective method to accelerate the classical iterative methods. In this study, we apply the idea of AAR for finding the unique positive solution quickly. We first present a tensor Richardson method based on tensor regular splittings, then apply Anderson acceleration to the tensor Richardson method and derive a tensor Anderson-Richardson method, finally, we periodically employ the tensor Anderson-Richardson method within the tensor Richardson method and propose a tensor AAR method. Numerical experiments show that the proposed method is effective in accelerating tensor splitting methods. ",
        "title": "A tensor Alternating Anderson-Richardson method for solving multilinear  systems with M-tensors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07603",
        "abstract_url": "http://arxiv.org/abs/2401.07603",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Heecheol"
            },
            {
                "last_name": "Ohmura",
                "first_name": "Yoshiyuki"
            },
            {
                "last_name": "Kuniyoshi",
                "first_name": "Yasuo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In the field of robotic manipulation, deep imitation learning is recognized as a promising approach for acquiring manipulation skills. Additionally, learning from diverse robot datasets is considered a viable method to achieve versatility and adaptability. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise, not addressing the fine-grained object manipulation that robots are expected to perform in the real world. This paper introduces a dataset of diverse object manipulations that includes dual-arm tasks and/or tasks requiring fine manipulation. To this end, we have generated dataset with 224k episodes (150 hours, 1,104 language instructions) which includes dual-arm fine tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data is publicly available. Additionally, this dataset includes visual attention signals as well as dual-action labels, a signal that separates actions into a robust reaching trajectory and precise interaction with objects, and language instructions to achieve robust and precise object manipulation. We applied the dataset to our Dual-Action and Attention (DAA), a model designed for fine-grained dual arm manipulation tasks and robust against covariate shifts. The model was tested with over 7k total trials in real robot manipulation tasks, demonstrating its capability in fine manipulation. ",
        "title": "Multi-task robot data for dual-arm fine manipulation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07604",
        "abstract_url": "http://arxiv.org/abs/2401.07604",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Wenqi"
            },
            {
                "last_name": "Bieker",
                "first_name": "Jacob"
            },
            {
                "last_name": "Arcucci",
                "first_name": "Rossella"
            },
            {
                "last_name": "Quilodr\u00e1n-Casas",
                "first_name": "C\u00e9sar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, the convergence of data-driven machine learning models with Data Assimilation (DA) offers a promising avenue for enhancing weather forecasting. This study delves into this emerging trend, presenting our methodologies and outcomes. We harnessed the UK's local ERA5 850 hPa temperature data and refined the U-STN12 global weather forecasting model, tailoring its predictions to the UK's climate nuances. From the ASOS network, we sourced T2m data, representing ground observations across the UK. We employed the advanced kriging method with a polynomial drift term for consistent spatial resolution. Furthermore, Gaussian noise was superimposed on the ERA5 T850 data, setting the stage for ensuing multi-time step synthetic observations. Probing into the assimilation impacts, the ASOS T2m data was integrated with the ERA5 T850 dataset. Our insights reveal that while global forecast models can adapt to specific regions, incorporating atmospheric data in DA significantly bolsters model accuracy. Conversely, the direct assimilation of surface temperature data tends to mitigate this enhancement, tempering the model's predictive prowess. ",
        "title": "Data Assimilation using ERA5, ASOS, and the U-STN model for Weather  Forecasting over the UK",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07606",
        "abstract_url": "http://arxiv.org/abs/2401.07606",
        "authors": [
            {
                "last_name": "Daniely",
                "first_name": "Amit"
            },
            {
                "last_name": "Schain",
                "first_name": "Mariano"
            },
            {
                "last_name": "Yehudai",
                "first_name": "Gilad"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Optimizing Neural networks is a difficult task which is still not well understood. On the other hand, fixed representation methods such as kernels and random features have provable optimization guarantees but inferior performance due to their inherent inability to learn the representations. In this paper, we aim at bridging this gap by presenting a novel architecture called RedEx (Reduced Expander Extractor) that is as expressive as neural networks and can also be trained in a layer-wise fashion via a convex program with semi-definite constraints and optimization guarantees. We also show that RedEx provably surpasses fixed representation methods, in the sense that it can efficiently learn a family of target functions which fixed representation methods cannot. ",
        "title": "RedEx: Beyond Fixed Representation Methods via Convex Optimization",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07611",
        "abstract_url": "http://arxiv.org/abs/2401.07611",
        "authors": [
            {
                "last_name": "Behravesh",
                "first_name": "Rasoul"
            },
            {
                "last_name": "Breitgand",
                "first_name": "David"
            },
            {
                "last_name": "Lorenz",
                "first_name": "Dean H."
            },
            {
                "last_name": "Raz",
                "first_name": "Danny"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Mobile edge computing offers a myriad of opportunities to innovate and introduce novel applications, thereby enhancing user experiences considerably. A critical issue extensively investigated in this domain is efficient deployment of Service Function Chains (SFCs) across the physical network, spanning from the edge to the cloud. This problem is known to be NP-hard. As a result of its practical importance, there is significant interest in the development of high-quality sub-optimal solutions.   In this paper, we consider this problem and propose a novel near-optimal heuristic that is extremely efficient and scalable. We compare our solution to the state-of-the-art heuristic and to the theoretical optimum. In our large-scale evaluations, we use realistic topologies which were previously reported in the literature. We demonstrate that the execution time offered by our solution grows slowly as the number of Virtual Network Function (VNF) forwarding graph embedding requests grows, and it handles one million requests in slightly more than 20 seconds for 100 nodes and 150 edges physical topology. ",
        "title": "A Practical Near Optimal Deployment of Service Function Chains in  Edge-to-Cloud Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07612",
        "abstract_url": "http://arxiv.org/abs/2401.07612",
        "authors": [
            {
                "last_name": "Suo",
                "first_name": "Xuchen"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate. This paper introduces the 'Signed-Prompt' method as a novel solution. The study involves signing sensitive instructions within command segments by authorized users, enabling the LLM to discern trusted instruction sources. The paper presents a comprehensive analysis of prompt injection attack patterns, followed by a detailed explanation of the Signed-Prompt concept, including its basic architecture and implementation through both prompt engineering and fine-tuning of LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method, showing substantial resistance to various types of prompt injection attacks, thus validating its potential as a robust defense strategy in AI security. ",
        "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks  Against LLM-Integrated Applications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07616",
        "abstract_url": "http://arxiv.org/abs/2401.07616",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Rewriting logic and its implementation Maude are an expressive framework for the formal specification and verification of software and other kinds of systems. Concurrency is naturally represented by nondeterministic local transformations produced by the application of rewriting rules over algebraic terms in an equational theory. Some aspects of the global behavior of the systems or additional constraints sometimes require restricting this nondeterminism. Rewriting strategies are used as a higher-level and modular resource to cleanly capture these requirements, which can be easily expressed in Maude with an integrated strategy language. However, strategy-aware specifications cannot be verified with the builtin LTL model checker, making strategies less useful and attractive.   In this paper, we discuss model checking for strategy-controlled systems, and present a strategy-aware extension of the Maude LTL model checker. The expressivity of the strategy language is discussed in relation to model checking, the model checker is illustrated with multiple application examples, and its performance is compared. ",
        "title": "Model checking strategy-controlled systems in rewriting logic",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07627",
        "abstract_url": "http://arxiv.org/abs/2401.07627",
        "authors": [
            {
                "last_name": "Ben\u00edtez-Pe\u00f1a",
                "first_name": "Sandra"
            },
            {
                "last_name": "Blanquero",
                "first_name": "Rafael"
            },
            {
                "last_name": "Carrizosa",
                "first_name": "Emilio"
            },
            {
                "last_name": "Ram\u00edrez-Cobo",
                "first_name": "Pepa"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Feature Selection is a crucial procedure in Data Science tasks such as Classification, since it identifies the relevant variables, making thus the classification procedures more interpretable, cheaper in terms of measurement and more effective by reducing noise and data overfit. The relevance of features in a classification procedure is linked to the fact that misclassifications costs are frequently asymmetric, since false positive and false negative cases may have very different consequences. However, off-the-shelf Feature Selection procedures seldom take into account such cost-sensitivity of errors.   In this paper we propose a mathematical-optimization-based Feature Selection procedure embedded in one of the most popular classification procedures, namely, Support Vector Machines, accommodating asymmetric misclassification costs. The key idea is to replace the traditional margin maximization by minimizing the number of features selected, but imposing upper bounds on the false positive and negative rates. The problem is written as an integer linear problem plus a quadratic convex problem for Support Vector Machines with both linear and radial kernels.   The reported numerical experience demonstrates the usefulness of the proposed Feature Selection procedure. Indeed, our results on benchmark data sets show that a substantial decrease of the number of features is obtained, whilst the desired trade-off between false positive and false negative rates is achieved. ",
        "title": "Cost-sensitive Feature Selection for Support Vector Machines",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07629",
        "abstract_url": "http://arxiv.org/abs/2401.07629",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zichen"
            },
            {
                "last_name": "Yang",
                "first_name": "Bo"
            },
            {
                "last_name": "Yue",
                "first_name": "Haonan"
            },
            {
                "last_name": "Ma",
                "first_name": "Zhenghao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Few-shot object detection (FSOD) aims at extending a generic detector for novel object detection with only a few training examples. It attracts great concerns recently due to the practical meanings. Meta-learning has been demonstrated to be an effective paradigm for this task. In general, methods based on meta-learning employ an additional support branch to encode novel examples (a.k.a. support images) into class prototypes, which are then fused with query branch to facilitate the model prediction. However, the class-level prototypes are difficult to precisely generate, and they also lack detailed information, leading to instability in performance.New methods are required to capture the distinctive local context for more robust novel object detection. To this end, we propose to distill the most representative support features into fine-grained prototypes. These prototypes are then assigned into query feature maps based on the matching results, modeling the detailed feature relations between two branches. This process is realized by our Fine-Grained Feature Aggregation (FFA) module. Moreover, in terms of high-level feature fusion, we propose Balanced Class-Agnostic Sampling (B-CAS) strategy and Non-Linear Fusion (NLF) module from differenct perspectives. They are complementary to each other and depict the high-level feature relations more effectively. Extensive experiments on PASCAL VOC and MS COCO benchmarks show that our method sets a new state-of-the-art performance in most settings. Our code is available at https://github.com/wangchen1801/FPD. ",
        "title": "Fine-Grained Prototypes Distillation for Few-Shot Object Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07631",
        "abstract_url": "http://arxiv.org/abs/2401.07631",
        "authors": [
            {
                "last_name": "Dutta",
                "first_name": "Pranjal"
            },
            {
                "last_name": "Gesmundo",
                "first_name": "Fulvio"
            },
            {
                "last_name": "Ikenmeyer",
                "first_name": "Christian"
            },
            {
                "last_name": "Jindal",
                "first_name": "Gorav"
            },
            {
                "last_name": "Lysikov",
                "first_name": "Vladimir"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  Border complexity measures are defined via limits (or topological closures), so that any function which can approximated arbitrarily closely by low complexity functions itself has low border complexity. Debordering is the task of proving an upper bound on some non-border complexity measure in terms of a border complexity measure, thus getting rid of limits.   Debordering is at the heart of understanding the difference between Valiant's determinant vs permanent conjecture, and Mulmuley and Sohoni's variation which uses border determinantal complexity. The debordering of matrix multiplication tensors by Bini played a pivotal role in the development of efficient matrix multiplication algorithms. Consequently, debordering finds applications in both establishing computational complexity lower bounds and facilitating algorithm design. Currently, very few debordering results are known.   In this work, we study the question of debordering the border Waring rank of polynomials. Waring and border Waring rank are very well studied measures in the context of invariant theory, algebraic geometry, and matrix multiplication algorithms. For the first time, we obtain a Waring rank upper bound that is exponential in the border Waring rank and only linear in the degree. All previous known results were exponential in the degree. For polynomials with constant border Waring rank, our results imply an upper bound on the Waring rank linear in degree, which previously was only known for polynomials with border Waring rank at most 5. ",
        "title": "Fixed-parameter debordering of Waring rank",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07636",
        "abstract_url": "http://arxiv.org/abs/2401.07636",
        "authors": [
            {
                "last_name": "Lindeberg",
                "first_name": "Anna"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  The AHU-algorithm solves the computationally difficult graph isomorphism problem for rooted trees, and does so with a linear time complexity. Although the AHU-algorithm has remained state of the art for almost 50 years, it has been criticized for being unclearly presented, and no complete proof of correctness has been given. In this text, that gap is filled: we formalize the algorithm's main point of assigning and compressing labels to provide a characterization of isomorphic rooted trees, and then proceed with proving the correctness and optimal runtime of the AHU-algorithm. ",
        "title": "Isomorphism Testing of Rooted Trees in Linear Time",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07639",
        "abstract_url": "http://arxiv.org/abs/2401.07639",
        "authors": [
            {
                "last_name": "N\u00e9meth",
                "first_name": "G\u00e1bor"
            },
            {
                "last_name": "Matuszka",
                "first_name": "Tam\u00e1s"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Active learning, a powerful paradigm in machine learning, aims at reducing labeling costs by selecting the most informative samples from an unlabeled dataset. However, the traditional active learning process often demands extensive computational resources, hindering scalability and efficiency. In this paper, we address this critical issue by presenting a novel method designed to alleviate the computational burden associated with active learning on massive datasets. To achieve this goal, we introduce a simple, yet effective method-agnostic framework that outlines how to strategically choose and annotate data points, optimizing the process for efficiency while maintaining model performance. Through case studies, we demonstrate the effectiveness of our proposed method in reducing computational costs while maintaining or, in some cases, even surpassing baseline model outcomes. Code is available at https://github.com/aimotive/Compute-Efficient-Active-Learning. ",
        "title": "Compute-Efficient Active Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07640",
        "abstract_url": "http://arxiv.org/abs/2401.07640",
        "authors": [
            {
                "last_name": "Milani",
                "first_name": "Marcelo Garlet"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We define and study a new structural parameter for directed graphs, which we call \\emph{ear anonymity}. Our parameter aims to generalize the useful properties of \\emph{funnels} to larger digraph classes. In particular, funnels are exactly the acyclic digraphs with ear anonymity one. We prove that computing the ear anonymity of a digraph is \\NP/-hard and that it can be solved in $O(m(n + m))$-time on acyclic digraphs (where \\(n\\) is the number of vertices and \\(m\\) is the number of arcs in the input digraph). It remains open where exactly in the polynomial hierarchy the problem of computing ear anonymity lies, however for a related problem we manage to show $\\Sigma_2^p$-completeness. ",
        "title": "Directed Ear Anonymity",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07641",
        "abstract_url": "http://arxiv.org/abs/2401.07641",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Mingxin"
            },
            {
                "last_name": "Peng",
                "first_name": "Dezhi"
            },
            {
                "last_name": "Li",
                "first_name": "Hongliang"
            },
            {
                "last_name": "Peng",
                "first_name": "Zhenghao"
            },
            {
                "last_name": "Liu",
                "first_name": "Chongyu"
            },
            {
                "last_name": "Lin",
                "first_name": "Dahua"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuliang"
            },
            {
                "last_name": "Bai",
                "first_name": "Xiang"
            },
            {
                "last_name": "Jin",
                "first_name": "Lianwen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  End-to-end scene text spotting, which aims to read the text in natural images, has garnered significant attention in recent years. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter v2, which seeks to find a better synergy between text detection and recognition. Specifically, we enhance the relationship between two tasks using novel Recognition Conversion and Recognition Alignment modules. Recognition Conversion explicitly guides text localization through recognition loss, while Recognition Alignment dynamically extracts text features for recognition through the detection predictions. This simple yet effective design results in a concise framework that requires neither an additional rectification module nor character-level annotations for the arbitrarily-shaped text. Furthermore, the parameters of the detector are greatly reduced without performance degradation by introducing a Box Selection Schedule. Qualitative and quantitative experiments demonstrate that SwinTextSpotter v2 achieved state-of-the-art performance on various multilingual (English, Chinese, and Vietnamese) benchmarks. The code will be available at \\href{https://github.com/mxin262/SwinTextSpotterv2}{SwinTextSpotter v2}. ",
        "title": "SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07643",
        "abstract_url": "http://arxiv.org/abs/2401.07643",
        "authors": [
            {
                "last_name": "Qazzaz",
                "first_name": "Mohammed M. H."
            },
            {
                "last_name": "Ku\u0142acz",
                "first_name": "\u0141ukasz"
            },
            {
                "last_name": "Kliks",
                "first_name": "Adrian"
            },
            {
                "last_name": "Zaidi",
                "first_name": "Syed A."
            },
            {
                "last_name": "Dryjanski",
                "first_name": "Marcin"
            },
            {
                "last_name": "McLernon",
                "first_name": "Des"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The disaggregated, distributed and virtualised implementation of radio access networks allows for dynamic resource allocation. These attributes can be realised by virtue of the Open Radio Access Networks (O-RAN) architecture. In this article, we tackle the issue of dynamic resource allocation using a data-driven approach by employing Machine Learning (ML). We present an xApp-based implementation for the proposed ML algorithm. The core aim of this work is to optimise resource allocation and fulfil Service Level Specifications (SLS). This is accomplished by dynamically adjusting the allocation of Physical Resource Blocks (PRBs) based on traffic demand and Quality of Service (QoS) requirements. The proposed ML model effectively selects the best allocation policy for each base station and enhances the performance of scheduler functionality in O-RAN - Distributed Unit (O-DU). We show that an xApp implementing the Random Forest Classifier can yield high (85\\%) performance accuracy for optimal policy selection. This can be attained using the O-RAN instance state input parameters over a short training duration. ",
        "title": "Machine Learning-based xApp for Dynamic Resource Allocation in O-RAN  Networks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07644",
        "abstract_url": "http://arxiv.org/abs/2401.07644",
        "authors": [
            {
                "last_name": "Amiri",
                "first_name": "Mojtaba"
            },
            {
                "last_name": "Vaezpour",
                "first_name": "Elaheh"
            },
            {
                "last_name": "Javadi",
                "first_name": "Sepideh"
            },
            {
                "last_name": "Mili",
                "first_name": "Mohammad Robat"
            },
            {
                "last_name": "Yanikomeroglu",
                "first_name": "Halim"
            },
            {
                "last_name": "Bennis",
                "first_name": "Mehdi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) is a cutting-edge concept for the sixth-generation (6G) wireless networks. In this letter, we propose a novel system that incorporates STAR-RIS with simultaneous wireless information and power transfer (SWIPT) using rate splitting multiple access (RSMA). The proposed system facilitates communication from a multi-antenna base station (BS) to single-antenna users in a downlink transmission. The BS concurrently sends energy and information signals to multiple energy harvesting receivers (EHRs) and information data receivers (IDRs) with the support of a deployed STAR-RIS. Furthermore, a multi-objective optimization is introduced to strike a balance between users' sum rate and the total harvested energy. To achieve this, an optimization problem is formulated to optimize the energy/information beamforming vectors at the BS, the phase shifts at the STAR-RIS, and the common message rate. Subsequently, we employ a meta deep deterministic policy gradient (Meta-DDPG) approach to solve the complex problem. Simulation results validate that the proposed algorithm significantly enhances both data rate and harvested energy in comparison to conventional DDPG. ",
        "title": "Multi-Objective Optimization in STAR-RIS-Aided SWIPT with RSMA via  Meta-Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07646",
        "abstract_url": "http://arxiv.org/abs/2401.07646",
        "authors": [
            {
                "last_name": "Seckler",
                "first_name": "Henrik"
            },
            {
                "last_name": "Metzler",
                "first_name": "Ralf"
            },
            {
                "last_name": "Kelty-Stephen",
                "first_name": "Damian G."
            },
            {
                "last_name": "Mangalam",
                "first_name": "Madhur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Anomalous diffusion processes pose a unique challenge in classification and characterization. Previously (Mangalam et al., 2023, Physical Review Research 5, 023144), we established a framework for understanding anomalous diffusion using multifractal formalism. The present study delves into the potential of multifractal spectral features for effectively distinguishing anomalous diffusion trajectories from five widely used models: fractional Brownian motion, scaled Brownian motion, continuous time random walk, annealed transient time motion, and L\\'evy walk. To accomplish this, we generate extensive datasets comprising $10^6$ trajectories from these five anomalous diffusion models and extract multiple multifractal spectra from each trajectory. Our investigation entails a thorough analysis of neural network performance, encompassing features derived from varying numbers of spectra. Furthermore, we explore the integration of multifractal spectra into traditional feature datasets, enabling us to assess their impact comprehensively. To ensure a statistically meaningful comparison, we categorize features into concept groups and train neural networks using features from each designated group. Notably, several feature groups demonstrate similar levels of accuracy, with the highest performance observed in groups utilizing moving-window characteristics and $p$-variation features. Multifractal spectral features, particularly those derived from three spectra involving different timescales and cutoffs, closely follow, highlighting their robust discriminatory potential. Remarkably, a neural network exclusively trained on features from a single multifractal spectrum exhibits commendable performance, surpassing other feature groups. Our findings underscore the diverse and potent efficacy of multifractal spectral features in enhancing classification of anomalous diffusion. ",
        "title": "Multifractal-spectral features enhance classification of anomalous  diffusion",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07654",
        "abstract_url": "http://arxiv.org/abs/2401.07654",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Ho Hin"
            },
            {
                "last_name": "Gu",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhao",
                "first_name": "Theodore"
            },
            {
                "last_name": "Xu",
                "first_name": "Yanbo"
            },
            {
                "last_name": "Yang",
                "first_name": "Jianwei"
            },
            {
                "last_name": "Usuyama",
                "first_name": "Naoto"
            },
            {
                "last_name": "Wong",
                "first_name": "Cliff"
            },
            {
                "last_name": "Wei",
                "first_name": "Mu"
            },
            {
                "last_name": "Landman",
                "first_name": "Bennett A."
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            },
            {
                "last_name": "Santamaria-Pang",
                "first_name": "Alberto"
            },
            {
                "last_name": "Poon",
                "first_name": "Hoifung"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios. ",
        "title": "Foundation Models for Biomedical Image Segmentation: A Survey",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07655",
        "abstract_url": "http://arxiv.org/abs/2401.07655",
        "authors": [
            {
                "last_name": "Zang",
                "first_name": "Runqiang"
            },
            {
                "last_name": "Guo",
                "first_name": "Hongcheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jian"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Li",
                "first_name": "Zhoujun"
            },
            {
                "last_name": "Zheng",
                "first_name": "Tieqiao"
            },
            {
                "last_name": "Shi",
                "first_name": "Xu"
            },
            {
                "last_name": "Zheng",
                "first_name": "Liangfan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Bo"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  In spite of the rapid advancements in unsupervised log anomaly detection techniques, the current mainstream models still necessitate specific training for individual system datasets, resulting in costly procedures and limited scalability due to dataset size, thereby leading to performance bottlenecks. Furthermore, numerous models lack cognitive reasoning capabilities, posing challenges in direct transferability to similar systems for effective anomaly detection. Additionally, akin to reconstruction networks, these models often encounter the \"identical shortcut\" predicament, wherein the majority of system logs are classified as normal, erroneously predicting normal classes when confronted with rare anomaly logs due to reconstruction errors.   To address the aforementioned issues, we propose MLAD, a novel anomaly detection model that incorporates semantic relational reasoning across multiple systems. Specifically, we employ Sentence-bert to capture the similarities between log sequences and convert them into highly-dimensional learnable semantic vectors. Subsequently, we revamp the formulas of the Attention layer to discern the significance of each keyword in the sequence and model the overall distribution of the multi-system dataset through appropriate vector space diffusion. Lastly, we employ a Gaussian mixture model to highlight the uncertainty of rare words pertaining to the \"identical shortcut\" problem, optimizing the vector space of the samples using the maximum expectation model. Experiments on three real-world datasets demonstrate the superiority of MLAD. ",
        "title": "MLAD: A Unified Model for Multi-system Log Anomaly Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07656",
        "abstract_url": "http://arxiv.org/abs/2401.07656",
        "authors": [
            {
                "last_name": "Bork",
                "first_name": "Alexander"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Debraj"
            },
            {
                "last_name": "Grover",
                "first_name": "Kush"
            },
            {
                "last_name": "Kretinsky",
                "first_name": "Jan"
            },
            {
                "last_name": "Mohr",
                "first_name": "Stefanie"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "LO"
        ],
        "abstract": "  Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable. ",
        "title": "Learning Explainable and Better Performing Representations of POMDP  Strategies",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07657",
        "abstract_url": "http://arxiv.org/abs/2401.07657",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Xiuyuan"
            },
            {
                "last_name": "Liu",
                "first_name": "Guoqing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model. ",
        "title": "Empirical Evidence for the Fragment level Understanding on Drug  Molecular Structure of LLMs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07658",
        "abstract_url": "http://arxiv.org/abs/2401.07658",
        "authors": [
            {
                "last_name": "Lim",
                "first_name": "Tian Yi"
            },
            {
                "last_name": "Ghignone",
                "first_name": "Edoardo"
            },
            {
                "last_name": "Baumann",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Magno",
                "first_name": "Michele"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This work introduces SynPF, an MCL-based algorithm tailored for high-speed racing environments. Benchmarked against Cartographer, a state-of-the-art pose-graph SLAM algorithm, SynPF leverages synergies from previous particle-filtering methods and synthesizes them for the high-performance racing domain. Our extensive in-field evaluations reveal that while Cartographer excels under nominal conditions, it struggles when subjected to wheel-slip, a common phenomenon in a racing scenario due to varying grip levels and aggressive driving behaviour. Conversely, SynPF demonstrates robustness in these challenging conditions and a low-latency computation time of 1.25 ms on on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled autonomous racing vehicle, this work not only highlights the vulnerabilities of existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also emphasizes the potential of SynPF as a viable alternative, especially in deteriorating odometry conditions. ",
        "title": "Robustness Evaluation of Localization Techniques for Autonomous Racing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07663",
        "abstract_url": "http://arxiv.org/abs/2401.07663",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lichen"
            },
            {
                "last_name": "Lu",
                "first_name": "Shuai"
            },
            {
                "last_name": "Duan",
                "first_name": "Nan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level project of the seL4 operating system microkernel. Selene provides a comprehensive framework for end-to-end evaluation and a lightweight verification environment. Our experimental results with advanced LLMs, such as GPT-3.5-turbo and GPT-4, highlight the capabilities of large language models (LLMs) in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors. ",
        "title": "Selene: Pioneering Automated Proof in Software Verification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07669",
        "abstract_url": "http://arxiv.org/abs/2401.07669",
        "authors": [
            {
                "last_name": "S",
                "first_name": "Darshan Singh"
            },
            {
                "last_name": "Khan",
                "first_name": "Zeeshan"
            },
            {
                "last_name": "Tapaswi",
                "first_name": "Makarand"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While contrastive language image pretraining (CLIP) have exhibited impressive performance by learning highly semantic and generalized representations, recent works have exposed a fundamental drawback in its syntactic properties, that includes interpreting fine-grained attributes, actions, spatial relations, states, and details that require compositional reasoning. One reason for this is that natural captions often do not capture all the visual details of a scene. This leads to unaddressed visual concepts being misattributed to the wrong words. And the pooled image and text features, ends up acting as a bag of words, hence losing the syntactic information. In this work, we ask: Is it possible to enhance CLIP's fine-grained and syntactic abilities without compromising its semantic properties? We show that this is possible by adapting CLIP efficiently on a high-quality, comprehensive, and relatively small dataset. We demonstrate our adaptation strategy on VidSitu, a video situation recognition dataset annotated with verbs and rich semantic role labels (SRL). We use the SRL and verb information to create rule-based detailed captions, making sure they capture most of the visual concepts. Combined with hard negatives and hierarchical losses, these annotations allow us to learn a powerful visual representation, dubbed Fine-Grained CLIP (FiGCLIP), that preserves semantic understanding while being detail-oriented. We evaluate on five diverse vision-language tasks in both fine-tuning and zero-shot settings, achieving consistent improvements over the base CLIP model. ",
        "title": "FiGCLIP: Fine-Grained CLIP Adaptation via Densely Annotated Videos",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07671",
        "abstract_url": "http://arxiv.org/abs/2401.07671",
        "authors": [
            {
                "last_name": "Pelke",
                "first_name": "Rebecca"
            },
            {
                "last_name": "Cubero-Cascante",
                "first_name": "Jose"
            },
            {
                "last_name": "Bosbach",
                "first_name": "Nils"
            },
            {
                "last_name": "Staudigl",
                "first_name": "Felix"
            },
            {
                "last_name": "Leupers",
                "first_name": "Rainer"
            },
            {
                "last_name": "Joseph",
                "first_name": "Jan Moritz"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR",
            "ET",
            "LG"
        ],
        "abstract": "  The demand for efficient machine learning (ML) accelerators is growing rapidly, driving the development of novel computing concepts such as resistive random access memory (RRAM)-based tiled computing-in-memory (CIM) architectures. CIM allows to compute within the memory unit, resulting in faster data processing and reduced power consumption. Efficient compiler algorithms are essential to exploit the potential of tiled CIM architectures. While conventional ML compilers focus on code generation for CPUs, GPUs, and other von Neumann architectures, adaptations are needed to cover CIM architectures. Cross-layer scheduling is a promising approach, as it enhances the utilization of CIM cores, thereby accelerating computations. Although similar concepts are implicitly used in previous work, there is a lack of clear and quantifiable algorithmic definitions for cross-layer scheduling for tiled CIM architectures. To close this gap, we present CLSA-CIM, a cross-layer scheduling algorithm for tiled CIM architectures. We integrate CLSA-CIM with existing weight-mapping strategies and compare performance against state-of-the-art (SOTA) scheduling algorithms. CLSA-CIM improves the utilization by up to 17.9 x , resulting in an overall speedup increase of up to 29.2 x compared to SOTA. ",
        "title": "CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory  Architectures",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07674",
        "abstract_url": "http://arxiv.org/abs/2401.07674",
        "authors": [
            {
                "last_name": "Koukis",
                "first_name": "Georgios"
            },
            {
                "last_name": "Skaperas",
                "first_name": "Sotiris"
            },
            {
                "last_name": "Kapetanidou",
                "first_name": "Ioanna Angeliki"
            },
            {
                "last_name": "Mamatas",
                "first_name": "Lefteris"
            },
            {
                "last_name": "Tsaoussidis",
                "first_name": "Vassilis"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Kubernetes (K8s) serves as a mature orchestration system for the seamless deployment and management of containerized applications spanning across cloud and edge environments. Since high-performance connectivity and minimal resource utilization become critical factors as we approach the edge, evaluating the performance of K8s networking in this context is essential. This paper contributes to this effort, by conducting a qualitative and quantitative performance evaluation of diverse Container Network Interface (CNI) plugins within different K8s environments, incorporating lightweight implementations designed for the Edge. Our experimental assessment was conducted in two distinct (intra- and inter-host) scenarios, revealing interesting insights for both researchers and practitioners. For example, the deployment of plugins across lightweight distributions does not necessarily lead to resource utilization improvements, e.g., in terms of CPU/memory or throughput. ",
        "title": "Performance Evaluation of Kubernetes Networking Approaches across  Constraint Edge Environments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07677",
        "abstract_url": "http://arxiv.org/abs/2401.07677",
        "authors": [
            {
                "last_name": "R\u00f8nneberg",
                "first_name": "Rasmus Carl"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Secure software architecture is increasingly important in a data-driven world. When security is neglected sensitive information might leak through unauthorized access. To mitigate this software architects needs tools and methods to quantify security risks in complex systems. This paper presents doctoral research in its early stages concerned with creating constructive methods for building secure component-based systems from a quantitative information flow specification. This research aim at developing a method that allows software architects to develop secure systems from a repository of secure components. Planned contributions are refinement rules for secure development of components from a specification and well-formedness rules for secure composition of said components. ",
        "title": "Quantitative Information Flow Control by Construction for  Component-Based Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07680",
        "abstract_url": "http://arxiv.org/abs/2401.07680",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Rewriting logic and its implementation Maude are a natural and expressive framework for the specification of concurrent systems and logics. Its nondeterministic local transformations are described by rewriting rules, which can be controlled at a higher level using a builtin strategy language added to Maude~3. This specification resource would not be of much interest without tools to analyze their models, so in a previous work, we extended the Maude LTL model checker to verify strategy-controlled systems. In this paper, CTL* and $\\mu$-calculus are added to the repertoire of supported logics, after discussing which adaptations are needed for branching-time properties. The new extension relies on some external model checkers that are exposed the Maude models through general and efficient connections, profitable for future extensions and further applications. The performance of these model checkers is compared. ",
        "title": "Strategies, model checking and branching-time properties in Maude",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07681",
        "abstract_url": "http://arxiv.org/abs/2401.07681",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Tong"
            },
            {
                "last_name": "Doclo",
                "first_name": "Simon"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Spatially selective active noise control (ANC) hearables are designed to reduce unwanted noise from certain directions while preserving desired sounds from other directions. In previous studies, the target signal has been defined either as the delayed desired component in one of the reference microphone signals or as the desired component in the error microphone signal without any delay. In this paper, we systematically investigate the influence of delays in different target signals on the ANC performance and provide an intuitive explanation for how the system obtains the desired signal. Simulations were conducted on a pair of open-fitting hearables for localized speech and noise sources in an anechoic environment. The performance was assessed in terms of noise reduction, signal quality and control effort. Results indicate that optimal performance is achieved without delays when the target signal is defined at the error microphone, whereas causality necessitates delays when the target signal is defined at the reference microphone. The optimal delay is found to be the acoustic delay between this reference microphone and the error microphone from the desired source. ",
        "title": "Effect of target signals and delays on spatially selective active noise  control for open-fitting hearables",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07682",
        "abstract_url": "http://arxiv.org/abs/2401.07682",
        "authors": [
            {
                "last_name": "Vandak",
                "first_name": "Samuel"
            },
            {
                "last_name": "Goodell",
                "first_name": "Geoffrey"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CE"
        ],
        "abstract": "  The landscape of payment methods in retail is a complex and evolving area. Vendors are motivated to conduct an appropriate analysis to decide what payment methods to accept out of a vast range of options. Many factors are included in this decision process, some qualitative and some quantitative. The following research project investigates vendors' acceptance of cards and cash from various viewpoints, all chosen to represent a novel perspective, including the barriers and preferences for each and correlations with external demographic factors. We observe that lower interchange fees, limited in this instance by the regulatory framework, play a crucial role in facilitating merchants' acceptance of card payments. The regulatory constraints on interchange fees create a favorable cost structure for merchants, making card payment adoption financially feasible. However, additional factors like technological readiness and consumer preferences might also play a significant role in their decision-making process. We also note that aggregate Merchant Service Providers (MSPs) have positively impacted the payment landscape by offering more competitive fee rates, particularly beneficial for small merchants and entrepreneurs. However, associated risks, such as account freezes or abrupt terminations, pose challenges and often lack transparency. Last, the quantitative analysis of the relationship between demographic variables and acceptance of payment types is presented. This analysis combines the current landscape of payment acceptance in the UK with data from the most recent census from 2021. We show that the unemployment rates shape card and cash acceptance, age affects contactless preference, and work-from-home impacts credit card preference. ",
        "title": "Cash and Card Acceptance in Retail Payments: Motivations and Factors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07683",
        "abstract_url": "http://arxiv.org/abs/2401.07683",
        "authors": [
            {
                "last_name": "Gohsen",
                "first_name": "Marcel"
            },
            {
                "last_name": "Stein",
                "first_name": "Benno"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository of millions of knowledge statements. However, domain-specific knowledge from fields such as history, physics, or medicine is significantly underrepresented in those graphs. Although few domain-specific knowledge graphs exist (e.g., Pubmed for medicine), developing specialized retrieval applications for many domains still requires constructing knowledge graphs from scratch. To facilitate knowledge graph construction, we introduce WAKA: a Web application that allows domain experts to create knowledge graphs through the medium with which they are most familiar: natural language. ",
        "title": "Assisted Knowledge Graph Authoring: Human-Supervised Knowledge Graph  Construction from Natural Language",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07685",
        "abstract_url": "http://arxiv.org/abs/2401.07685",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Binh Vinh Duc"
            },
            {
                "last_name": "Moere",
                "first_name": "Andrew Vande"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  The WeWatt bike serves as an energy station that enables passers-by to charge their mobile devices through physical activity. However, despite multiple people using it simultaneously, the bike is typically used individually. To address this limitation, we developed the WeWattTree, an installation utilising human-powered energy to filter environmental air. Through the orchestration of subtle motion gestures, our goal is to entice passers-by to participate and encourage them to socially interact, synchronising their pace. In this work-in-progress, we provide insights into the prototyping process, combining physical experimentation and computational simulation, and delve into the underlying concepts of our grammar of motion gestures. We highlight how a single design effectively merged multiple functionalities, how the role of material characteristics shaped the interaction design, and discuss the potential for social performances as captivating public displays. ",
        "title": "A Human-Powered Public Display that Nudges Social Biking via Motion  Gesturing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07691",
        "abstract_url": "http://arxiv.org/abs/2401.07691",
        "authors": [
            {
                "last_name": "Buchanan",
                "first_name": "William J"
            },
            {
                "last_name": "Grierson",
                "first_name": "Sam"
            },
            {
                "last_name": "Uribe",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Biometric data is often highly sensitive, and a leak of this data can lead to serious privacy breaches. Some of the most sensitive of this type of data relates to the usage of DNA data on individuals. A leak of this type of data without consent could lead to privacy breaches of data protection laws. Along with this, there have been several recent data breaches related to the leak of DNA information, including from 23andMe and Ancestry. It is thus fundamental that a citizen should have the right to know if their DNA data is contained within a DNA database and ask for it to be removed if they are concerned about its usage. This paper outlines a method of hashing the core information contained within the data stores - known as Single-Nucleotide Polymorphisms (SNPs) - into a bilinear group accumulator in batch mode, which can then be searched by a trusted entity for matches. The time to create the witness proof and to verify were measured at 0.86 ms and 10.90 ms, respectively. ",
        "title": "Privacy-Aware Single-Nucleotide Polymorphisms (SNPs) using Bilinear  Group Accumulators in Batch Mode",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07694",
        "abstract_url": "http://arxiv.org/abs/2401.07694",
        "authors": [
            {
                "last_name": "Powell",
                "first_name": "William G."
            },
            {
                "last_name": "Lyu",
                "first_name": "Hanbaek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  For obtaining optimal first-order convergence guarantee for stochastic optimization, it is necessary to use a recurrent data sampling algorithm that samples every data point with sufficient frequency. Most commonly used data sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed recurrent under mild assumptions. In this work, we show that for a particular class of stochastic optimization algorithms, we do not need any other property (e.g., independence, exponential mixing, and reshuffling) than recurrence in data sampling algorithms to guarantee the optimal rate of first-order convergence. Namely, using regularized versions of Minimization by Incremental Surrogate Optimization (MISO), we show that for non-convex and possibly non-smooth objective functions, the expected optimality gap converges at an optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes. Furthermore, the implied constant depends explicitly on the `speed of recurrence', measured by the expected amount of time to visit a given data point either averaged (`target time') or supremized (`hitting time') over the current location. We demonstrate theoretically and empirically that convergence can be accelerated by selecting sampling algorithms that cover the data set most effectively. We discuss applications of our general framework to decentralized optimization and distributed non-negative matrix factorization. ",
        "title": "Stochastic optimization with arbitrary recurrent data sampling",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07696",
        "abstract_url": "http://arxiv.org/abs/2401.07696",
        "authors": [
            {
                "last_name": "Shome",
                "first_name": "Arumoy"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "van Deursen",
                "first_name": "Arie"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployment. In a prior study, we mined $54,070$ Jupyter notebooks from Github and created a catalogue of $269$ semantically related visualisation-assertion (VA) pairs. Building on this catalogue, we propose to build a taxonomy that organises the VA pairs based on ML verification tasks. The input feature space comprises of a rich source of information mined from the Jupyter notebooks -- visualisations, Python source code, and associated markdown text. The effectiveness of various AI models, including traditional NLP4Code models and modern Large Language Models, will be compared using established machine translation metrics and evaluated through a qualitative study with human participants. The paper also plans to address the challenge of extending the existing VA pair dataset with additional pairs from Kaggle and to compare the tool's effectiveness with commercial generative AI models like ChatGPT. This research not only contributes to the field of ML system validation but also explores novel ways to leverage AI for automating and enhancing software engineering practices in ML. ",
        "title": "Towards Automatic Translation of Machine Learning Visual Insights to  Analytical Assertions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07697",
        "abstract_url": "http://arxiv.org/abs/2401.07697",
        "authors": [
            {
                "last_name": "Shome",
                "first_name": "Arumoy"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "van Deursen",
                "first_name": "Arie"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY",
            "SE"
        ],
        "abstract": "  Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a ``cheap'' and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs. ",
        "title": "Data vs. Model Machine Learning Fairness Testing: An Empirical Study",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07698",
        "abstract_url": "http://arxiv.org/abs/2401.07698",
        "authors": [
            {
                "last_name": "Mari\u0107",
                "first_name": "Ante"
            },
            {
                "last_name": "Li",
                "first_name": "Yiming"
            },
            {
                "last_name": "Calinon",
                "first_name": "Sylvain"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Reasoning about distance is indispensable for establishing or avoiding contact in manipulation tasks. To this end, we present an online method for learning implicit representations of signed distance using piecewise polynomial basis functions. Starting from an arbitrary prior shape, our approach incrementally constructs a continuous representation from incoming point cloud data. It offers fast access to distance and analytical gradients without the need to store training data. We assess the accuracy of our model on a diverse set of household objects and compare it to neural network and Gaussian process counterparts. Distance reconstruction and real-time updates are further evaluated in a physical experiment by simultaneously collecting sparse point cloud data and using the evolving model to control a manipulator. ",
        "title": "Online Learning of Piecewise Polynomial Signed Distance Fields for  Manipulation Tasks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07702",
        "abstract_url": "http://arxiv.org/abs/2401.07702",
        "authors": [
            {
                "last_name": "Davis",
                "first_name": "Christopher"
            },
            {
                "last_name": "Caines",
                "first_name": "Andrew"
            },
            {
                "last_name": "Andersen",
                "first_name": "\u00d8istein"
            },
            {
                "last_name": "Taslimipoor",
                "first_name": "Shiva"
            },
            {
                "last_name": "Yannakoudakis",
                "first_name": "Helen"
            },
            {
                "last_name": "Yuan",
                "first_name": "Zheng"
            },
            {
                "last_name": "Bryant",
                "first_name": "Christopher"
            },
            {
                "last_name": "Rei",
                "first_name": "Marek"
            },
            {
                "last_name": "Buttery",
                "first_name": "Paula"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Thanks to recent advances in generative AI, we are able to prompt large language models (LLMs) to produce texts which are fluent and grammatical. In addition, it has been shown that we can elicit attempts at grammatical error correction (GEC) from LLMs when prompted with ungrammatical input sentences. We evaluate how well LLMs can perform at GEC by measuring their performance on established benchmark datasets. We go beyond previous studies, which only examined GPT* models on a selection of English GEC datasets, by evaluating seven open-source and three commercial LLMs on four established GEC benchmarks. We investigate model performance and report results against individual error types. Our results indicate that LLMs do not always outperform supervised English GEC models except in specific contexts -- namely commercial LLMs on benchmarks annotated with fluency corrections as opposed to minimal edits. We find that several open-source models outperform commercial ones on minimal edit benchmarks, and that in some settings zero-shot prompting is just as competitive as few-shot prompting. ",
        "title": "Prompting open-source and commercial language models for grammatical  error correction of English learner text",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07704",
        "abstract_url": "http://arxiv.org/abs/2401.07704",
        "authors": [
            {
                "last_name": "Oxenhorn",
                "first_name": "Arthur"
            },
            {
                "last_name": "Mor",
                "first_name": "Almog"
            },
            {
                "last_name": "Stern",
                "first_name": "Uri"
            },
            {
                "last_name": "Feitelson",
                "first_name": "Dror G."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Perhaps the most widely used form of code documentation is function header comments. We performed a large-scale survey of 367 developers to catalog their expectations from such documentation and to chronicle actual practice. Paradoxically, we found that developers appreciate the value of header comments and estimate that they are worth the investment in time, but nevertheless they tend not to write such documentation in their own code. Reasons for not writing header comments vary from the belief that code should be self-documenting to concern that documentation will not be kept up-to-date. A possible outcome of this situation is that developers may evade requirements to write documentation by using templates to generate worthless comments that do not provide any real information. We define a simple metric for information-less documentation based on its similarity to the function signature. Applying this to 21,140 files in GitHub Python projects shows that most functions are undocumented, but when header comments are written they typically do contain additional information beyond the function signature. ",
        "title": "The Paradox of Function Header Comments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07706",
        "abstract_url": "http://arxiv.org/abs/2401.07706",
        "authors": [
            {
                "last_name": "Della Rossa",
                "first_name": "Matteo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The topic of this manuscript is the stability analysis of continuous-time switched nonlinear systems with constraints on the admissible switching signals. Our particular focus lies in considering signals characterized by upper and lower bounds on the length of the switching intervals. We adapt and extend the existing theory of multiple Lyapunov functions, providing converse results and thus a complete characterization of uniform stability for this class of systems. We specify our results in the context of switched linear systems, providing the equivalence of exponential stability and the existence of multiple Lyapunov norms. By restricting the class of candidate Lyapunov functions to the set of quadratic functions, we are able to provide semidefinite-optimization-based numerical schemes to check the proposed conditions. We provide numerical examples to illustrate our approach and highlight its advantages over existing methods. ",
        "title": "Converse Lyapunov Results for Switched Systems with Lower and Upper  Bounds on Switching Intervals",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07709",
        "abstract_url": "http://arxiv.org/abs/2401.07709",
        "authors": [
            {
                "last_name": "Zou",
                "first_name": "Siyu"
            },
            {
                "last_name": "Tang",
                "first_name": "Jiji"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yiyi"
            },
            {
                "last_name": "He",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhao",
                "first_name": "Chaoyi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Rongsheng"
            },
            {
                "last_name": "Hu",
                "first_name": "Zhipeng"
            },
            {
                "last_name": "Sun",
                "first_name": "Xiaoshuai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which often applies a semantic mask to control the target area for diffusion-based editing. However, most existing solutions obtain these masks via manual operations or off-line processing, greatly reducing their efficiency. In this paper, we propose a novel and efficient image editing method for Text-to-Image (T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In particular, InstDiffEdit aims to employ the cross-modal attention ability of existing diffusion models to achieve instant mask guidance during the diffusion steps. To reduce the noise of attention maps and realize the full automatics, we equip InstDiffEdit with a training-free refinement scheme to adaptively aggregate the attention distributions for the automatic yet accurate mask generation. Meanwhile, to supplement the existing evaluations of DIE, we propose a new benchmark called Editing-Mask to examine the mask accuracy and local editing ability of existing methods. To validate InstDiffEdit, we also conduct extensive experiments on ImageNet and Imagen, and compare it with a bunch of the SOTA methods. The experimental results show that InstDiffEdit not only outperforms the SOTA methods in both image quality and editing results, but also has a much faster inference speed, i.e., +5 to +6 times. Our code available at https://anonymous.4open.science/r/InstDiffEdit-C306/ ",
        "title": "Towards Efficient Diffusion-Based Image Editing with Instant Attention  Masks",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07710",
        "abstract_url": "http://arxiv.org/abs/2401.07710",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Junlin"
            },
            {
                "last_name": "Mannion",
                "first_name": "Patrick"
            },
            {
                "last_name": "Mason",
                "first_name": "Karl"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Reinforcement learning is commonly applied in residential energy management, particularly for optimizing energy costs. However, RL agents often face challenges when dealing with deceptive and sparse rewards in the energy control domain, especially with stochastic rewards. In such situations, thorough exploration becomes crucial for learning an optimal policy. Unfortunately, the exploration mechanism can be misled by deceptive reward signals, making thorough exploration difficult. Go-Explore is a family of algorithms which combines planning methods and reinforcement learning methods to achieve efficient exploration. We use the Go-Explore algorithm to solve the cost-saving task in residential energy management problems and achieve an improvement of up to 19.84\\% compared to the well-known reinforcement learning algorithms. ",
        "title": "Go-Explore for Residential Energy Management",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07711",
        "abstract_url": "http://arxiv.org/abs/2401.07711",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Zerui"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Toshihisa"
            },
            {
                "last_name": "Zhao",
                "first_name": "Qibin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In numerous applications, binary reactions or event counts are observed and stored within high-order tensors. Tensor decompositions (TDs) serve as a powerful tool to handle such high-dimensional and sparse data. However, many traditional TDs are explicitly or implicitly designed based on the Gaussian distribution, which is unsuitable for discrete data. Moreover, most TDs rely on predefined multi-linear structures, such as CP and Tucker formats. Therefore, they may not be effective enough to handle complex real-world datasets. To address these issues, we propose ENTED, an \\underline{E}fficient \\underline{N}onparametric \\underline{TE}nsor \\underline{D}ecomposition for binary and count tensors. Specifically, we first employ a nonparametric Gaussian process (GP) to replace traditional multi-linear structures. Next, we utilize the \\pg augmentation which provides a unified framework to establish conjugate models for binary and count distributions. Finally, to address the computational issue of GPs, we enhance the model by incorporating sparse orthogonal variational inference of inducing points, which offers a more effective covariance approximation within GPs and stochastic natural gradient updates for nonparametric models. We evaluate our model on several real-world tensor completion tasks, considering binary and count datasets. The results manifest both better performance and computational advantages of the proposed model. ",
        "title": "Efficient Nonparametric Tensor Decomposition for Binary and Count Data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07713",
        "abstract_url": "http://arxiv.org/abs/2401.07713",
        "authors": [
            {
                "last_name": "Gast",
                "first_name": "Nicolas"
            },
            {
                "last_name": "van Houdt",
                "first_name": "Benny"
            }
        ],
        "primary_category": "PF",
        "categories": [
            "PF"
        ],
        "abstract": "  As job redundancy has been recognized as an effective means to improve performance of large-scale computer systems, queueing systems with redundancy have been studied by various authors. Existing results include methods to compute the queue length distribution and response time but only when the service discipline is First-Come-First-Served (FCFS). For other service disciplines, such as Processor Sharing (PS), or Last-Come-First-Served (LCFS), only the stability conditions are known. In this paper we develop the first methods to approximate the queue length distribution in a queueing system with redundancy under various service disciplines. We focus on a system with exponential job sizes, i.i.d. copies, and a large number of servers. We first derive a mean field approximation that is independent of the scheduling policy. In order to study the impact of service discipline, we then derive refinements of this approximation to specific scheduling policies. In the case of Processor Sharing, we provide a pair and a triplet approximation. The pair approximation can be regarded as a refinement of the classic mean field approximation and takes the service discipline into account, while the triplet approximation further refines the pair approximation. We also develop a pair approximation for three other service disciplines: First-Come-First-Served, Limited Processor Sharing and Last-Come-First-Served. We present numerical evidence that shows that all the approximations presented in the paper are highly accurate, but that none of them are asymptotically exact (as the number of servers goes to infinity). This makes these approximations suitable to study the impact of the service discipline on the queue length distribution. Our results show that FCFS yields the shortest queue length, and that the differences are more substantial at higher loads. ",
        "title": "Approximations to Study the Impact of the Service Discipline in Systems  with Redundancy",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07717",
        "abstract_url": "http://arxiv.org/abs/2401.07717",
        "authors": [
            {
                "last_name": "Skaperas",
                "first_name": "Sotiris"
            },
            {
                "last_name": "Koukis",
                "first_name": "George"
            },
            {
                "last_name": "Kapetanidou",
                "first_name": "Ioanna Angeliki"
            },
            {
                "last_name": "Tsaoussidis",
                "first_name": "Vassilis"
            },
            {
                "last_name": "Mamatas",
                "first_name": "Lefteris"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Anomaly detection (AD) has been recently employed in the context of edge cloud computing, e.g., for intrusion detection and identification of performance issues. However, state-of-the-art anomaly detection procedures do not systematically consider restrictions and performance requirements inherent to the edge, such as system responsiveness and resource consumption. In this paper, we attempt to investigate the performance of change-point based detectors, i.e., a class of lightweight and accurate AD methods, in relation to the requirements of edge cloud systems. Firstly, we review the theoretical properties of two major categories of change point approaches, i.e., Bayesian and cumulative sum (CUSUM), also discussing their suitability for edge systems. Secondly, we introduce a novel experimental methodology and apply it over two distinct edge cloud test-beds to evaluate the performance of such mechanisms in real-world edge environments. Our experimental results reveal important insights and trade-offs for the applicability and the online performance of the selected change point detectors. ",
        "title": "A Pragmatical Approach to Anomaly Detection Evaluation in Edge Cloud  Systems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07718",
        "abstract_url": "http://arxiv.org/abs/2401.07718",
        "authors": [
            {
                "last_name": "Peshkovskaya",
                "first_name": "Anastasia"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yu-Tao"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC",
            "SI"
        ],
        "abstract": "  In the light of increasing clues on social media impact on self-harm and suicide risks, there is still no evidence on who are and how factually engaged in suicide-related online behaviors. This study reports new findings of high-performance supercomputing investigation of publicly accessible big data sourced from one of the world-largest social networking site. Three-month supercomputer searching resulted in 570,156 young adult users who consumed suicide-related information on social media. Most of them were 21-24 year olds with higher share of females (58%) of predominantly younger age. Every eight user was alarmingly engrossed with up to 15 suicide-related online groups. Evidently, suicide groups on social media are highly underrated public health issue that might weaken the prevention efforts. Suicide prevention strategies that target social media users must be implemented extensively. While major gap in functional understanding of technologies relevance for use in public mental health still exists, current findings act for better understanding digital technologies utility for translational advance and offer relevant evidence-based framework for improving suicide prevention in general population. ",
        "title": "How Social Media Big Data Can Improve Suicide Prevention",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07721",
        "abstract_url": "http://arxiv.org/abs/2401.07721",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Hao"
            },
            {
                "last_name": "Shao",
                "first_name": "Ling"
            },
            {
                "last_name": "Sebe",
                "first_name": "Nicu"
            },
            {
                "last_name": "Van Gool",
                "first_name": "Luc"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a novel graph Transformer generative adversarial network (GTGAN) to learn effective graph node relations in an end-to-end fashion for challenging graph-constrained architectural layout generation tasks. The proposed graph-Transformer-based generator includes a novel graph Transformer encoder that combines graph convolutions and self-attentions in a Transformer to model both local and global interactions across connected and non-connected graph nodes. Specifically, the proposed connected node attention (CNA) and non-connected node attention (NNA) aim to capture the global relations across connected nodes and non-connected nodes in the input graph, respectively. The proposed graph modeling block (GMB) aims to exploit local vertex interactions based on a house layout topology. Moreover, we propose a new node classification-based discriminator to preserve the high-level semantic and discriminative node features for different house components. To maintain the relative spatial relationships between ground truth and predicted graphs, we also propose a novel graph-based cycle-consistency loss. Finally, we propose a novel self-guided pre-training method for graph representation learning. This approach involves simultaneous masking of nodes and edges at an elevated mask ratio (i.e., 40%) and their subsequent reconstruction using an asymmetric graph-centric autoencoder architecture. This method markedly improves the model's learning proficiency and expediency. Experiments on three challenging graph-constrained architectural layout generation tasks (i.e., house layout generation, house roof generation, and building layout generation) with three public datasets demonstrate the effectiveness of the proposed method in terms of objective quantitative scores and subjective visual realism. New state-of-the-art results are established by large margins on these three tasks. ",
        "title": "Graph Transformer GANs with Graph Masked Modeling for Architectural  Layout Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07722",
        "abstract_url": "http://arxiv.org/abs/2401.07722",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Junlin"
            },
            {
                "last_name": "Mannion",
                "first_name": "Patrick"
            },
            {
                "last_name": "Mason",
                "first_name": "Karl"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  It is often challenging for a user to articulate their preferences accurately in multi-objective decision-making problems. Demonstration-based preference inference (DemoPI) is a promising approach to mitigate this problem. Understanding the behaviours and values of energy customers is an example of a scenario where preference inference can be used to gain insights into the values of energy customers with multiple objectives, e.g. cost and comfort. In this work, we applied the state-of-art DemoPI method, i.e., the dynamic weight-based preference inference (DWPI) algorithm in a multi-objective residential energy consumption setting to infer preferences from energy consumption demonstrations by simulated users following a rule-based approach. According to our experimental results, the DWPI model achieves accurate demonstration-based preference inferring in three scenarios. These advancements enhance the usability and effectiveness of multi-objective reinforcement learning (MORL) in energy management, enabling more intuitive and user-friendly preference specifications, and opening the door for DWPI to be applied in real-world settings. ",
        "title": "Inferring Preferences from Demonstrations in Multi-Objective Residential  Energy Management",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07725",
        "abstract_url": "http://arxiv.org/abs/2401.07725",
        "authors": [
            {
                "last_name": "Siddik",
                "first_name": "Md. Abubakar"
            },
            {
                "last_name": "Hasi",
                "first_name": "Most. Anju Ara"
            },
            {
                "last_name": "Islam",
                "first_name": "Md. Rajiul"
            },
            {
                "last_name": "Nitu",
                "first_name": "Jakia Akter"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The recently released IEEE 802.15.6 standard specifies several physical (PHY) layer and medium access control (MAC) layer protocols for variety of medical and non-medical applications of Wireless Body Area Networks (WBAN). The most suitable way for enhancing network performance is to be the choice of different MAC and PHY parameters based on quality of service (QoS) requirements of different applications. The impact of different MAC and PHY parameters on the network performance and the trade-off relationship between the parameters are essential to overcome the limitations of exiting carrier sense multiple access with collision avoidance (CSMA/CA) scheme of IEEE 802.15.6 standard. To address this issue, we develop a Markov chain-based analytical model of IEEE 802.15.6 CSMA/CA for all user priorities (UPs) and apply this general model to different network scenarios to investigate the effects of the packet arrival rate, channel condition, payload size, access phase length, access mechanism and number of nodes on the performance parameters viz. reliability, normalized throughput, energy consumption and average access delay. Moreover, we conclude the effectiveness of different access phases, access mechanisms and user priorities of intra-WBAN. ",
        "title": "Effects Investigation of MAC and PHY Layer Parameters on the Performance  of IEEE 802.15.6 CSMA/CA",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07726",
        "abstract_url": "http://arxiv.org/abs/2401.07726",
        "authors": [
            {
                "last_name": "Garcia",
                "first_name": "Paulo"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "RO"
        ],
        "abstract": "  We evaluate the use of software interpretation to push High Level Synthesis of application-specific accelerators toward a higher level of abstraction. Our methodology is supported by a formal power consumption model that computes the power consumption of accelerator components, accurately predicting the power consumption on new designs from prior optimization estimations. We demonstrate how our approach simplifies the re-use of power optimizations across distinct designs, by leveraging the higher level of design abstraction, using two accelerators representative of the robotics domain, implemented through the Bambu High Level Synthesis tool. Results support the research hypothesis, achieving predictions accurate within +/- 1%. ",
        "title": "Preserving Power Optimizations Across the High Level Synthesis of  Distinct Application-Specific Circuits",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07727",
        "abstract_url": "http://arxiv.org/abs/2401.07727",
        "authors": [
            {
                "last_name": "Mercier",
                "first_name": "Antoine"
            },
            {
                "last_name": "Nakhli",
                "first_name": "Ramin"
            },
            {
                "last_name": "Reddy",
                "first_name": "Mahesh"
            },
            {
                "last_name": "Yasarla",
                "first_name": "Rajeev"
            },
            {
                "last_name": "Cai",
                "first_name": "Hong"
            },
            {
                "last_name": "Porikli",
                "first_name": "Fatih"
            },
            {
                "last_name": "Berger",
                "first_name": "Guillaume"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite the latest remarkable advances in generative modeling, efficient generation of high-quality 3D assets from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of assets, while their 2D counterparts contain billions of text-image pairs. To address this, we propose a novel approach which harnesses the power of large, pretrained 2D diffusion models. More specifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding latent triplane. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization, and can infer high-quality and diverse objects from textual prompts in 7 seconds, offering significantly better quality-to-latency trade-offs when comparing to existing approaches. Furthermore, HexaGen3D demonstrates strong generalization to new objects or compositions. ",
        "title": "HexaGen3D: StableDiffusion is just one step away from Fast and Diverse  Text-to-3D Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07729",
        "abstract_url": "http://arxiv.org/abs/2401.07729",
        "authors": [
            {
                "last_name": "Bhattacharyya",
                "first_name": "Prarthana"
            },
            {
                "last_name": "Huang",
                "first_name": "Chengjie"
            },
            {
                "last_name": "Czarnecki",
                "first_name": "Krzysztof"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  This paper addresses motion forecasting in multi-agent environments, pivotal for ensuring safety of autonomous vehicles. Traditional as well as recent data-driven marginal trajectory prediction methods struggle to properly learn non-linear agent-to-agent interactions. We present SSL-Interactions that proposes pretext tasks to enhance interaction modeling for trajectory prediction. We introduce four interaction-aware pretext tasks to encapsulate various aspects of agent interactions: range gap prediction, closest distance prediction, direction of movement prediction, and type of interaction prediction. We further propose an approach to curate interaction-heavy scenarios from datasets. This curated data has two advantages: it provides a stronger learning signal to the interaction model, and facilitates generation of pseudo-labels for interaction-centric pretext tasks. We also propose three new metrics specifically designed to evaluate predictions in interactive scenes. Our empirical evaluations indicate SSL-Interactions outperforms state-of-the-art motion forecasting methods quantitatively with up to 8% improvement, and qualitatively, for interaction-heavy scenarios. ",
        "title": "SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07732",
        "abstract_url": "http://arxiv.org/abs/2401.07732",
        "authors": [
            {
                "last_name": "Fournier",
                "first_name": "Ga\u00ebtan"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Marc"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We study a variant of the Hotelling-Downs model of spatial competition between firms where consumer choices are influenced by their individual preferences as well as the popularity of the firms. In general, a multiplicity of market equilibria might exist due to the popularity effect. To elucidate firm decision-making, we explore three distinct behavioral attitudes towards this multiplicity of equilibria: optimistic, neutral, and pessimistic. For each behavior, we characterize the set of Nash equilibria and measure the impact of the selfish behavior on the social welfare by means of the price of anarchy and price of stability. ",
        "title": "Popularity in location games",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07733",
        "abstract_url": "http://arxiv.org/abs/2401.07733",
        "authors": [
            {
                "last_name": "Jaber",
                "first_name": "Edgar"
            },
            {
                "last_name": "Blot",
                "first_name": "Vincent"
            },
            {
                "last_name": "Brunel",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Chabridon",
                "first_name": "Vincent"
            },
            {
                "last_name": "Remy",
                "first_name": "Emmanuel"
            },
            {
                "last_name": "Iooss",
                "first_name": "Bertrand"
            },
            {
                "last_name": "Lucor",
                "first_name": "Didier"
            },
            {
                "last_name": "Mougeot",
                "first_name": "Mathilde"
            },
            {
                "last_name": "Leite",
                "first_name": "Alessandro"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Gaussian processes (GPs) are a Bayesian machine learning approach widely used to construct surrogate models for the uncertainty quantification of computer simulation codes in industrial applications. It provides both a mean predictor and an estimate of the posterior prediction variance, the latter being used to produce Bayesian credibility intervals. Interpreting these intervals relies on the Gaussianity of the simulation model as well as the well-specification of the priors which are not always appropriate. We propose to address this issue with the help of conformal prediction. In the present work, a method for building adaptive cross-conformal prediction intervals is proposed by weighting the non-conformity score with the posterior standard deviation of the GP. The resulting conformal prediction intervals exhibit a level of adaptivity akin to Bayesian credibility sets and display a significant correlation with the surrogate model local approximation error, while being free from the underlying model assumptions and having frequentist coverage guarantees. These estimators can thus be used for evaluating the quality of a GP surrogate model and can assist a decision-maker in the choice of the best prior for the specific application of the GP. The performance of the method is illustrated through a panel of numerical examples based on various reference databases. Moreover, the potential applicability of the method is demonstrated in the context of surrogate modeling of an expensive-to-evaluate simulator of the clogging phenomenon in steam generators of nuclear reactors. ",
        "title": "Conformal Approach To Gaussian Process Surrogate Evaluation With  Coverage Guarantees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07743",
        "abstract_url": "http://arxiv.org/abs/2401.07743",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Membrane systems are a biologically-inspired computational model based on the structure of biological cells and the way chemicals interact and traverse their membranes. Although their dynamics are described by rules, encoding membrane systems into rewriting logic is not straightforward due to its complex control mechanisms. Multiple alternatives have been proposed in the literature and implemented in the Maude specification language. The recent release of the Maude strategy language and its associated strategy-aware model checker allow specifying these systems more easily, so that they become executable and verifiable for free. An easily-extensible interactive environment transforms membrane specifications into rewrite theories controlled by appropriate strategies, and allows simulating and verifying membrane computations by means of them. ",
        "title": "Simulating and model checking membrane systems using strategies in Maude",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07744",
        "abstract_url": "http://arxiv.org/abs/2401.07744",
        "authors": [
            {
                "last_name": "Ghidalia",
                "first_name": "Sarah"
            },
            {
                "last_name": "Narsis",
                "first_name": "Ouassila Labbani"
            },
            {
                "last_name": "Bertaux",
                "first_name": "Aur\u00e9lie"
            },
            {
                "last_name": "Nicolle",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Motivated by the desire to explore the process of combining inductive and deductive reasoning, we conducted a systematic literature review of articles that investigate the integration of machine learning and ontologies. The objective was to identify diverse techniques that incorporate both inductive reasoning (performed by machine learning) and deductive reasoning (performed by ontologies) into artificial intelligence systems. Our review, which included the analysis of 128 studies, allowed us to identify three main categories of hybridization between machine learning and ontologies: learning-enhanced ontologies, semantic data mining, and learning and reasoning systems. We provide a comprehensive examination of all these categories, emphasizing the various machine learning algorithms utilized in the studies. Furthermore, we compared our classification with similar recent work in the field of hybrid AI and neuro-symbolic approaches. ",
        "title": "Combining Machine Learning and Ontology: A Systematic Literature Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07745",
        "abstract_url": "http://arxiv.org/abs/2401.07745",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Mi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jiazhao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Yan"
            },
            {
                "last_name": "Wang",
                "first_name": "He"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Open-vocabulary 3D instance segmentation has emerged as a frontier topic due to its capability to segment 3D instances beyond a predefined set of categories. However, compared to significant progress in the 2D domain, methods for 3D open-vocabulary instance segmentation are hindered by the limited scale of high-quality annotated 3D data. To harness the capabilities of 2D models, recent efforts have focused on merging 2D masks based on metrics such as geometric and semantic similarity to form 3D instances. In contrast to these local metrics, we propose a novel metric called view consensus to better exploit multi-view observation. The key insight is that two 2D masks should be considered as belonging to the same instance if a considerable number of other 2D masks from other views contain both these two masks. Based on this metric, we build a global mask graph and iteratively cluster masks, prioritizing mask pairs with solid view consensus. The corresponding 3D points cluster of these 2D mask clusters can be regarded as 3D instances, along with the fused open-vocabulary features from clustered 2D masks. Through this multi-view verification and fusion mechanism, our method effectively leverages the prior instance knowledge from massive 2D masks predicted by visual foundation models, eliminating the need for training on 3D data. Experiments on publicly available datasets, including ScanNet200 and MatterPort3D, demonstrate that our method achieves state-of-the-art performance in both open-vocabulary instance segmentation and class-agnostic mask generation. Our project page is at https://pku-epic.github.io/MaskClustering. ",
        "title": "MaskClustering: View Consensus based Mask Graph Clustering for  Open-Vocabulary 3D Instance Segmentation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07746",
        "abstract_url": "http://arxiv.org/abs/2401.07746",
        "authors": [
            {
                "last_name": "Valera",
                "first_name": "Patris"
            },
            {
                "last_name": "Vizca\u00edno",
                "first_name": "Josu\u00e9 Page"
            },
            {
                "last_name": "Lasser",
                "first_name": "Tobias"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single-molecule localization microscopy techniques, like stochastic optical reconstruction microscopy (STORM), visualize biological specimens by stochastically exciting sparse blinking emitters. The raw images suffer from unwanted background fluorescence, which must be removed to achieve super-resolution. We introduce a sparsity-based background removal method by adapting a neural network (SLNet) from a different microscopy domain. The SLNet computes a low-rank representation of the images, and then, by subtracting it from the raw images, the sparse component is computed, representing the frames without the background. We compared our approach with widely used background removal methods, such as the median background removal or the rolling ball algorithm, on two commonly used STORM datasets, one glial cell, and one microtubule dataset. The SLNet delivers STORM frames with less background, leading to higher emitters' localization precision and higher-resolution reconstructed images than commonly used methods. Notably, the SLNet is lightweight and easily trainable (<5 min). Since it is trained in an unsupervised manner, no prior information is required and can be applied to any STORM dataset. We uploaded a pre-trained SLNet to the Bioimage model zoo, easily accessible through ImageJ. Our results show that our sparse decomposition method could be an essential and efficient STORM pre-processing tool. ",
        "title": "Sparsity-based background removal for STORM super-resolution images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07749",
        "abstract_url": "http://arxiv.org/abs/2401.07749",
        "authors": [
            {
                "last_name": "Rubio",
                "first_name": "Rub\u00e9n"
            },
            {
                "last_name": "Mart\u00ed-Oliet",
                "first_name": "Narciso"
            },
            {
                "last_name": "Pita",
                "first_name": "Isabel"
            },
            {
                "last_name": "Verdejo",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  In the reflective Maude specification language, based on rewriting logic, a strategy language has been introduced to control rule rewriting while avoiding complex and verbose metalevel programs. However, just as multiple levels of reflection are required for some metaprogramming tasks, reflective manipulation and generation of strategies are convenient in multiple situations. Some examples of reflective strategy transformations are presented, which implement special forms of evaluation or extend the strategy language while preserving its advantages. ",
        "title": "Metalevel transformation of strategies",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07751",
        "abstract_url": "http://arxiv.org/abs/2401.07751",
        "authors": [
            {
                "last_name": "Ruiz-Perez",
                "first_name": "Marina"
            },
            {
                "last_name": "Morell-Ortega",
                "first_name": "Sergio"
            },
            {
                "last_name": "Gadea",
                "first_name": "Marien"
            },
            {
                "last_name": "Vivo-Hernando",
                "first_name": "Roberto"
            },
            {
                "last_name": "Rubio",
                "first_name": "Gregorio"
            },
            {
                "last_name": "Aparici",
                "first_name": "Fernando"
            },
            {
                "last_name": "de la Iglesia-Vaya",
                "first_name": "Mariam"
            },
            {
                "last_name": "Tourdias",
                "first_name": "Thomas"
            },
            {
                "last_name": "Coup\u00e9",
                "first_name": "Pierrick"
            },
            {
                "last_name": "Manj\u00f3n",
                "first_name": "Jos\u00e9 V."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The implication of the thalamus in multiple neurological pathologies makes it a structure of interest for volumetric analysis. In the present work, we have designed and implemented a multimodal volumetric deep neural network for the segmentation of thalamic nuclei at ultra-high resolution (0.125 mm3). Current tools either operate at standard resolution (1 mm3) or use monomodal data. To achieve the proposed objective, first, a database of semiautomatically segmented thalamic nuclei was created using ultra-high resolution T1, T2 and White Matter nulled (WMn) images. Then, a novel Deep learning based strategy was designed to obtain the automatic segmentations and trained to improve its robustness and accuaracy using a semisupervised approach. The proposed method was compared with a related state-of-the-art method showing competitive results both in terms of segmentation quality and efficiency. To make the proposed method fully available to the scientific community, a full pipeline able to work with monomodal standard resolution T1 images is also proposed. ",
        "title": "DeepThalamus: A novel deep learning method for automatic segmentation of  brain thalamic nuclei from multimodal ultra-high resolution MRI",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07753",
        "abstract_url": "http://arxiv.org/abs/2401.07753",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Minghua"
            },
            {
                "last_name": "Qin",
                "first_name": "Xiangdong"
            },
            {
                "last_name": "Du",
                "first_name": "Shuangli"
            },
            {
                "last_name": "Bai",
                "first_name": "Xuefei"
            },
            {
                "last_name": "Lyu",
                "first_name": "Jiahao"
            },
            {
                "last_name": "Liu",
                "first_name": "Yiguang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unlike single image task, stereo image enhancement can use another view information, and its key stage is how to perform cross-view feature interaction to extract useful information from another view. However, complex noise in low-light image and its impact on subsequent feature encoding and interaction are ignored by the existing methods. In this paper, a method is proposed to perform enhancement and de-noising simultaneously. First, to reduce unwanted noise interference, a low-frequency information enhanced module (IEM) is proposed to suppress noise and produce a new image space. Additionally, a cross-channel and spatial context information mining module (CSM) is proposed to encode long-range spatial dependencies and to enhance inter-channel feature interaction. Relying on CSM, an encoder-decoder structure is constructed, incorporating cross-view and cross-scale feature interactions to perform enhancement in the new image space. Finally, the network is trained with the constraints of both spatial and frequency domain losses. Extensive experiments on both synthesized and real datasets show that our method obtains better detail recovery and noise removal compared with state-of-the-art methods. In addition, a real stereo image enhancement dataset is captured with stereo camera ZED2. The code and dataset are publicly available at: https://www.github.com/noportraits/LFENet. ",
        "title": "Low-light Stereo Image Enhancement and De-noising in the Low-frequency  Information Enhanced Image Space",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07756",
        "abstract_url": "http://arxiv.org/abs/2401.07756",
        "authors": [
            {
                "last_name": "Marnissi",
                "first_name": "Ouiame"
            },
            {
                "last_name": "Hammouti",
                "first_name": "Hajar EL"
            },
            {
                "last_name": "Bergou",
                "first_name": "El Houcine"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we study the performance of federated learning over wireless networks, where devices with a limited energy budget train a machine learning model. The federated learning performance depends on the selection of the clients participating in the learning at each round. Most existing studies suggest deterministic approaches for the client selection, resulting in challenging optimization problems that are usually solved using heuristics, and therefore without guarantees on the quality of the final solution. We formulate a new probabilistic approach to jointly select clients and allocate power optimally so that the expected number of participating clients is maximized. To solve the problem, a new alternating algorithm is proposed, where at each step, the closed-form solutions for user selection probabilities and power allocations are obtained. Our numerical results show that the proposed approach achieves a significant performance in terms of energy consumption, completion time and accuracy as compared to the studied benchmarks. ",
        "title": "Joint Probability Selection and Power Allocation for Federated Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07760",
        "abstract_url": "http://arxiv.org/abs/2401.07760",
        "authors": [
            {
                "last_name": "Ghaddar",
                "first_name": "Abbas"
            },
            {
                "last_name": "Langlais",
                "first_name": "Philippe"
            },
            {
                "last_name": "Rezagholizadeh",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Chen",
                "first_name": "Boxing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Pretraining monolingual language models have been proven to be vital for performance in Arabic Natural Language Processing (NLP) tasks. In this paper, we conduct a comprehensive study on the role of data in Arabic Pretrained Language Models (PLMs). More precisely, we reassess the performance of a suite of state-of-the-art Arabic PLMs by retraining them on massive-scale, high-quality Arabic corpora. We have significantly improved the performance of the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in their respective model categories. In addition, our analysis strongly suggests that pretraining data by far is the primary contributor to performance, surpassing other factors. Our models and source code are publicly available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch. ",
        "title": "On the importance of Data Scale in Pretraining Arabic Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07761",
        "abstract_url": "http://arxiv.org/abs/2401.07761",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Zhaohong"
            },
            {
                "last_name": "Yamada",
                "first_name": "Naoyuki"
            },
            {
                "last_name": "Takenami",
                "first_name": "Yoshihiro"
            },
            {
                "last_name": "Moriwaki",
                "first_name": "Daisuke"
            },
            {
                "last_name": "Yokoo",
                "first_name": "Makoto"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We study a practical two-sided matching problem of allocating children to daycare centers, which has significant social implications. We are cooperating with several municipalities in Japan and our goal is to devise a reliable and trustworthy clearing algorithm to deal with the problem. In this paper, we describe the design of our new algorithm that minimizes the number of unmatched children while ensuring stability. We evaluate our algorithm using real-life data sets, and experimental results demonstrate that our algorithm surpasses the commercial software that currently dominates the market in terms of both the number of matched children and the number of blocking coalitions (measuring stability). Our findings have been reported to local governments, and some are considering adopting our proposed algorithm in the near future, instead of the existing solution. Moreover, our model and algorithm have broader applicability to other important matching markets, such as hospital-doctor matching with couples and school choice with siblings. ",
        "title": "Stable Matchings in Practice: A Constraint Programming Approach",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07762",
        "abstract_url": "http://arxiv.org/abs/2401.07762",
        "authors": [
            {
                "last_name": "Ying",
                "first_name": "Jun"
            },
            {
                "last_name": "Dong",
                "first_name": "Xin"
            },
            {
                "last_name": "Li",
                "first_name": "Bowei"
            },
            {
                "last_name": "Tian",
                "first_name": "Zihan"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Traffic flow prediction is widely used in travel decision making, traffic control, roadway system planning, business sectors, and government agencies. ARX models have proved to be highly effective and versatile. In this research, we investigated the applications of ARX models in prediction for real traffic flow in New York City. The ARX models were constructed by linear/polynomial or neural networks. Comparative studies were carried out based on the results by the efficiency, accuracy, and training computational demand of the algorithms. ",
        "title": "Auto-Regressive Model with Exogenous Input--ARX--based traffic-flow  prediction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07764",
        "abstract_url": "http://arxiv.org/abs/2401.07764",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Minrui"
            },
            {
                "last_name": "Dusit",
                "first_name": "Niyato"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zehui"
            },
            {
                "last_name": "Mao",
                "first_name": "Shiwen"
            },
            {
                "last_name": "Han",
                "first_name": "Zhu"
            },
            {
                "last_name": "Kim",
                "first_name": "Dong In"
            },
            {
                "last_name": "Letaief",
                "first_name": "Khaled B."
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents. ",
        "title": "When Large Language Model Agents Meet 6G Networks: Perception,  Grounding, and Alignment",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07768",
        "abstract_url": "http://arxiv.org/abs/2401.07768",
        "authors": [
            {
                "last_name": "Kudo",
                "first_name": "Momonari"
            },
            {
                "last_name": "Yokoyama",
                "first_name": "Kazuhiro"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC"
        ],
        "abstract": "  Gr\\\"{o}bner bases are nowadays central tools for solving various problems in commutative algebra and algebraic geometry. A typical use of Gr\\\"{o}bner bases is the multivariate polynomial system solving, which enables us to construct algebraic attacks against post-quantum cryptographic protocols. Therefore, the determination of the complexity of computing Gr\\\"{o}bner bases is very important both in theory and in practice: One of the most important cases is the case where input polynomials compose an (overdetermined) affine semi-regular sequence. The first part of this paper aims to present a survey on the Gr\\\"{o}bner basis computation and its complexity. In the second part, we shall give an explicit formula on the (truncated) Hilbert-Poincar\\'{e} series associated to the homogenization of an affine semi-regular sequence. Based on the formula, we also study (reduced) Gr\\\"{o}bner bases of the ideals generated by an affine semi-regular sequence and its homogenization. Some of our results are considered to give mathematically rigorous proofs of the correctness of methods for computing Gr\\\"{o}bner bases of the ideal generated by an affine semi-regular sequence. ",
        "title": "On Hilbert-Poincar\\'{e} series of affine semi-regular polynomial  sequences and related Gr\\\"{o}bner bases",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07769",
        "abstract_url": "http://arxiv.org/abs/2401.07769",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Zhibo"
            },
            {
                "last_name": "Yang",
                "first_name": "Luwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tao"
            },
            {
                "last_name": "Jiang",
                "first_name": "Wen"
            },
            {
                "last_name": "Ning",
                "first_name": "Wei"
            },
            {
                "last_name": "Yang",
                "first_name": "Yujiu"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "LG"
        ],
        "abstract": "  The recommendation has been playing a key role in many industries, e.g., e-commerce, streaming media, social media, etc. Recently, a new recommendation scenario, called Trigger-Induced Recommendation (TIR), where users are able to explicitly express their instant interests via trigger items, is emerging as an essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon. Without explicitly modeling the user's instant interest, traditional recommendation methods usually obtain sub-optimal results in TIR. Even though there are a few methods considering the trigger and target items simultaneously to solve this problem, they still haven't taken into account temporal information of user behaviors, the dynamic change of user instant interest when the user scrolls down and the interactions between the trigger and target items. To tackle these problems, we propose a novel method -- Deep Evolutional Instant Interest Network (DEI2N), for click-through rate prediction in TIR scenarios. Specifically, we design a User Instant Interest Modeling Layer to predict the dynamic change of the intensity of instant interest when the user scrolls down. Temporal information is utilized in user behavior modeling. Moreover, an Interaction Layer is introduced to learn better interactions between the trigger and target items. We evaluate our method on several offline and real-world industrial datasets. Experimental results show that our proposed DEI2N outperforms state-of-the-art baselines. In addition, online A/B testing demonstrates the superiority over the existing baseline in real-world production environments. ",
        "title": "Deep Evolutional Instant Interest Network for CTR Prediction in  Trigger-Induced Recommendation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07770",
        "abstract_url": "http://arxiv.org/abs/2401.07770",
        "authors": [
            {
                "last_name": "Ramrakhya",
                "first_name": "Ram"
            },
            {
                "last_name": "Kembhavi",
                "first_name": "Aniruddha"
            },
            {
                "last_name": "Batra",
                "first_name": "Dhruv"
            },
            {
                "last_name": "Kira",
                "first_name": "Zsolt"
            },
            {
                "last_name": "Zeng",
                "first_name": "Kuo-Hao"
            },
            {
                "last_name": "Weihs",
                "first_name": "Luca"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Computer vision tasks typically involve describing what is present in an image (e.g. classification, detection, segmentation, and captioning). We study a visual common sense task that requires understanding what is not present. Specifically, given an image (e.g. of a living room) and name of an object (\"cushion\"), a vision system is asked to predict semantically-meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely be placed by humans (e.g. on the sofa). We call this task: Semantic Placement (SP) and believe that such common-sense visual understanding is critical for assitive robots (tidying a house), and AR devices (automatically rendering an object in the user's space). Studying the invisible is hard. Datasets for image description are typically constructed by curating relevant images and asking humans to annotate the contents of the image; neither of those two steps are straightforward for objects not present in the image. We overcome this challenge by operating in the opposite direction: we start with an image of an object in context from web, and then remove that object from the image via inpainting. This automated pipeline converts unstructured web data into a dataset comprising pairs of images with/without the object. Using this, we collect a novel dataset, with ${\\sim}1.3$M images across $9$ object categories, and train a SP prediction model called CLIP-UNet. CLIP-UNet outperforms existing VLMs and baselines that combine semantic priors with object detectors on real-world and simulated images. In our user studies, we find that the SP masks predicted by CLIP-UNet are favored $43.7\\%$ and $31.3\\%$ times when comparing against the $4$ SP baselines on real and simulated images. In addition, we demonstrate leveraging SP mask predictions from CLIP-UNet enables downstream applications like building tidying robots in indoor environments. ",
        "title": "Seeing the Unseen: Visual Common Sense for Semantic Placement",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07774",
        "abstract_url": "http://arxiv.org/abs/2401.07774",
        "authors": [
            {
                "last_name": "Franco",
                "first_name": "Nicola"
            },
            {
                "last_name": "Sakhnenko",
                "first_name": "Alona"
            },
            {
                "last_name": "Stolpmann",
                "first_name": "Leon"
            },
            {
                "last_name": "Thuerck",
                "first_name": "Daniel"
            },
            {
                "last_name": "Petsch",
                "first_name": "Fabian"
            },
            {
                "last_name": "R\u00fcll",
                "first_name": "Annika"
            },
            {
                "last_name": "Lorenz",
                "first_name": "Jeanette Miriam"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Quantum Machine Learning (QML) has emerged as a promising intersection of quantum computing and classical machine learning, anticipated to drive breakthroughs in computational tasks. This paper discusses the question which security concerns and strengths are connected to QML by means of a systematic literature review. We categorize and review the security of QML models, their vulnerabilities inherent to quantum architectures, and the mitigation strategies proposed. The survey reveals that while QML possesses unique strengths, it also introduces novel attack vectors not seen in classical systems. Techniques like adversarial training, quantum noise exploitation, and quantum differential privacy have shown potential in enhancing QML robustness. Our review discuss the need for continued and rigorous research to ensure the secure deployment of QML in real-world applications. This work serves as a foundational reference for researchers and practitioners aiming to navigate the security aspects of QML. ",
        "title": "Predominant Aspects on Security for Quantum Machine Learning: Literature  Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07776",
        "abstract_url": "http://arxiv.org/abs/2401.07776",
        "authors": [
            {
                "last_name": "Aubian",
                "first_name": "Guillaume"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  The clique number of a tournament is the maximum clique number of a graph formed by keeping backwards arcs in an ordering of its vertices. We study the time complexity of computing the clique number of a tournament and prove that, for any integer $k \\geq 3$, deciding whether a tournament has clique number at most $k$ is NP-complete. This answers an interrogation of Nguyen, Scott and Seymour. To do so, we make use of a construction which we then modify to provide a counterexample to a conjecture of Aboulker, Aubian, Charbit and Lopes. ",
        "title": "Computing the clique number of tournaments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07777",
        "abstract_url": "http://arxiv.org/abs/2401.07777",
        "authors": [
            {
                "last_name": "Buonaiuto",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Guarasci",
                "first_name": "Raffaele"
            },
            {
                "last_name": "Minutolo",
                "first_name": "Aniello"
            },
            {
                "last_name": "De Pietro",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Esposito",
                "first_name": "Massimo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Hybrid quantum-classical classifiers promise to positively impact critical aspects of natural language processing tasks, particularly classification-related ones. Among the possibilities currently investigated, quantum transfer learning, i.e., using a quantum circuit for fine-tuning pre-trained classical models for a specific task, is attracting significant attention as a potential platform for proving quantum advantage.   This work shows potential advantages, both in terms of performance and expressiveness, of quantum transfer learning algorithms trained on embedding vectors extracted from a large language model to perform classification on a classical Linguistics task: acceptability judgments. Acceptability judgment is the ability to determine whether a sentence is considered natural and well-formed by a native speaker. The approach has been tested on sentences extracted from ItaCoLa, a corpus that collects Italian sentences labeled with their acceptability judgment. The evaluation phase shows results for the quantum transfer learning pipeline comparable to state-of-the-art classical transfer learning algorithms, proving current quantum computers' capabilities to tackle NLP tasks for ready-to-use applications. Furthermore, a qualitative linguistic analysis, aided by explainable AI methods, reveals the capabilities of quantum transfer learning algorithms to correctly classify complex and more structured sentences, compared to their classical counterpart. This finding sets the ground for a quantifiable quantum advantage in NLP in the near future. ",
        "title": "Quantum Transfer Learning for Acceptability Judgements",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07780",
        "abstract_url": "http://arxiv.org/abs/2401.07780",
        "authors": [
            {
                "last_name": "Chatzikiriakos",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Wabersich",
                "first_name": "Kim P."
            },
            {
                "last_name": "Berkel",
                "first_name": "Felix"
            },
            {
                "last_name": "Pauli",
                "first_name": "Patricia"
            },
            {
                "last_name": "Iannelli",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Model Predictive Control (MPC) can be applied to safety-critical control problems, providing closed-loop safety and performance guarantees. Implementation of MPC controllers requires solving an optimization problem at every sampling instant, which is challenging to execute on embedded hardware. To address this challenge, we propose a framework that combines a tightened soft constrained MPC formulation with supervised learning to approximate the MPC value function. This combination enables us to obtain a corresponding optimal control law, which can be implemented efficiently on embedded platforms. The framework ensures stability and constraint satisfaction for various nonlinear systems. While the design effort is similar to that of nominal MPC, the proposed formulation provides input-to-state stability (ISS) with respect to the approximation error of the value function. Furthermore, we prove that the value function corresponding to the soft constrained MPC problem is Lipschitz continuous for Lipschitz continuous systems, even if the optimal control law may be discontinuous. This serves two purposes: First, it allows to relate approximation errors to a sufficiently large constraint tightening to obtain constraint satisfaction guarantees. Second, it paves the way for an efficient supervised learning procedure to obtain a continuous value function approximation. We demonstrate the effectiveness of the method using a nonlinear numerical example. ",
        "title": "Learning Soft Constrained MPC Value Functions: Efficient MPC Design and  Implementation providing Stability and Safety Guarantees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07781",
        "abstract_url": "http://arxiv.org/abs/2401.07781",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jay Zhangjie"
            },
            {
                "last_name": "Fang",
                "first_name": "Guian"
            },
            {
                "last_name": "Wu",
                "first_name": "Haoning"
            },
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "Ge",
                "first_name": "Yixiao"
            },
            {
                "last_name": "Cun",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Zhang",
                "first_name": "David Junhao"
            },
            {
                "last_name": "Liu",
                "first_name": "Jia-Wei"
            },
            {
                "last_name": "Gu",
                "first_name": "Yuchao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Rui"
            },
            {
                "last_name": "Lin",
                "first_name": "Weisi"
            },
            {
                "last_name": "Hsu",
                "first_name": "Wynne"
            },
            {
                "last_name": "Shan",
                "first_name": "Ying"
            },
            {
                "last_name": "Shou",
                "first_name": "Mike Zheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Generative models have demonstrated remarkable capability in synthesizing high-quality text, images, and videos. For video generation, contemporary text-to-video models exhibit impressive capabilities, crafting visually stunning videos. Nonetheless, evaluating such videos poses significant challenges. Current research predominantly employs automated metrics such as FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis, particularly in the temporal assessment of video content, thus rendering them unreliable indicators of true video quality. Furthermore, while user studies have the potential to reflect human perception accurately, they are hampered by their time-intensive and laborious nature, with outcomes that are often tainted by subjective bias. In this paper, we investigate the limitations inherent in existing metrics and introduce a novel evaluation pipeline, the Text-to-Video Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video Alignment, which scrutinizes the fidelity of the video in representing the given text description, and (2) Video Quality, which evaluates the video's overall production caliber with a mixture of experts. Moreover, to evaluate the proposed metrics and facilitate future improvements on them, we present the TVGE dataset, collecting human judgements of 2,543 text-to-video generated videos on the two criteria. Experiments on the TVGE dataset demonstrate the superiority of the proposed T2VScore on offering a better metric for text-to-video generation. ",
        "title": "Towards A Better Metric for Text-to-Video Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07782",
        "abstract_url": "http://arxiv.org/abs/2401.07782",
        "authors": [
            {
                "last_name": "Hackstein",
                "first_name": "Jakob"
            },
            {
                "last_name": "Sumbul",
                "first_name": "Gencer"
            },
            {
                "last_name": "Clasen",
                "first_name": "Kai Norman"
            },
            {
                "last_name": "Demir",
                "first_name": "Beg\u00fcm"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Self-supervised learning through masked autoencoders (MAEs) has recently attracted great attention for remote sensing (RS) image representation learning, and thus embodies a significant potential for content-based image retrieval (CBIR) from ever-growing RS image archives. However, the existing studies on MAEs in RS assume that the considered RS images are acquired by a single image sensor, and thus are only suitable for uni-modal CBIR problems. The effectiveness of MAEs for cross-sensor CBIR, which aims to search semantically similar images across different image modalities, has not been explored yet. In this paper, we take the first step to explore the effectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a systematic overview on the possible adaptations of the vanilla MAE to exploit masked image modeling on multi-sensor RS image archives (denoted as cross-sensor masked autoencoders [CSMAEs]). Based on different adjustments applied to the vanilla MAE, we introduce different CSMAE models. We also provide an extensive experimental analysis of these CSMAE models. We finally derive a guideline to exploit masked image modeling for uni-modal and cross-modal CBIR problems in RS. The code of this work is publicly available at https://github.com/jakhac/CSMAE. ",
        "title": "Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in  Remote Sensing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07783",
        "abstract_url": "http://arxiv.org/abs/2401.07783",
        "authors": [
            {
                "last_name": "Giaretta",
                "first_name": "Alberto"
            },
            {
                "last_name": "Loutfi",
                "first_name": "Amy"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "RO"
        ],
        "abstract": "  Modern robots are stepping away from monolithic entities built using ad-hoc sensors and actuators, due to new technologies and communication paradigms, such as the Internet of Things (IoT) and the Robotic Operating System (ROS). Using such paradigms, robots can be built by acquiring heterogeneous standard devices and putting them in communication with each other. This approach brings high degrees of modularity, but it also yields uncertainty of providing cybersecurity assurances, and guarantees on the integrity of the embodiment. In this paper, we first illustrate how cyberattacks on different devices can have radically different consequences on the robot's ability to complete its tasks and preserve its embodiment. We also claim that modern robots should have self-awareness for what it concerns such aspects, and formulate the different characteristics that robots should integrate for doing so. Then, we show that achieving these propositions requires that robots possess at least three properties that conceptually link devices and tasks. Last, we reflect on how these three properties could be achieved in a larger conceptual framework. ",
        "title": "Cybersecurity and Embodiment Integrity for Modern Robots: A Conceptual  Framework",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07784",
        "abstract_url": "http://arxiv.org/abs/2401.07784",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yingjian"
            },
            {
                "last_name": "Wen",
                "first_name": "Xiangyong"
            },
            {
                "last_name": "Gao",
                "first_name": "Fei"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Bearing measurements,as the most common modality in nature, have recently gained traction in multi-robot systems to enhance mutual localization and swarm collaboration. Despite their advantages, challenges such as sensory noise, obstacle occlusion, and uncoordinated swarm motion persist in real-world scenarios, potentially leading to erroneous state estimation and undermining the system's flexibility, practicality, and robustness.In response to these challenges, in this paper we address theoretical and practical problem related to both mutual localization and swarm planning.Firstly, we propose a certifiable mutual localization algorithm.It features a concise problem formulation coupled with lossless convex relaxation, enabling independence from initial values and globally optimal relative pose recovery.Then, to explore how detection noise and swarm motion influence estimation optimality, we conduct a comprehensive analysis on the interplay between robots' mutual spatial relationship and mutual localization. We develop a differentiable metric correlated with swarm trajectories to explicitly evaluate the noise resistance of optimal estimation.By establishing a finite and pre-computable threshold for this metric and accordingly generating swarm trajectories, the estimation optimality can be strictly guaranteed under arbitrary noise. Based on these findings, an optimization-based swarm planner is proposed to generate safe and smooth trajectories, with consideration of both inter-robot visibility and estimation optimality.Through numerical simulations, we evaluate the optimality and certifiablity of our estimator, and underscore the significance of our planner in enhancing estimation performance.The results exhibit considerable potential of our methods to pave the way for advanced closed-loop intelligence in swarm systems. ",
        "title": "Certifiable Mutual Localization and Trajectory Planning for  Bearing-Based Robot Swarm",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07787",
        "abstract_url": "http://arxiv.org/abs/2401.07787",
        "authors": [
            {
                "last_name": "Fleischhacker",
                "first_name": "David"
            },
            {
                "last_name": "Goederle",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Kern",
                "first_name": "Roman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This paper addresses a major challenge to historical research on the 19th century. Large quantities of sources have become digitally available for the first time, while extraction techniques are lagging behind. Therefore, we researched machine learning (ML) models to recognise and extract complex data structures in a high-value historical primary source, the Schematismus. It records every single person in the Habsburg civil service above a certain hierarchical level between 1702 and 1918 and documents the genesis of the central administration over two centuries. Its complex and intricate structure as well as its enormous size have so far made any more comprehensive analysis of the administrative and social structure of the later Habsburg Empire on the basis of this source impossible. We pursued two central objectives: Primarily, the improvement of the OCR quality, for which we considered an improved structure recognition to be essential; in the further course, it turned out that this also made the extraction of the data structure possible. We chose Faster R-CNN as base for the ML architecture for structure recognition. In order to obtain the required amount of training data quickly and economically, we synthesised Hof- und Staatsschematismus-style data, which we used to train our model. The model was then fine-tuned with a smaller set of manually annotated historical source data. We then used Tesseract-OCR, which was further optimised for the style of our documents, to complete the combined structure extraction and OCR process. Results show a significant decrease in the two standard parameters of OCR-performance, WER and CER (where lower values are better). Combined structure detection and fine-tuned OCR improved CER and WER values by remarkable 71.98 percent (CER) respectively 52.49 percent (WER). ",
        "title": "Improving OCR Quality in 19th Century Historical Documents Using a  Combined Machine Learning Based Approach",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07788",
        "abstract_url": "http://arxiv.org/abs/2401.07788",
        "authors": [
            {
                "last_name": "Rudakov",
                "first_name": "Mikhail"
            },
            {
                "last_name": "Beznosikov",
                "first_name": "Aleksandr"
            },
            {
                "last_name": "Kholodov",
                "first_name": "Yaroslav"
            },
            {
                "last_name": "Gasnikov",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\\%$ is the lowest TopK compression level, which does not harm model convergence severely. Experiments also show that models trained with TopK perform well only when compression is also applied during inference. We find that error feedback techniques do not improve model-parallel training compared to plain compression, but allow model inference without compression with almost no quality drop. Finally, when applied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens model performance significantly. ",
        "title": "Activations and Gradients Compression for Model-Parallel Training",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07793",
        "abstract_url": "http://arxiv.org/abs/2401.07793",
        "authors": [
            {
                "last_name": "Shao",
                "first_name": "Ninglu"
            },
            {
                "last_name": "Xiao",
                "first_name": "Shitao"
            },
            {
                "last_name": "Liu",
                "first_name": "Zheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Peitian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models (LLMs) are in need of sufficient contexts to handle many critical applications, such as retrieval augmented generation and few-shot learning. However, due to the constrained window size, the LLMs can only access to the information within a limited context. Although the size of context window can be extended by fine-tuning, it will result in a substantial cost in both training and inference stage. In this paper, we present Extensible Tokenization as an alternative method which realizes the flexible scaling of LLMs' context. Extensible Tokenization stands as a midware in between of the tokenized context and the LLM, which transforms the raw token embeddings into the extensible embeddings. Such embeddings provide a more compact representation for the long context, on top of which the LLM is able to perceive more information with the same context window. Extensible Tokenization is also featured by its flexibility: the scaling factor can be flexibly determined within a feasible scope, leading to the extension of an arbitrary context length at the inference time. Besides, Extensible Tokenization is introduced as a drop-in component, which can be seamlessly plugged into not only the LLM itself and but also its fine-tuned derivatives, bringing in the extended contextual information while fully preserving the LLM's existing capabilities. We perform comprehensive experiments on long-context language modeling and understanding tasks, which verify Extensible Tokenization as an effective, efficient, flexible, and compatible method to extend LLM's context. Our model and source code will be made publicly available. ",
        "title": "Flexibly Scaling Large Language Models Contexts Through Extensible  Tokenization",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07796",
        "abstract_url": "http://arxiv.org/abs/2401.07796",
        "authors": [
            {
                "last_name": "Painchaud",
                "first_name": "Nathan"
            },
            {
                "last_name": "Courand",
                "first_name": "Pierre-Yves"
            },
            {
                "last_name": "Jodoin",
                "first_name": "Pierre-Marc"
            },
            {
                "last_name": "Duchateau",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Bernard",
                "first_name": "Olivier"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Deep learning now enables automatic and robust extraction of cardiac function descriptors from echocardiographic sequences, such as ejection fraction or strain. These descriptors provide fine-grained information that physicians consider, in conjunction with more global variables from the clinical record, to assess patients' condition. Drawing on novel transformer models applied to tabular data (e.g., variables from electronic health records), we propose a method that considers all descriptors extracted from medical records and echocardiograms to learn the representation of a difficult-to-characterize cardiovascular pathology, namely hypertension. Our method first projects each variable into its own representation space using modality-specific approaches. These standardized representations of multimodal data are then fed to a transformer encoder, which learns to merge them into a comprehensive representation of the patient through a pretext task of predicting a clinical rating. This pretext task is formulated as an ordinal classification to enforce a pathological continuum in the representation space. We observe the major trends along this continuum for a cohort of 239 hypertensive patients to describe, with unprecedented gradation, the effect of hypertension on a number of cardiac function descriptors. Our analysis shows that i) pretrained weights from a foundation model allow to reach good performance (83% accuracy) even with limited data (less than 200 training samples), ii) trends across the population are reproducible between trainings, and iii) for descriptors whose interactions with hypertension are well documented, patterns are consistent with prior physiological knowledge. ",
        "title": "Fusing Echocardiography Images and Medical Records for Continuous  Patient Stratification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07801",
        "abstract_url": "http://arxiv.org/abs/2401.07801",
        "authors": [
            {
                "last_name": "Ghari",
                "first_name": "Bahareh"
            },
            {
                "last_name": "Tourani",
                "first_name": "Ali"
            },
            {
                "last_name": "Shahbahrami",
                "first_name": "Asadollah"
            },
            {
                "last_name": "Gaydadjiev",
                "first_name": "Georgi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Pedestrian detection remains a critical problem in various domains, such as computer vision, surveillance, and autonomous driving. In particular, accurate and instant detection of pedestrians in low-light conditions and reduced visibility is of utmost importance for autonomous vehicles to prevent accidents and save lives. This paper aims to comprehensively survey various pedestrian detection approaches, baselines, and datasets that specifically target low-light conditions. The survey discusses the challenges faced in detecting pedestrians at night and explores state-of-the-art methodologies proposed in recent years to address this issue. These methodologies encompass a diverse range, including deep learning-based, feature-based, and hybrid approaches, which have shown promising results in enhancing pedestrian detection performance under challenging lighting conditions. Furthermore, the paper highlights current research directions in the field and identifies potential solutions that merit further investigation by researchers. By thoroughly examining pedestrian detection techniques in low-light conditions, this survey seeks to contribute to the advancement of safer and more reliable autonomous driving systems and other applications related to pedestrian safety. Accordingly, most of the current approaches in the field use deep learning-based image fusion methodologies (i.e., early, halfway, and late fusion) for accurate and reliable pedestrian detection. Moreover, the majority of the works in the field (approximately 48%) have been evaluated on the KAIST dataset, while the real-world video feeds recorded by authors have been used in less than six percent of the works. ",
        "title": "Pedestrian Detection in Low-Light Conditions: A Comprehensive Survey",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07803",
        "abstract_url": "http://arxiv.org/abs/2401.07803",
        "authors": [
            {
                "last_name": "Reich",
                "first_name": "Daniel"
            },
            {
                "last_name": "Schultz",
                "first_name": "Tanja"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model's reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits.   In this work, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that the potential benefits of these methods are severely underestimated as a result. ",
        "title": "Uncovering the Full Potential of Visual Grounding Methods in VQA",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07805",
        "abstract_url": "http://arxiv.org/abs/2401.07805",
        "authors": [
            {
                "last_name": "Schreter-Fleischhacker",
                "first_name": "Magdalena"
            },
            {
                "last_name": "Munch",
                "first_name": "Peter"
            },
            {
                "last_name": "Much",
                "first_name": "Nils"
            },
            {
                "last_name": "Kronbichler",
                "first_name": "Martin"
            },
            {
                "last_name": "Wall",
                "first_name": "Wolfgang A."
            },
            {
                "last_name": "Meier",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  We present accurate and mathematically consistent formulations of a diffuse-interface model for two-phase flow problems involving rapid evaporation. The model addresses challenges including discontinuities in the density field by several orders of magnitude, leading to high velocity and pressure jumps across the liquid-vapor interface, along with dynamically changing interface topologies. To this end, we integrate an incompressible Navier--Stokes solver combined with a conservative level-set formulation and a regularized, i.e., diffuse, representation of discontinuities into a matrix-free adaptive finite element framework. The achievements are three-fold: First, this work proposes mathematically consistent definitions for the level-set transport velocity in the diffuse interface region by extrapolating the velocity from the liquid or gas phase, which exhibit superior prediction accuracy for the evaporated mass and the resulting interface dynamics compared to a local velocity evaluation, especially for highly curved interfaces. Second, we show that accurate prediction of the evaporation-induced pressure jump requires a consistent, namely a reciprocal, density interpolation across the interface, which satisfies local mass conservation. Third, the combination of diffuse interface models for evaporation with standard Stokes-type constitutive relations for viscous flows leads to significant pressure artifacts in the diffuse interface region. To mitigate these, we propose a modification for such constitutive model types. Through selected analytical and numerical examples, the aforementioned properties are validated. The presented model promises new insights in simulation-based prediction of melt-vapor interactions in thermal multiphase flows such as in laser-based powder bed fusion of metals. ",
        "title": "A consistent diffuse-interface model for two-phase flow problems with  rapid evaporation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07806",
        "abstract_url": "http://arxiv.org/abs/2401.07806",
        "authors": [
            {
                "last_name": "Jin",
                "first_name": "Ruhui"
            },
            {
                "last_name": "Guerra",
                "first_name": "Martin"
            },
            {
                "last_name": "Li",
                "first_name": "Qin"
            },
            {
                "last_name": "Wright",
                "first_name": "Stephen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Optimal experimental design (OED) has far-reaching impacts in many scientific domains. We study OED over a continuous-valued design space, a setting that occurs often in practice. Optimization of a distributional function over an infinite-dimensional probability measure space is conceptually distinct from the discrete OED tasks that are conventionally tackled. We propose techniques based on optimal transport and Wasserstein gradient flow. A practical computational approach is derived from the Monte Carlo simulation, which transforms the infinite-dimensional optimization problem to a finite-dimensional problem over Euclidean space, to which gradient descent can be applied. We discuss first-order criticality and study the convexity properties of the OED objective. We apply our algorithm to the tomography inverse problem, where the solution reveals optimal sensor placements for imaging. ",
        "title": "Optimal experimental design via gradient flow",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07807",
        "abstract_url": "http://arxiv.org/abs/2401.07807",
        "authors": [
            {
                "last_name": "Heimann",
                "first_name": "Fabian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a higher order space-time unfitted finite element method for convection-diffusion problems on coupled (surface and bulk) domains. In that way, we combine a method suggested by Heimann, Lehrenfeld, Preu{\\ss} (SIAM J. Sci. Comput. 45(2), 2023, B139 - B165) for the bulk case with a method suggested by Sass, Reusken (Comput. Math. Appl. 146(15), 2023, 253-270) for the surface case. The geometry is allowed to change with time, and the higher order discrete approximation of this geometry is ensured by a time-dependent isoparametric mapping. The space-time discretisation approach allows for straightforward handling of arbitrary high orders. In that way, we also generalise results of Hansbo, Larson, Zahedi (Comput. Methods Appl. Mech. Engrg. 307, 2016, 96-116) to higher orders. The convergence of the proposed higher order discretisations is confirmed numerically. ",
        "title": "A Higher Order Unfitted Space-Time Finite Element Method for Coupled  Surface-Bulk problems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07809",
        "abstract_url": "http://arxiv.org/abs/2401.07809",
        "authors": [
            {
                "last_name": "Medyakov",
                "first_name": "Daniil"
            },
            {
                "last_name": "Molodtsov",
                "first_name": "Gleb"
            },
            {
                "last_name": "Beznosikov",
                "first_name": "Aleksandr"
            },
            {
                "last_name": "Gasnikov",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The distributed optimization problem has become increasingly relevant recently. It has a lot of advantages such as processing a large amount of data in less time compared to non-distributed methods. However, most distributed approaches suffer from a significant bottleneck - the cost of communications. Therefore, a large amount of research has recently been directed at solving this problem. One such approach uses local data similarity. In particular, there exists an algorithm provably optimally exploiting the similarity property. But this result, as well as results from other works solve the communication bottleneck by focusing only on the fact that communication is significantly more expensive than local computing and does not take into account the various capacities of network devices and the different relationship between communication time and local computing expenses. We consider this setup and the objective of this study is to achieve an optimal ratio of distributed data between the server and local machines for any costs of communications and local computations. The running times of the network are compared between uniform and optimal distributions. The superior theoretical performance of our solutions is experimentally validated. ",
        "title": "Optimal Data Splitting in Distributed Optimization for Machine Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07810",
        "abstract_url": "http://arxiv.org/abs/2401.07810",
        "authors": [
            {
                "last_name": "Saha",
                "first_name": "Sougata"
            },
            {
                "last_name": "Srihari",
                "first_name": "Rohini"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counter-arguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristics-based qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora. ",
        "title": "Consolidating Strategies for Countering Hate Speech Using Persuasive  Dialogues",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07812",
        "abstract_url": "http://arxiv.org/abs/2401.07812",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Kunpeng"
            },
            {
                "last_name": "Diefenbach",
                "first_name": "Dennis"
            },
            {
                "last_name": "Gourru",
                "first_name": "Antoine"
            },
            {
                "last_name": "Gravier",
                "first_name": "Christophe"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Wikidata has grown to a knowledge graph with an impressive size. To date, it contains more than 17 billion triples collecting information about people, places, films, stars, publications, proteins, and many more. On the other side, most of the information on the Web is not published in highly structured data repositories like Wikidata, but rather as unstructured and semi-structured content, more concretely in HTML pages containing text and tables. Finding, monitoring, and organizing this data in a knowledge graph is requiring considerable work from human editors. The volume and complexity of the data make this task difficult and time-consuming. In this work, we present a framework that is able to identify and extract new facts that are published under multiple Web domains so that they can be proposed for validation by Wikidata editors. The framework is relying on question-answering technologies. We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages. For achieving this, we demonstrate that language models can be adapted to extract facts not only from textual collections but also from Web pages. By exploiting the information already contained in Wikidata the proposed framework can be trained without the need for any additional learning signals and can extract new facts for a wide range of properties and domains. Following this path, Wikidata can be used as a seed to extract facts on the Web. Our experiments show that we can achieve a mean performance of 84.07 at F1-score. Moreover, our estimations show that we can potentially extract millions of facts that can be proposed for human validation. The goal is to help editors in their daily tasks and contribute to the completion of the Wikidata knowledge graph. ",
        "title": "Wikidata as a seed for Web Extraction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07815",
        "abstract_url": "http://arxiv.org/abs/2401.07815",
        "authors": [
            {
                "last_name": "Card\u00f3",
                "first_name": "Carles"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  Context-free languages can be characterized in several ways. This article studies projective linearisations of languages of simple dependency trees, i.e., dependency trees in which a node can govern at most one node with a given syntactic function. We prove that the projective linearisations of local languages of simple dependency trees coincide with the context-free languages.   Simple dependency trees suggest alternative dual notions of locality and projectivity, which permits defining a dual language for each context-free language. We call this new class of languages anti-context-free. These languages are related to some linguistic constructions exhibiting the so-called cross-serial dependencies that were historically important for the development of computational linguistics. We propose that this duality could be a relevant linguistic phenomenon. ",
        "title": "Anti-Context-Free languages",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07817",
        "abstract_url": "http://arxiv.org/abs/2401.07817",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Huang",
                "first_name": "Shujian"
            },
            {
                "last_name": "Yuan",
                "first_name": "Fei"
            },
            {
                "last_name": "She",
                "first_name": "Shuaijie"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Birch",
                "first_name": "Alexandra"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of chain-of-thought and mathematical reasoning instructions. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English question data. In this way we perform targetted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3\\% and 16.1\\% accuracy across ten languages on the MGSM and MSVAMP maths reasoning benchmarks (The project will be available at: https://github.com/NJUNLP/QAlign). ",
        "title": "Question Translation Training for Better Multilingual Reasoning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07819",
        "abstract_url": "http://arxiv.org/abs/2401.07819",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Zhongjie"
            },
            {
                "last_name": "De Persis",
                "first_name": "Claudio"
            },
            {
                "last_name": "Tesi",
                "first_name": "Pietro"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present data-based conditions for enforcing contractivity via feedback control and obtain desired asymptotic properties of the closed-loop system. We focus on unknown nonlinear control systems whose vector fields are expressible via a dictionary of functions and derive data-dependent semidefinite programs whose solution returns the controller that guarantees contractivity. When data are perturbed by disturbances that are linear combination of sinusoids of known frequencies (but unknown amplitude and phase) and constants, we remarkably obtain conditions for contractivity that do not depend on the magnitude of the disturbances, with imaginable positive consequences for the synthesis of the controller. Finally, we show how to design from data an integral controller for nonlinear systems that achieves constant reference tracking and constant disturbance rejection. ",
        "title": "Enforcing contraction via data",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07823",
        "abstract_url": "http://arxiv.org/abs/2401.07823",
        "authors": [
            {
                "last_name": "Febrianto",
                "first_name": "Eky"
            },
            {
                "last_name": "Sistek",
                "first_name": "Jakub"
            },
            {
                "last_name": "Kus",
                "first_name": "Pavel"
            },
            {
                "last_name": "Kecman",
                "first_name": "Matija"
            },
            {
                "last_name": "Cirak",
                "first_name": "Fehmi"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The automated finite element analysis of complex CAD models using boundary-fitted meshes is rife with difficulties. Immersed finite element methods are intrinsically more robust but usually less accurate. In this work, we introduce an efficient, robust, high-order immersed finite element method for complex CAD models. Our approach relies on three adaptive structured grids: a geometry grid for representing the implicit geometry, a finite element grid for discretising physical fields and a quadrature grid for evaluating the finite element integrals. The geometry grid is a sparse VDB (Volumetric Dynamic B+ tree) grid that is highly refined close to physical domain boundaries. The finite element grid consists of a forest of octree grids distributed over several processors, and the quadrature grid in each finite element cell is an octree grid constructed in a bottom-up fashion. We discretise physical fields on the finite element grid using high-order Lagrange basis functions. The resolution of the quadrature grid ensures that finite element integrals are evaluated with sufficient accuracy and that any sub-grid geometric features, like small holes or corners, are resolved up to a desired resolution. The conceptual simplicity and modularity of our approach make it possible to reuse open-source libraries, i.e. openVDB and p4est for implementing the geometry and finite element grids, respectively, and BDDCML for iteratively solving the discrete systems of equations in parallel using domain decomposition. We demonstrate the efficiency and robustness of the proposed approach by solving the Poisson equation on domains given by complex CAD models and discretised with tens of millions of degrees of freedom. ",
        "title": "A three-grid high-order immersed finite element method for the analysis  of CAD models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07825",
        "abstract_url": "http://arxiv.org/abs/2401.07825",
        "authors": [
            {
                "last_name": "Ramezanpour",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Robertson",
                "first_name": "Anne M."
            },
            {
                "last_name": "Tobe",
                "first_name": "Yasutaka"
            },
            {
                "last_name": "Jia",
                "first_name": "Xiaowei"
            },
            {
                "last_name": "Cebral",
                "first_name": "Juan R."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vascular calcification is implicated as an important factor in major adverse cardiovascular events (MACE), including heart attack and stroke. A controversy remains over how to integrate the diverse forms of vascular calcification into clinical risk assessment tools. Even the commonly used calcium score for coronary arteries, which assumes risk scales positively with total calcification, has important inconsistencies. Fundamental studies are needed to determine how risk is influenced by the diverse calcification phenotypes. However, studies of these kinds are hindered by the lack of high-throughput, objective, and non-destructive tools for classifying calcification in imaging data sets. Here, we introduce a new classification system for phenotyping calcification along with a semi-automated, non-destructive pipeline that can distinguish these phenotypes in even atherosclerotic tissues. The pipeline includes a deep-learning-based framework for segmenting lipid pools in noisy micro-CT images and an unsupervised clustering framework for categorizing calcification based on size, clustering, and topology. This approach is illustrated for five vascular specimens, providing phenotyping for thousands of calcification particles across as many as 3200 images in less than seven hours. Average Dice Similarity Coefficients of 0.96 and 0.87 could be achieved for tissue and lipid pool, respectively, with training and validation needed on only 13 images despite the high heterogeneity in these tissues. By introducing an efficient and comprehensive approach to phenotyping calcification, this work enables large-scale studies to identify a more reliable indicator of the risk of cardiovascular events, a leading cause of global mortality and morbidity. ",
        "title": "Phenotyping calcification in vascular tissues using artificial  intelligence",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07835",
        "abstract_url": "http://arxiv.org/abs/2401.07835",
        "authors": [
            {
                "last_name": "Baghban",
                "first_name": "Akram"
            },
            {
                "last_name": "Newman",
                "first_name": "Marc"
            },
            {
                "last_name": "Horlemann",
                "first_name": "Anna-Lena"
            },
            {
                "last_name": "Ghiyasvand",
                "first_name": "Mehdi"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work focuses on sequential locally recoverable codes (SLRCs), a special family of locally repairable codes, capable of correcting multiple code symbol erasures, which are commonly used for distributed storage systems. First, we construct an extended $q$-ary family of non-binary SLRCs using code products with a novel maximum number of recoverable erasures $t$ and a minimal repair alternativity $A$. Second, we study how MDS and BCH codes can be used to construct $q$-ary SLRCs. Finally, we compare our codes to other LRCs. ",
        "title": "$q$-ary Sequential Locally Recoverable Codes from the Product  Construction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07836",
        "abstract_url": "http://arxiv.org/abs/2401.07836",
        "authors": [
            {
                "last_name": "Kasirzadeh",
                "first_name": "Atoosa"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "LG"
        ],
        "abstract": "  The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional \"decisive AI x-risk hypothesis\" with an \"accumulative AI x-risk hypothesis.\" While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic erosion of econopolitical structures. The accumulative hypothesis suggests a boiling frog scenario where incremental AI risks slowly converge, undermining resilience until a triggering event results in irreversible collapse. Through systems analysis, this paper examines the distinct assumptions differentiating these two hypotheses. It is then argued that the accumulative view reconciles seemingly incompatible perspectives on AI risks. The implications of differentiating between these causal pathways -- the decisive and the accumulative -- for the governance of AI risks as well as long-term AI safety are discussed. ",
        "title": "Two Types of AI Existential Risk: Decisive and Accumulative",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07841",
        "abstract_url": "http://arxiv.org/abs/2401.07841",
        "authors": [
            {
                "last_name": "Deubert",
                "first_name": "Darius"
            },
            {
                "last_name": "Klingel",
                "first_name": "Lars"
            },
            {
                "last_name": "Selig",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The importance of simulation at machine level in industrial environments is steadily increasing especially in the design and commissioning phase. Using models during the operation phase together with the real machine or plant is referred to as online simulation. Online simulation is used for system monitoring, predictive analyses, decision support or online optimization and therefore has various advantages and a wide field of applications. This paper has the aim to characterize online simulation at machine level in industrial automation focusing on key technologies and common applications. Therefore, a set of 65 relevant publications, which are focusing on this subject, is found by database search, expert consultation, and snowballing. As key technological aspects, the used model types, interfaces and platforms, and the aspects of initialization and synchronization are further investigated. The results are interpreted and limitations, knowledge gaps and future prospects are discussed. The potential of online simulation at machine level especially arises due to the increasing availability of component and machine models from the design and commissioning phase, which can be reused for online simulation. Remaining challenges are identified concerning implementation, simulation platforms, model maintenance and especially in the field of synchronization. ",
        "title": "Online Simulation at Machine Level: A Systematic Review",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07842",
        "abstract_url": "http://arxiv.org/abs/2401.07842",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Peng"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Recent advancements in low-Earth orbit (LEO) satellites represented by large constellations and advanced payloads provide great promises for enabling beyond 5G and 6G telecommunications and high-quality and ubiquitous Internet connectivity to everyone anywhere on Earth. LEO satellite networks are envisioned to bridge the urban-rural connectivity gap for the digital divide. However, the digital divide can hardly be closed by only providing connectivity to rural and remote areas. Various unprecedented challenges brought by the emerging satellite Internet still need to be resolved, such as inconsistent end-to-end performance guarantees and a lack of efficient management and operations in these areas, which are referred to as \"performance gap\" and \"management gap\", respectively. This position paper will briefly discuss these gaps, approaches to addressing the gaps, and some research directions based on our recent works. ",
        "title": "Closing the Performance and Management Gaps with Satellite Internet:  Challenges, Approaches, and Future Directions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07844",
        "abstract_url": "http://arxiv.org/abs/2401.07844",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Shuze"
            },
            {
                "last_name": "Chen",
                "first_name": "Shuhang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shangtong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible. ",
        "title": "The ODE Method for Stochastic Approximation and Reinforcement Learning  with Markovian Noise",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07847",
        "abstract_url": "http://arxiv.org/abs/2401.07847",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Saptarshi"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Shreya"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            },
            {
                "last_name": "Tamiti",
                "first_name": "Tarikul Islam"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Sentiment Analysis (SA) refers to the task of associating a view polarity (usually, positive, negative, or neutral; or even fine-grained such as slightly angry, sad, etc.) to a given text, essentially breaking it down to a supervised (since we have the view labels apriori) classification task. Although heavily studied in resource-rich languages such as English thus pushing the SOTA by leaps and bounds, owing to the arrival of the Transformer architecture, the same cannot be said for resource-poor languages such as Bengali (BN). For a language spoken by roughly 300 million people, the technology enabling them to run trials on their favored tongue is severely lacking. In this paper, we analyze the SOTA for SA in Bengali, particularly, Transformer-based models. We discuss available datasets, their drawbacks, the nuances associated with Bengali i.e. what makes this a challenging language to apply SA on, and finally provide insights for future direction to mitigate the limitations in the field. ",
        "title": "Milestones in Bengali Sentiment Analysis leveraging Transformer-models:  Fundamentals, Challenges and Future Directions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07849",
        "abstract_url": "http://arxiv.org/abs/2401.07849",
        "authors": [
            {
                "last_name": "Fejgin",
                "first_name": "Daniel"
            },
            {
                "last_name": "Hadad",
                "first_name": "Elior"
            },
            {
                "last_name": "Gannot",
                "first_name": "Sharon"
            },
            {
                "last_name": "Koldovsk\u00fd",
                "first_name": "Zbyn\u011bk"
            },
            {
                "last_name": "Doclo",
                "first_name": "Simon"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  To estimate the direction of arrival (DOA) of multiple speakers with methods that use prototype transfer functions, frequency-dependent spatial spectra (SPS) are usually constructed. To make the DOA estimation robust, SPS from different frequencies can be combined. According to how the SPS are combined, frequency fusion mechanisms are categorized into narrowband, broadband, or speaker-grouped, where the latter mechanism requires a speaker-wise grouping of frequencies. For a binaural hearing aid setup, in this paper we propose an interaural time difference (ITD)-based speaker-grouped frequency fusion mechanism. By exploiting the DOA dependence of ITDs, frequencies can be grouped according to a common ITD and be used for DOA estimation of the respective speaker. We apply the proposed ITD-based speaker-grouped frequency fusion mechanism for different DOA estimation methods, namely the multiple signal classification, steered response power and a recently published method based on relative transfer function (RTF) vectors. In our experiments, we compare DOA estimation with different fusion mechanisms. For all considered DOA estimation methods, the proposed ITD-based speaker-grouped frequency fusion mechanism results in a higher DOA estimation accuracy compared with the narrowband and broadband fusion mechanisms. ",
        "title": "Comparison of Frequency-Fusion Mechanisms for Binaural  Direction-of-Arrival Estimation for Multiple Speakers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07851",
        "abstract_url": "http://arxiv.org/abs/2401.07851",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Heming"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Dong",
                "first_name": "Qingxiu"
            },
            {
                "last_name": "Wang",
                "first_name": "Peiyi"
            },
            {
                "last_name": "Li",
                "first_name": "Yongqi"
            },
            {
                "last_name": "Ge",
                "first_name": "Tao"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianyu"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjie"
            },
            {
                "last_name": "Sui",
                "first_name": "Zhifang"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first efficiently drafts several future tokens and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, including current leading techniques, the challenges faced, and potential future directions in this field. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference. ",
        "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive  Survey of Speculative Decoding",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07853",
        "abstract_url": "http://arxiv.org/abs/2401.07853",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Rongyu"
            },
            {
                "last_name": "Cai",
                "first_name": "Zefan"
            },
            {
                "last_name": "Yang",
                "first_name": "Huanrui"
            },
            {
                "last_name": "Liu",
                "first_name": "Zidong"
            },
            {
                "last_name": "Gudovskiy",
                "first_name": "Denis"
            },
            {
                "last_name": "Okuno",
                "first_name": "Tomoyuki"
            },
            {
                "last_name": "Nakata",
                "first_name": "Yohei"
            },
            {
                "last_name": "Keutzer",
                "first_name": "Kurt"
            },
            {
                "last_name": "Chang",
                "first_name": "Baobao"
            },
            {
                "last_name": "Du",
                "first_name": "Yuan"
            },
            {
                "last_name": "Du",
                "first_name": "Li"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shanghang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Finetuning a pretrained vision model (PVM) is a common technique for learning downstream vision tasks. The conventional finetuning process with the randomly sampled data points results in diminished training efficiency. To address this drawback, we propose a novel approach, VLM-empowered Collaborative Active Finetuning (VeCAF). VeCAF optimizes a parametric data selection model by incorporating the training objective of the model being tuned. Effectively, this guides the PVM towards the performance goal with improved data and computational efficiency. As vision-language models (VLMs) have achieved significant advancements by establishing a robust connection between image and language domains, we exploit the inherent semantic richness of the text embedding space and utilize text embedding of pretrained VLM models to augment PVM image features for better data selection and finetuning. Furthermore, the flexibility of text-domain augmentation gives VeCAF a unique ability to handle out-of-distribution scenarios without external augmented data. Extensive experiments show the leading performance and high efficiency of VeCAF that is superior to baselines in both in-distribution and out-of-distribution image classification tasks. On ImageNet, VeCAF needs up to 3.3x less training batches to reach the target performance compared to full finetuning and achieves 2.8% accuracy improvement over SOTA methods with the same number of batches. ",
        "title": "VeCAF: VLM-empowered Collaborative Active Finetuning with Training  Objective Awareness",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07854",
        "abstract_url": "http://arxiv.org/abs/2401.07854",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Quan"
            },
            {
                "last_name": "Yao",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Yao",
                "first_name": "Lisha"
            },
            {
                "last_name": "Chen",
                "first_name": "Xin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jingren"
            },
            {
                "last_name": "Lu",
                "first_name": "Le"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ling"
            },
            {
                "last_name": "Liu",
                "first_name": "Zaiyi"
            },
            {
                "last_name": "Huo",
                "first_name": "Yuankai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Colorectal cancer (CRC) micro-satellite instability (MSI) prediction on histopathology images is a challenging weakly supervised learning task that involves multi-instance learning on gigapixel images. To date, radiology images have proven to have CRC MSI information and efficient patient imaging techniques. Different data modalities integration offers the opportunity to increase the accuracy and robustness of MSI prediction. Despite the progress in representation learning from the whole slide images (WSI) and exploring the potential of making use of radiology data, CRC MSI prediction remains a challenge to fuse the information from multiple data modalities (e.g., pathology WSI and radiology CT image). In this paper, we propose $M^{2}$Fusion: a Bayesian-based multimodal multi-level fusion pipeline for CRC MSI. The proposed fusion model $M^{2}$Fusion is capable of discovering more novel patterns within and across modalities that are beneficial for predicting MSI than using a single modality alone, as well as other fusion methods. The contribution of the paper is three-fold: (1) $M^{2}$Fusion is the first pipeline of multi-level fusion on pathology WSI and 3D radiology CT image for MSI prediction; (2) CT images are the first time integrated into multimodal fusion for CRC MSI prediction; (3) feature-level fusion strategy is evaluated on both Transformer-based and CNN-based method. Our approach is validated on cross-validation of 352 cases and outperforms either feature-level (0.8177 vs. 0.7908) or decision-level fusion strategy (0.8177 vs. 0.7289) on AUC score. ",
        "title": "$M^{2}$Fusion: Bayesian-based Multimodal Multi-level Fusion on  Colorectal Cancer Microsatellite Instability Prediction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07855",
        "abstract_url": "http://arxiv.org/abs/2401.07855",
        "authors": [
            {
                "last_name": "Suulker",
                "first_name": "Cem"
            },
            {
                "last_name": "Skach",
                "first_name": "Sophie"
            },
            {
                "last_name": "Kaleel",
                "first_name": "Danyaal"
            },
            {
                "last_name": "Abrar",
                "first_name": "Taqi"
            },
            {
                "last_name": "Murtaza",
                "first_name": "Zain"
            },
            {
                "last_name": "Suulker",
                "first_name": "Dilara"
            },
            {
                "last_name": "Althoefer",
                "first_name": "Kaspar"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Here we present a flexible tip mount for eversion (vine) robots. This soft cap allows attaching a payload to an eversion robot while allowing moving through narrow openings, as well as the eversion of protruding objects, and expanded surfaces. ",
        "title": "Deformable Tip Mount for Soft Growing Eversion Robots",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07856",
        "abstract_url": "http://arxiv.org/abs/2401.07856",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Bijie"
            },
            {
                "last_name": "Lee",
                "first_name": "Ryan"
            },
            {
                "last_name": "Li",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Gan",
                "first_name": "Tianyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuntian"
            },
            {
                "last_name": "Jarrahi",
                "first_name": "Mona"
            },
            {
                "last_name": "Ozcan",
                "first_name": "Aydogan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Data protection methods like cryptography, despite being effective, inadvertently signal the presence of secret communication, thereby drawing undue attention. Here, we introduce an optical information hiding camera integrated with an electronic decoder, optimized jointly through deep learning. This information hiding-decoding system employs a diffractive optical processor as its front-end, which transforms and hides input images in the form of ordinary-looking patterns that deceive/mislead human observers. This information hiding transformation is valid for infinitely many combinations of secret messages, all of which are transformed into ordinary-looking output patterns, achieved all-optically through passive light-matter interactions within the optical processor. By processing these ordinary-looking output images, a jointly-trained electronic decoder neural network accurately reconstructs the original information hidden within the deceptive output pattern. We numerically demonstrated our approach by designing an information hiding diffractive camera along with a jointly-optimized convolutional decoder neural network. The efficacy of this system was demonstrated under various lighting conditions and noise levels, showing its robustness. We further extended this information hiding camera to multi-spectral operation, allowing the concealment and decoding of multiple images at different wavelengths, all performed simultaneously in a single feed-forward operation. The feasibility of our framework was also demonstrated experimentally using THz radiation. This optical encoder-electronic decoder-based co-design provides a novel information hiding camera interface that is both high-speed and energy-efficient, offering an intriguing solution for visual information security. ",
        "title": "Information hiding cameras: optical concealment of object information  into ordinary images",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07861",
        "abstract_url": "http://arxiv.org/abs/2401.07861",
        "authors": [
            {
                "last_name": "Fernandes",
                "first_name": "Joao B."
            },
            {
                "last_name": "da Silva",
                "first_name": "Felipe H. S."
            },
            {
                "last_name": "Xavier-de-Souza",
                "first_name": "Samuel"
            },
            {
                "last_name": "Assis",
                "first_name": "Italo A. S."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Programs with high levels of complexity often face challenges in adjusting execution parameters, particularly when these parameters vary based on the execution context. These dynamic parameters significantly impact the program's performance, such as loop granularity, which can vary depending on factors like the execution environment, program input, or the choice of compiler. Given the expensive nature of testing each case individually, one viable solution is to automate parameter adjustments using optimization methods. This article introduces PATSMA, a parameter auto-tuning tool that leverages Coupled Simulated Annealing (CSA) and Nelder-Mead (NM) optimization methods to fine-tune existing parameters in an application. We demonstrate how auto-tuning can contribute to the real-time optimization of parallel algorithms designed for shared memory systems. PATSMA is a C++ library readily available under the MIT license. ",
        "title": "PATSMA: Parameter Auto-tuning for Shared Memory Algorithms",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07862",
        "abstract_url": "http://arxiv.org/abs/2401.07862",
        "authors": [
            {
                "last_name": "Lamarque",
                "first_name": "Maxence"
            },
            {
                "last_name": "Bhan",
                "first_name": "Luke"
            },
            {
                "last_name": "Shi",
                "first_name": "Yuanyuan"
            },
            {
                "last_name": "Krstic",
                "first_name": "Miroslav"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  To stabilize PDEs, feedback controllers require gain kernel functions, which are themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on the PDE plants' functional coefficients. The functional coefficients in PDE plants are often unknown. This requires an adaptive approach to PDE control, i.e., an estimation of the plant coefficients conducted concurrently with control, where a separate PDE for the gain kernel must be solved at each timestep upon the update in the plant coefficient function estimate. Solving a PDE at each timestep is computationally expensive and a barrier to the implementation of real-time adaptive control of PDEs. Recently, results in neural operator (NO) approximations of functional mappings have been introduced into PDE control, for replacing the computation of the gain kernel with a neural network that is trained, once offline, and reused in real-time for rapid solution of the PDEs. In this paper, we present the first result on applying NOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with recirculation. We establish global stabilization via Lyapunov analysis, in the plant and parameter error states, and also present an alternative approach, via passive identifiers, which avoids the strong assumptions on kernel differentiability. We then present numerical simulations demonstrating stability and observe speedups up to three orders of magnitude, highlighting the real-time efficacy of neural operators in adaptive control. Our code (Github) is made publicly available for future researchers. ",
        "title": "Adaptive Neural-Operator Backstepping Control of a Benchmark Hyperbolic  PDE",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07867",
        "abstract_url": "http://arxiv.org/abs/2401.07867",
        "authors": [
            {
                "last_name": "Macko",
                "first_name": "Dominik"
            },
            {
                "last_name": "Moro",
                "first_name": "Robert"
            },
            {
                "last_name": "Uchendu",
                "first_name": "Adaku"
            },
            {
                "last_name": "Srba",
                "first_name": "Ivan"
            },
            {
                "last_name": "Lucas",
                "first_name": "Jason Samuel"
            },
            {
                "last_name": "Yamashita",
                "first_name": "Michiharu"
            },
            {
                "last_name": "Tripto",
                "first_name": "Nafis Irtiza"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongwon"
            },
            {
                "last_name": "Simko",
                "first_name": "Jakub"
            },
            {
                "last_name": "Bielikova",
                "first_name": "Maria"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  High-quality text generation capability of latest Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause detection evasion in all tested languages, where homoglyph attacks are especially successful. ",
        "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07868",
        "abstract_url": "http://arxiv.org/abs/2401.07868",
        "authors": [
            {
                "last_name": "Sakib",
                "first_name": "Md Sadman"
            },
            {
                "last_name": "Sun",
                "first_name": "Yu"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CL"
        ],
        "abstract": "  The inherent probabilistic nature of Large Language Models (LLMs) introduces an element of unpredictability, raising concerns about potential discrepancies in their output. This paper introduces an innovative approach aims to generate correct and optimal robotic task plans for diverse real-world demands and scenarios. LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps. The proposed approach uses LLM to generate a number of task plans as trees and amalgamates them into a graph by removing questionable paths. Then an optimal task tree can be retrieved to circumvent questionable and high-cost nodes, thereby improving planning accuracy and execution efficiency. The approach is further improved by incorporating a large knowledge network. Leveraging GPT-4 further, the high-level task plan is converted into a low-level Planning Domain Definition Language (PDDL) plan executable by a robot. Evaluation results highlight the superior accuracy and efficiency of our approach compared to previous methodologies in the field of task planning. ",
        "title": "Consolidating Trees of Robotic Plans Generated Using Large Language  Models to Improve Reliability",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07870",
        "abstract_url": "http://arxiv.org/abs/2401.07870",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Mouxiang"
            },
            {
                "last_name": "Tian",
                "first_name": "Hao"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhongxin"
            },
            {
                "last_name": "Ren",
                "first_name": "Xiaoxue"
            },
            {
                "last_name": "Sun",
                "first_name": "Jianling"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel modelagnostic framework that enables online modification and non-sequential generation to augment the code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an infill-first, judge-later strategy, which experiments with filling at the $k$ most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple benchmarks consistently indicate significant improvements over all baselines. Notably, JumpCoder assists code LLMs in achieving up to a 3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the multilingual HumanEval benchmarks. Our code is public at https://github.com/Keytoyze/JumpCoder. ",
        "title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07871",
        "abstract_url": "http://arxiv.org/abs/2401.07871",
        "authors": [
            {
                "last_name": "Cummins",
                "first_name": "Logan"
            },
            {
                "last_name": "Sommers",
                "first_name": "Alex"
            },
            {
                "last_name": "Ramezani",
                "first_name": "Somayeh Bakhtiari"
            },
            {
                "last_name": "Mittal",
                "first_name": "Sudip"
            },
            {
                "last_name": "Jabour",
                "first_name": "Joseph"
            },
            {
                "last_name": "Seale",
                "first_name": "Maria"
            },
            {
                "last_name": "Rahimi",
                "first_name": "Shahram"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  Predictive maintenance is a well studied collection of techniques that aims to prolong the life of a mechanical system by using artificial intelligence and machine learning to predict the optimal time to perform maintenance. The methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep. As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system. This attracts the field of Explainable AI (XAI) to introduce explainability and interpretability into the predictive system. XAI brings methods to the field of predictive maintenance that can amplify trust in the users while maintaining well-performing systems. This survey on explainable predictive maintenance (XPM) discusses and presents the current methods of XAI as applied to predictive maintenance while following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We categorize the different XPM methods into groups that follow the XAI literature. Additionally, we include current challenges and a discussion on future research directions in XPM. ",
        "title": "Explainable Predictive Maintenance: A Survey of Current Methods,  Challenges and Opportunities",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07872",
        "abstract_url": "http://arxiv.org/abs/2401.07872",
        "authors": [
            {
                "last_name": "Pawar",
                "first_name": "Saurav"
            },
            {
                "last_name": "Tonmoy",
                "first_name": "S. M Towhidul Islam"
            },
            {
                "last_name": "Zaman",
                "first_name": "S M Mehedi"
            },
            {
                "last_name": "Jain",
                "first_name": "Vinija"
            },
            {
                "last_name": "Chadha",
                "first_name": "Aman"
            },
            {
                "last_name": "Das",
                "first_name": "Amitava"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The advent of Large Language Models (LLMs) represents a notable breakthrough in Natural Language Processing (NLP), contributing to substantial progress in both text comprehension and generation. However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation. Understanding and extending the context length for LLMs is crucial in enhancing their performance across various NLP applications. In this survey paper, we delve into the multifaceted aspects of exploring why it is essential, and the potential transformations that superior techniques could bring to NLP applications. We study the inherent challenges associated with extending context length and present an organized overview of the existing strategies employed by researchers. Additionally, we discuss the intricacies of evaluating context extension techniques and highlight the open challenges that researchers face in this domain. Furthermore, we explore whether there is a consensus within the research community regarding evaluation standards and identify areas where further agreement is needed. This comprehensive survey aims to serve as a valuable resource for researchers, guiding them through the nuances of context length extension techniques and fostering discussions on future advancements in this evolving field. ",
        "title": "The What, Why, and How of Context Length Extension Techniques in Large  Language Models -- A Detailed Survey",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07874",
        "abstract_url": "http://arxiv.org/abs/2401.07874",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Z. N. D."
            },
            {
                "last_name": "Hansen",
                "first_name": "A. C."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In deep learning (DL) the instability phenomenon is widespread and well documented, most commonly using the classical measure of stability, the Lipschitz constant. While a small Lipchitz constant is traditionally viewed as guarantying stability, it does not capture the instability phenomenon in DL for classification well. The reason is that a classification function -- which is the target function to be approximated -- is necessarily discontinuous, thus having an 'infinite' Lipchitz constant. As a result, the classical approach will deem every classification function unstable, yet basic classification functions a la 'is there a cat in the image?' will typically be locally very 'flat' -- and thus locally stable -- except at the decision boundary. The lack of an appropriate measure of stability hinders a rigorous theory for stability in DL, and consequently, there are no proper approximation theoretic results that can guarantee the existence of stable networks for classification functions. In this paper we introduce a novel stability measure $\\mathscr{S}(f)$, for any classification function $f$, appropriate to study the stability of discontinuous functions and their approximations. We further prove two approximation theorems: First, for any $\\epsilon > 0$ and any classification function $f$ on a \\emph{compact set}, there is a neural network (NN) $\\psi$, such that $\\psi - f \\neq 0$ only on a set of measure $< \\epsilon$, moreover, $\\mathscr{S}(\\psi) \\geq \\mathscr{S}(f) - \\epsilon$ (as accurate and stable as $f$ up to $\\epsilon$). Second, for any classification function $f$ and $\\epsilon > 0$, there exists a NN $\\psi$ such that $\\psi = f$ on the set of points that are at least $\\epsilon$ away from the decision boundary. ",
        "title": "Do stable neural networks exist for classification problems? -- A new  view on stability in AI",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07875",
        "abstract_url": "http://arxiv.org/abs/2401.07875",
        "authors": [
            {
                "last_name": "Wright",
                "first_name": "Ryan"
            },
            {
                "last_name": "Parekh",
                "first_name": "Sagar"
            },
            {
                "last_name": "White",
                "first_name": "Robin"
            },
            {
                "last_name": "Losey",
                "first_name": "Dylan P."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Labor shortages in the United States are impacting a number of industries including the meat processing sector. Collaborative technologies that work alongside humans while increasing production abilities may support the industry by enhancing automation and improving job quality. However, existing automation technologies used in the meat industry have limited collaboration potential, low flexibility, and high cost. The objective of this work was to explore the use of a robot arm to collaboratively work alongside a human and complete tasks performed in a meat processing facility. Toward this objective, we demonstrated proof-of-concept approaches to ensure human safety while exploring the capacity of the robot arm to perform example meat processing tasks. In support of human safety, we developed a knife instrumentation system to detect when the cutting implement comes into contact with meat within the collaborative space. To demonstrate the capability of the system to flexibly conduct a variety of basic meat processing tasks, we developed vision and control protocols to execute slicing, trimming, and cubing of pork loins. We also collected a subjective evaluation of the actions from experts within the U.S. meat processing industry. On average the experts rated the robot's performance as adequate. Moreover, the experts generally preferred the cuts performed in collaboration with a human worker to cuts completed autonomously, highlighting the benefits of robotic technologies that assist human workers rather than replace them. Video demonstrations of our proposed framework can be found here: https://youtu.be/56mdHjjYMVc ",
        "title": "Safely and Autonomously Cutting Meat with a Collaborative Robot Arm",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07877",
        "abstract_url": "http://arxiv.org/abs/2401.07877",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mingjie"
            },
            {
                "last_name": "Verspoor",
                "first_name": "Karin"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Information extraction techniques, including named entity recognition (NER) and relation extraction (RE), are crucial in many domains to support making sense of vast amounts of unstructured text data by identifying and connecting relevant information. Such techniques can assist researchers in extracting valuable insights. In this paper, we introduce the Entity-aware Masking for Biomedical Relation Extraction (EMBRE) method for biomedical relation extraction, as applied in the context of the BioRED challenge Task 1, in which human-annotated entities are provided as input. Specifically, we integrate entity knowledge into a deep neural network by pretraining the backbone model with an entity masking objective. We randomly mask named entities for each instance and let the model identify the masked entity along with its type. In this way, the model is capable of learning more specific knowledge and more robust representations. Then, we utilize the pre-trained model as our backbone to encode language representations and feed these representations into two multilayer perceptron (MLPs) to predict the logits for relation and novelty, respectively. The experimental results demonstrate that our proposed method can improve the performances of entity pair, relation and novelty extraction over our baseline. ",
        "title": "EMBRE: Entity-aware Masking for Biomedical Relation Extraction",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07879",
        "abstract_url": "http://arxiv.org/abs/2401.07879",
        "authors": [
            {
                "last_name": "Pandey",
                "first_name": "Ashutosh"
            },
            {
                "last_name": "Xu",
                "first_name": "Buye"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  We present a novel model designed for resource-efficient multichannel speech enhancement in the time domain, with a focus on low latency, lightweight, and low computational requirements. The proposed model incorporates explicit spatial and temporal processing within deep neural network (DNN) layers. Inspired by frequency-dependent multichannel filtering, our spatial filtering process applies multiple trainable filters to each hidden unit across the spatial dimension, resulting in a multichannel output. The temporal processing is applied over a single-channel output stream from the spatial processing using a Long Short-Term Memory (LSTM) network. The output from the temporal processing stage is then further integrated into the spatial dimension through elementwise multiplication. This explicit separation of spatial and temporal processing results in a resource-efficient network design. Empirical findings from our experiments show that our proposed model significantly outperforms robust baseline models while demanding far fewer parameters and computations, while achieving an ultra-low algorithmic latency of just 2 milliseconds. ",
        "title": "Decoupled Spatial and Temporal Processing for Resource Efficient  Multichannel Speech Enhancement",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07882",
        "abstract_url": "http://arxiv.org/abs/2401.07882",
        "authors": [
            {
                "last_name": "Hsieh",
                "first_name": "Tsun-An"
            },
            {
                "last_name": "Donley",
                "first_name": "Jacob"
            },
            {
                "last_name": "Wong",
                "first_name": "Daniel"
            },
            {
                "last_name": "Xu",
                "first_name": "Buye"
            },
            {
                "last_name": "Pandey",
                "first_name": "Ashutosh"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  We introduce a time-domain framework for efficient multichannel speech enhancement, emphasizing low latency and computational efficiency. This framework incorporates two compact deep neural networks (DNNs) surrounding a multichannel neural Wiener filter (NWF). The first DNN enhances the speech signal to estimate NWF coefficients, while the second DNN refines the output from the NWF. The NWF, while conceptually similar to the traditional frequency-domain Wiener filter, undergoes a training process optimized for low-latency speech enhancement, involving fine-tuning of both analysis and synthesis transforms. Our research results illustrate that the NWF output, having minimal nonlinear distortions, attains performance levels akin to those of the first DNN, deviating from conventional Wiener filter paradigms. Training all components jointly outperforms sequential training, despite its simplicity. Consequently, this framework achieves superior performance with fewer parameters and reduced computational demands, making it a compelling solution for resource-efficient multichannel speech enhancement. ",
        "title": "On the Importance of Neural Wiener Filter for Resource Efficient  Multichannel Speech Enhancement",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07883",
        "abstract_url": "http://arxiv.org/abs/2401.07883",
        "authors": [
            {
                "last_name": "Finardi",
                "first_name": "Paulo"
            },
            {
                "last_name": "Avila",
                "first_name": "Leonardo"
            },
            {
                "last_name": "Castaldoni",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Gengo",
                "first_name": "Pedro"
            },
            {
                "last_name": "Larcher",
                "first_name": "Celio"
            },
            {
                "last_name": "Piau",
                "first_name": "Marcos"
            },
            {
                "last_name": "Costa",
                "first_name": "Pablo"
            },
            {
                "last_name": "Carid\u00e1",
                "first_name": "Vinicius"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "IR"
        ],
        "abstract": "  Retrieval Augmented Generation (RAG) has become one of the most popular paradigms for enabling LLMs to access external data, and also as a mechanism for grounding to mitigate against hallucinations. When implementing RAG you can face several challenges like effective integration of retrieval models, efficient representation learning, data diversity, computational efficiency optimization, evaluation, and quality of text generation. Given all these challenges, every day a new technique to improve RAG appears, making it unfeasible to experiment with all combinations for your problem. In this context, this paper presents good practices to implement, optimize, and evaluate RAG for the Brazilian Portuguese language, focusing on the establishment of a simple pipeline for inference and experiments. We explored a diverse set of methods to answer questions about the first Harry Potter book. To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview, gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to the baseline. When optimizing the input size in the application, we observed that it is possible to further enhance it by 2.4%. Finally, we present the complete architecture of the RAG with our recommendations. As result, we moved from a baseline of 57.88% to a maximum relative score of 98.61%. ",
        "title": "The Chronicles of RAG: The Retriever, the Chunk and the Generator",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07886",
        "abstract_url": "http://arxiv.org/abs/2401.07886",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Hooper",
                "first_name": "Coleman"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoxuan"
            },
            {
                "last_name": "Kim",
                "first_name": "Sehoon"
            },
            {
                "last_name": "Keutzer",
                "first_name": "Kurt"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "DC"
        ],
        "abstract": "  Many applications must provide low-latency LLM service to users or risk unacceptable user experience. However, over-provisioning resources to serve fluctuating request patterns is often prohibitively expensive. In this work, we present a best-effort serving system that employs deep reinforcement learning to adjust service quality based on the task distribution and system load. Our best-effort system can maintain availability with over 10x higher client request rates, serves above 96% of peak performance 4.1x more often, and serves above 98% of peak performance 2.3x more often than static serving on unpredictable workloads. Our learned router is robust to shifts in both the arrival and task distribution. Compared to static serving, learned best-effort serving allows for cost-efficient serving through increased hardware utility. Additionally, we argue that learned best-effort LLM serving is applicable in wide variety of settings and provides application developers great flexibility to meet their specific needs. ",
        "title": "Learned Best-Effort LLM Serving",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07888",
        "abstract_url": "http://arxiv.org/abs/2401.07888",
        "authors": [
            {
                "last_name": "Heinlein",
                "first_name": "Alexander"
            },
            {
                "last_name": "Howard",
                "first_name": "Amanda A."
            },
            {
                "last_name": "Beecroft",
                "first_name": "Damien"
            },
            {
                "last_name": "Stinis",
                "first_name": "Panos"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multiscale problems are challenging for neural network-based discretizations of differential equations, such as physics-informed neural networks (PINNs). This can be (partly) attributed to the so-called spectral bias of neural networks. To improve the performance of PINNs for time-dependent problems, a combination of multifidelity stacking PINNs and domain decomposition-based finite basis PINNs are employed. In particular, to learn the high-fidelity part of the multifidelity model, a domain decomposition in time is employed. The performance is investigated for a pendulum and a two-frequency problem as well as the Allen-Cahn equation. It can be observed that the domain decomposition approach clearly improves the PINN and stacking PINN approaches. ",
        "title": "Multifidelity domain decomposition-based physics-informed neural  networks for time-dependent problems",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07889",
        "abstract_url": "http://arxiv.org/abs/2401.07889",
        "authors": [
            {
                "last_name": "Cho",
                "first_name": "Ryan"
            },
            {
                "last_name": "Patel",
                "first_name": "Sunil"
            },
            {
                "last_name": "Cho",
                "first_name": "Kyu Taek"
            },
            {
                "last_name": "Hwang",
                "first_name": "Jaejin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This study investigated the use of forearm EMG data for distinguishing eight hand gestures, employing the Neural Network and Random Forest algorithms on data from ten participants. The Neural Network achieved 97 percent accuracy with 1000-millisecond windows, while the Random Forest achieved 85 percent accuracy with 200-millisecond windows. Larger window sizes improved gesture classification due to increased temporal resolution. The Random Forest exhibited faster processing at 92 milliseconds, compared to the Neural Network's 124 milliseconds. In conclusion, the study identified a Neural Network with a 1000-millisecond stream as the most accurate (97 percent), and a Random Forest with a 200-millisecond stream as the most efficient (85 percent). Future research should focus on increasing sample size, incorporating more hand gestures, and exploring different feature extraction methods and modeling algorithms to enhance system accuracy and efficiency. ",
        "title": "Machine Learning Techniques to Identify Hand Gestures amidst Forearm  Muscle Signals",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07890",
        "abstract_url": "http://arxiv.org/abs/2401.07890",
        "authors": [
            {
                "last_name": "Shahbazi",
                "first_name": "Alireza"
            },
            {
                "last_name": "Mirsanei",
                "first_name": "Seyyed Ahmad"
            },
            {
                "last_name": "Sarraf",
                "first_name": "Malikeh Haj Khan Mirzaye"
            },
            {
                "last_name": "Bidgoli",
                "first_name": "Behrouz Minaei"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Planning and reasoning about actions and processes, in addition to reasoning about propositions, are important issues in recent logical and computer science studies. The widespread use of actions in everyday life such as IoT, semantic web services, etc., and the limitations and issues in the action formalisms are two factors that lead us to study about how actions are represented.   Since 2007, there was some ideas to integrate Description Logic (DL) and action formalisms for representing both static and dynamic knowledge. In meanwhile, time is an important factor in dynamic situations, and actions change states over time. In this study, on the one hand, we examined related logical structures such as extensions of description logics (DLs), temporal formalisms, and action formalisms. On the other hand, we analyzed possible tools for designing and developing the Knowledge and Action Base (KAB).   For representation and reasoning about actions, we embedded actions into DLs (such as Dynamic-ALC and its extensions). We propose a terminable algorithm for action projection, planning, checking the satisfiability, consistency, realizability, and executability, and also querying from KAB. Actions in this framework were modeled with SPIN and added to state space. This framework has also been implemented as a plugin for the Prot\\'eg\\'e ontology editor.   During the last two decades, various algorithms have been presented, but due to the high computational complexity, we face many problems in implementing dynamic ontologies. In addition, an algorithm to detect the inconsistency of actions' effects was not explicitly stated. In the proposed strategy, the interactions of actions with other parts of modeled knowledge, and a method to check consistency between the effects of actions are presented. With this framework, the ramification problem can be well handled in future works. ",
        "title": "A Strategy for Implementing description Temporal Dynamic Algorithms in  Dynamic Knowledge Graphs by SPIN",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07892",
        "abstract_url": "http://arxiv.org/abs/2401.07892",
        "authors": [
            {
                "last_name": "Asif",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Ali",
                "first_name": "Noman"
            },
            {
                "last_name": "Mishra",
                "first_name": "Sudhakar"
            },
            {
                "last_name": "Dandawate",
                "first_name": "Anushka"
            },
            {
                "last_name": "Tiwary",
                "first_name": "Uma Shanker"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Recently, the representation of emotions in the Valence, Arousal and Dominance (VAD) space has drawn enough attention. However, the complex nature of emotions and the subjective biases in self-reported values of VAD make the emotion model too specific to a particular experiment. This study aims to develop a generic model representing emotions using a fuzzy VAD space and improve emotion recognition by utilizing this representation. We partitioned the crisp VAD space into a fuzzy VAD space using low, medium and high type-2 fuzzy dimensions to represent emotions. A framework that integrates fuzzy VAD space with EEG data has been developed to recognize emotions. The EEG features were extracted using spatial and temporal feature vectors from time-frequency spectrograms, while the subject-reported values of VAD were also considered. The study was conducted on the DENS dataset, which includes a wide range of twenty-four emotions, along with EEG data and subjective ratings. The study was validated using various deep fuzzy framework models based on type-2 fuzzy representation, cuboid probabilistic lattice representation and unsupervised fuzzy emotion clusters. These models resulted in emotion recognition accuracy of 96.09\\%, 95.75\\% and 95.31\\%, respectively, for the classes of 24 emotions. The study also included an ablation study, one with crisp VAD space and the other without VAD space. The result with crisp VAD space performed better, while the deep fuzzy framework outperformed both models. The model was extended to predict cross-subject cases of emotions, and the results with 78.37\\% accuracy are promising, proving the generality of our model. The generic nature of the developed model, along with its successful cross-subject predictions, gives direction for real-world applications in the areas such as affective computing, human-computer interaction, and mental health monitoring. ",
        "title": "Deep Fuzzy Framework for Emotion Recognition using EEG Signals and  Emotion Representation in Type-2 Fuzzy VAD Space",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07897",
        "abstract_url": "http://arxiv.org/abs/2401.07897",
        "authors": [
            {
                "last_name": "van Deemter",
                "first_name": "Kees"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in Data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs. ",
        "title": "The Pitfalls of Defining Hallucination",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07898",
        "abstract_url": "http://arxiv.org/abs/2401.07898",
        "authors": [
            {
                "last_name": "Yavuz",
                "first_name": "Tuba"
            },
            {
                "last_name": "Khor",
                "first_name": "Chin"
            },
            {
                "last_name": "Ken",
                "first_name": ""
            },
            {
                "last_name": "Bai",
                "first_name": ""
            },
            {
                "last_name": "Lutz",
                "first_name": "Robyn"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Testing configurable systems continues to be challenging and costly. Generation of configurations for testing tends to use either techniques based on semantic sampling (e.g., logical formulas over configuration variables, often called presence conditions) or structural code metrics (e.g., code coverage). In this paper we describe our hybrid approaches that combine these two kinds of techniques to good effect. We present new configuration-generation algorithms that leverage constraint solving (SAT and MaxSAT) and configuration fuzzing, and implement our approach in a configuration-generation framework, CONFIZZ. CONFIZZ both enables the generation of maximal configurations (maximal sets of presence conditions that can be satisfied together) and performs code-metric guided configuration fuzzing. Results from evaluation on BusyBox, a highly configurable benchmark, show that our MaxSAT-based configuration generation achieves better coverage for several code metrics. Results also show that, when high coverage of multiple configurations is needed, CONFIZZ's presence-condition fuzzing outperforms alternatives. ",
        "title": "Generating Maximal Configurations and Their Variants Using Code Metrics",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07912",
        "abstract_url": "http://arxiv.org/abs/2401.07912",
        "authors": [
            {
                "last_name": "Weggemans",
                "first_name": "Jordi"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  In unitary property testing a quantum algorithm, also known as a tester, is given query access to a black-box unitary and has to decide whether it satisfies some property. We propose a new technique for proving lower bounds on the quantum query complexity of unitary property testing and related problems, which utilises the connection between unitary property testing and unitary channel discrimination. The main advantage of this technique is that all obtained lower bounds hold for any $\\mathsf{C}$-tester with $\\mathsf{C} \\subseteq \\mathsf{QMA}(\\text{poly(n)} / \\mathsf{qpoly}$, showing that even having access to both (unentangled) quantum proofs and advice does not help for many unitary problems. We apply our technique to prove lower bounds for problems like quantum phase estimation, the entanglement entropy problem, quantum Gibbs sampling and more, removing all logarithmic factors in the lower bounds obtained by the sample-to-query lifting theorem of Wang and Zhang (2023). As a direct corollary, we show that there exists a quantum oracle relative to which $\\mathsf{QMA}(\\text{poly(n)} / \\mathsf{qpoly} \\not\\supset \\mathsf{SBQP}$. ",
        "title": "Lower Bounds for Unitary Property Testing with Proofs and Advice",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07914",
        "abstract_url": "http://arxiv.org/abs/2401.07914",
        "authors": [
            {
                "last_name": "Booth",
                "first_name": "Robert I."
            },
            {
                "last_name": "Carette",
                "first_name": "Titouan"
            },
            {
                "last_name": "Comfort",
                "first_name": "Cole"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We give complete presentations for the dagger-compact props of affine Lagrangian and coisotropic relations over an arbitrary field. This provides a unified family of graphical languages for both affinely constrained classical mechanical systems, as well as odd-prime-dimensional stabiliser quantum circuits. To this end, we present affine Lagrangian relations by a particular class of undirected coloured graphs. In order to reason about composite systems, we introduce a powerful scalable notation where the vertices of these graphs are themselves coloured by graphs. In the setting of stabiliser quantum mechanics, this scalable notation gives an extremely concise description of graph states, which can be composed via ``phased spider fusion.'' Likewise, in the classical mechanical setting of electrical circuits, we show that impedance matrices for reciprocal networks are presented in essentially the same way. ",
        "title": "Graphical Symplectic Algebra",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07915",
        "abstract_url": "http://arxiv.org/abs/2401.07915",
        "authors": [
            {
                "last_name": "Weinberg",
                "first_name": "Abraham Itzhak"
            },
            {
                "last_name": "Shirizly",
                "first_name": "Alon"
            },
            {
                "last_name": "Azulay",
                "first_name": "Osher"
            },
            {
                "last_name": "Sintov",
                "first_name": "Avishai"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Human dexterity is an invaluable capability for precise manipulation of objects in complex tasks. The capability of robots to similarly grasp and perform in-hand manipulation of objects is critical for their use in the ever changing human environment, and for their ability to replace manpower. In recent decades, significant effort has been put in order to enable in-hand manipulation capabilities to robotic systems. Initial robotic manipulators followed carefully programmed paths, while later attempts provided a solution based on analytical modeling of motion and contact. However, these have failed to provide practical solutions due to inability to cope with complex environments and uncertainties. Therefore, the effort has shifted to learning-based approaches where data is collected from the real world or through a simulation, during repeated attempts to complete various tasks. The vast majority of learning approaches focused on learning data-based models that describe the system to some extent or Reinforcement Learning (RL). RL, in particular, has seen growing interest due to the remarkable ability to generate solutions to problems with minimal human guidance. In this survey paper, we track the developments of learning approaches for in-hand manipulations and, explore the challenges and opportunities. This survey is designed both as an introduction for novices in the field with a glossary of terms as well as a guide of novel advances for advanced practitioners. ",
        "title": "Survey of Learning Approaches for Robotic In-Hand Manipulation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07923",
        "abstract_url": "http://arxiv.org/abs/2401.07923",
        "authors": [
            {
                "last_name": "Gow-Smith",
                "first_name": "Edward"
            },
            {
                "last_name": "Phelps",
                "first_name": "Dylan"
            },
            {
                "last_name": "Madabushi",
                "first_name": "Harish Tayyar"
            },
            {
                "last_name": "Scarton",
                "first_name": "Carolina"
            },
            {
                "last_name": "Villavicencio",
                "first_name": "Aline"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  All existing transformer-based approaches to NLP using subword tokenisation algorithms encode whitespace (word boundary information) through the use of special space symbols (such as \\#\\# or \\_) forming part of tokens. These symbols have been shown to a) lead to reduced morphological validity of tokenisations, and b) give substantial vocabulary redundancy. As such, removing these symbols has been shown to have a beneficial effect on the processing of morphologically complex words for transformer encoders in the pretrain-finetune paradigm. In this work, we explore whether word boundary information is at all useful to such models. In particular, we train transformer encoders across four different training scales, and investigate several alternative approaches to including word boundary information, evaluating on a range of tasks across different domains and problem set-ups: GLUE (for sentence-level classification), NER (for token-level classification), and two classification datasets involving complex words (Superbizarre and FLOTA). Overall, through an extensive experimental setup that includes the pre-training of 29 models, we find no substantial improvements from our alternative approaches, suggesting that modifying tokenisers to remove word boundary information isn't leading to a loss of useful information. ",
        "title": "Word Boundary Information Isn't Useful for Encoder Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07927",
        "abstract_url": "http://arxiv.org/abs/2401.07927",
        "authors": [
            {
                "last_name": "Madsen",
                "first_name": "Andreas"
            },
            {
                "last_name": "Chandar",
                "first_name": "Sarath"
            },
            {
                "last_name": "Reddy",
                "first_name": "Siva"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to three types of self-explanations: counterfactuals, importance measures, and redactions. Our work demonstrate that faithfulness is both task and model dependent, e.g., for sentiment classification, counterfactual explanations are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B. Finally, our findings are robust to prompt-variations. ",
        "title": "Can Large Language Models Explain Themselves?",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07928",
        "abstract_url": "http://arxiv.org/abs/2401.07928",
        "authors": [
            {
                "last_name": "Klein",
                "first_name": "Emily"
            },
            {
                "last_name": "Golbeck",
                "first_name": "Jennifer"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Incels are an extremist online community of men who believe in an ideology rooted in misogyny, racism, the glorification of violence, and dehumanization. In their online forums, they use an extensive, evolving cryptolect - a set of ingroup terms that have meaning within the group, reflect the ideology, demonstrate membership in the community, and are difficult for outsiders to understand. This paper presents a lexicon with terms and definitions for common incel root words, prefixes, and affixes. The lexicon is text-based for use in automated analysis and is derived via a Qualitative Content Analysis of the most frequent incel words, their structure, and their meaning on five of the most active incel communities from 2016 to 2023. This lexicon will support future work examining radicalization and deradicalization/disengagement within the community. ",
        "title": "A Lexicon for Studying Radicalization in Incel Communities",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07929",
        "abstract_url": "http://arxiv.org/abs/2401.07929",
        "authors": [
            {
                "last_name": "Akanda",
                "first_name": "Md Rakibul Karim"
            },
            {
                "last_name": "Reynolds",
                "first_name": "Joshua"
            },
            {
                "last_name": "Jackson",
                "first_name": "Treylin"
            },
            {
                "last_name": "Gray",
                "first_name": "Milijah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Machine learning based object detection as well as tracking that object have been performed in this paper. The authors were able to set a range of interest (ROI) around an object using Open Computer Vision, better known as OpenCV. Next a tracking algorithm has been used to maintain tracking on an object while simultaneously operating two servo motors to keep the object centered in the frame. Detailed procedure and code are included in this paper. ",
        "title": "Machine Learning Based Object Tracking",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07930",
        "abstract_url": "http://arxiv.org/abs/2401.07930",
        "authors": [
            {
                "last_name": "L\u00f3pez",
                "first_name": "Jos\u00e9 Antonio Hern\u00e1ndez"
            },
            {
                "last_name": "Chen",
                "first_name": "Boqi"
            },
            {
                "last_name": "Sharma",
                "first_name": "Tushar"
            },
            {
                "last_name": "Varr\u00f3",
                "first_name": "D\u00e1niel"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Motivation. Large language models (LLMs) have exhibited remarkable proficiency in diverse software engineering (SE) tasks. Handling such tasks typically involves acquiring foundational coding knowledge on large, general-purpose datasets during a pre-training phase, and subsequently refining on smaller, task-specific datasets as part of a fine-tuning phase.   Problem statement. Data leakage is a well-known issue in training of machine learning models. A manifestation of this issue is the intersection of the training and testing splits. While intra-dataset code duplication examines this intersection within a given dataset and has been addressed in prior research, inter-dataset code duplication, which gauges the overlap between different datasets, remains largely unexplored. If this phenomenon exists, it could compromise the integrity of LLM evaluations because of the inclusion of fine-tuning test samples that were already encountered during pre-training, resulting in inflated performance metrics.   Contribution. This paper explores the phenomenon of inter-dataset code duplication and its impact on evaluating LLMs across diverse SE tasks.   Study design. We conduct an empirical study using the CSN dataset, a widely adopted pre-training dataset, and five fine-tuning datasets used for various SE tasks. We first identify the intersection between the pre-training and fine-tuning datasets using a deduplication process. Then, we fine-tune four models pre-trained on CSN to evaluate their performance on samples encountered during pre-training and those unseen during that phase.   Results. Our findings reveal a potential threat to the evaluation of various LLMs across multiple SE tasks, stemming from the inter-dataset code duplication phenomenon. Moreover, we demonstrate that this threat is accentuated by factors like the LLM's size and the chosen fine-tuning technique. ",
        "title": "On Inter-dataset Code Duplication and Data Leakage in Large Language  Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07931",
        "abstract_url": "http://arxiv.org/abs/2401.07931",
        "authors": [
            {
                "last_name": "Mandal",
                "first_name": "Paul K."
            },
            {
                "last_name": "Leo",
                "first_name": "Cole"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "DC",
            "LG"
        ],
        "abstract": "  With the popularization of AI solutions for image based problems, there has been a growing concern for both data privacy and acquisition. In a large number of cases, information is located on separate data silos and it can be difficult for a developer to consolidate all of it in a fashion that is appropriate for machine learning model development. Alongside this, a portion of these localized data regions may not have access to a labelled ground truth. This indicates that they have the capacity to reach conclusions numerically, but are not able to assign classifications amid a lack of pertinent information. Such a determination is often negligible, especially when attempting to develop image based solutions that often necessitate this capability. With this being the case, we propose an innovative vertical federated learning (VFL) model architecture that can operate under this common set of conditions. This is the first (and currently the only) implementation of a system that can work under the constraints of a VFL environment and perform image segmentation while maintaining nominal accuracies. We achieved this by utilizing an FCN that boasts the ability to operate on federates that lack labelled data and privately share the respective weights with a central server, that of which hosts the necessary features for classification. Tests were conducted on the CamVid dataset in order to determine the impact of heavy feature compression required for the transfer of information between federates, as well as to reach nominal conclusions about the overall performance metrics when working under such constraints. ",
        "title": "Vertical Federated Image Segmentation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07935",
        "abstract_url": "http://arxiv.org/abs/2401.07935",
        "authors": [
            {
                "last_name": "S\u00f3ti",
                "first_name": "Gergely"
            },
            {
                "last_name": "Huang",
                "first_name": "Xi"
            },
            {
                "last_name": "Wurll",
                "first_name": "Christian"
            },
            {
                "last_name": "Hein",
                "first_name": "Bj\u00f6rn"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We address the problem of robotic grasping of known and unknown objects using implicit behavior cloning. We train a grasp evaluation model from a small number of demonstrations that outputs higher values for grasp candidates that are more likely to succeed in grasping. This evaluation model serves as an objective function, that we maximize to identify successful grasps. Key to our approach is the utilization of learned implicit representations of visual and geometric features derived from a pre-trained NeRF. Though trained exclusively in a simulated environment with simplified objects and 4-DoF top-down grasps, our evaluation model and optimization procedure demonstrate generalization to 6-DoF grasps and novel objects both in simulation and in real-world settings, without the need for additional data. Supplementary material is available at: https://gergely-soti.github.io/grasp ",
        "title": "6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from  NeRFs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07936",
        "abstract_url": "http://arxiv.org/abs/2401.07936",
        "authors": [
            {
                "last_name": "Tschernutter",
                "first_name": "Daniel"
            },
            {
                "last_name": "Kraus",
                "first_name": "Mathias"
            },
            {
                "last_name": "Feuerriegel",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  We propose an algorithm for optimizing the parameters of single hidden layer neural networks. Specifically, we derive a blockwise difference-of-convex (DC) functions representation of the objective function. Based on the latter, we propose a block coordinate descent (BCD) approach that we combine with a tailored difference-of-convex functions algorithm (DCA). We prove global convergence of the proposed algorithm. Furthermore, we mathematically analyze the convergence rate of parameters and the convergence rate in value (i.e., the training loss). We give conditions under which our algorithm converges linearly or even faster depending on the local shape of the loss function. We confirm our theoretical derivations numerically and compare our algorithm against state-of-the-art gradient-based solvers in terms of both training loss and test loss. ",
        "title": "A Globally Convergent Algorithm for Neural Network Parameter  Optimization Based on Difference-of-Convex Functions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07937",
        "abstract_url": "http://arxiv.org/abs/2401.07937",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Shihao"
            },
            {
                "last_name": "Zeng",
                "first_name": "Andy G. X."
            },
            {
                "last_name": "Haibe-Kains",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Goldenberg",
                "first_name": "Anna"
            },
            {
                "last_name": "Dick",
                "first_name": "John E"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  High-throughput omics profiling advancements have greatly enhanced cancer patient stratification. However, incomplete data in multi-omics integration presents a significant challenge, as traditional methods like sample exclusion or imputation often compromise biological diversity and dependencies. Furthermore, the critical task of accurately classifying new patients with partial omics data into existing subtypes is commonly overlooked. To address these issues, we introduce IntegrAO (Integrate Any Omics), an unsupervised framework for integrating incomplete multi-omics data and classifying new samples. IntegrAO first combines partially overlapping patient graphs from diverse omics sources and utilizes graph neural networks to produce unified patient embeddings. Our systematic evaluation across five cancer cohorts involving six omics modalities demonstrates IntegrAO's robustness to missing data and its accuracy in classifying new samples with partial profiles. An acute myeloid leukemia case study further validates its capability to uncover biological and clinical heterogeneity in incomplete datasets. IntegrAO's ability to handle heterogeneous and incomplete data makes it an essential tool for precision oncology, offering a holistic approach to patient characterization. ",
        "title": "Integrate Any Omics: Towards genome-wide data integration for patient  stratification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07942",
        "abstract_url": "http://arxiv.org/abs/2401.07942",
        "authors": [
            {
                "last_name": "Moradi",
                "first_name": "Morteza"
            },
            {
                "last_name": "Palazzo",
                "first_name": "Simone"
            },
            {
                "last_name": "Spampinato",
                "first_name": "Concetto"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "MM"
        ],
        "abstract": "  In recent years, finding an effective and efficient strategy for exploiting spatial and temporal information has been a hot research topic in video saliency prediction (VSP). With the emergence of spatio-temporal transformers, the weakness of the prior strategies, e.g., 3D convolutional networks and LSTM-based networks, for capturing long-range dependencies has been effectively compensated. While VSP has drawn benefits from spatio-temporal transformers, finding the most effective way for aggregating temporal features is still challenging. To address this concern, we propose a transformer-based video saliency prediction approach with high temporal dimension decoding network (THTD-Net). This strategy accounts for the lack of complex hierarchical interactions between features that are extracted from the transformer-based spatio-temporal encoder: in particular, it does not require multiple decoders and aims at gradually reducing temporal features' dimensions in the decoder. This decoder-based architecture yields comparable performance to multi-branch and over-complicated models on common benchmarks such as DHF1K, UCF-sports and Hollywood-2. ",
        "title": "Transformer-based Video Saliency Prediction with High Temporal Dimension  Decoding",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07944",
        "abstract_url": "http://arxiv.org/abs/2401.07944",
        "authors": [
            {
                "last_name": "Das",
                "first_name": "Rupak Kumar"
            },
            {
                "last_name": "Pedersen",
                "first_name": "Dr. Ted"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper uses the BERT model, which is a transformer-based architecture, to solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017. BERT is a very powerful large language model for classification tasks when the amount of training data is small. For this experiment, we have used the BERT{\\textsubscript{\\tiny BASE}} model, which has 12 hidden layers. This model provides better accuracy, precision, recall, and f1 score than the Naive Bayes baseline model. It performs better in binary classification subtasks than the multi-class classification subtasks. We also considered all kinds of ethical issues during this experiment, as Twitter data contains personal and sensible information. The dataset and code used in our experiment can be found in this GitHub repository. ",
        "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07947",
        "abstract_url": "http://arxiv.org/abs/2401.07947",
        "authors": [
            {
                "last_name": "Akanda",
                "first_name": "Md Rakibul Karim"
            },
            {
                "last_name": "Lazo",
                "first_name": "Jason"
            },
            {
                "last_name": "Carter",
                "first_name": "Quintwon"
            },
            {
                "last_name": "Roberts",
                "first_name": "Haineef"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The project we embarked on is making an electronic robot that can deliver a package along a set route through infrared sensors. It uses the infrared sensors to determine if the path it is following is correct or if it is off course. This is determined by sending off a photon to reflect off the path and determines if it is on a light surface by the amount of light emitted back or if it is a dark surface by the amount of light that is not present. In addition to following a line, the user can stop and start the robot at any interval through the infrared remote control. The project is a combination of the practical parts of machinery with the software part of coding in Arduino which is a coding subsect of C++. This can lead to endless possibilities that could help a wide variety of people from all ranges of life, especially with those that live with disabilities ",
        "title": "Delivery Line Tracking Robot",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07950",
        "abstract_url": "http://arxiv.org/abs/2401.07950",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dan"
            },
            {
                "last_name": "Hu",
                "first_name": "Ziniu"
            },
            {
                "last_name": "Zhoubian",
                "first_name": "Sining"
            },
            {
                "last_name": "Du",
                "first_name": "Zhengxiao"
            },
            {
                "last_name": "Yang",
                "first_name": "Kaiyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Zihan"
            },
            {
                "last_name": "Yue",
                "first_name": "Yisong"
            },
            {
                "last_name": "Dong",
                "first_name": "Yuxiao"
            },
            {
                "last_name": "Tang",
                "first_name": "Jie"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  \\label{sec:abstract} Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing mathematics, physics, chemistry, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing their capabilities in scientific and mathematical reasoning. Remarkably, SciGLM consistently improves both the base model (ChatGLM3-6B-Base) and larger-scale models (12B and 32B), without sacrificing the language understanding capabilities of the base model. This makes SciGLM a suitable foundational model to facilitate diverse scientific discovery tasks. For the benefit of the wider research community, we release SciInstruct, SciGLM, alongside a self-reflective framework and fine-tuning code at \\url{https://github.com/THUDM/SciGLM}. ",
        "title": "SciGLM: Training Scientific Language Models with Self-Reflective  Instruction Annotation and Tuning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07951",
        "abstract_url": "http://arxiv.org/abs/2401.07951",
        "authors": [
            {
                "last_name": "Liao",
                "first_name": "Zukang"
            },
            {
                "last_name": "Chen",
                "first_name": "Min"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Image similarity has been extensively studied in computer vision. In recently years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling similarity, assigning a numerical score to a pair of images is less intuitive than determining if an image A is closer to a reference image R than another image B. In this work, we present a novel approach for building an image similarity model based on labelled data in the form of A:R vs B:R. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. In particular, we employed two ML techniques to construct such an ensemble model, namely dimensionality reduction and MLP regressors. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the model trained with mixed imagery data as well as existing similarity models, e.g., CLIP and DINO. This work demonstrate that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling. ",
        "title": "Image Similarity using An Ensemble of Context-Sensitive Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07955",
        "abstract_url": "http://arxiv.org/abs/2401.07955",
        "authors": [
            {
                "last_name": "Khatun",
                "first_name": "Aisha"
            },
            {
                "last_name": "Brown",
                "first_name": "Daniel G."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The widespread adoption of Large Language Models (LLMs) has become commonplace, particularly with the emergence of open-source models. More importantly, smaller models are well-suited for integration into consumer devices and are frequently employed either as standalone solutions or as subroutines in various AI tasks. Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations. In this study, we tackle one of the most widely used tasks - answering Multiple Choice Question (MCQ). We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent. These results are rather alarming given the extensive use of MCQ tests with these models. We recommend exercising caution and testing task understanding before using MCQ to evaluate LLMs in any field whatsoever. ",
        "title": "A Study on Large Language Models' Limitations in Multiple-Choice  Question Answering",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07957",
        "abstract_url": "http://arxiv.org/abs/2401.07957",
        "authors": [
            {
                "last_name": "Jacobellis",
                "first_name": "Dan"
            },
            {
                "last_name": "Cummings",
                "first_name": "Daniel"
            },
            {
                "last_name": "Yadwadkar",
                "first_name": "Neeraja J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG",
            "SD"
        ],
        "abstract": "  In the field of neural data compression, the prevailing focus has been on optimizing algorithms for either classical distortion metrics, such as PSNR or SSIM, or human perceptual quality. With increasing amounts of data consumed by machines rather than humans, a new paradigm of machine-oriented compression$\\unicode{x2013}$which prioritizes the retention of features salient for machine perception over traditional human-centric criteria$\\unicode{x2013}$has emerged, creating several new challenges to the development, evaluation, and deployment of systems utilizing lossy compression. In particular, it is unclear how different approaches to lossy compression will affect the performance of downstream machine perception tasks. To address this under-explored area, we evaluate various perception models$\\unicode{x2013}$including image classification, image segmentation, speech recognition, and music source separation$\\unicode{x2013}$under severe lossy compression. We utilize several popular codecs spanning conventional, neural, and generative compression architectures. Our results indicate three key findings: (1) using generative compression, it is feasible to leverage highly compressed data while incurring a negligible impact on machine perceptual quality; (2) machine perceptual quality correlates strongly with deep similarity metrics, indicating a crucial role of these metrics in the development of machine-oriented codecs; and (3) using lossy compressed datasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive scenarios where lossy compression increases machine perceptual quality rather than degrading it. To encourage engagement on this growing area of research, our code and experiments are available at: https://github.com/danjacobellis/MPQ. ",
        "title": "Machine Perceptual Quality: Evaluating the Impact of Severe Lossy  Compression on Audio and Image Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07958",
        "abstract_url": "http://arxiv.org/abs/2401.07958",
        "authors": [
            {
                "last_name": "Vatamany",
                "first_name": "Lorand"
            },
            {
                "last_name": "Mehrkanoon",
                "first_name": "Siamak"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Accurate precipitation nowcasting is essential for various purposes, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolutional operations. This enhancement enables the model to directly process the high-dimensional spatiotemporal graph of precipitation maps and exploits higher-order correlations between the data dimensions. We evaluate our model on seven years of precipitation maps across Europe and its neighboring areas collected from the ERA5 dataset, provided by Copernicus. The model receives a fully connected graph in which each node represents historical observations from a specific region on the map. Consequently, each node contains a 3D tensor with time, height, and width dimensions. Experimental results demonstrate that the proposed GD-CAF model outperforms the other examined models. Furthermore, the averaged seasonal spatial and temporal attention scores over the test set are visualized to provide additional insights about the strongest connections between different regions or time steps. These visualizations shed light on the decision-making process of our model. ",
        "title": "GD-CAF: Graph Dual-stream Convolutional Attention Fusion for  Precipitation Nowcasting",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07960",
        "abstract_url": "http://arxiv.org/abs/2401.07960",
        "authors": [
            {
                "last_name": "Kumar",
                "first_name": "Vimal"
            },
            {
                "last_name": "Mayo",
                "first_name": "Juliette"
            },
            {
                "last_name": "Bahiss",
                "first_name": "Khadija"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Machine learning (ML) and artificial intelligence (AI) techniques have now become commonplace in software products and services. When threat modelling a system, it is therefore important that we consider threats unique to ML and AI techniques, in addition to threats to our software. In this paper, we present a threat model that can be used to systematically uncover threats to AI based software. The threat model consists of two main parts, a model of the software development process for AI based software and an attack taxonomy that has been developed using attacks found in adversarial AI research. We apply the threat model to two real life AI based software and discuss the process and the threats found. ",
        "title": "ADMIn: Attacks on Dataset, Model and Input. A Threat Model for AI Based  Software",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07961",
        "abstract_url": "http://arxiv.org/abs/2401.07961",
        "authors": [
            {
                "last_name": "Teter",
                "first_name": "Alexis M. H."
            },
            {
                "last_name": "Nodozi",
                "first_name": "Iman"
            },
            {
                "last_name": "Halder",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection of the OMT with the Schr\\\"odinger bridge problem (SBP). This also shows that the probabilistic Lambert problem with additive dynamic process noise is in fact a generalized SBP, and can be solved numerically using the so-called Schr\\\"odinger factors, as we do in this work. We explain how the resulting analysis leads to solving a boundary-coupled system of reaction-diffusion PDEs where the nonlinear gravitational potential appears as the reaction rate. We propose novel algorithms for the same, and present illustrative numerical results. Our analysis and the algorithmic framework are nonparametric, i.e., we make neither statistical (e.g., Gaussian, first few moments, mixture or exponential family, finite dimensionality of the sufficient statistic) nor dynamical (e.g., Taylor series) approximations. ",
        "title": "Solution of the Probabilistic Lambert Problem: Connections with Optimal  Mass Transport, Schr\\\"odinger Bridge and Reaction-Diffusion PDEs",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07962",
        "abstract_url": "http://arxiv.org/abs/2401.07962",
        "authors": [
            {
                "last_name": "Beam",
                "first_name": "Chris"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jincheng"
            },
            {
                "last_name": "Kakavitsas",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Hague",
                "first_name": "Collin"
            },
            {
                "last_name": "Wolek",
                "first_name": "Artur"
            },
            {
                "last_name": "Willis",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  This article discusses the use of a simulated environment to predict algorithm results in the real world. Simulators are crucial in allowing researchers to test algorithms, sensor integration, and navigation systems without deploying expensive hardware. This article examines how the AirSim simulator, Unreal Engine, and Cesium plugin can be used to generate simulated digital twin models of real-world locations. Several technical challenges in completing the analysis are discussed and the technical solutions are detailed in this article. Work investigates how to assess mapping results for a real-life experiment using Cesium Tiles provided by digital twins of the experimental location. This is accompanied by a description of a process for duplicating real-world flights in simulation. The performance of these methods is evaluated by analyzing real-life and experimental image telemetry with the Direct Sparse Odometry (DSO) mapping algorithm. Results indicate that Cesium Tiles environments can provide highly accurate models of ground truth geometry after careful alignment. Further, results from real-life and simulated telemetry analysis indicate that the virtual simulation results accurately predict real-life results. Findings indicate that the algorithm results in real life and in the simulated duplicate exhibited a high degree of similarity. This indicates that the use of Cesium Tiles environments as a virtual digital twin for real-life experiments will provide representative results for such algorithms. The impact of this can be significant, potentially allowing expansive virtual testing of robotic systems at specific deployment locations to develop solutions that are tailored to the environment and potentially outperforming solutions meant to work in completely generic environments. ",
        "title": "Cesium Tiles for High-realism Simulation and Comparing SLAM Results in  Corresponding Virtual and Real-world Environments",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07964",
        "abstract_url": "http://arxiv.org/abs/2401.07964",
        "authors": [
            {
                "last_name": "Mollo",
                "first_name": "Dimitri Coelho"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no less relevant to intelligence research, to those hypothesised for humans. ",
        "title": "AI-as-exploration: Navigating intelligence space",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07967",
        "abstract_url": "http://arxiv.org/abs/2401.07967",
        "authors": [
            {
                "last_name": "Kimelman",
                "first_name": "Robert G."
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "CL",
            "HC"
        ],
        "abstract": "  A novel freestyle rap software, MCMChaos 0.0.1, based on rap music transcriptions created in previous research is presented. The software has three different versions, each making use of different mathematical simulation methods: collapsed gibbs sampler and lorenz attractor simulation. As far as we know, these simulation methods have never been used in rap music generation before. The software implements Python Text-to-Speech processing (pyttxs) to convert text wrangled from the MCFlow corpus into English speech. In each version, values simulated from each respective mathematical model alter the rate of speech, volume, and (in the multiple voice case) the voice of the text-to-speech engine on a line-by-line basis. The user of the software is presented with a real-time graphical user interface (GUI) which instantaneously changes the initial values read into the mathematical simulation methods. Future research might attempt to allow for more user control and autonomy. ",
        "title": "MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07969",
        "abstract_url": "http://arxiv.org/abs/2401.07969",
        "authors": [
            {
                "last_name": "Battle",
                "first_name": "Steve"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a novel form of Liquid Automata, using this to simulate autopoiesis, whereby living machines self-organise in the physical realm. This simulation is based on an earlier Cellular Automaton described by Francisco Varela. The basis of Liquid Automata is a particle simulation with additional rules about how particles are transformed on collision with other particles. Unlike cellular automata, there is no fixed grid or time-step, only particles moving about and colliding with each other in a continuous space/time. ",
        "title": "Simulated Autopoiesis in Liquid Automata",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07971",
        "abstract_url": "http://arxiv.org/abs/2401.07971",
        "authors": [
            {
                "last_name": "Koval",
                "first_name": "Karina"
            },
            {
                "last_name": "Herzog",
                "first_name": "Roland"
            },
            {
                "last_name": "Scheichl",
                "first_name": "Robert"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a flexible method for computing Bayesian optimal experimental designs (BOEDs) for inverse problems with intractable posteriors. The approach is applicable to a wide range of BOED problems and can accommodate various optimality criteria, prior distributions and noise models. The key to our approach is the construction of a transport-map-based surrogate to the joint probability law of the design, observational and inference random variables. This order-preserving transport map is constructed using tensor trains and can be used to efficiently sample from (and evaluate approximate densities of) conditional distributions that are used to define many commonly-used optimality criteria. The algorithm is also extended to sequential data acquisition problems, where experiments can be performed in sequence and used to update the state of knowledge about the unknown parameters. The sequential BOED problem is made computationally feasible by preconditioning the approximation of the joint density at the current stage using transport maps constructed at previous stages. The flexibility of our approach in finding optimal designs is illustrated with some numerical examples inspired by disease modeling and the reconstruction of subsurface structures in aquifers. ",
        "title": "Tractable Optimal Experimental Design using Transport Maps",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07974",
        "abstract_url": "http://arxiv.org/abs/2401.07974",
        "authors": [
            {
                "last_name": "Zhandry",
                "first_name": "Mark"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  General quantum computation consists of unitary operations and also measurements. It is well known that intermediate quantum measurements can be deferred to the end of the computation, resulting in an equivalent purely unitary computation. While time efficient, this transformation blows up the space to linear in the running time, which could be super-polynomial for low-space algorithms. Fefferman and Remscrim (STOC'21) and Girish, Raz and Zhan (ICALP'21) show different transformations which are space efficient, but blow up the running time by a factor that is exponential in the space. This leaves the case of algorithms with small-but-super-logarithmic space as incurring a large blowup in either time or space complexity. We show that such a blowup is likely inherent, demonstrating that any \"black-box\" transformation which removes intermediate measurements must significantly blow up either space or time. ",
        "title": "The Space-Time Cost of Purifying Quantum Computations",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07977",
        "abstract_url": "http://arxiv.org/abs/2401.07977",
        "authors": [
            {
                "last_name": "Sengupta",
                "first_name": "Saptarshi"
            },
            {
                "last_name": "Heaton",
                "first_name": "Connor"
            },
            {
                "last_name": "Mitra",
                "first_name": "Prasenjit"
            },
            {
                "last_name": "Sarkar",
                "first_name": "Soumalya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Machine Reading Comprehension (MRC) has been a long-standing problem in NLP and, with the recent introduction of the BERT family of transformer based language models, it has come a long way to getting solved. Unfortunately, however, when BERT variants trained on general text corpora are applied to domain-specific text, their performance inevitably degrades on account of the domain shift i.e. genre/subject matter discrepancy between the training and downstream application data. Knowledge graphs act as reservoirs for either open or closed domain information and prior studies have shown that they can be used to improve the performance of general-purpose transformers in domain-specific applications. Building on existing work, we introduce a method using Multi-Layer Perceptrons (MLPs) for aligning and integrating embeddings extracted from knowledge graphs with the embeddings spaces of pre-trained language models (LMs). We fuse the aligned embeddings with open-domain LMs BERT and RoBERTa, and fine-tune them for two MRC tasks namely span detection (COVID-QA) and multiple-choice questions (PubMedQA). On the COVID-QA dataset, we see that our approach allows these models to perform similar to their domain-specific counterparts, Bio/Sci-BERT, as evidenced by the Exact Match (EM) metric. With regards to PubMedQA, we observe an overall improvement in accuracy while the F1 stays relatively the same over the domain-specific models. ",
        "title": "Leveraging External Knowledge Resources to Enable Domain-Specific  Comprehension",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07985",
        "abstract_url": "http://arxiv.org/abs/2401.07985",
        "authors": [
            {
                "last_name": "Barbie",
                "first_name": "Alexander"
            },
            {
                "last_name": "Hasselbring",
                "first_name": "Wilhelm"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The transformation to Industry 4.0 also transforms the processes of how we develop intelligent manufacturing production systems. To advance the software development of these new (embedded) software systems, digital twins may be employed. However, there is no consensual definition of what a digital twin is. In this paper, we give an overview of the current state of the digital twin concept and formalize the digital twin concept using the Object-Z notation. This formalization includes the concepts of physical twins, digital models, digital templates, digital threads, digital shadows, digital twins, and digital twin prototypes. The relationships between all these concepts are visualized as UML class diagrams.   Our digital twin prototype (DTP) approach supports engineers during the development and automated testing of complex embedded software systems. This approach enable engineers to test embedded software systems in a virtual context, without the need of a connection to a physical object. In continuous integration / continuous deployment pipelines such digital twin prototypes can be used for automated integration testing and, thus, allow for an agile verification and validation process.   In this paper, we demonstrate and report on how to apply and implement a digital twin by the example of two real-world field studies (ocean observation systems and smart farming). For independent replication and extension of our approach by other researchers, we provide a lab study published open source on GitHub. ",
        "title": "From Digital Twins to Digital Twin Prototypes: Concepts, Formalization,  and Applications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07986",
        "abstract_url": "http://arxiv.org/abs/2401.07986",
        "authors": [
            {
                "last_name": "Cherubini",
                "first_name": "Giacomo"
            },
            {
                "last_name": "Micheli",
                "first_name": "Giacomo"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Let $n$ be a prime power, $r$ be a prime with $r\\mid n-1$, and $\\varepsilon\\in (0,1/2)$. Using the theory of multiplicative character sums and superelliptic curves, we construct new codes over $\\mathbb F_r$ having length $n$, relative distance $(r-1)/r+O(n^{-\\varepsilon})$ and rate $n^{-1/2-\\varepsilon}$. When $r=2$, our binary codes have exponential size when compared to all previously known families of linear and non-linear codes with relative distance asymptotic to $1/2$, such as Delsarte--Goethals codes. Moreover, our codes are linear. ",
        "title": "A New Class of Linear Codes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07988",
        "abstract_url": "http://arxiv.org/abs/2401.07988",
        "authors": [
            {
                "last_name": "Gavriilidis",
                "first_name": "Panagiotis"
            },
            {
                "last_name": "Atzeni",
                "first_name": "Italo"
            },
            {
                "last_name": "Alexandropoulos",
                "first_name": "George C."
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "IT"
        ],
        "abstract": "  The massive Multiple-Input Multiple-Output (mMIMO) concept has been recently moving forward to extreme scales to address the envisioned requirements of next generation networks. However, the extension of conventional architectures will result in significant cost and power consumption. To this end, metasurface-based transceivers, consisting of microstrips of metamaterials, have recently emerged as an efficient enabler of extreme mMIMO systems. In this paper, we consider metasurface-based receivers with a $1$-bit Analog-to-Digital Converter (ADC) per microstrip and develop an analytical framework for the optimization of the analog and digital combining matrices. Our numerical results, including comparisons with fully digital, infinite-resolution MIMO, provide useful insights into the role of various system parameters. ",
        "title": "Metasurface-Based Receivers with $1$-bit ADCs for Multi-User Uplink  Communications",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07990",
        "abstract_url": "http://arxiv.org/abs/2401.07990",
        "authors": [
            {
                "last_name": "Khanal",
                "first_name": "Bidur"
            },
            {
                "last_name": "Bhattarai",
                "first_name": "Binod"
            },
            {
                "last_name": "Khanal",
                "first_name": "Bishesh"
            },
            {
                "last_name": "Linte",
                "first_name": "Cristian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Noisy labels can significantly impact medical image classification, particularly in deep learning, by corrupting learned features. Self-supervised pretraining, which doesn't rely on labeled data, can enhance robustness against noisy labels. However, this robustness varies based on factors like the number of classes, dataset complexity, and training size. In medical images, subtle inter-class differences and modality-specific characteristics add complexity. Previous research hasn't comprehensively explored the interplay between self-supervised learning and robustness against noisy labels in medical image classification, considering all these factors. In this study, we address three key questions: i) How does label noise impact various medical image classification datasets? ii) Which types of medical image datasets are more challenging to learn and more affected by label noise? iii) How do different self-supervised pretraining methods enhance robustness across various medical image datasets? Our results show that DermNet, among five datasets (Fetal plane, DermNet, COVID-DU-Ex, MURA, NCT-CRC-HE-100K), is the most challenging but exhibits greater robustness against noisy labels. Additionally, contrastive learning stands out among the eight self-supervised methods as the most effective approach to enhance robustness against noisy labels. ",
        "title": "How does self-supervised pretraining improve robustness against noisy  labels across various medical image classification datasets?",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07991",
        "abstract_url": "http://arxiv.org/abs/2401.07991",
        "authors": [
            {
                "last_name": "Hamidi",
                "first_name": "Shayan Mohajer"
            },
            {
                "last_name": "Ye",
                "first_name": "Linfeng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Deep neural networks (DNNs) could be deceived by generating human-imperceptible perturbations of clean samples. Therefore, enhancing the robustness of DNNs against adversarial attacks is a crucial task. In this paper, we aim to train robust DNNs by limiting the set of outputs reachable via a norm-bounded perturbation added to a clean sample. We refer to this set as adversarial polytope, and each clean sample has a respective adversarial polytope. Indeed, if the respective polytopes for all the samples are compact such that they do not intersect the decision boundaries of the DNN, then the DNN is robust against adversarial samples. Hence, the inner-working of our algorithm is based on learning \\textbf{c}onfined \\textbf{a}dversarial \\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we demonstrate the effectiveness of CAP over existing adversarial robustness methods in improving the robustness of models against state-of-the-art attacks including AutoAttack. ",
        "title": "Robustness Against Adversarial Attacks via Learning Confined Adversarial  Polytopes",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07992",
        "abstract_url": "http://arxiv.org/abs/2401.07992",
        "authors": [
            {
                "last_name": "\u00d6z",
                "first_name": "Burak"
            },
            {
                "last_name": "Gebele",
                "first_name": "Jonas"
            },
            {
                "last_name": "Singh",
                "first_name": "Parshant"
            },
            {
                "last_name": "Rezabek",
                "first_name": "Filip"
            },
            {
                "last_name": "Matthes",
                "first_name": "Florian"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Maximal Extractable Value (MEV) searching has gained prominence on the Ethereum blockchain since the surge in Decentralized Finance activities. In Ethereum, MEV extraction primarily hinges on fee payments to block proposers. However, in First-Come-First-Served (FCFS) blockchain networks, the focus shifts to latency optimizations, akin to High-Frequency Trading in Traditional Finance. This paper illustrates the dynamics of the MEV extraction game in an FCFS network, specifically Algorand. We introduce an arbitrage detection algorithm tailored to the unique time constraints of FCFS networks and assess its effectiveness. Additionally, our experiments investigate potential optimizations in Algorand's network layer to secure optimal execution positions.   Our analysis reveals that while the states of relevant trading pools are updated approximately every six blocks on median, pursuing MEV at the block state level is not viable on Algorand, as arbitrage opportunities are typically executed within the blocks they appear. Our algorithm's performance under varying time constraints underscores the importance of timing in arbitrage discovery. Furthermore, our network-level experiments identify critical transaction prioritization strategies for Algorand's FCFS network. Key among these is reducing latency in connections with relays that are well-connected to high-staked proposers. ",
        "title": "Playing the MEV Game on a First-Come-First-Served Blockchain",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07993",
        "abstract_url": "http://arxiv.org/abs/2401.07993",
        "authors": [
            {
                "last_name": "Kruthoff",
                "first_name": "Jorrit"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only models, we observe the same implementation and provide suggestive evidence for its existence in three 7B large language models. ",
        "title": "Carrying over algorithm in transformers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07994",
        "abstract_url": "http://arxiv.org/abs/2401.07994",
        "authors": [
            {
                "last_name": "Ruiz",
                "first_name": "Fernando Vallecillos"
            },
            {
                "last_name": "Grishina",
                "first_name": "Anastasiia"
            },
            {
                "last_name": "Hort",
                "first_name": "Max"
            },
            {
                "last_name": "Moonen",
                "first_name": "Leon"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "LG"
        ],
        "abstract": "  Research shows that grammatical mistakes in a sentence can be corrected by translating it to another language and back using neural machine translation with language models. We investigate whether this correction capability of Large Language Models (LLMs) extends to Automatic Program Repair (APR). Current generative models for APR are pre-trained on source code and fine-tuned for repair. This paper proposes bypassing the fine-tuning step and using Round-Trip Translation (RTT): translation of code from one programming language to another programming or natural language, and back. We hypothesize that RTT with LLMs restores the most commonly seen patterns in code during pre-training, i.e., performs a regression toward the mean, which removes bugs as they are a form of noise w.r.t. the more frequent, natural, bug-free code in the training data. To test this hypothesis, we employ eight recent LLMs pre-trained on code, including the latest GPT versions, and four common program repair benchmarks in Java. We find that RTT with English as an intermediate language repaired 101 of 164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are unique bugs that are not repaired by other LLMs fine-tuned for APR. Our findings highlight the viability of round-trip translation with LLMs as a technique for automated program repair and its potential for research in software engineering.   Keywords: automated program repair, large language model, machine translation ",
        "title": "A Novel Approach for Automatic Program Repair using Round-Trip  Translation with Large Language Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07995",
        "abstract_url": "http://arxiv.org/abs/2401.07995",
        "authors": [
            {
                "last_name": "Varlioglu",
                "first_name": "Said"
            },
            {
                "last_name": "Elsayed",
                "first_name": "Nelly"
            },
            {
                "last_name": "Varlioglu",
                "first_name": "Eva Ruhsar"
            },
            {
                "last_name": "Ozer",
                "first_name": "Murat"
            },
            {
                "last_name": "ElSayed",
                "first_name": "Zag"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Fileless malware predominantly relies on PowerShell scripts, leveraging the native capabilities of Windows systems to execute stealthy attacks that leave no traces on the victim's system. The effectiveness of the fileless method lies in its ability to remain operational on victim endpoints through memory execution, even if the attacks are detected, and the original malicious scripts are removed. Threat actors have increasingly utilized this technique, particularly since 2017, to conduct cryptojacking attacks. With the emergence of new Remote Code Execution (RCE) vulnerabilities in ubiquitous libraries, widespread cryptocurrency mining attacks have become prevalent, often employing fileless techniques. This paper provides a comprehensive analysis of PowerShell scripts of fileless cryptojacking, dissecting the common malicious patterns based on the MITRE ATT&CK framework. ",
        "title": "The Pulse of Fileless Cryptojacking Attacks: Malicious PowerShell  Scripts",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.07996",
        "abstract_url": "http://arxiv.org/abs/2401.07996",
        "authors": [
            {
                "last_name": "Aiswarya",
                "first_name": "C"
            },
            {
                "last_name": "Mal",
                "first_name": "Soumodev"
            },
            {
                "last_name": "Saivasan",
                "first_name": "Prakash"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "LO"
        ],
        "abstract": "  We study the satisfiability of string constraints where context-free membership constraints may be imposed on variables. Additionally a variable may be constrained to be a subword of a word obtained by shuffling variables and their transductions. The satisfiability problem is known to be undecidable even without rational transductions. It is known to be NExptime-complete without transductions, if the subword relations between variables do not have a cyclic dependency between them. We show that the satisfiability problem stays decidable in this fragment even when rational transductions are added. It is 2NExptime-complete with context-free membership, and NExptime-complete with only regular membership. For the lower bound we prove a technical lemma that is of independent interest: The length of the shortest word in the intersection of a pushdown automaton (of size $O(n)$) and $n$ finite-state automata (each of size $O(n)$) can be double exponential in $n$. ",
        "title": "Satisfiability of Context-free String Constraints with Subword-ordering  and Transducers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08001",
        "abstract_url": "http://arxiv.org/abs/2401.08001",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Donghyun"
            },
            {
                "last_name": "Yin",
                "first_name": "Ruokai"
            },
            {
                "last_name": "Kim",
                "first_name": "Youngeun"
            },
            {
                "last_name": "Moitra",
                "first_name": "Abhishek"
            },
            {
                "last_name": "Li",
                "first_name": "Yuhang"
            },
            {
                "last_name": "Panda",
                "first_name": "Priyadarshini"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  Spiking Neural Networks (SNNs) have gained significant attention as a potentially energy-efficient alternative for standard neural networks with their sparse binary activation. However, SNNs suffer from memory and computation overhead due to spatio-temporal dynamics and multiple backpropagation computations across timesteps during training. To address this issue, we introduce Tensor Train Decomposition for Spiking Neural Networks (TT-SNN), a method that reduces model size through trainable weight decomposition, resulting in reduced storage, FLOPs, and latency. In addition, we propose a parallel computation pipeline as an alternative to the typical sequential tensor computation, which can be flexibly integrated into various existing SNN architectures. To the best of our knowledge, this is the first of its kind application of tensor decomposition in SNNs. We validate our method using both static and dynamic datasets, CIFAR10/100 and N-Caltech101, respectively. We also propose a TT-SNN-tailored training accelerator to fully harness the parallelism in TT-SNN. Our results demonstrate substantial reductions in parameter size (7.98X), FLOPs (9.25X), training time (17.7%), and training energy (28.3%) during training for the N-Caltech101 dataset, with negligible accuracy degradation. ",
        "title": "TT-SNN: Tensor Train Decomposition for Efficient Spiking Neural Network  Training",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08002",
        "abstract_url": "http://arxiv.org/abs/2401.08002",
        "authors": [
            {
                "last_name": "Ghaderi",
                "first_name": "Hamid"
            },
            {
                "last_name": "Foreman",
                "first_name": "Brandon"
            },
            {
                "last_name": "Reddy",
                "first_name": "Chandan K."
            },
            {
                "last_name": "Subbian",
                "first_name": "Vignesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traumatic Brain Injury (TBI) presents a broad spectrum of clinical presentations and outcomes due to its inherent heterogeneity, leading to diverse recovery trajectories and varied therapeutic responses. While many studies have delved into TBI phenotyping for distinct patient populations, identifying TBI phenotypes that consistently generalize across various settings and populations remains a critical research gap. Our research addresses this by employing multivariate time-series clustering to unveil TBI's dynamic intricates. Utilizing a self-supervised learning-based approach to clustering multivariate time-Series data with missing values (SLAC-Time), we analyzed both the research-centric TRACK-TBI and the real-world MIMIC-IV datasets. Remarkably, the optimal hyperparameters of SLAC-Time and the ideal number of clusters remained consistent across these datasets, underscoring SLAC-Time's stability across heterogeneous datasets. Our analysis revealed three generalizable TBI phenotypes ({\\alpha}, \\b{eta}, and {\\gamma}), each exhibiting distinct non-temporal features during emergency department visits, and temporal feature profiles throughout ICU stays. Specifically, phenotype {\\alpha} represents mild TBI with a remarkably consistent clinical presentation. In contrast, phenotype \\b{eta} signifies severe TBI with diverse clinical manifestations, and phenotype {\\gamma} represents a moderate TBI profile in terms of severity and clinical diversity. Age is a significant determinant of TBI outcomes, with older cohorts recording higher mortality rates. Importantly, while certain features varied by age, the core characteristics of TBI manifestations tied to each phenotype remain consistent across diverse populations. ",
        "title": "Discovery of Generalizable TBI Phenotypes Using Multivariate Time-Series  Clustering",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08003",
        "abstract_url": "http://arxiv.org/abs/2401.08003",
        "authors": [
            {
                "last_name": "Alcalde-Llergo",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Yeguas-Bol\u00edvar",
                "first_name": "Enrique"
            },
            {
                "last_name": "Zingoni",
                "first_name": "Andrea"
            },
            {
                "last_name": "Fuerte-Jurado",
                "first_name": "Alejandro"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Jewelry recognition is a complex task due to the different styles and designs of accessories. Precise descriptions of the various accessories is something that today can only be achieved by experts in the field of jewelry. In this work, we propose an approach for jewelry recognition using computer vision techniques and image captioning, trying to simulate this expert human behavior of analyzing accessories. The proposed methodology consist on using different image captioning models to detect the jewels from an image and generate a natural language description of the accessory. Then, this description is also utilized to classify the accessories at different levels of detail. The generated caption includes details such as the type of jewel, color, material, and design. To demonstrate the effectiveness of the proposed method in accurately recognizing different types of jewels, a dataset consisting of images of accessories belonging to jewelry stores in C\\'ordoba (Spain) has been created. After testing the different image captioning architectures designed, the final model achieves a captioning accuracy of 95\\%. The proposed methodology has the potential to be used in various applications such as jewelry e-commerce, inventory management or automatic jewels recognition to analyze people's tastes and social status. ",
        "title": "Jewelry Recognition via Encoder-Decoder Models",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08008",
        "abstract_url": "http://arxiv.org/abs/2401.08008",
        "authors": [
            {
                "last_name": "Alcalde-Llergo",
                "first_name": "Jos\u00e9 M."
            },
            {
                "last_name": "Garc\u00eda-Mart\u00ednez",
                "first_name": "Carlos"
            },
            {
                "last_name": "Vaquero-Abell\u00e1n",
                "first_name": "Manuel"
            },
            {
                "last_name": "Aparicio-Mart\u00ednez",
                "first_name": "Pilar"
            },
            {
                "last_name": "Yeguas-Bol\u00edvar",
                "first_name": "Enrique"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Homelessness is a social and health problem with great repercussions in Europe. Many non-governmental organisations help homeless people by collecting and analysing large amounts of information about them. However, these tasks are not always easy to perform, and hinder other of the organisations duties. The SINTECH project was created to tackle this issue proposing two different tools: a mobile application to quickly and easily collect data; and a software based on artificial intelligence which obtains interesting information from the collected data. The first one has been distributed to some Spanish organisations which are using it to conduct surveys of homeless people. The second tool implements different feature selection and association rules mining methods. These artificial intelligence techniques have allowed us to identify the most relevant features and some interesting association rules from previously collected homeless data. ",
        "title": "Analysing the Needs of Homeless People Using Feature Selection and  Mining Association Rules",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08013",
        "abstract_url": "http://arxiv.org/abs/2401.08013",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jiayang"
            },
            {
                "last_name": "Wang",
                "first_name": "Qianni"
            },
            {
                "last_name": "Feng",
                "first_name": "Liyang"
            },
            {
                "last_name": "Xie",
                "first_name": "Jun"
            },
            {
                "last_name": "Nie",
                "first_name": "Yu Marco"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "MA"
        ],
        "abstract": "  The lack of a unique user equilibrium (UE) route flow in traffic assignment has posed a significant challenge to many transportation applications. The maximum-entropy principle, which advocates for the consistent selection of the most likely solution as a representative, is often used to address the challenge. Built on a recently proposed day-to-day (DTD) discrete-time dynamical model called cumulative logit (CULO), this study provides a new behavioral underpinning for the maximum-entropy UE (MEUE) route flow. It has been proven that CULO can reach a UE state without presuming travelers are perfectly rational. Here, we further establish that CULO always converges to the MEUE route flow if (i) travelers have zero prior information about routes and thus are forced to give all routes an equal choice probability, or (ii) all travelers gather information from the same source such that the so-called general proportionality condition is satisfied. Thus, CULO may be used as a practical solution algorithm for the MEUE problem. To put this idea into practice, we propose to eliminate the route enumeration requirement of the original CULO model through an iterative route discovery scheme. We also examine the discrete-time versions of four popular continuous-time dynamical models and compare them to CULO. The analysis shows that the replicator dynamic is the only one that has the potential to reach the MEUE solution with some regularity. The analytical results are confirmed through numerical experiments. ",
        "title": "A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium  Problem",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08014",
        "abstract_url": "http://arxiv.org/abs/2401.08014",
        "authors": [
            {
                "last_name": "Sharma",
                "first_name": "Manish"
            },
            {
                "last_name": "Heard",
                "first_name": "Jamison"
            },
            {
                "last_name": "Saber",
                "first_name": "Eli"
            },
            {
                "last_name": "Markopoulos",
                "first_name": "Panos P."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  While Convolutional Neural Networks (CNNs) excel at learning complex latent-space representations, their over-parameterization can lead to overfitting and reduced performance, particularly with limited data. This, alongside their high computational and memory demands, limits the applicability of CNNs for edge deployment. Low-rank matrix approximation has emerged as a promising approach to reduce CNN parameters, but its application presents challenges including rank selection and performance loss. To address these issues, we propose an efficient training method for CNN compression via dynamic parameter rank pruning. Our approach integrates efficient matrix factorization and novel regularization techniques, forming a robust framework for dynamic rank reduction and model compression. We use Singular Value Decomposition (SVD) to model low-rank convolutional filters and dense weight matrices and we achieve model compression by training the SVD factors with back-propagation in an end-to-end way. We evaluate our method on an array of modern CNNs, including ResNet-18, ResNet-20, and ResNet-32, and datasets like CIFAR-10, CIFAR-100, and ImageNet (2012), showcasing its applicability in computer vision. Our experiments show that the proposed method can yield substantial storage savings while maintaining or even enhancing classification performance. ",
        "title": "Convolutional Neural Network Compression via Dynamic Parameter Rank  Pruning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08015",
        "abstract_url": "http://arxiv.org/abs/2401.08015",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Quanquan C."
            },
            {
                "last_name": "Shun",
                "first_name": "Julian"
            },
            {
                "last_name": "Zablotchi",
                "first_name": "Igor"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "DS"
        ],
        "abstract": "  Maintaining a dynamic $k$-core decomposition is an important problem that identifies dense subgraphs in dynamically changing graphs. Recent work by Liu et al. [SPAA 2022] presents a parallel batch-dynamic algorithm for maintaining an approximate $k$-core decomposition. In their solution, both reads and updates need to be batched, and therefore each type of operation can incur high latency waiting for the other type to finish. To tackle most real-world workloads, which are dominated by reads, this paper presents a novel hybrid concurrent-parallel dynamic $k$-core data structure where asynchronous reads can proceed concurrently with batches of updates, leading to significantly lower read latencies. Our approach is based on tracking causal dependencies between updates, so that causally related groups of updates appear atomic to concurrent readers. Our data structure guarantees linearizability and liveness for both reads and updates, and maintains the same approximation guarantees as prior work. Our experimental evaluation on a 30-core machine shows that our approach reduces read latency by orders of magnitude compared to the batch-dynamic algorithm, up to a $\\left(4.05 \\cdot 10^{5}\\right)$-factor. Compared to an unsynchronized (non-linearizable) baseline, our read latency overhead is only up to a $3.21$-factor greater, while improving accuracy of coreness estimates by up to a factor of $52.7$. ",
        "title": "Parallel $k$-Core Decomposition with Batched Updates and Asynchronous  Reads",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08016",
        "abstract_url": "http://arxiv.org/abs/2401.08016",
        "authors": [
            {
                "last_name": "Pacchiano",
                "first_name": "Aldo"
            },
            {
                "last_name": "Ghavamzadeh",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Bartlett",
                "first_name": "Peter"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We study contextual bandits in the presence of a stage-wise constraint (a constraint at each round), when the constraint must be satisfied both with high probability and in expectation. Obviously the setting where the constraint is in expectation is a relaxation of the one with high probability. We start with the linear case where both the contextual bandit problem (reward function) and the stage-wise constraint (cost function) are linear. In each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $T$-round regret bound for it. Our algorithms balance exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. In the high probability setting, we describe the minimum requirements for the action set in order for our algorithm to be tractable. In the setting that the constraint is in expectation, we further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting with regret analysis. Finally, we extend our results to the case where the reward and cost functions are both non-linear. We propose an algorithm for this case and prove a regret bound for it that characterize the function class complexity by the eluder dimension. ",
        "title": "Contextual Bandits with Stage-wise Constraints",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08017",
        "abstract_url": "http://arxiv.org/abs/2401.08017",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Ji"
            },
            {
                "last_name": "Wang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The main challenge for small object detection algorithms is to ensure accuracy while pursuing real-time performance. The RT-DETR model performs well in real-time object detection, but performs poorly in small object detection accuracy. In order to compensate for the shortcomings of the RT-DETR model in small object detection, two key improvements are proposed in this study. Firstly, The RT-DETR utilises a Transformer that receives input solely from the final layer of Backbone features. This means that the Transformer's input only receives semantic information from the highest level of abstraction in the Deep Network, and ignores detailed information such as edges, texture or color gradients that are critical to the location of small objects at lower levels of abstraction. Including only deep features can introduce additional background noise. This can have a negative impact on the accuracy of small object detection. To address this issue, we propose the fine-grained path augmentation method. This method helps to locate small objects more accurately by providing detailed information to the deep network. So, the input to the transformer contains both semantic and detailed information. Secondly, In RT-DETR, the decoder takes feature maps of different levels as input after concatenating them with equal weight. However, this operation is not effective in dealing with the complex relationship of multi-scale information captured by feature maps of different sizes. Therefore, we propose an adaptive feature fusion algorithm that assigns learnable parameters to each feature map from different levels. This allows the model to adaptively fuse feature maps from different levels and effectively integrate feature information from different scales. This enhances the model's ability to capture object features at different scales, thereby improving the accuracy of detecting small objects. ",
        "title": "Small Object Detection by DETR via Information Augmentation and Adaptive  Feature Fusion",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08018",
        "abstract_url": "http://arxiv.org/abs/2401.08018",
        "authors": [
            {
                "last_name": "Zaman",
                "first_name": "Mobasshira"
            },
            {
                "last_name": "Hwang",
                "first_name": "Jaejin"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This study evaluated the effect of virtual hand representation on the typing performance, upper extremity angle, neck muscle activity, and usability during virtual reality (VR) typing. A total of 15 participants (7 females and 8 males) performed a typing task with and without virtual hand representations. The optical motion capture data was utilized to capture the upper extremity angles, and electromyography device was used to collect the neck muscle activities (left and right splenius capitis). The results showed that the typing performance, upper extremity angle, neck muscle activity, and usability were significantly different with and without virtual hand representations. With the virtual hand representation, net typing speed (WPM) and usability increased significantly by 171.4% and 25% compared to the without hand representation. Without the virtual hand representation, participants showed increased wrist extension, and decreased right shoulder abduction angles. The variability of the neck rotation was increased while typing without the virtual hand representation. The neck muscle activities were increased with the virtual hand representation. The results suggest that typing with the virtual hand representation could positively affect the typing performance and usability and reduce the risk of the musculoskeletal disorders of the upper extremity. Future study could explore the effect of the virtual hand representation for users with varying levels of typing skills. ",
        "title": "Effects of Virtual Hand Representation on the Typing Performance, Upper  Extremity Angle, and Neck Muscle Activity during Virtual Reality Typing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08019",
        "abstract_url": "http://arxiv.org/abs/2401.08019",
        "authors": [
            {
                "last_name": "Phosavanh",
                "first_name": "Johnson"
            },
            {
                "last_name": "Matsypura",
                "first_name": "Dmytro"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CC"
        ],
        "abstract": "  The degree centrality of a node, defined to be the number of nodes adjacent to it, can be used as a measure of importance of a node to the structure of a network. This metric can be extended to paths in a network, where the degree centrality of a path is defined to be the number of nodes adjacent to it. In this paper, we reconsider the problem of finding the most degree-central shortest path in an unweighted network. We propose a polynomial algorithm with the worst-case running time of $O(|V|^3(\\Delta(G))^2)$, where $|V|$ is the number of vertices in the network and $\\Delta(G)$ is the maximum degree of the graph. We conduct a numerical study of our algorithm on synthetic and real-world networks and compare our results to the existing literature. In addition, we show that the same problem is NP-hard when a weighted graph is considered. ",
        "title": "A polynomial algorithm for the most degree-central shortest path problem",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08020",
        "abstract_url": "http://arxiv.org/abs/2401.08020",
        "authors": [
            {
                "last_name": "Salim",
                "first_name": "Shahreen"
            },
            {
                "last_name": "Hoque",
                "first_name": "Md Naimul"
            },
            {
                "last_name": "Mueller",
                "first_name": "Klaus"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Causal belief is a cognitive practice that humans apply everyday to reason about cause and effect relations between factors, phenomena, or events. Like optical illusions, humans are prone to drawing causal relations between events that are only coincidental (i.e., causal illusions). Researchers in domains such as cognitive psychology and healthcare often use logistically expensive experiments to understand causal beliefs and illusions. In this paper, we propose Belief Miner, a crowdsourcing method for evaluating people's causal beliefs and illusions. Our method uses the (dis)similarities between the causal relations collected from the crowds and experts to surface the causal beliefs and illusions. Through an iterative design process, we developed a web-based interface for collecting causal relations from a target population. We then conducted a crowdsourced experiment with 101 workers on Amazon Mechanical Turk and Prolific using this interface and analyzed the collected data with Belief Miner. We discovered a variety of causal beliefs and potential illusions, and we report the design implications for future research. ",
        "title": "Belief Miner: A Methodology for Discovering Causal Beliefs and Causal  Illusions from General Populations",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08021",
        "abstract_url": "http://arxiv.org/abs/2401.08021",
        "authors": [
            {
                "last_name": "Tsai",
                "first_name": "Chia Hsuan"
            },
            {
                "last_name": "Elyasi",
                "first_name": "Fatemeh"
            },
            {
                "last_name": "Ren",
                "first_name": "Peng"
            },
            {
                "last_name": "Manduchi",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  We introduce two iOS apps that have been designed to support wayfinding and backtracking for blind travelers navigating in indoor building environments. Wayfinding involves determining and following a route through the building's corridors to reach a destination, and assumes that the app has access to the floor plan of the building. Backtracking one's route, on the other hand, requires no map knowledge. Our apps only use the inertial and magnetic sensors of the smartphone, and thus require no infrastructure modification (e.g., installation and support of BLE beacons). Unlike systems that use the phone's camera, users of our apps can conveniently keep their phone tucked inside a pocket, while interacting with the apps using a smartwatch. Routing directions are given via speech. Both apps were tested in a user study with seven blind participants, who used them while navigating a campus building. ",
        "title": "All the Way There and Back: Inertial-Based, Phone-in-Pocket Indoor  Wayfinding and Backtracking Apps for Blind Travelers",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08022",
        "abstract_url": "http://arxiv.org/abs/2401.08022",
        "authors": [
            {
                "last_name": "Natarajan",
                "first_name": "Ramkumar"
            },
            {
                "last_name": "Yang",
                "first_name": "Hanlan"
            },
            {
                "last_name": "Xie",
                "first_name": "Qintong"
            },
            {
                "last_name": "Oza",
                "first_name": "Yash"
            },
            {
                "last_name": "Das",
                "first_name": "Manash Pratim"
            },
            {
                "last_name": "Islam",
                "first_name": "Fahad"
            },
            {
                "last_name": "Saleem",
                "first_name": "Muhammad Suhail"
            },
            {
                "last_name": "Choset",
                "first_name": "Howie"
            },
            {
                "last_name": "Likhachev",
                "first_name": "Maxim"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We are interested in studying sports with robots and starting with the problem of intercepting a projectile moving toward a robot manipulator equipped with a shield. To successfully perform this task, the robot needs to (i) detect the incoming projectile, (ii) predict the projectile's future motion, (iii) plan a minimum-time rapid trajectory that can evade obstacles and intercept the projectile, and (iv) execute the planned trajectory. These four steps must be performed under the manipulator's dynamic limits and extreme time constraints (<350ms in our setting) to successfully intercept the projectile. In addition, we want these trajectories to be smooth to reduce the robot's joint torques and the impulse on the platform on which it is mounted. To this end, we propose a kinodynamic motion planning framework that preprocesses smooth trajectories offline to allow real-time collision-free executions online. We present an end-to-end pipeline along with our planning framework, including perception, prediction, and execution modules. We evaluate our framework experimentally in simulation and show that it has a higher blocking success rate than the baselines. Further, we deploy our pipeline on a robotic system comprising an industrial arm (ABB IRB-1600) and an onboard stereo camera (ZED 2i), which achieves a 78% success rate in projectile interceptions. ",
        "title": "Preprocessing-based Kinodynamic Motion Planning Framework for  Intercepting Projectiles using a Robot Manipulator",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08023",
        "abstract_url": "http://arxiv.org/abs/2401.08023",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Lihao"
            },
            {
                "last_name": "Sun",
                "first_name": "Haijian"
            },
            {
                "last_name": "Zeng",
                "first_name": "Yong"
            },
            {
                "last_name": "Hu",
                "first_name": "Rose Qingyang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "CV"
        ],
        "abstract": "  As 5G technology becomes increasingly established, the anticipation for 6G is growing, which promises to deliver faster and more reliable wireless connections via cutting-edge radio technologies. However, efficient management method of the large-scale antenna arrays deployed by those radio technologies is crucial. Traditional management methods are mainly reactive, usually based on feedback from users to adapt to the dynamic wireless channel. However, a more promising approach lies in the prediction of spatial channel state information (spatial-CSI), which is an all-inclusive channel characterization and consists of all the feasible line-of-sight (LoS) and non-line-of-sight (NLoS) paths between the transmitter (Tx) and receiver (Rx), with the three-dimension (3D) trajectory, attenuation, phase shift, delay, and polarization of each path. Advances in hardware and neural networks make it possible to predict such spatial-CSI using precise environmental information, and further look into the possibility of holographic communication, which implies complete control over every aspect of the radio waves emitted. Based on the integration of holographic communication and digital twin, we proposed a new framework, digital radio twin, which takes advantages from both the digital world and deterministic control over radio waves, supporting a wide range of high-level applications. As a preliminary attempt towards this visionary direction, in this paper, we explore the use of generative artificial intelligence (AI) to pinpoint the valid paths in a given environment, demonstrating promising results, and highlighting the potential of this approach in driving forward the evolution of 6G wireless communication technologies. ",
        "title": "Spatial Channel State Information Prediction with Generative AI: Towards  Holographic Communication and Digital Radio Twin",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08025",
        "abstract_url": "http://arxiv.org/abs/2401.08025",
        "authors": [
            {
                "last_name": "Akter",
                "first_name": "Syeda Nahida"
            },
            {
                "last_name": "Madaan",
                "first_name": "Aman"
            },
            {
                "last_name": "Lee",
                "first_name": "Sangwu"
            },
            {
                "last_name": "Yang",
                "first_name": "Yiming"
            },
            {
                "last_name": "Nyberg",
                "first_name": "Eric"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The potential of Vision-Language Models (\\textsc{vlm}s) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose \\textsc{Self-Imagine}. We leverage a single Vision-Language Model (\\textsc{vlm}) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same \\vlm to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach in three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art \\textsc{vlm}. Our approach boosts the performance of \\textsc{vlm} on all math tasks (\\gsm: +4.62\\%; \\asdiv: +4.49\\%; \\svamp: +9.30\\%) and the majority of the general-purpose reasoning tasks by 0.4\\% to 13.20\\% while achieving comparable performance in other tasks.   Code and data at https://github.com/snat1505027/self-imagine . ",
        "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using  Self-Imagination",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08026",
        "abstract_url": "http://arxiv.org/abs/2401.08026",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Fengzhu"
            },
            {
                "last_name": "Gao",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation is previously oversimplified as summarization of fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim for \\underline{Ex}plainable fact-checking of real-world \\underline{Claim}s, and introduce JustiLM, a novel few-shot \\underline{Justi}fication generation based on retrieval-augmented \\underline{L}anguage \\underline{M}odel by using fact-check articles as auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension. ",
        "title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking  of Real-world Claims",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08032",
        "abstract_url": "http://arxiv.org/abs/2401.08032",
        "authors": [
            {
                "last_name": "Omee",
                "first_name": "Sadman Sadeed"
            },
            {
                "last_name": "Fu",
                "first_name": "Nihang"
            },
            {
                "last_name": "Dong",
                "first_name": "Rongzhi"
            },
            {
                "last_name": "Hu",
                "first_name": "Ming"
            },
            {
                "last_name": "Hu",
                "first_name": "Jianjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In real-world material research, machine learning (ML) models are usually expected to predict and discover novel exceptional materials that deviate from the known materials. It is thus a pressing question to provide an objective evaluation of ML model performances in property prediction of out-of-distribution (OOD) materials that are different from the training set distribution. Traditional performance evaluation of materials property prediction models through random splitting of the dataset frequently results in artificially high performance assessments due to the inherent redundancy of typical material datasets. Here we present a comprehensive benchmark study of structure-based graph neural networks (GNNs) for extrapolative OOD materials property prediction. We formulate five different categories of OOD ML problems for three benchmark datasets from the MatBench study. Our extensive experiments show that current state-of-the-art GNN algorithms significantly underperform for the OOD property prediction tasks on average compared to their baselines in the MatBench study, demonstrating a crucial generalization gap in realistic material prediction tasks. We further examine the latent physical spaces of these GNN models and identify the sources of CGCNN, ALIGNN, and DeeperGATGNN's significantly more robust OOD performance than those of the current best models in the MatBench study (coGN and coNGN), and provide insights to improve their performance. ",
        "title": "Structure-based out-of-distribution (OOD) materials property prediction:  a benchmark study",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08035",
        "abstract_url": "http://arxiv.org/abs/2401.08035",
        "authors": [
            {
                "last_name": "Saha",
                "first_name": "Chandrika"
            },
            {
                "last_name": "Rahman",
                "first_name": "Md. Mostafijur"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Handwritten character recognition is a crucial task because of its abundant applications. The recognition task of Bangla handwritten characters is especially challenging because of the cursive nature of Bangla characters and the presence of compound characters with more than one way of writing. In this paper, a classification model based on the ensembling of several Convolutional Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic characters, compound characters, numerals, and modifiers. Three different models based on the idea of state-of-the-art CNN models like Inception, ResNet, and DenseNet have been trained with both augmented and non-augmented inputs. Finally, all these models are averaged or ensembled to get the finishing model. Rigorous experimentation on three benchmark Bangla handwritten characters datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited significant recognition accuracies compared to some recent CNN-based research. The top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and the top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb, BanglaLekha-Isolated, and Ekush datasets respectively. ",
        "title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of  Convolutional Neural Network",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08036",
        "abstract_url": "http://arxiv.org/abs/2401.08036",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Haibin"
            },
            {
                "last_name": "Chang",
                "first_name": "Jun"
            },
            {
                "last_name": "Lu",
                "first_name": "Tao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Huabing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  3D lanes offer a more comprehensive understanding of the road surface geometry than 2D lanes, thereby providing crucial references for driving decisions and trajectory planning. While many efforts aim to improve prediction accuracy, we recognize that an efficient network can bring results closer to lane modeling. However, if the modeling data is imprecise, the results might not accurately capture the real-world scenario. Therefore, accurate lane modeling is essential to align prediction results closely with the environment. This study centers on efficient and accurate lane modeling, proposing a joint modeling approach that combines Bezier curves and interpolation methods. Furthermore, based on this lane modeling approach, we developed a Global2Local Lane Matching method with Bezier Control-Point and Key-Point, which serve as a comprehensive solution that leverages hierarchical features with two mathematical models to ensure a precise match. We also introduce a novel 3D Spatial Constructor, representing an exploration of 3D surround-view lane detection research. The framework is suitable for front-view or surround-view 3D lane detection. By directly outputting the key points of lanes in 3D space, it overcomes the limitations of anchor-based methods, enabling accurate prediction of closed-loop or U-shaped lanes and effective adaptation to complex road conditions. This innovative method establishes a new benchmark in front-view 3D lane detection on the Openlane dataset and achieves competitive performance in surround-view 2D lane detection on the Argoverse2 dataset. ",
        "title": "3D Lane Detection from Front or Surround-View using Joint-Modeling &  Matching",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08037",
        "abstract_url": "http://arxiv.org/abs/2401.08037",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Akash Deep"
            },
            {
                "last_name": "Wang",
                "first_name": "Brian"
            },
            {
                "last_name": "Garcia",
                "first_name": "Luis"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiang"
            },
            {
                "last_name": "Srivastava",
                "first_name": "Mani"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  While IoT sensors in physical spaces have provided utility and comfort in our lives, their instrumentation in private and personal spaces has led to growing concerns regarding privacy. The existing notion behind IoT privacy is that the sensors whose data can easily be understood and interpreted by humans (such as cameras) are more privacy-invasive than sensors that are not human-understandable, such as RF (radio-frequency) sensors. However, given recent advancements in machine learning, we can not only make sensitive inferences on RF data but also translate between modalities. Thus, the existing notions of privacy for IoT sensors need to be revisited. In this paper, our goal is to understand what factors affect the privacy notions of a non-expert user (someone who is not well-versed in privacy concepts). To this regard, we conduct an online study of 162 participants from the USA to find out what factors affect the privacy perception of a user regarding an RF-based device or a sensor. Our findings show that a user's perception of privacy not only depends upon the data collected by the sensor but also on the inferences that can be made on that data, familiarity with the device and its form factor as well as the control a user has over the device design and its data policies. When the data collected by the sensor is not human-interpretable, it is the inferences that can be made on the data and not the data itself that users care about when making informed decisions regarding device privacy. ",
        "title": "Understanding factors behind IoT privacy -- A user's perspective on RF  sensors",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08038",
        "abstract_url": "http://arxiv.org/abs/2401.08038",
        "authors": [
            {
                "last_name": "Qiu",
                "first_name": "Wenjun"
            },
            {
                "last_name": "Lie",
                "first_name": "David"
            },
            {
                "last_name": "Austin",
                "first_name": "Lisa"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CR",
            "HC",
            "LG"
        ],
        "abstract": "  A significant challenge to training accurate deep learning models on privacy policies is the cost and difficulty of obtaining a large and comprehensive set of training data. To address these challenges, we present Calpric , which combines automatic text selection and segmentation, active learning and the use of crowdsourced annotators to generate a large, balanced training set for privacy policies at low cost. Automated text selection and segmentation simplifies the labeling task, enabling untrained annotators from crowdsourcing platforms, like Amazon's Mechanical Turk, to be competitive with trained annotators, such as law students, and also reduces inter-annotator agreement, which decreases labeling cost. Having reliable labels for training enables the use of active learning, which uses fewer training samples to efficiently cover the input space, further reducing cost and improving class and data category balance in the data set. The combination of these techniques allows Calpric to produce models that are accurate over a wider range of data categories, and provide more detailed, fine-grain labels than previous work. Our crowdsourcing process enables Calpric to attain reliable labeled data at a cost of roughly $0.92-$1.71 per labeled text segment. Calpric 's training process also generates a labeled data set of 16K privacy policy text segments across 9 Data categories with balanced positive and negative samples. ",
        "title": "Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with  Crowdsourcing and Active Learning",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08041",
        "abstract_url": "http://arxiv.org/abs/2401.08041",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Jiaming"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Duong Thuy Anh"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Duong Tung"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Edge computing (EC) promises to deliver low-latency and ubiquitous computation to numerous devices at the network edge. This paper aims to jointly optimize edge node (EN) placement and resource allocation for an EC platform, considering demand uncertainty. Diverging from existing approaches treating uncertainties as exogenous, we propose a novel two-stage decision-dependent distributionally robust optimization (DRO) framework to effectively capture the interdependence between EN placement decisions and uncertain demands. The first stage involves making EN placement decisions, while the second stage optimizes resource allocation after uncertainty revelation. We present an exact mixed-integer linear program reformulation for solving the underlying ``min-max-min\" two-stage model. We further introduce a valid inequality method to enhance computational efficiency, especially for large-scale networks. Extensive numerical experiments demonstrate the benefits of considering endogenous uncertainties and the advantages of the proposed model and approach. ",
        "title": "Two-Stage Distributionally Robust Edge Node Placement Under Endogenous  Demand Uncertainty",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08043",
        "abstract_url": "http://arxiv.org/abs/2401.08043",
        "authors": [
            {
                "last_name": "Zuo",
                "first_name": "Yi-Fan"
            },
            {
                "last_name": "Xu",
                "first_name": "Wanting"
            },
            {
                "last_name": "Wang",
                "first_name": "Xia"
            },
            {
                "last_name": "Wang",
                "first_name": "Yifu"
            },
            {
                "last_name": "Kneip",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras. ",
        "title": "Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging  Conditions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08045",
        "abstract_url": "http://arxiv.org/abs/2401.08045",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Xu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haiming"
            },
            {
                "last_name": "Cai",
                "first_name": "Yingjie"
            },
            {
                "last_name": "Guo",
                "first_name": "Jingming"
            },
            {
                "last_name": "Qiu",
                "first_name": "Weichao"
            },
            {
                "last_name": "Gao",
                "first_name": "Bin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Kaiqiang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yue"
            },
            {
                "last_name": "Jin",
                "first_name": "Huan"
            },
            {
                "last_name": "Gao",
                "first_name": "Jiantao"
            },
            {
                "last_name": "Li",
                "first_name": "Zhen"
            },
            {
                "last_name": "Jiang",
                "first_name": "Lihui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongbo"
            },
            {
                "last_name": "Dai",
                "first_name": "Dengxin"
            },
            {
                "last_name": "Liu",
                "first_name": "Bingbing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. ",
        "title": "Forging Vision Foundation Models for Autonomous Driving: Challenges,  Methodologies, and Opportunities",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08046",
        "abstract_url": "http://arxiv.org/abs/2401.08046",
        "authors": [
            {
                "last_name": "Dou",
                "first_name": "Zhicheng"
            },
            {
                "last_name": "Guo",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chang",
                "first_name": "Ching-Chun"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Huy H."
            },
            {
                "last_name": "Echizen",
                "first_name": "Isao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The emergence of large language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and broader community. While these models offer numerous advantages in terms of revolutionizing work and study methods, they have also garnered significant attention due to their potential negative consequences. One example is generating academic reports or papers with little to no human contribution. Consequently, researchers have focused on developing detectors to address the misuse of LLMs. However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount. In this paper, we present a comprehensive analysis of the impact of prompts on the text generated by LLMs and highlight the potential lack of robustness in one of the current state-of-the-art GPT detectors. To mitigate these issues concerning the misuse of LLMs in academic writing, we propose a reference-based Siamese detector named Synthetic-Siamese which takes a pair of texts, one as the inquiry and the other as the reference. Our method effectively addresses the lack of robustness of previous detectors (OpenAI detector and DetectGPT) and significantly improves the baseline performances in realistic academic writing scenarios by approximately 67% to 95%. ",
        "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic  Writing: A Comprehensive Analysis",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08047",
        "abstract_url": "http://arxiv.org/abs/2401.08047",
        "authors": [
            {
                "last_name": "Chowdhury",
                "first_name": "Somnath Basu Roy"
            },
            {
                "last_name": "Monath",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Dubey",
                "first_name": "Avinava"
            },
            {
                "last_name": "Zaheer",
                "first_name": "Manzil"
            },
            {
                "last_name": "McCallum",
                "first_name": "Andrew"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Amr"
            },
            {
                "last_name": "Chaturvedi",
                "first_name": "Snigdha"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Extractive opinion summarization involves automatically producing a summary of text about an entity (e.g., a product's reviews) by extracting representative sentences that capture prevalent opinions in the review set. Typically, in online marketplaces user reviews accrue over time, and opinion summaries need to be updated periodically to provide customers with up-to-date information. In this work, we study the task of extractive opinion summarization in an incremental setting, where the underlying review set evolves over time. Many of the state-of-the-art extractive opinion summarization approaches are centrality-based, such as CentroidRank. CentroidRank performs extractive summarization by selecting a subset of review sentences closest to the centroid in the representation space as the summary. However, these methods are not capable of operating efficiently in an incremental setting, where reviews arrive one at a time. In this paper, we present an efficient algorithm for accurately computing the CentroidRank summaries in an incremental setting. Our approach, CoverSumm, relies on indexing review representations in a cover tree and maintaining a reservoir of candidate summary review sentences. CoverSumm's efficacy is supported by a theoretical and empirical analysis of running time. Empirically, on a diverse collection of data (both real and synthetically created to illustrate scaling considerations), we demonstrate that CoverSumm is up to 25x faster than baseline methods, and capable of adapting to nuanced changes in data distribution. We also conduct human evaluations of the generated summaries and find that CoverSumm is capable of producing informative summaries consistent with the underlying review set. ",
        "title": "Incremental Extractive Opinion Summarization Using Cover Trees",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08049",
        "abstract_url": "http://arxiv.org/abs/2401.08049",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Bingyuan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xulong"
            },
            {
                "last_name": "Cheng",
                "first_name": "Ning"
            },
            {
                "last_name": "Yu",
                "first_name": "Jun"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jing"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianzong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "SD"
        ],
        "abstract": "  In recent years, the field of talking faces generation has attracted considerable attention, with certain methods adept at generating virtual faces that convincingly imitate human expressions. However, existing methods face challenges related to limited generalization, particularly when dealing with challenging identities. Furthermore, methods for editing expressions are often confined to a singular emotion, failing to adapt to intricate emotions. To overcome these challenges, this paper proposes EmoTalker, an emotionally editable portraits animation approach based on the diffusion model. EmoTalker modifies the denoising process to ensure preservation of the original portrait's identity during inference. To enhance emotion comprehension from text input, Emotion Intensity Block is introduced to analyze fine-grained emotions and strengths derived from prompts. Additionally, a crafted dataset is harnessed to enhance emotion comprehension within prompts. Experiments show the effectiveness of EmoTalker in generating high-quality, emotionally customizable facial expressions. ",
        "title": "EmoTalker: Emotionally Editable Talking Face Generation via Diffusion  Model",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08053",
        "abstract_url": "http://arxiv.org/abs/2401.08053",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zhixuan"
            },
            {
                "last_name": "Schaldenbrand",
                "first_name": "Peter"
            },
            {
                "last_name": "Okogwu",
                "first_name": "Beverley-Claire"
            },
            {
                "last_name": "Peng",
                "first_name": "Wenxuan"
            },
            {
                "last_name": "Yun",
                "first_name": "Youngsik"
            },
            {
                "last_name": "Hundt",
                "first_name": "Andrew"
            },
            {
                "last_name": "Kim",
                "first_name": "Jihie"
            },
            {
                "last_name": "Oh",
                "first_name": "Jean"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate representation in media is known to improve the well-being of the people who consume it. Generative image models trained on large web-crawled datasets such as LAION are known to produce images with harmful stereotypes and misrepresentations of cultures. We improve inclusive representation in generated images by (1) engaging with communities to collect a culturally representative dataset that we call the Cross-Cultural Understanding Benchmark (CCUB) and (2) proposing a novel Self-Contrastive Fine-Tuning (SCoFT) method that leverages the model's known biases to self-improve. SCoFT is designed to prevent overfitting on small datasets, encode only high-level information from the data, and shift the generated distribution away from misrepresentations encoded in a pretrained model. Our user study conducted on 51 participants from 5 different countries based on their self-selected national cultural affiliation shows that fine-tuning on CCUB consistently generates images with higher cultural relevance and fewer stereotypes when compared to the Stable Diffusion baseline, which is further improved with our SCoFT technique. ",
        "title": "SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08054",
        "abstract_url": "http://arxiv.org/abs/2401.08054",
        "authors": [
            {
                "last_name": "Suzuki",
                "first_name": "Taro"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper proposes a novel positioning technique suitable for use in mobile robots in urban environments in which large global navigation satellite system (GNSS) positioning errors occur because of multipath signals. During GNSS positioning, the GNSS satellites that are obstructed by buildings emit reflection and diffraction signals, which are called non-line-of-sight (NLOS) multipath signals. These multipath signals cause major positioning errors. The key concept considered in this paper is the estimation of a user's position using the likelihood of the position hypotheses computed from the GNSS pseudoranges, consisting only of LOS signals based on the analysis of the pseudorange residuals. To determine the NLOS GNSS signals from the pseudorange residuals at the user's position, it is necessary to accurately determine the position before the computation of the pseudorange residuals. This problem is solved using a particle filter. We propose a likelihood estimation method using the Mahalanobis distance between the hypotheses of the user's position computed from only the LOS pseudoranges and the particles. To confirm the effectiveness of the proposed technique, a positioning test was performed in a real-world urban environment. The results demonstrated that the proposed method is effective for accurately estimating the user's position in urban canyons. ",
        "title": "Mobile robot localization with GNSS multipath detection using  pseudorange residuals",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08056",
        "abstract_url": "http://arxiv.org/abs/2401.08056",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Haoran"
            },
            {
                "last_name": "Xu",
                "first_name": "Chang"
            },
            {
                "last_name": "Yang",
                "first_name": "Wen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ruixiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Xia",
                "first_name": "Gui-Song"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Precise detection of tiny objects in remote sensing imagery remains a significant challenge due to their limited visual information and frequent occurrence within scenes. This challenge is further exacerbated by the practical burden and inherent errors associated with manual annotation: annotating tiny objects is laborious and prone to errors (i.e., label noise). Training detectors for such objects using noisy labels often leads to suboptimal performance, with networks tending to overfit on noisy labels. In this study, we address the intricate issue of tiny object detection under noisy label supervision. We systematically investigate the impact of various types of noise on network training, revealing the vulnerability of object detectors to class shifts and inaccurate bounding boxes for tiny objects. To mitigate these challenges, we propose a DeNoising Tiny Object Detector (DN-TOD), which incorporates a Class-aware Label Correction (CLC) scheme to address class shifts and a Trend-guided Learning Strategy (TLS) to handle bounding box noise. CLC mitigates inaccurate class supervision by identifying and filtering out class-shifted positive samples, while TLS reduces noisy box-induced erroneous supervision through sample reweighting and bounding box regeneration. Additionally, Our method can be seamlessly integrated into both one-stage and two-stage object detection pipelines. Comprehensive experiments conducted on synthetic (i.e., noisy AI-TOD-v2.0 and DOTA-v2.0) and real-world (i.e., AI-TOD) noisy datasets demonstrate the robustness of DN-TOD under various types of label noise. Notably, when applied to the strong baseline RFLA, DN-TOD exhibits a noteworthy performance improvement of 4.9 points under 40% mixed noise. Datasets, codes, and models will be made publicly available. ",
        "title": "Robust Tiny Object Detection in Aerial Images amidst Label Noise",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08058",
        "abstract_url": "http://arxiv.org/abs/2401.08058",
        "authors": [
            {
                "last_name": "Gamble",
                "first_name": "Cooper"
            },
            {
                "last_name": "Faghani",
                "first_name": "Shahriar"
            },
            {
                "last_name": "Erickson",
                "first_name": "Bradley J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  As deep learning (DL) continues to demonstrate its ability in radiological tasks, it is critical that we optimize clinical DL solutions to include safety. One of the principal concerns in the clinical adoption of DL tools is trust. This study aims to apply conformal prediction as a step toward trustworthiness for DL in radiology. This is a retrospective study of 491 non-contrast head CTs from the CQ500 dataset, in which three senior radiologists annotated slices containing intracranial hemorrhage (ICH). The dataset was split into definite and challenging subsets, where challenging images were defined to those in which there was disagreement among readers. A DL model was trained on 146 patients (10,815 slices) from the definite data (training dataset) to perform ICH localization and classification for five classes of ICH. To develop an uncertainty-aware DL model, 1,546 cases of the definite data (calibration dataset) was used for Mondrian conformal prediction (MCP). The uncertainty-aware DL model was tested on 8,401 definite and challenging cases to assess its ability to identify challenging cases. After the MCP procedure, the model achieved an F1 score of 0.920 for ICH classification on the test dataset. Additionally, it correctly identified 6,837 of the 6,856 total challenging cases as challenging (99.7% accuracy). It did not incorrectly label any definite cases as challenging. The uncertainty-aware ICH detector performs on par with state-of-the-art models. MCP's performance in detecting challenging cases demonstrates that it is useful in automated ICH detection and promising for trustworthiness in radiological DL. ",
        "title": "Toward Clinically Trustworthy Deep Learning: Applying Conformal  Prediction to Intracranial Hemorrhage Detection",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08061",
        "abstract_url": "http://arxiv.org/abs/2401.08061",
        "authors": [
            {
                "last_name": "Duan",
                "first_name": "Lei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Carlson",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Fusing abundant satellite data with sparse ground measurements constitutes a major challenge in climate modeling. To address this, we propose a strategy to augment the training dataset by introducing unlabeled satellite images paired with pseudo-labels generated through a spatial interpolation technique known as ordinary kriging, thereby making full use of the available satellite data resources. We show that the proposed data augmentation strategy helps enhance the performance of the state-of-the-art convolutional neural network-random forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy improvement in spatial correlation and a reduction in prediction error. ",
        "title": "Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label  Generation",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08064",
        "abstract_url": "http://arxiv.org/abs/2401.08064",
        "authors": [
            {
                "last_name": "Allen",
                "first_name": "Scott E."
            },
            {
                "last_name": "Kizilcec",
                "first_name": "Ren\u00e9 F."
            },
            {
                "last_name": "Redish",
                "first_name": "A. David"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  More than 30 years of research has firmly established the vital role of trust in human organizations and relationships, but the underlying mechanisms by which people build, lose, and rebuild trust remains incompletely understood. We propose a mechanistic model of trust that is grounded in the modern neuroscience of decision making. Since trust requires anticipating the future actions of others, any mechanistic model must be built upon up-to-date theories on how the brain learns, represents, and processes information about the future within its decision-making systems. Contemporary neuroscience has revealed that decision making arises from multiple parallel systems that perform distinct, complementary information processing. Each system represents information in different forms, and therefore learns via different mechanisms. When an act of trust is reciprocated or violated, this provides new information that can be used to anticipate future actions. The taxonomy of neural information representations that is the basis for the system boundaries between neural decision-making systems provides a taxonomy for categorizing different forms of trust and generating mechanistic predictions about how these forms of trust are learned and manifested in human behavior. Three key predictions arising from our model are (1) strategic risk-taking can reveal how to best proceed in a relationship, (2) human organizations and environments can be intentionally designed to encourage trust among their members, and (3) violations of trust need not always degrade trust, but can also provide opportunities to build trust. ",
        "title": "A new model of trust based on neural information processing",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08066",
        "abstract_url": "http://arxiv.org/abs/2401.08066",
        "authors": [
            {
                "last_name": "Chiu",
                "first_name": "Ching-Hao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yu-Jen"
            },
            {
                "last_name": "Wu",
                "first_name": "Yawen"
            },
            {
                "last_name": "Shi",
                "first_name": "Yiyu"
            },
            {
                "last_name": "Ho",
                "first_name": "Tsung-Yi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In medical image diagnosis, fairness has become increasingly crucial. Without bias mitigation, deploying unfair AI would harm the interests of the underprivileged population and potentially tear society apart. Recent research addresses prediction biases in deep learning models concerning demographic groups (e.g., gender, age, and race) by utilizing demographic (sensitive attribute) information during training. However, many sensitive attributes naturally exist in dermatological disease images. If the trained model only targets fairness for a specific attribute, it remains unfair for other attributes. Moreover, training a model that can accommodate multiple sensitive attributes is impractical due to privacy concerns. To overcome this, we propose a method enabling fair predictions for sensitive attributes during the testing phase without using such information during training. Inspired by prior work highlighting the impact of feature entanglement on fairness, we enhance the model features by capturing the features related to the sensitive and target attributes and regularizing the feature entanglement between corresponding classes. This ensures that the model can only classify based on the features related to the target attribute without relying on features associated with sensitive attributes, thereby improving fairness and accuracy. Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets. ",
        "title": "Achieve Fairness without Demographics for Dermatological Disease  Diagnosis",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08067",
        "abstract_url": "http://arxiv.org/abs/2401.08067",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zuotian"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiang"
            },
            {
                "last_name": "Tang",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Pengyue"
            },
            {
                "last_name": "Jin",
                "first_name": "Nanxin"
            },
            {
                "last_name": "Eadon",
                "first_name": "Michael"
            },
            {
                "last_name": "Song",
                "first_name": "Qianqian"
            },
            {
                "last_name": "Chen",
                "first_name": "Yingjie"
            },
            {
                "last_name": "Su",
                "first_name": "Jing"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Objective: Our objective is to develop and validate TrajVis, an interactive tool that assists clinicians in using artificial intelligence (AI) models to leverage patients' longitudinal electronic medical records (EMR) for personalized precision management of chronic disease progression. Methods: We first perform requirement analysis with clinicians and data scientists to determine the visual analytics tasks of the TrajVis system as well as its design and functionalities. A graph AI model for chronic kidney disease (CKD) trajectory inference named DEPOT is used for system development and demonstration. TrajVis is implemented as a full-stack web application with synthetic EMR data derived from the Atrium Health Wake Forest Baptist Translational Data Warehouse and the Indiana Network for Patient Care research database. A case study with a nephrologist and a user experience survey of clinicians and data scientists are conducted to evaluate the TrajVis system. Results: The TrajVis clinical information system is composed of four panels: the Patient View for demographic and clinical information, the Trajectory View to visualize the DEPOT-derived CKD trajectories in latent space, the Clinical Indicator View to elucidate longitudinal patterns of clinical features and interpret DEPOT predictions, and the Analysis View to demonstrate personal CKD progression trajectories. System evaluations suggest that TrajVis supports clinicians in summarizing clinical data, identifying individualized risk predictors, and visualizing patient disease progression trajectories, overcoming the barriers of AI implementation in healthcare. Conclusion: TrajVis bridges the gap between the fast-growing AI/ML modeling and the clinical use of such models for personalized and precision management of chronic diseases. ",
        "title": "TrajVis: a visual clinical decision support system to translate  artificial intelligence trajectory models in the precision management of  chronic kidney disease",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08068",
        "abstract_url": "http://arxiv.org/abs/2401.08068",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Beibei"
            },
            {
                "last_name": "Li",
                "first_name": "Weiling"
            },
            {
                "last_name": "Fang",
                "first_name": "Yan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Event cameras are neuromorphic sensors that capture asynchronous and sparse event stream when per-pixel brightness changes. The state-of-the-art processing methods for event signals typically aggregate events into a frame or a grid. However, events are dense in time, these works are limited to local information of events due to the stacking. In this paper, we present a novel spatiotemporal representation learning method which can capture the global correlations of all events in the event stream simultaneously by tensor decomposition. In addition, with the events are sparse in space, we propose an Elastic Net-incorporated tensor network (ENTN) model to obtain more spatial and temporal details about event stream. Empirically, the results indicate that our method can represent the spatiotemporal correlation of events with high quality, and can achieve effective results in applications like filtering noise compared with the state-of-the-art methods. ",
        "title": "Representation Learning on Event Stream via an Elastic Net-incorporated  Tensor Network",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08073",
        "abstract_url": "http://arxiv.org/abs/2401.08073",
        "authors": [
            {
                "last_name": "Ramanathan",
                "first_name": "Alagappan"
            },
            {
                "last_name": "Sankaran",
                "first_name": "Rishika"
            },
            {
                "last_name": "Jyothi",
                "first_name": "Sangeetha Abdu"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  A resilient Internet infrastructure is critical in our highly interconnected society. However, the Internet faces several vulnerabilities, ranging from natural disasters to human activities, that can impact the physical layer and, in turn, the higher network layers, such as IP links. In this paper, we introduce Xaminer, the first Internet cross-layer resilience analysis tool, to evaluate the interplay between physical- and network-layer failures. Using a cross-layer Internet map and a failure event model, Xaminer generates a risk profile encompassing a cross-layer impact report, critical infrastructure identification at each layer, and the discovery of trends and patterns under different failure event settings. Xaminer's key strengths lie in its adaptability to diverse disaster scenarios, the ability to assess risks at various granularities, and the capability to generate joint risk profiles for multiple events. We demonstrate Xaminer's capabilities in cross-layer analysis across a spectrum of disaster event models and regions, showcasing its potential role in facilitating well-informed decision-making for resilience planning and deployments. ",
        "title": "Xaminer: An Internet Cross-Layer Resilience Analysis Tool",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08077",
        "abstract_url": "http://arxiv.org/abs/2401.08077",
        "authors": [
            {
                "last_name": "Singh",
                "first_name": "Shubham"
            },
            {
                "last_name": "Bhat",
                "first_name": "Mayur"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments. ",
        "title": "Transformer-based approach for Ethereum Price Prediction Using  Crosscurrency correlation and Sentiment Analysis",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08079",
        "abstract_url": "http://arxiv.org/abs/2401.08079",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Huafeng"
            },
            {
                "last_name": "Wu",
                "first_name": "Yiquan"
            },
            {
                "last_name": "El-Yacoubi",
                "first_name": "Mounim A."
            },
            {
                "last_name": "Wang",
                "first_name": "Jun"
            },
            {
                "last_name": "Yang",
                "first_name": "Guangxiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Vein recognition has received increasing attention due to its high security and privacy. Recently, deep neural networks such as Convolutional neural networks (CNN) and Transformers have been introduced for vein recognition and achieved state-of-the-art performance. Despite the recent advances, however, existing solutions for finger-vein feature extraction are still not optimal due to scarce training image samples. To overcome this problem, in this paper, we propose an adversarial masking contrastive learning (AMCL) approach, that generates challenging samples to train a more robust contrastive learning model for the downstream palm-vein recognition task, by alternatively optimizing the encoder in the contrastive learning model and a set of latent variables. First, a huge number of masks are generated to train a robust generative adversarial network (GAN). The trained generator transforms a latent variable from the latent variable space into a mask space. Then, we combine the trained generator with a contrastive learning model to obtain our AMCL, where the generator produces challenging masking images to increase the contrastive loss and the contrastive learning model is trained based on the harder images to learn a more robust feature representation. After training, the trained encoder in the contrastive learning model is combined with a classification layer to build a classifier, which is further fine-tuned on labeled training data for vein recognition. The experimental results on three databases demonstrate that our approach outperforms existing contrastive learning approaches in terms of improving identification accuracy of vein classifiers and achieves state-of-the-art recognition results. ",
        "title": "Adversarial Masking Contrastive Learning for vein recognition",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08080",
        "abstract_url": "http://arxiv.org/abs/2401.08080",
        "authors": [
            {
                "last_name": "Costa",
                "first_name": "Alberto"
            }
        ],
        "primary_category": "MS",
        "categories": [
            "MS"
        ],
        "abstract": "  Two approximations of the integral of a class of sinusoidal composite functions, for which an explicit form does not exist, are derived. Numerical experiments show that the proposed approximations yield an error that does not depend on the width of the integration interval. Using such approximations, definite integrals can be computed in almost real-time. ",
        "title": "Approximations of the integral of a class of sinusoidal composite  functions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08081",
        "abstract_url": "http://arxiv.org/abs/2401.08081",
        "authors": [
            {
                "last_name": "Nezhadettehad",
                "first_name": "Alireza"
            },
            {
                "last_name": "Zaslavsky",
                "first_name": "Arkady"
            },
            {
                "last_name": "Abdur",
                "first_name": "Rakib"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Siraj Ahmed"
            },
            {
                "last_name": "Loke",
                "first_name": "Seng W."
            },
            {
                "last_name": "Huang",
                "first_name": "Guang-Li"
            },
            {
                "last_name": "Hassani",
                "first_name": "Alireza"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Predicting the future location of mobile objects reinforces location-aware services with proactive intelligence and helps businesses and decision-makers with better planning and near real-time scheduling in different applications such as traffic congestion control, location-aware advertisements, and monitoring public health and well-being. The recent developments in the smartphone and location sensors technology and the prevalence of using location-based social networks alongside the improvements in artificial intelligence and machine learning techniques provide an excellent opportunity to exploit massive amounts of historical and real-time contextual information to recognise mobility patterns and achieve more accurate and intelligent predictions. This survey provides a comprehensive overview of the next useful location prediction problem with context-awareness. First, we explain the concepts of context and context-awareness and define the next location prediction problem. Then we analyse nearly thirty studies in this field concerning the prediction method, the challenges addressed, the datasets and metrics used for training and evaluating the model, and the types of context incorporated. Finally, we discuss the advantages and disadvantages of different approaches, focusing on the usefulness of the predicted location and identifying the open challenges and future work on this subject by introducing two potential use cases of next location prediction in the automotive industry. ",
        "title": "Predicting Next Useful Location With Context-Awareness: The  State-Of-The-Art",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08083",
        "abstract_url": "http://arxiv.org/abs/2401.08083",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yu"
            },
            {
                "last_name": "Lin",
                "first_name": "Yuming"
            },
            {
                "last_name": "Liao",
                "first_name": "Qingming"
            },
            {
                "last_name": "Li",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM. ",
        "title": "UV-SAM: Adapting Segment Anything Model for Urban Village Identification",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08086",
        "abstract_url": "http://arxiv.org/abs/2401.08086",
        "authors": [
            {
                "last_name": "Su",
                "first_name": "Yukun"
            },
            {
                "last_name": "Cao",
                "first_name": "Yiwen"
            },
            {
                "last_name": "Deng",
                "first_name": "Jingliang"
            },
            {
                "last_name": "Rao",
                "first_name": "Fengyun"
            },
            {
                "last_name": "Wu",
                "first_name": "Qingyao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  A large amount of User Generated Content (UGC) is uploaded to the Internet daily and displayed to people world-widely through the client side (e.g., mobile and PC). This requires the cropping algorithms to produce the aesthetic thumbnail within a specific aspect ratio on different devices. However, existing image cropping works mainly focus on landmark or landscape images, which fail to model the relations among the multi-objects with the complex background in UGC. Besides, previous methods merely consider the aesthetics of the cropped images while ignoring the content integrity, which is crucial for UGC cropping. In this paper, we propose a Spatial-Semantic Collaborative cropping network (S2CNet) for arbitrary user generated content accompanied by a new cropping benchmark. Specifically, we first mine the visual genes of the potential objects. Then, the suggested adaptive attention graph recasts this task as a procedure of information association over visual nodes. The underlying spatial and semantic relations are ultimately centralized to the crop candidate through differentiable message passing, which helps our network efficiently to preserve both the aesthetics and the content integrity. Extensive experiments on the proposed UGCrop5K and other public datasets demonstrate the superiority of our approach over state-of-the-art counterparts. Our project is available at https://github.com/suyukun666/S2CNet. ",
        "title": "Spatial-Semantic Collaborative Cropping for User Generated Content",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08088",
        "abstract_url": "http://arxiv.org/abs/2401.08088",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yachao"
            },
            {
                "last_name": "Li",
                "first_name": "Junhui"
            },
            {
                "last_name": "Jiang",
                "first_name": "Jing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Min"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Existing large language models (LLMs) for machine translation are typically fine-tuned on sentence-level translation instructions and achieve satisfactory performance at the sentence level. However, when applied to document-level translation, these models face a significant challenge, particularly when dealing with documents containing over 512 tokens. This challenge arises from the issue of sentence-level coverage, where subsequent sentences in the document remain untranslated. As a result, the document-level translation capability of LLMs fine-tuned on sentence-level translation instructions is significantly limited. We conjecture that the primary cause of LLMs' weak document-level translation performance is the absence of document-to-document mapping ability. To address the issue, we propose an approach that combines sentence-level and document-level translation instructions of varying lengths to fine-tune LLMs. Our proposed translation mixed-instructions enable LLMs (Llama-2~7B and 13B) to maintain consistent translation performance from the sentence level to documents containing as many as 2048 tokens. Extensive experimental results show that the proposed approach significantly enhances the document-level translation capabilities of LLMs on 10 language pairs, effectively mitigating the sentence-level coverage issue in document-level translation. Experimentation on discourse phenomena has demonstrated that our document-level translation approach significantly improves translation quality, both in terms of BLEU score and discourse coherence. ",
        "title": "Enhancing Document-level Translation of Large Language Model via  Translation Mixed-instructions",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08089",
        "abstract_url": "http://arxiv.org/abs/2401.08089",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Fu"
            },
            {
                "last_name": "Wang",
                "first_name": "Xueying"
            },
            {
                "last_name": "Li",
                "first_name": "Bin"
            },
            {
                "last_name": "Wu",
                "first_name": "Yunlong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanzhen"
            },
            {
                "last_name": "Yi",
                "first_name": "Xiaodong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "RO"
        ],
        "abstract": "  This paper presents an innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks. The conventional manual BT generation method is inefficient and heavily reliant on domain expertise. On the other hand, existing automatic BT generation technologies encounter bottlenecks related to task complexity, model adaptability, and reliability. In order to overcome these challenges, we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs. The core contribution of this paper lies in the design of a BT generation framework based on LLM, which encompasses the entire process, from data synthesis and model training to application developing and data verification. Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance. In order to ensure the effectiveness and executability of the generated BTs, we emphasize the importance of data verification and introduce a multilevel verification strategy. Additionally, we explore a range of agent design and development schemes with LLM as the central element. We hope that the work in this paper may provide a reference for the researchers who are interested in BT generation based on LLMs. ",
        "title": "A Study on Training and Developing Large Language Models for Behavior  Tree Generation",
        "date": "2024-01-15",
        "group": "cs"
    }
]
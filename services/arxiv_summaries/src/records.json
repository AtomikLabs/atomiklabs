[
    {
        "identifier": "oai:arXiv.org:1309.7583",
        "abstract_url": "http://arxiv.org/abs/1309.7583",
        "authors": [
            {
                "last_name": "H\u00e4ger",
                "first_name": "Christian"
            },
            {
                "last_name": "Amat",
                "first_name": "Alexandre Graell i"
            },
            {
                "last_name": "Alvarado",
                "first_name": "Alex"
            },
            {
                "last_name": "Br\u00e4nnstr\u00f6m",
                "first_name": "Fredrik"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In many practical communication systems, one binary encoder/decoder pair is used to communicate over a set of parallel channels. Examples of this setup include multi-carrier transmission, rate-compatible puncturing of turbo-like codes, and bit-interleaved coded modulation (BICM). A bit mapper is commonly employed to determine how the coded bits are allocated to the channels. In this paper, we study spatially coupled low-density parity check codes over parallel channels and optimize the bit mapper using BICM as the driving example. For simplicity, the parallel bit channels that arise in BICM are replaced by independent binary erasure channels (BECs). For two parallel BECs modeled according to a 4-PAM constellation labeled by the binary reflected Gray code, the optimization results show that the decoding threshold can be improved over a uniform random bit mapper, or, alternatively, the spatial chain length of the code can be reduced for a given gap to capacity. It is also shown that for rate-loss free, circular (tail-biting) ensembles, a decoding wave effect can be initiated using only an optimized bit mapper. ",
        "title": "Optimized Bit Mappings for Spatially Coupled LDPC Codes over Parallel  Binary Erasure Channels",
        "date": "2013-09-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1310.4149",
        "abstract_url": "http://arxiv.org/abs/1310.4149",
        "authors": [
            {
                "last_name": "Alvarado",
                "first_name": "Alex"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We study achievable rates for four-dimensional (4D) constellations for spectrally efficient optical systems based on a (suboptimal) bit-wise receiver. We show that PM-QPSK outperforms the best 4D constellation designed for uncoded transmission by approximately 1 dB. Numerical results using LDPC codes validate the analysis. ",
        "title": "Achievable Rates for Four-Dimensional Coded Modulation with a Bit-Wise  Receiver",
        "date": "2013-10-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1312.3092",
        "abstract_url": "http://arxiv.org/abs/1312.3092",
        "authors": [
            {
                "last_name": "H\u00e4ger",
                "first_name": "Christian"
            },
            {
                "last_name": "Beygi",
                "first_name": "Lotfollah"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            },
            {
                "last_name": "Johannisson",
                "first_name": "Pontus"
            },
            {
                "last_name": "Karlsson",
                "first_name": "Magnus"
            },
            {
                "last_name": "Amat",
                "first_name": "Alexandre Graell i"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  A low-complexity detector is introduced for polarization-multiplexed M-ary phase shift keying modulation in a fiber-optical channel impaired by nonlinear phase noise, generalizing a previous result by Lau and Kahn for single-polarization signals. The proposed detector uses phase compensation based on both received signal amplitudes in conjunction with simple straight-line rather than four-dimensional maximum-likelihood decision boundaries. ",
        "title": "A Low-Complexity Detector for Memoryless Polarization-Multiplexed  Fiber-Optical Channels",
        "date": "2013-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1503.05477",
        "abstract_url": "http://arxiv.org/abs/1503.05477",
        "authors": [
            {
                "last_name": "Alvarado",
                "first_name": "Alex"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            },
            {
                "last_name": "Lavery",
                "first_name": "Domanic"
            },
            {
                "last_name": "Maher",
                "first_name": "Robert"
            },
            {
                "last_name": "Bayvel",
                "first_name": "Polina"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The FEC limit paradigm is the prevalent practice for designing optical communication systems to attain a certain bit-error rate (BER) without forward error correction (FEC). This practice assumes that there is an FEC code that will reduce the BER after decoding to the desired level. In this paper, we challenge this practice and show that the concept of a channel-independent FEC limit is invalid for soft-decision bit-wise decoding. It is shown that for low code rates and high order modulation formats, the use of the soft FEC limit paradigm can underestimate the spectral efficiencies by up to 20%. A better predictor for the BER after decoding is the generalized mutual information, which is shown to give consistent post-FEC BER predictions across different channel conditions and modulation formats. Extensive optical full-field simulations and experiments are carried out in both the linear and nonlinear transmission regimes to confirm the theoretical analysis. ",
        "title": "Replacing the Soft FEC Limit Paradigm in the Design of Optical  Communication Systems",
        "date": "2015-03-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1512.09207",
        "abstract_url": "http://arxiv.org/abs/1512.09207",
        "authors": [
            {
                "last_name": "Cojocaru",
                "first_name": "Liliana"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  We introduce a normal form for context-free grammars, called Dyck normal form. This is a syntactical restriction of the Chomsky normal form, in which the two nonterminals occurring on the right-hand side of a rule are paired nonterminals. This pairwise property allows to define a homomorphism from Dyck words to words generated by a grammar in Dyck normal form. We prove that for each context-free language L, there exist an integer K and a homomorphism h such that L=h(D'_K), where D'_K is a subset of the one-sided Dyck language over K letters. Through a transition-like diagram for a context-free grammar in Dyck normal form, we effectively build a regular language R that satisfies the Chomsky-Schutzenberger theorem. Using graphical approaches we refine R such that the Chomsky-Schutzenberger theorem still holds. Based on this readjustment we sketch a transition diagram for a regular grammar that generates a regular superset approximation for the initial context-free language. ",
        "title": "Around Context-Free Grammars -- a Normal Form, a Representation Theorem,  and a Regular Approximation",
        "date": "2015-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1601.05927",
        "abstract_url": "http://arxiv.org/abs/1601.05927",
        "authors": [
            {
                "last_name": "Czegledi",
                "first_name": "Cristian B."
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            },
            {
                "last_name": "Karlsson",
                "first_name": "Magnus"
            },
            {
                "last_name": "Johannisson",
                "first_name": "Pontus"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The state of polarization and the carrier phase drift dynamically during transmission in a random fashion in coherent optical fiber communications. The typical digital signal processing solution to mitigate these impairments consists of two separate blocks that track each phenomenon independently. Such algorithms have been developed without taking into account mathematical models describing the impairments. We study a blind, model-based tracking algorithm to compensate for these impairments. The algorithm dynamically recovers the carrier phase and state of polarization jointly for an arbitrary modulation format. Simulation results show the effectiveness of the proposed algorithm, having a fast convergence rate and an excellent tolerance to phase noise and dynamic drift of the polarization. The computational complexity of the algorithm is lower compared to state-of-the-art algorithms at similar or better performance, which makes it a strong candidate for future optical systems. ",
        "title": "Modulation Format Independent Joint Polarization and Phase Tracking for  Coherent Receivers",
        "date": "2016-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1606.01689",
        "abstract_url": "http://arxiv.org/abs/1606.01689",
        "authors": [
            {
                "last_name": "Liga",
                "first_name": "Gabriele"
            },
            {
                "last_name": "Alvarado",
                "first_name": "Alex"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            },
            {
                "last_name": "Bayvel",
                "first_name": "Polina"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  A comprehensive study of the coded performance of long-haul spectrally-efficient WDM optical fiber transmission systems with different coded modulation decoding structures is presented. Achievable information rates are derived for three different square QAM formats and the optimal format is identified as a function of distance and specific decoder implementation. The four cases analyzed combine hard-decision (HD) or soft-decision (SD) decoding together with either a bit-wise or a symbol-wise demapper, the last two suitable for binary and nonbinary codes, respectively. The information rates achievable for each scheme are calculated based on the mismatched decoder principle. These quantities represent true indicators of the coded performance of the system for specific decoder implementations and when the modulation format and its input distribution are fixed. In combination with the structure of the decoder, two different receiver-side equalization strategies are also analyzed: electronic dispersion compensation and digital backpropagation. We show that, somewhat unexpectedly, schemes based on nonbinary HD codes can achieve information rates comparable to SD decoders and that, when SD is used, switching from a symbol-wise to a bit-wise decoder results in a negligible penalty. Conversely, from an information-theoretic standpoint, HD binary decoders are shown to be unsuitable for spectrally-efficient, long-haul systems. ",
        "title": "Information Rates of Next-Generation Long-Haul Optical Fiber Systems  Using Coded Modulation",
        "date": "2016-06-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1807.08011",
        "abstract_url": "http://arxiv.org/abs/1807.08011",
        "authors": [
            {
                "last_name": "Dereniowski",
                "first_name": "Dariusz"
            },
            {
                "last_name": "Kubiak",
                "first_name": "Wieslaw"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  We study shared processor scheduling of $\\textit{multiprocessor}$ weighted jobs where each job can be executed on its private processor and simultaneously on possibly $\\textit{many}$ processors shared by all jobs in order to reduce their completion times due to processing time overlap. Each of $m$ shared processors may charge different fee but otherwise the processors are identical. The total weighted overlap of all jobs is to be maximized. This problem is key to subcontractor scheduling in extended enterprises and supply chains, and divisible load scheduling in computing. We prove that, quite surprisingly, $\\textit{synchronized}$ schedules that complete each job using shared processors at the same time on its private and shared processors include optimal schedules. We show that optimal $\\alpha$-$\\textit{private}$ schedules that require each job to use its private processor for at least $\\alpha=1/2+1/(4(m+1))$ of the time required by the job guarantee more than an $\\alpha$ fraction of the total weighted overlap of the optimal schedules. This gives an $\\alpha$-approximation algorithm that runs in strongly polynomial time for the problem, and improves the $1/2$-approximation reported recently in the literature to $5/8$-approximation for a single shared processor problem. The computational complexity of the problem, both single and multi-shared processor, remains open. We show however an LP-based optimal algorithm for $\\textit{antithetical}$ instances where for any pair of jobs $j$ and $i$, if the processing time of $j$ is smaller than or equal to the processing time of $i$, then the weight of $j$ is greater than or equal to the weight of $i$. ",
        "title": "Shared Processor Scheduling of Multiprocessor Jobs",
        "date": "2018-07-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1810.00197",
        "abstract_url": "http://arxiv.org/abs/1810.00197",
        "authors": [
            {
                "last_name": "Keykhosravi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Rastegarfar",
                "first_name": "Houman"
            },
            {
                "last_name": "Peyghambarian",
                "first_name": "Nasser"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Modular optical switch architectures combining wavelength routing based on arrayed waveguide grating (AWG) devices and multicasting based on star couplers hold promise for flexibly addressing the exponentially growing traffic demands in a cost- and power-efficient fashion. In a default switching scenario, an input port of the AWG is connected to an output port via a single wavelength. This can severely limit the capacity between broadcast domains, resulting in interdomain traffic switching bottlenecks. In this paper, we examine the possibility of resolving capacity bottlenecks by exploiting multiple AWG free spectral ranges (FSRs), i.e., setting up multiple parallel connections between each pair of broadcast domains. To this end, we introduce a multi-FSR scheduling algorithm for interconnecting broadcast domains by fairly distributing the wavelength resources among them. We develop a general-purpose analytical framework to study the blocking probabilities in a multistage switching scenario and compare our results with Monte Carlo simulations. Our study points to significant improvements with a moderate increase in the number of FSRs. We show that an FSR count beyond four results in diminishing returns. Furthermore, to investigate the trade-offs between the network- and physical-layer effects, we conduct a cross-layer analysis, taking into account pulse amplitude modulation (PAM) and rate-adaptive forward error correction (FEC). We illustrate how the effective bit rate per port increases with an increase in the number of FSRs. %We also look at the advantages of an impairment-aware scheduling strategy in a multi-FSR switching scenario. ",
        "title": "Exploiting AWG Free Spectral Range Periodicity in Distributed Multicast  Architectures",
        "date": "2018-09-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1811.08942",
        "abstract_url": "http://arxiv.org/abs/1811.08942",
        "authors": [
            {
                "last_name": "Secondini",
                "first_name": "Marco"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            },
            {
                "last_name": "Forestieri",
                "first_name": "Enrico"
            },
            {
                "last_name": "Marsella",
                "first_name": "Domenico"
            },
            {
                "last_name": "Camara",
                "first_name": "Menelaos Ralli"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  After reviewing models and mitigation strategies for interchannel nonlinear interference (NLI), we focus on the frequency-resolved logarithmic perturbation model to study the coherence properties of NLI. Based on this study, we devise an NLI mitigation strategy which exploits the synergic effect of phase and polarization noise compensation (PPN) and subcarrier multiplexing with symbol-rate optimization. This synergy persists even for high-order modulation alphabets and Gaussian symbols. A particle method for the computation of the resulting achievable information rate and spectral efficiency (SE) is presented and employed to lower-bound the channel capacity. The dependence of the SE on the link length, amplifier spacing, and presence or absence of inline dispersion compensation is studied. Single-polarization and dual-polarization scenarios with either independent or joint processing of the two polarizations are considered. Numerical results show that, in links with ideal distributed amplification, an SE gain of about 1 bit/s/Hz/polarization can be obtained (or, in alternative, the system reach can be doubled at a given SE) with respect to single-carrier systems without PPN mitigation. The gain is lower with lumped amplification, increases with the number of spans, decreases with the span length, and is further reduced by in-line dispersion compensation. For instance, considering a dispersion-unmanaged link with lumped amplification and an amplifier spacing of 60 km, the SE after 80 spans can be be increased from 4.5 to 4.8 bit/s/Hz/polarization, or the reach raised up to 100 spans (+25%) for a fixed SE. ",
        "title": "Nonlinearity Mitigation in WDM Systems: Models, Strategies, and  Achievable Rates",
        "date": "2018-11-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1812.03556",
        "abstract_url": "http://arxiv.org/abs/1812.03556",
        "authors": [
            {
                "last_name": "Keykhosravi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Secondini",
                "first_name": "Marco"
            },
            {
                "last_name": "Durisi",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Deploying periodic inline chromatic dispersion compensation enables reducing the complexity of the digital back propagation (DBP) algorithm. However, compared with nondispersion-managed (NDM) links, dispersion-managed (DM) ones suffer a stronger cross-phase modulation (XPM). Utilizing per-channel dispersion-managed (CDM) links (e.g., using fiber Bragg grating) allows for a complexity reduction of DBP, while abating XPM compared to DM links. In this paper, we show for the first time that CDM links enable also a more effective XPM compensation compared to NDM ones, allowing a higher achievable information rate (AIR). This is explained by resorting to the frequency-resolved logarithmic perturbation model and showing that per-channel dispersion compensation increases the frequency correlation of the distortions induced by XPM over the channel bandwidth, making them more similar to a conventional phase noise. We compare the performance (in terms of the AIR) of a DM, an NDM, and a CDM link, considering two types of mismatched receivers: one neglects the XPM phase distortion and the other compensates for it. With the former, the CDM link is inferior to the NDM one due to an increased in-band signal--noise interaction. However, with the latter, a higher AIR is obtained with the CDM link than with the NDM one owing to a higher XPM frequency correlation. The DM link has the lowest AIR for both receivers because of a stronger XPM. ",
        "title": "How to Increase the Achievable Information Rate by Per-Channel  Dispersion Compensation",
        "date": "2018-12-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1904.10850",
        "abstract_url": "http://arxiv.org/abs/1904.10850",
        "authors": [
            {
                "last_name": "Czyzowicz",
                "first_name": "Jurek"
            },
            {
                "last_name": "Dereniowski",
                "first_name": "Dariusz"
            },
            {
                "last_name": "Pelc",
                "first_name": "Andrzej"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  A robot modeled as a deterministic finite automaton has to build a structure from material available to it. The robot navigates in the infinite oriented grid $\\mathbb{Z} \\times \\mathbb{Z}$. Some cells of the grid are full (contain a brick) and others are empty. The subgraph of the grid induced by full cells, called the field, is initially connected. The (Manhattan) distance between the farthest cells of the field is called its span. The robot starts at a full cell. It can carry at most one brick at a time. At each step it can pick a brick from a full cell, move to an adjacent cell and drop a brick at an empty cell. The aim of the robot is to construct the most compact possible structure composed of all bricks, i.e., a nest. That is, the robot has to move all bricks in such a way that the span of the resulting field be the smallest. Our main result is the design of a deterministic finite automaton that accomplishes this task and subsequently stops, for every initially connected field, in time $O(sz)$, where $s$ is the span of the initial field and $z$ is the number of bricks. We show that this complexity is optimal. ",
        "title": "Building a Nest by an Automaton",
        "date": "2019-04-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1912.03507",
        "abstract_url": "http://arxiv.org/abs/1912.03507",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Asif Ali"
            },
            {
                "last_name": "Goens",
                "first_name": "Andres"
            },
            {
                "last_name": "Hameed",
                "first_name": "Fazal"
            },
            {
                "last_name": "Castrillon",
                "first_name": "Jeronimo"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Ultra-dense non-volatile racetrack memories (RTMs) have been investigated at various levels in the memory hierarchy for improved performance and reduced energy consumption. However, the innate shift operations in RTMs hinder their applicability to replace low-latency on-chip memories. Recent research has demonstrated that intelligent placement of memory objects in RTMs can significantly reduce the amount of shifts with no hardware overhead, albeit for specific system setups. However, existing placement strategies may lead to sub-optimal performance when applied to different architectures. In this paper we look at generalized data placement mechanisms that improve upon existing ones by taking into account the underlying memory architecture and the timing and liveliness information of memory objects. We propose a novel heuristic and a formulation using genetic algorithms that optimize key performance parameters. We show that, on average, our generalized approach improves the number of shifts, performance and energy consumption by 4.3x, 46% and 55% respectively compared to the state-of-the-art. ",
        "title": "Generalized Data Placement Strategies for Racetrack Memories",
        "date": "2019-12-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:1912.05638",
        "abstract_url": "http://arxiv.org/abs/1912.05638",
        "authors": [
            {
                "last_name": "G\u00fcm\u00fcs",
                "first_name": "Kadir"
            },
            {
                "last_name": "Alvarado",
                "first_name": "Alex"
            },
            {
                "last_name": "Chen",
                "first_name": "Bin"
            },
            {
                "last_name": "H\u00e4ger",
                "first_name": "Christian"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  GMI-based end-to-end learning is shown to be highly nonconvex. We apply gradient descent initialized with Gray-labeled APSK constellations directly to the constellation coordinates. State-of-the-art constellations in 2D and 4D are found providing reach increases up to 26\\% w.r.t. to QAM. ",
        "title": "End-to-End Learning of Geometrical Shaping Maximizing Generalized Mutual  Information",
        "date": "2019-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2002.00560",
        "abstract_url": "http://arxiv.org/abs/2002.00560",
        "authors": [
            {
                "last_name": "Yoshida",
                "first_name": "Tsuyoshi"
            },
            {
                "last_name": "Mazur",
                "first_name": "Mikael"
            },
            {
                "last_name": "Schr\u00f6der",
                "first_name": "Jochen"
            },
            {
                "last_name": "Karlsson",
                "first_name": "Magnus"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We investigated a suitable auxiliary channel setting and the gap between Q-factors with hard and soft demapping. The system margin definition should be reconsidered for systems employing complex coded modulation with soft forward error correction. ",
        "title": "On the Performance under Hard and Soft Bitwise Mismatched-Decoding",
        "date": "2020-02-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2104.13353",
        "abstract_url": "http://arxiv.org/abs/2104.13353",
        "authors": [
            {
                "last_name": "Zumaya",
                "first_name": "Martin"
            },
            {
                "last_name": "Guerrero",
                "first_name": "Rita"
            },
            {
                "last_name": "Islas",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Pineda",
                "first_name": "Omar"
            },
            {
                "last_name": "Gershenson",
                "first_name": "Carlos"
            },
            {
                "last_name": "I\u00f1iguez",
                "first_name": "Gerardo"
            },
            {
                "last_name": "Pineda",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Mexico has kept electronic records of all taxable transactions since 2014. Anonymized data collected by the Mexican federal government comprises more than 80 million contributors (individuals and companies) and almost 7 billion monthly-aggregations of invoices among contributors between January 2015 and December 2018. This data includes a list of almost ten thousand contributors already identified as tax evaders, due to their activities fabricating invoices for non-existing products or services so that recipients can evade taxes. Harnessing this extensive dataset, we build monthly and yearly temporal networks where nodes are contributors and directed links are invoices produced in a given time slice. Exploring the properties of the network neighborhoods around tax evaders, we show that their interaction patterns differ from those of the majority of contributors. In particular, invoicing loops between tax evaders and their clients are over-represented. With this insight, we use two machine-learning methods to classify other contributors as suspects of tax evasion: deep neural networks and random forests. We train each method with a portion of the tax evader list and test it with the rest, obtaining more than 0.9 accuracy with both methods. By using the complete dataset of contributors, each method classifies more than 100 thousand suspects of tax evasion, with more than 40 thousand suspects classified by both methods. We further reduce the number of suspects by focusing on those with a short network distance from known tax evaders. We thus obtain a list of highly suspicious contributors sorted by the amount of evaded tax, valuable information for the authorities to further investigate illegal tax activity in Mexico. With our methods, we estimate previously undetected tax evasion in the order of \\$10 billion USD per year by about 10 thousand contributors. ",
        "title": "Identifying tax evasion in Mexico with tools from network science and  machine learning",
        "date": "2021-04-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2105.02609",
        "abstract_url": "http://arxiv.org/abs/2105.02609",
        "authors": [
            {
                "last_name": "Dereniowski",
                "first_name": "Dariusz"
            },
            {
                "last_name": "Kuszner",
                "first_name": "\u0141ukasz"
            },
            {
                "last_name": "Ostrowski",
                "first_name": "Robert"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  In this work we introduce and study a pursuit-evasion game in which the search is performed by heterogeneous entities. We incorporate heterogeneity into the classical edge search problem by considering edge-labeled graphs: once a search strategy initially assigns labels to the searchers, each searcher can be only present on an edge of its own label. We prove that this problem is not monotone even for trees and we give instances in which the number of recontamination events is asymptotically quadratic in the tree size. Other negative results regard the NP-completeness of the monotone, and NP-hardness of an arbitrary (i.e., non-monotone) heterogeneous search in trees. These properties show that this problem behaves very differently from the classical edge search. On the other hand, if all edges of a particular label form a (connected) subtree of the input tree, then we show that optimal heterogeneous search strategy can be computed efficiently. ",
        "title": "Searching by Heterogeneous Agents",
        "date": "2021-05-06",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2112.12661",
        "abstract_url": "http://arxiv.org/abs/2112.12661",
        "authors": [
            {
                "last_name": "Farsi",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Karlsson",
                "first_name": "Magnus"
            },
            {
                "last_name": "Agrell",
                "first_name": "Erik"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In optical fiber communication, due to the random variation of the environment, the state of polarization (SOP) fluctuates randomly with time leading to distortion and performance degradation. The memory-less SOP fluctuations can be regarded as a two-by-two random unitary matrix. In this paper, for what we believe to be the first time, the capacity of the polarization drift channel under an average power constraint with imperfect channel knowledge is characterized. An achievable information rate (AIR) is derived when imperfect channel knowledge is available and is shown to be highly dependent on the channel estimation technique. It is also shown that a tighter lower bound can be achieved when a unitary estimation of the channel is available. However, the conventional estimation algorithms do not guarantee a unitary channel estimation. Therefore, by considering the unitary constraint of the channel, a data-aided channel estimator based on the Kabsch algorithm is proposed, and its performance is numerically evaluated in terms of AIR. Monte Carlo simulations show that Kabsch outperforms the least-square error algorithm. In particular, with complex, Gaussian inputs and eight pilot symbols per block, Kabsch improves the AIR by 0:2 to 0:35 bits/symbol throughout the range of studied signal-to-noise ratios. ",
        "title": "Capacity Bounds under Imperfect Polarization Tracking",
        "date": "2021-12-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2201.04542",
        "abstract_url": "http://arxiv.org/abs/2201.04542",
        "authors": [
            {
                "last_name": "Shurup",
                "first_name": "A. S."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this work the numerical solution of acoustic tomography problem based on the iterative and functional-analytical algorithms is considered. The mathematical properties of these algorithms were previously described in works of R.G.Novikov for the case of the Schr\\\"odinger equation. In the present work, for the case of two-dimensional scalar Helmholtz equation, the efficiency of the iterative algorithm in reconstruction of middle strength scatterers and advantages of the functional-analytical approach in recovering strong scatterers are demonstrated. A filtering procedure is considered in the space of wave vectors, which additionally increases the convergence of the iterative algorithm. Reconstruction results of sound speed perturbations demonstrate the comparable noise immunity and resolution of the considered algorithms when reconstructing middle strength scatterers. A comparative numerical investigation of the iterative and functional-analytical algorithms in inverse acoustic scattering problems is implemented in this work for the first time. ",
        "title": "Numerical comparison of iterative and functional-analytical algorithms  for inverse acoustic scattering",
        "date": "2022-01-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2301.08809",
        "abstract_url": "http://arxiv.org/abs/2301.08809",
        "authors": [
            {
                "last_name": "Shurup",
                "first_name": "A. S."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In practice of acoustic tomography, for example, in medical applications and ocean tomography, the relative deviation of sound speed from its background value usually does not exceed 10-30%. At the same time, in electromagnetic applications, the equivalent contrasts can be noticeably higher than 60%. Since the inverse electromagnetic problem can be reduced in some approximation to Helmholtz equation, a formal comparison of reconstruction results obtained for different \"acoustic\" contrast and corresponding \"dielectric\" contrast is possible. In this work examples of such reconstructions are presented, which were obtained by using the functional-analytical algorithm described in works of R.G. Novikov. Previously, the advantages of this algorithm for solving practical problems of acoustic tomography were demonstrated. Results obtained in the present work show that functional-analytical algorithm can also be applied to reconstructing inhomogeneities with high \"dielectric\" contrast. Moreover, the functional algorithm also perfectly reconstructs very small \"dielectric\" contrast, recovering of which can be difficult for other approaches due to weak backscattering. ",
        "title": "Functional-analytical reconstruction of high contrast inhomogeneities",
        "date": "2023-01-20",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.02025",
        "abstract_url": "http://arxiv.org/abs/2302.02025",
        "authors": [
            {
                "last_name": "Kozlov",
                "first_name": "Igor"
            },
            {
                "last_name": "Rivkin",
                "first_name": "Dmitriy"
            },
            {
                "last_name": "Chang",
                "first_name": "Wei-Di"
            },
            {
                "last_name": "Wu",
                "first_name": "Di"
            },
            {
                "last_name": "Liu",
                "first_name": "Xue"
            },
            {
                "last_name": "Dudek",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Radio Access Networks (RANs) for telecommunications represent large agglomerations of interconnected hardware consisting of hundreds of thousands of transmitting devices (cells). Such networks undergo frequent and often heterogeneous changes caused by network operators, who are seeking to tune their system parameters for optimal performance. The effects of such changes are challenging to predict and will become even more so with the adoption of 5G/6G networks. Therefore, RAN monitoring is vital for network operators. We propose a self-supervised learning framework that leverages self-attention and self-distillation for this task. It works by detecting changes in Performance Measurement data, a collection of time-varying metrics which reflect a set of diverse measurements of the network performance at the cell level. Experimental results show that our approach outperforms the state of the art by 4% on a real-world based dataset consisting of about hundred thousands timeseries. It also has the merits of being scalable and generalizable. This allows it to provide deep insight into the specifics of mode of operation changes while relying minimally on expert knowledge. ",
        "title": "Self-Supervised Transformer Architecture for Change Detection in Radio  Access Networks",
        "date": "2023-02-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.06043",
        "abstract_url": "http://arxiv.org/abs/2302.06043",
        "authors": [
            {
                "last_name": "Xing",
                "first_name": "Xin"
            },
            {
                "last_name": "Lin",
                "first_name": "Lin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We provide the first rigorous study of the finite-size error in the simplest and representative coupled cluster theory, namely the coupled cluster doubles (CCD) theory, for gapped periodic systems. Assuming that the CCD equations are solved using exact Hartree-Fock orbitals and orbital energies, we prove that the convergence rate of finite-size error scales as $\\mathscr{O}(N_\\mathbf{k}^{-\\frac13})$, where $N_{\\mathbf{k}}$ is the number of discretization point in the Brillouin zone and characterizes the system size. Our analysis shows that the dominant error lies in the coupled cluster amplitude calculation, and the convergence of the finite-size error in energy calculations can be boosted to $\\mathscr{O}(N_\\mathbf{k}^{-1})$ with accurate amplitudes. This also provides the first proof of the scaling of the finite-size error in the third order M{\\o}ller-Plesset perturbation theory (MP3) for periodic systems. ",
        "title": "Finite-size effects in periodic coupled cluster calculations",
        "date": "2023-02-12",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2302.13972",
        "abstract_url": "http://arxiv.org/abs/2302.13972",
        "authors": [
            {
                "last_name": "I\u00f1iguez",
                "first_name": "Gerardo"
            },
            {
                "last_name": "Heydari",
                "first_name": "Sara"
            },
            {
                "last_name": "Kert\u00e9sz",
                "first_name": "J\u00e1nos"
            },
            {
                "last_name": "Saram\u00e4ki",
                "first_name": "Jari"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Tie strengths in social networks are heterogeneous, with strong and weak ties playing different roles at both the network and the individual level. Egocentric networks, networks of relationships around a focal individual, exhibit a small number of strong ties and a larger number of weaker ties, a pattern that is evident in electronic communication records, such as mobile phone calls. Mobile phone data has also revealed persistent individual differences within this pattern. However, the generality and the driving mechanisms of this tie strength heterogeneity remain unclear. Here, we study tie strengths in egocentric networks across multiple datasets containing records of interactions between millions of people over time periods ranging from months to years. Our findings reveal a remarkable universality in the distribution of tie strengths and their individual-level variation across different modes of communication, even in channels that may not reflect offline social relationships. With the help of an analytically tractable model of egocentric network evolution, we show that the observed universality can be attributed to the competition between cumulative advantage and random choice, two general mechanisms of tie reinforcement whose balance determines the amount of heterogeneity in tie strengths. Our results provide new insights into the driving mechanisms of tie strength heterogeneity in social networks and have implications for the understanding of social network structure and individual behavior. ",
        "title": "Universal patterns in egocentric communication networks",
        "date": "2023-02-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.05290",
        "abstract_url": "http://arxiv.org/abs/2304.05290",
        "authors": [
            {
                "last_name": "Amico",
                "first_name": "Ambra"
            },
            {
                "last_name": "Verginer",
                "first_name": "Luca"
            },
            {
                "last_name": "Casiraghi",
                "first_name": "Giona"
            },
            {
                "last_name": "Vaccario",
                "first_name": "Giacomo"
            },
            {
                "last_name": "Schweitzer",
                "first_name": "Frank"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Supply chain disruptions cause shortages of raw material and products. To increase resilience, i.e., the ability to cope with shocks, substituting goods in established supply chains can become an effective alternative to creating new distribution links. We demonstrate its impact on supply deficits through a detailed analysis of the US opioid distribution system. Reconstructing 40 billion empirical distribution paths, our data-driven model allows a unique inspection of policies that increase the substitution flexibility. Our approach enables policymakers to quantify the trade-off between increasing flexibility, i.e., reduced supply deficits, and increasing complexity of the supply chain, which could make it more expensive to operate. ",
        "title": "Adapting to Disruptions: Flexibility as a Pillar of Supply Chain  Resilience",
        "date": "2023-04-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2304.06826",
        "abstract_url": "http://arxiv.org/abs/2304.06826",
        "authors": [
            {
                "last_name": "Venturini",
                "first_name": "Sara"
            },
            {
                "last_name": "Sikdar",
                "first_name": "Satyaki"
            },
            {
                "last_name": "Rinaldi",
                "first_name": "Francesco"
            },
            {
                "last_name": "Tudisco",
                "first_name": "Francesco"
            },
            {
                "last_name": "Fortunato",
                "first_name": "Santo"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Collaboration is a key driver of science and innovation. Mainly motivated by the need to leverage different capacities and expertise to solve a scientific problem, collaboration is also an excellent source of information about the future behavior of scholars. In particular, it allows us to infer the likelihood that scientists choose future research directions via the intertwined mechanisms of selection and social influence. Here we thoroughly investigate the interplay between collaboration and topic switches. We find that the probability for a scholar to start working on a new topic increases with the number of previous collaborators, with a pattern showing that the effects of individual collaborators are not independent. The higher the productivity and the impact of authors, the more likely their coworkers will start working on new topics. The average number of coauthors per paper is also inversely related to the topic switch probability, suggesting a dilution of this effect as the number of collaborators increases. ",
        "title": "Collaboration and topic switches in science",
        "date": "2023-04-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.11584",
        "abstract_url": "http://arxiv.org/abs/2305.11584",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yan"
            },
            {
                "last_name": "Shen",
                "first_name": "Li"
            },
            {
                "last_name": "Chen",
                "first_name": "Shixiang"
            },
            {
                "last_name": "Ding",
                "first_name": "Liang"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the global Sharpness Aware Minimization (SAM) optimizer to search for the consistent flat minima. Our theoretical analysis indicates that {\\ttfamily FedSMOO} achieves fast $\\mathcal{O}(1/T)$ convergence rate with low generalization bound. Extensive numerical studies are conducted on the real-world dataset to verify its peerless efficiency and excellent generality. ",
        "title": "Dynamic Regularized Sharpness Aware Minimization in Federated Learning:  Approaching Global Consistency and Smooth Landscape",
        "date": "2023-05-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.16978",
        "abstract_url": "http://arxiv.org/abs/2305.16978",
        "authors": [
            {
                "last_name": "Vadher",
                "first_name": "Pratik"
            },
            {
                "last_name": "Sacco",
                "first_name": "Giulia"
            },
            {
                "last_name": "Nikolayev",
                "first_name": "Denys"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a dual-band frequency scanning meandering microstrip leaky-wave antenna with linear polarization in the Ku-band and circular polarization in the K-band. This is achieved by making use of two spatial harmonics for radiation. The unit cell of the periodic microstrip antenna contains three meanders with mitred corners. To ensure circular polarization, a theoretical formulation is developed taking into account the delay caused by microstrip length intervals. It defines the unit cell geometry by determining the length of the meanders to ensure that axial ratio remains below 3 dB throughout the operational band. Moreover, the meanders are used to provide better control over scanning rate (the ratio of change of angle of maximum radiation with frequency) and reduce spurious radiation of harmonics by ensuring single harmonic operation within the operational band. To guarantee continuous scanning through broadside direction, open stopband is suppressed using mitered angles. The antenna is designed on a 0.254-mm substrate making it suitable for conformal applications. The fabricated antenna shows a backward to forward beam steering range of 72 deg (-42 deg to 30 deg) in the K-band (19.4-27.5 GHz) with circular polarization and of 75 deg (-15 deg to 60 deg) in the Ku-band (11-15.5 GHz) with linear polarization. ",
        "title": "Meandering microstrip leaky-wave antenna with dual-band linear-circular  polarization and suppressed open stopband",
        "date": "2023-05-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2305.18469",
        "abstract_url": "http://arxiv.org/abs/2305.18469",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Fei"
            },
            {
                "last_name": "Chen",
                "first_name": "Chaochao"
            },
            {
                "last_name": "Lyu",
                "first_name": "Lingjuan"
            },
            {
                "last_name": "Yao",
                "first_name": "Binhui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Split learning is a simple solution for Vertical Federated Learning (VFL), which has drawn substantial attention in both research and application due to its simplicity and efficiency. However, communication efficiency is still a crucial issue for split learning. In this paper, we investigate multiple communication reduction methods for split learning, including cut layer size reduction, top-k sparsification, quantization, and L1 regularization. Through analysis of the cut layer size reduction and top-k sparsification, we further propose randomized top-k sparsification, to make the model generalize and converge better. This is done by selecting top-k elements with a large probability while also having a small probability to select non-top-k elements. Empirical results show that compared with other communication-reduction methods, our proposed randomized top-k sparsification achieves a better model performance under the same compression level. ",
        "title": "Reducing Communication for Split Learning by Randomized Top-k  Sparsification",
        "date": "2023-05-29",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.02800",
        "abstract_url": "http://arxiv.org/abs/2306.02800",
        "authors": [
            {
                "last_name": "Hekler",
                "first_name": "Achim"
            },
            {
                "last_name": "Maron",
                "first_name": "Roman C."
            },
            {
                "last_name": "Haggenm\u00fcller",
                "first_name": "Sarah"
            },
            {
                "last_name": "Schmitt",
                "first_name": "Max"
            },
            {
                "last_name": "Wies",
                "first_name": "Christoph"
            },
            {
                "last_name": "Utikal",
                "first_name": "Jochen S."
            },
            {
                "last_name": "Meier",
                "first_name": "Friedegund"
            },
            {
                "last_name": "Hobelsberger",
                "first_name": "Sarah"
            },
            {
                "last_name": "Gellrich",
                "first_name": "Frank F."
            },
            {
                "last_name": "Sergon",
                "first_name": "Mildred"
            },
            {
                "last_name": "Hauschild",
                "first_name": "Axel"
            },
            {
                "last_name": "French",
                "first_name": "Lars E."
            },
            {
                "last_name": "Heinzerling",
                "first_name": "Lucie"
            },
            {
                "last_name": "Schlager",
                "first_name": "Justin G."
            },
            {
                "last_name": "Ghoreschi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Schlaak",
                "first_name": "Max"
            },
            {
                "last_name": "Hilke",
                "first_name": "Franz J."
            },
            {
                "last_name": "Poch",
                "first_name": "Gabriela"
            },
            {
                "last_name": "Korsing",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Berking",
                "first_name": "Carola"
            },
            {
                "last_name": "Heppt",
                "first_name": "Markus V."
            },
            {
                "last_name": "Erdmann",
                "first_name": "Michael"
            },
            {
                "last_name": "Haferkamp",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Drexler",
                "first_name": "Konstantin"
            },
            {
                "last_name": "Schadendorf",
                "first_name": "Dirk"
            },
            {
                "last_name": "Sondermann",
                "first_name": "Wiebke"
            },
            {
                "last_name": "Goebeler",
                "first_name": "Matthias"
            },
            {
                "last_name": "Schilling",
                "first_name": "Bastian"
            },
            {
                "last_name": "Kather",
                "first_name": "Jakob N."
            },
            {
                "last_name": "Krieghoff-Henning",
                "first_name": "Eva"
            },
            {
                "last_name": "Brinker",
                "first_name": "Titus J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Background: Convolutional neural network (CNN)-based melanoma classifiers face several challenges that limit their usefulness in clinical practice. Objective: To investigate the impact of multiple real-world dermoscopic views of a single lesion of interest on a CNN-based melanoma classifier.   Methods: This study evaluated 656 suspected melanoma lesions. Classifier performance was measured using area under the receiver operating characteristic curve (AUROC), expected calibration error (ECE) and maximum confidence change (MCC) for (I) a single-view scenario, (II) a multiview scenario using multiple artificially modified images per lesion and (III) a multiview scenario with multiple real-world images per lesion.   Results: The multiview approach with real-world images significantly increased the AUROC from 0.905 (95% CI, 0.879-0.929) in the single-view approach to 0.930 (95% CI, 0.909-0.951). ECE and MCC also improved significantly from 0.131 (95% CI, 0.105-0.159) to 0.072 (95% CI: 0.052-0.093) and from 0.149 (95% CI, 0.125-0.171) to 0.115 (95% CI: 0.099-0.131), respectively. Comparing multiview real-world to artificially modified images showed comparable diagnostic accuracy and uncertainty estimation, but significantly worse robustness for the latter.   Conclusion: Using multiple real-world images is an inexpensive method to positively impact the performance of a CNN-based melanoma classifier. ",
        "title": "Using Multiple Dermoscopic Photographs of One Lesion Improves Melanoma  Classification via Deep Learning: A Prognostic Diagnostic Accuracy Study",
        "date": "2023-06-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.08373",
        "abstract_url": "http://arxiv.org/abs/2306.08373",
        "authors": [
            {
                "last_name": "Jiang",
                "first_name": "Baoxing"
            },
            {
                "last_name": "Liang",
                "first_name": "Shehui"
            },
            {
                "last_name": "Liu",
                "first_name": "Peiyu"
            },
            {
                "last_name": "Dong",
                "first_name": "Kaifang"
            },
            {
                "last_name": "Li",
                "first_name": "Hongye"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Aspect sentiment triplet extraction (ASTE) is a crucial subtask of aspect-based sentiment analysis (ABSA) that aims to comprehensively identify sentiment triplets. Previous research has focused on enhancing ASTE through innovative table-filling strategies. However, these approaches often overlook the multi-perspective nature of language expressions, resulting in a loss of valuable interaction information between aspects and opinions. To address this limitation, we propose a framework that leverages both a basic encoder, primarily based on BERT, and a particular encoder comprising a Bi-LSTM network and graph convolutional network (GCN ). The basic encoder captures the surface-level semantics of linguistic expressions, while the particular encoder extracts deeper semantics, including syntactic and lexical information. By modeling the dependency tree of comments and considering the part-of-speech and positional information of words, we aim to capture semantics that are more relevant to the underlying intentions of the sentences. An interaction strategy combines the semantics learned by the two encoders, enabling the fusion of multiple perspectives and facilitating a more comprehensive understanding of aspect--opinion relationships. Experiments conducted on benchmark datasets demonstrate the state-of-the-art performance of our proposed framework. ",
        "title": "A semantically enhanced dual encoder for aspect sentiment triplet  extraction",
        "date": "2023-06-14",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.10718",
        "abstract_url": "http://arxiv.org/abs/2306.10718",
        "authors": [
            {
                "last_name": "Imamura",
                "first_name": "Kanami"
            },
            {
                "last_name": "Nakamura",
                "first_name": "Tomohiko"
            },
            {
                "last_name": "Takamune",
                "first_name": "Norihiro"
            },
            {
                "last_name": "Yatabe",
                "first_name": "Kohei"
            },
            {
                "last_name": "Saruwatari",
                "first_name": "Hiroshi"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  In this paper, we propose algorithms for handling non-integer strides in sampling-frequency-independent (SFI) convolutional and transposed convolutional layers. The SFI layers have been developed for handling various sampling frequencies (SFs) by a single neural network. They are replaceable with their non-SFI counterparts and can be introduced into various network architectures. However, they could not handle some specific configurations when combined with non-SFI layers. For example, an SFI extension of Conv-TasNet, a standard audio source separation model, cannot handle some pairs of trained and target SFs because the strides of the SFI layers become non-integers. This problem cannot be solved by simple rounding or signal resampling, resulting in the significant performance degradation. To overcome this problem, we propose algorithms for handling non-integer strides by using windowed sinc interpolation. The proposed algorithms realize the continuous-time representations of features using the interpolation and enable us to sample instants with the desired stride. Experimental results on music source separation showed that the proposed algorithms outperformed the rounding- and signal-resampling-based methods at SFs lower than the trained SF. ",
        "title": "Algorithms of Sampling-Frequency-Independent Layers for Non-integer  Strides",
        "date": "2023-06-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.12817",
        "abstract_url": "http://arxiv.org/abs/2306.12817",
        "authors": [
            {
                "last_name": "Fan",
                "first_name": "Daiwei"
            },
            {
                "last_name": "Bolderman",
                "first_name": "Max"
            },
            {
                "last_name": "Koekebakker",
                "first_name": "Sjirk"
            },
            {
                "last_name": "Butler",
                "first_name": "Hans"
            },
            {
                "last_name": "Lazar",
                "first_name": "Mircea"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Rotary motors, such as hybrid stepper motors (HSMs), are widely used in industries varying from printing applications to robotics. The increasing need for productivity and efficiency without increasing the manufacturing costs calls for innovative control design. Feedforward control is typically used in tracking control problems, where the desired reference is known in advance. In most applications, this is the case for HSMs, which need to track a periodic angular velocity and angular position reference. Performance achieved by feedforward control is limited by the accuracy of the available model describing the inverse system dynamics. In this work, we develop a physics-guided neural network (PGNN) feedforward controller for HSMs, which can learn the effect of parasitic forces from data and compensate for it, resulting in improved accuracy. Indeed, experimental results on an HSM used in printing industry show that the PGNN outperforms conventional benchmarks in terms of the mean-absolute tracking error. ",
        "title": "Physics-guided neural networks for inversion-based feedforward control  applied to hybrid stepper motors",
        "date": "2023-06-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.14018",
        "abstract_url": "http://arxiv.org/abs/2306.14018",
        "authors": [
            {
                "last_name": "Vu",
                "first_name": "Linh"
            },
            {
                "last_name": "Vu",
                "first_name": "Tuyen"
            },
            {
                "last_name": "Vu",
                "first_name": "Thanh-Long"
            },
            {
                "last_name": "Srivastava",
                "first_name": "Anurag"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper addresses the load restoration problem after power outage events. Our primary proposed methodology is using multi-agent deep reinforcement learning to optimize the load restoration process in distribution systems, modeled as networked microgrids, via determining the optimal operational sequence of circuit breakers (switches). An innovative invalid action masking technique is incorporated into the multi-agent method to handle both the physical constraints in the restoration process and the curse of dimensionality as the action space of operational decisions grows exponentially with the number of circuit breakers. The features of our proposed method include centralized training for multi-agents to overcome non-stationary environment problems, decentralized execution to ease the deployment, and zero constraint violations to prevent harmful actions. Our simulations are performed in OpenDSS and Python environments to demonstrate the effectiveness of the proposed approach using the IEEE 13, 123, and 8500-node distribution test feeders. The results show that the proposed algorithm can achieve a significantly better learning curve and stability than the conventional methods. ",
        "title": "Multi-agent Deep Reinforcement Learning for Distributed Load Restoration",
        "date": "2023-06-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2306.14553",
        "abstract_url": "http://arxiv.org/abs/2306.14553",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongmyoung"
            },
            {
                "last_name": "Chappell",
                "first_name": "Digby"
            },
            {
                "last_name": "Rojas",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  When performing cloth-related tasks, such as garment hanging, it is often important to identify and grasp certain structural regions -- a shirt's collar as opposed to its sleeve, for instance. However, due to cloth deformability, these manipulation activities, which are essential in domestic, health care, and industrial contexts, remain challenging for robots. In this paper, we focus on how to segment and grasp structural regions of clothes to enable manipulation tasks, using hanging tasks as case study. To this end, a neural network-based perception system is proposed to segment a shirt's collar from areas that represent the rest of the scene in a depth image. With a 10-minute video of a human manipulating shirts to train it, our perception system is capable of generalizing to other shirts regardless of texture as well as to other types of collared garments. A novel grasping strategy is then proposed based on the segmentation to determine grasping pose. Experiments demonstrate that our proposed grasping strategy achieves 92\\%, 80\\%, and 50\\% grasping success rates with one folded garment, one crumpled garment and three crumpled garments, respectively.   Our grasping strategy performs considerably better than tested baselines that do not take into account the structural nature of the garments. With the proposed region segmentation and grasping strategy, challenging garment hanging tasks are successfully implemented using an open-loop control policy. Supplementary material is available at https://sites.google.com/view/garment-hanging ",
        "title": "Learning to Grasp Clothing Structural Regions for Garment Manipulation  Tasks",
        "date": "2023-06-26",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2308.13294",
        "abstract_url": "http://arxiv.org/abs/2308.13294",
        "authors": [
            {
                "last_name": "Bialas",
                "first_name": "Piotr"
            },
            {
                "last_name": "Korcyl",
                "first_name": "Piotr"
            },
            {
                "last_name": "Stebel",
                "first_name": "Tomasz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Machine learning techniques, in particular the so-called normalizing flows, are becoming increasingly popular in the context of Monte Carlo simulations as they can effectively approximate target probability distributions. In the case of lattice field theories (LFT) the target distribution is given by the exponential of the action. The common loss function's gradient estimator based on the \"reparametrization trick\" requires the calculation of the derivative of the action with respect to the fields. This can present a significant computational cost for complicated, non-local actions like e.g. fermionic action in QCD. In this contribution, we propose an estimator for normalizing flows based on the REINFORCE algorithm that avoids this issue. We apply it to two dimensional Schwinger model with Wilson fermions at criticality and show that it is up to ten times faster in terms of the wall-clock time as well as requiring up to $30\\%$ less memory than the reparameterization trick estimator. It is also more numerically stable allowing for single precision calculations and the use of half-float tensor cores. We present an in-depth analysis of the origins of those improvements. We believe that these benefits will appear also outside the realm of the LFT, in each case where the target probability distribution is computationally intensive. ",
        "title": "Training normalizing flows with computationally intensive target  probability distributions",
        "date": "2023-08-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.01590",
        "abstract_url": "http://arxiv.org/abs/2309.01590",
        "authors": [
            {
                "last_name": "Park",
                "first_name": "Dogyun"
            },
            {
                "last_name": "Kim",
                "first_name": "Suhyun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Assessing the fidelity and diversity of the generative model is a difficult but important issue for technological advancement. So, recent papers have introduced k-Nearest Neighbor ($k$NN) based precision-recall metrics to break down the statistical distance into fidelity and diversity. While they provide an intuitive method, we thoroughly analyze these metrics and identify oversimplified assumptions and undesirable properties of kNN that result in unreliable evaluation, such as susceptibility to outliers and insensitivity to distributional changes. Thus, we propose novel metrics, P-precision and P-recall (PP\\&PR), based on a probabilistic approach that address the problems. Through extensive investigations on toy experiments and state-of-the-art generative models, we show that our PP\\&PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics. The codes are available at \\url{https://github.com/kdst-team/Probablistic_precision_recall}. ",
        "title": "Probabilistic Precision and Recall Towards Reliable Evaluation of  Generative Models",
        "date": "2023-09-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2309.09419",
        "abstract_url": "http://arxiv.org/abs/2309.09419",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Jin Sung"
            },
            {
                "last_name": "Quan",
                "first_name": "Ying Shuai"
            },
            {
                "last_name": "Chung",
                "first_name": "Chung Choo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper proposes a method for uncertainty quantification of an autoencoder-based Koopman operator. The main challenge of using the Koopman operator is to design the basis functions for lifting the state. To this end, this paper builds an autoencoder to automatically search the optimal lifting basis functions with a given loss function. We approximate the Koopman operator in a finite-dimensional space with the autoencoder, while the approximated Koopman has an approximation uncertainty. To resolve the problem, we compute a robust positively invariant set for the approximated Koopman operator to consider the approximation error. Then, the decoder of the autoencoder is analyzed by robustness certification against approximation error using the Lipschitz constant in the reconstruction phase. The forced Van der Pol model is used to show the validity of the proposed method. From the numerical simulation results, we confirmed that the trajectory of the true state stays in the uncertainty set centered by the reconstructed state. ",
        "title": "Uncertainty Quantification of Autoencoder-based Koopman Operator",
        "date": "2023-09-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2312.02505",
        "abstract_url": "http://arxiv.org/abs/2312.02505",
        "authors": [
            {
                "last_name": "Onat",
                "first_name": "Emin Burak"
            },
            {
                "last_name": "Bulusu",
                "first_name": "Vishwanath"
            },
            {
                "last_name": "Chakrabarty",
                "first_name": "Anjan"
            },
            {
                "last_name": "Hansen",
                "first_name": "Mark"
            },
            {
                "last_name": "Sengupta",
                "first_name": "Raja"
            },
            {
                "last_name": "Sridar",
                "first_name": "Banavar"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Urban Air Mobility (UAM) represents a promising solution for future transportation. In this study, we introduce VertiSim, an advanced event-driven simulator developed to evaluate e-VTOL transportation networks. Uniquely, VertiSim simultaneously models passenger, aircraft, and energy flows, reflecting the interrelated complexities of UAM systems. We utilized VertiSim to assess 19 operational scenarios serving a daily demand for 2,834 passengers with varying fleet sizes and vertiport distances. The study aims to support stakeholders in making informed decisions about fleet size, network design, and infrastructure development by understanding tradeoffs in passenger delay time, operational costs, and fleet utilization. Our simulations, guided by a heuristic dispatch and charge policy, indicate that fleet size significantly influences passenger delay and energy consumption within UAM networks. We find that increasing the fleet size can reduce average passenger delays, but this comes at the cost of higher operational expenses due to an increase in the number of repositioning flights. Additionally, our analysis highlights how vertiport distances impact fleet utilization: longer distances result in reduced total idle time and increased cruise and charge times, leading to more efficient fleet utilization but also longer passenger delays. These findings are important for UAM network planning, especially in balancing fleet size with vertiport capacity and operational costs. Simulator demo is available at: https://tinyurl.com/vertisim-vis ",
        "title": "Evaluating eVTOL Network Performance and Fleet Dynamics through  Simulation-Based Analysis",
        "date": "2023-12-05",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.01009",
        "abstract_url": "http://arxiv.org/abs/2401.01009",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Hanyu"
            },
            {
                "last_name": "Tan",
                "first_name": "Bochen"
            },
            {
                "last_name": "Cong",
                "first_name": "Jason"
            },
            {
                "last_name": "De Micheli",
                "first_name": "Giovanni"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Minimizing the use of CNOT gates in quantum state preparation is a crucial step in quantum compilation, as they introduce coupling constraints and more noise than single-qubit gates. Reducing the number of CNOT gates can lead to more efficient and accurate quantum computations. However, the lack of compatibility to model superposition and entanglement challenges the scalability and optimality of CNOT optimization algorithms on classical computers. In this paper, we propose an effective state preparation algorithm using an exact CNOT synthesis formulation. Our method represents a milestone as the first design automation algorithm to surpass manual design, reducing the best CNOT numbers to prepare a Dicke state by 2x. For general states with up to 20 qubits, our method reduces the CNOT number by 9% and 32% for dense and sparse states, on average, compared to the latest algorithms. ",
        "title": "Quantum State Preparation Using an Exact CNOT Synthesis Formulation",
        "date": "2024-01-01",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.08916",
        "abstract_url": "http://arxiv.org/abs/2401.08916",
        "authors": [
            {
                "last_name": "Raju",
                "first_name": "Anirudh"
            },
            {
                "last_name": "Khare",
                "first_name": "Aparna"
            },
            {
                "last_name": "He",
                "first_name": "Di"
            },
            {
                "last_name": "Sklyar",
                "first_name": "Ilya"
            },
            {
                "last_name": "Chen",
                "first_name": "Long"
            },
            {
                "last_name": "Alptekin",
                "first_name": "Sam"
            },
            {
                "last_name": "Trinh",
                "first_name": "Viet Anh"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhe"
            },
            {
                "last_name": "Vaz",
                "first_name": "Colin"
            },
            {
                "last_name": "Ravichandran",
                "first_name": "Venkatesh"
            },
            {
                "last_name": "Maas",
                "first_name": "Roland"
            },
            {
                "last_name": "Rastrow",
                "first_name": "Ariya"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Endpoint (EP) detection is a key component of far-field speech recognition systems that assist the user through voice commands. The endpoint detector has to trade-off between accuracy and latency, since waiting longer reduces the cases of users being cut-off early. We propose a novel two-pass solution for endpointing, where the utterance endpoint detected from a first pass endpointer is verified by a 2nd-pass model termed EP Arbitrator. Our method improves the trade-off between early cut-offs and latency over a baseline endpointer, as tested on datasets including voice-assistant transactional queries, conversational speech, and the public SLURP corpus. We demonstrate that our method shows improvements regardless of the first-pass EP model used. ",
        "title": "Two-pass Endpoint Detection for Speech Recognition",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.09239",
        "abstract_url": "http://arxiv.org/abs/2401.09239",
        "authors": [
            {
                "last_name": "Reyzabal",
                "first_name": "Mikel De Iturrate"
            },
            {
                "last_name": "Chen",
                "first_name": "Mingcong"
            },
            {
                "last_name": "Huang",
                "first_name": "Wei"
            },
            {
                "last_name": "Ourselin",
                "first_name": "Sebastien"
            },
            {
                "last_name": "Liu",
                "first_name": "Hongbin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Precisely determining the contact force during safe interaction in Minimally Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired by post-operative qualitative analysis from surgical videos, the use of cross-modality data driven deep neural network models has been one of the newest approaches to predict sensorless force trends. However, these methods required for large and variable datasets which are not currently available. In this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft environments for the training of deep neural models. In order to reduce the bias from a single dataset, we present a pipeline to generalize different vision and state data inputs for mixed dataset training, using a previously validated dataset with different setup. Finally, we present a variable encoder-decoder architecture to predict the forces done by the laparoscopic tool using single input or sequence of inputs. For input sequence, we use a recurrent decoder, named with the prefix R, and a new temporal sampling to represent the acceleration of the tool. During our training, we demonstrate that single dataset training tends to overfit to the training data domain, but has difficulties on translating the results across new domains. However, dataset mixing presents a good translation with a mean relative estimated force error of 5% and 12% for the recurrent and non-recurrent models respectively. Our method, also marginally increase the effectiveness of transformers for force estimation up to a maximum of ~15%, as the volume of available data is increase by 150%. In conclusion, we demonstrate that mixing experimental set ups for vision-state force estimation in MIRS is a possible approach towards the general solution of the problem. ",
        "title": "DaFoEs: Mixing Datasets towards the generalization of vision-state  deep-learning Force Estimation in Minimally Invasive Robotic Surgery",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.10963",
        "abstract_url": "http://arxiv.org/abs/2401.10963",
        "authors": [
            {
                "last_name": "de Campos",
                "first_name": "Luis M."
            },
            {
                "last_name": "Fern\u00e1ndez-Luna",
                "first_name": "Juan M."
            },
            {
                "last_name": "Huete",
                "first_name": "Juan F."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In this paper, we examine the problem of building a user profile from a set of documents. This profile will consist of a subset of the most representative terms in the documents that best represent user preferences or interests. Inspired by the discrete concentration theory we have conducted an axiomatic study of seven properties that a selection function should fulfill: the minimum and maximum uncertainty principle, invariant to adding zeros, invariant to scale transformations, principle of nominal increase, transfer principle and the richest get richer inequality. We also present a novel selection function based on the use of similarity metrics, and more specifically the cosine measure which is commonly used in information retrieval, and demonstrate that this verifies six of the properties in addition to a weaker variant of the transfer principle, thereby representing a good selection approach. The theoretical study was complemented with an empirical study to compare the performance of different selection criteria (weight- and unweight-based) using real data in a parliamentary setting. In this study, we analyze the performance of the different functions focusing on the two main factors affecting the selection process: profile size (number of terms) and weight distribution. These profiles are then used in a document filtering task to show that our similarity-based approach performs well in terms not only of recommendation accuracy but also efficiency (we obtain smaller profiles and consequently faster recommendations). ",
        "title": "On the selection of the correct number of terms for profile  construction: theoretical and empirical analysis",
        "date": "2024-01-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.11838",
        "abstract_url": "http://arxiv.org/abs/2401.11838",
        "authors": [
            {
                "last_name": "Nwankwo",
                "first_name": "Linus"
            },
            {
                "last_name": "Rueckert",
                "first_name": "Elmar"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  In recent years, autonomous agents have surged in real-world environments such as our homes, offices, and public spaces. However, natural human-robot interaction remains a key challenge. In this paper, we introduce an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue. We leveraged the LLMs to decode the high-level natural language instructions from humans and abstract them into precise robot actionable commands or queries. Further, we utilised the VLMs to provide a visual and semantic understanding of the robot's task environment. Our results with 99.13% command recognition accuracy and 97.96% commands execution success show that our approach can enhance human-robot interaction in real-world applications. The video demonstrations of this paper can be found at https://osf.io/wzyf6 and the code is available at our GitHub repository (https://github.com/LinusNEP/TCC_IRoNL.git). ",
        "title": "The Conversation is the Command: Interacting with Real-World Autonomous  Robot Through Natural Language",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.11898",
        "abstract_url": "http://arxiv.org/abs/2401.11898",
        "authors": [
            {
                "last_name": "Gonzalez",
                "first_name": "Salwa Tabet"
            },
            {
                "last_name": "Jani\u010di\u0107",
                "first_name": "Predrag"
            },
            {
                "last_name": "Narboux",
                "first_name": "Julien"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Conjecturing and theorem proving are activities at the center of mathematical practice and are difficult to separate. In this paper, we propose a framework for completing incomplete conjectures and incomplete proofs. The framework can turn a conjecture with missing assumptions and with an under-specified goal into a proper theorem. Also, the proposed framework can help in completing a proof sketch into a human-readable and machine-checkable proof. Our approach is focused on synthetic geometry, and uses coherent logic and constraint solving. The proposed approach is uniform for all three kinds of tasks, flexible and, to our knowledge, unique such approach. ",
        "title": "Automated Completion of Statements and Proofs in Synthetic Geometry: an  Approach based on Constraint Solving",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.11900",
        "abstract_url": "http://arxiv.org/abs/2401.11900",
        "authors": [
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Zolt\u00e1n"
            },
            {
                "last_name": "Recio",
                "first_name": "Tom\u00e1s"
            },
            {
                "last_name": "V\u00e9lez",
                "first_name": "M. Pilar"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC",
            "CG"
        ],
        "abstract": "  In our contribution we describe some on-going improvements concerning the Automated Reasoning Tools developed in GeoGebra Discovery, providing different examples of the performance of these new features. We describe the new ShowProof command, that outputs both the sequence of the different steps performed by GeoGebra Discovery to confirm a certain statement, as well as a number intending to grade the difficulty or interest of the assertion. The proposal of this assessment measure, involving the comparison of the expression of the thesis (or conclusion) as a combination of the hypotheses, will be developed. ",
        "title": "Showing Proofs, Assessing Difficulty with GeoGebra Discovery",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.11906",
        "abstract_url": "http://arxiv.org/abs/2401.11906",
        "authors": [
            {
                "last_name": "Ari\u00f1o-Morera",
                "first_name": "Bel\u00e9n"
            },
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Zolt\u00e1n"
            },
            {
                "last_name": "Recio",
                "first_name": "Tom\u00e1s"
            },
            {
                "last_name": "Tolmos",
                "first_name": "Piedad"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC",
            "CG"
        ],
        "abstract": "  We address, through the automated reasoning tools in GeoGebra Discovery, a problem from a regional phase of the Austrian Mathematics Olympiad 2023. Trying to solve this problem gives rise to four different kind of feedback: the almost instantaneous, automated solution of the proposed problem; the measure of its complexity, according to some recent proposals; the automated discovery of a generalization of the given assertion, showing that the same statement is true over more general polygons than those mentioned in the problem; and the difficulties associated to the analysis of the surprising and involved high number of degenerate cases that appear when using the LocusEquation command in this problem. In our communication we will describe and reflect on these diverse issues, enhancing its exemplar role for showing some of the advantages, problems, and current fields of development of GeoGebra Discovery. ",
        "title": "Solving with GeoGebra Discovery an Austrian Mathematics Olympiad  problem: Lessons Learned",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.11908",
        "abstract_url": "http://arxiv.org/abs/2401.11908",
        "authors": [
            {
                "last_name": "K\u00e4ferb\u00f6ck",
                "first_name": "Anna"
            },
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Zolt\u00e1n"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CG",
            "SC"
        ],
        "abstract": "  We give an example of automated geometry reasoning for an imaginary classroom project by using the free software package GeoGebra Discovery. The project is motivated by a publicly available toy, a rocking camel, installed at a medical center in Upper Austria. We explain how the process of a false conjecture, experimenting, modeling, a precise mathematical setup, and then a proof by automated reasoning could help extend mathematical knowledge at secondary school level and above. ",
        "title": "The Locus Story of a Rocking Camel in a Medical Center in the City of  Freistadt",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.11909",
        "abstract_url": "http://arxiv.org/abs/2401.11909",
        "authors": [
            {
                "last_name": "Dana-Picard",
                "first_name": "Thierry"
            },
            {
                "last_name": "Tejera",
                "first_name": "Matias"
            },
            {
                "last_name": "Ulbrich",
                "first_name": "Eva"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            "SC"
        ],
        "abstract": "  We present simple models of trajectories in space, both in 2D and in 3D. The first examples, which model bicircular moves in the same direction, are classical curves (epicycloids, etc.). Then, we explore bicircular moves in reverse direction and tricircular moves in 2D and 3D, to explore complex visualisations of extraplanetary movements. These moves are studied in a plane setting. Then, adding increasing complexity, we explore them in a non planar setting (which is a closer model of the real situation). The exploration is followed by using these approaches for creating mathematical art in 2D and 3D printed objects, providing new ways of mathematical representations. Students' activities are organized around this exploration. ",
        "title": "3D Space Trajectories and beyond: Abstract Art Creation with 3D Printing",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12023",
        "abstract_url": "http://arxiv.org/abs/2401.12023",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Neil"
            },
            {
                "last_name": "Brockner",
                "first_name": "Emilee"
            },
            {
                "last_name": "Winslow",
                "first_name": "Asia"
            },
            {
                "last_name": "Seraydarian",
                "first_name": "Megan"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  The classic question of whether one should walk or run in the rain to remain the least wet has inspired a myriad of solutions ranging from physically performing test runs in raining conditions to mathematically modeling human movement through rain. This manuscript approaches the classical problem by simulating movement through rainfall using MATLAB. Our simulation was generalizable to include snowfall as well. An increase in walking speed resulted in a corresponding decrease in raindrop and snowflake collisions. When raindrops or snowflakes were given a horizontal movement vector due to wind, a local minimum in collisions was achieved when moving in parallel with the same horizontal speed as the raindrop; no local minimum was detected with antiparallel movement. In general, our simulation revealed that the faster one moves, the drier one remains. ",
        "title": "A Simulation of Optimal Dryness When Moving in the Rain or Snow Using  MATLAB",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12255",
        "abstract_url": "http://arxiv.org/abs/2401.12255",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Jiashu"
            },
            {
                "last_name": "Wang",
                "first_name": "Fei"
            },
            {
                "last_name": "Ma",
                "first_name": "Mingyu Derek"
            },
            {
                "last_name": "Koh",
                "first_name": "Pang Wei"
            },
            {
                "last_name": "Xiao",
                "first_name": "Chaowei"
            },
            {
                "last_name": "Chen",
                "first_name": "Muhao"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CL",
            "LG"
        ],
        "abstract": "  The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/. ",
        "title": "Instructional Fingerprinting of Large Language Models",
        "date": "2024-01-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12509",
        "abstract_url": "http://arxiv.org/abs/2401.12509",
        "authors": [
            {
                "last_name": "Puri",
                "first_name": "Prateek"
            },
            {
                "last_name": "Hassler",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Shenk",
                "first_name": "Anton"
            },
            {
                "last_name": "Katragadda",
                "first_name": "Sai"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  We develop a simulation framework for studying misinformation spread within online social networks that blends agent-based modeling and natural language processing techniques. While many other agent-based simulations exist in this space, questions over their fidelity and generalization to existing networks in part hinders their ability to provide actionable insights. To partially address these concerns, we create a 'digital clone' of a known misinformation sharing network by downloading social media histories for over ten thousand of its users. We parse these histories to both extract the structure of the network and model the nuanced ways in which information is shared and spread among its members. Unlike many other agent-based methods in this space, information sharing between users in our framework is sensitive to topic of discussion, user preferences, and online community dynamics. To evaluate the fidelity of our method, we seed our cloned network with a set of posts recorded in the base network and compare propagation dynamics between the two, observing reasonable agreement across the twin networks over a variety of metrics. Lastly, we explore how the cloned network may serve as a flexible, low-cost testbed for misinformation countermeasure evaluation and red teaming analysis. We hope the tools explored here augment existing efforts in the space and unlock new opportunities for misinformation countermeasure evaluation, a field that may become increasingly important to consider with the anticipated rise of misinformation campaigns fueled by generative artificial intelligence. ",
        "title": "Digital cloning of online social networks for language-sensitive  agent-based modeling of misinformation spread",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12592",
        "abstract_url": "http://arxiv.org/abs/2401.12592",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Hongchi"
            },
            {
                "last_name": "Fu",
                "first_name": "Yang"
            },
            {
                "last_name": "Liu",
                "first_name": "Sifei"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaolong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a new RGB-D object dataset captured in the wild called WildRGB-D. Unlike most existing real-world object-centric datasets which only come with RGB capturing, the direct capture of the depth channel allows better 3D annotations and broader downstream applications. WildRGB-D comprises large-scale category-level RGB-D object videos, which are taken using an iPhone to go around the objects in 360 degrees. It contains around 8500 recorded objects and nearly 20000 RGB-D videos across 46 common object categories. These videos are taken with diverse cluttered backgrounds with three setups to cover as many real-world scenarios as possible: (i) a single object in one video; (ii) multiple objects in one video; and (iii) an object with a static hand in one video. The dataset is annotated with object masks, real-world scale camera poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark four tasks with WildRGB-D including novel view synthesis, camera pose estimation, object 6d pose estimation, and object surface reconstruction. Our experiments show that the large-scale capture of RGB-D objects provides a large potential to advance 3D object learning. Our project page is https://wildrgbd.github.io/. ",
        "title": "RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from  RGB-D Videos",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12722",
        "abstract_url": "http://arxiv.org/abs/2401.12722",
        "authors": [
            {
                "last_name": "Tae",
                "first_name": "Ki Hyun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hantian"
            },
            {
                "last_name": "Park",
                "first_name": "Jaeyoung"
            },
            {
                "last_name": "Rong",
                "first_name": "Kexin"
            },
            {
                "last_name": "Whang",
                "first_name": "Steven Euijong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from \"target groups\" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood of postponing due to undesired label prediction, and the optimal balance varies per dataset. We capture the trade-off between informativeness and postpone rate as policies and propose to automatically select the best policy using adversarial multi-armed bandit methods, given their computational efficiency and theoretical guarantees. Experiments show that Falcon significantly outperforms existing fair active learning approaches in terms of fairness and accuracy and is more efficient. In particular, only Falcon supports a proper trade-off between accuracy and fairness where its maximum fairness score is 1.8-4.5x higher than the second-best results. ",
        "title": "Falcon: Fair Active Learning using Multi-armed Bandits",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12764",
        "abstract_url": "http://arxiv.org/abs/2401.12764",
        "authors": [
            {
                "last_name": "Doan",
                "first_name": "Thinh T."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed. Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples. The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution. Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $O(1/k)$, where $k$ is the number of iterations. Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $O(1/k^{2/3})$. ",
        "title": "Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving  $O(1/k)$ Finite-Sample Complexity",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12843",
        "abstract_url": "http://arxiv.org/abs/2401.12843",
        "authors": [
            {
                "last_name": "Dall'Amico",
                "first_name": "Lorenzo"
            },
            {
                "last_name": "Barrat",
                "first_name": "Alain"
            },
            {
                "last_name": "Cattuto",
                "first_name": "Ciro"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "LG"
        ],
        "abstract": "  We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks. We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes. We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties. Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs. ",
        "title": "An embedding-based distance for temporal graphs",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12980",
        "abstract_url": "http://arxiv.org/abs/2401.12980",
        "authors": [
            {
                "last_name": "Lima",
                "first_name": "Vinicius"
            },
            {
                "last_name": "de Oliveira",
                "first_name": "Jaque Almeida"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Femicide refers to the killing of a female victim, often perpetrated by an intimate partner or family member, and is also associated with gender-based violence. Studies have shown that there is a pattern of escalating violence leading up to these killings, highlighting the potential for prevention if the level of danger to the victim can be assessed. Machine learning offers a promising approach to address this challenge by predicting risk levels based on textual descriptions of the violence. In this study, we employed the Long Short Term Memory (LSTM) technique to identify patterns of behavior in Brazilian police reports preceding femicides. Our first objective was to classify the content of these reports as indicating either a lower or higher risk of the victim being murdered, achieving an accuracy of 66%. In the second approach, we developed a model to predict the next action a victim might experience within a sequence of patterned events. Both approaches contribute to the understanding and assessment of the risks associated with domestic violence, providing authorities with valuable insights to protect women and prevent situations from escalating. ",
        "title": "Identifying Risk Patterns in Brazilian Police Reports Preceding  Femicides: A Long Short Term Memory (LSTM) Based Analysis",
        "date": "2024-01-04",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12981",
        "abstract_url": "http://arxiv.org/abs/2401.12981",
        "authors": [
            {
                "last_name": "Yan",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Alterovitz",
                "first_name": "Gil"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recent advancements in machine learning and natural language processing have led to the rapid development of artificial intelligence (AI) as a valuable tool in the healthcare industry. Using large language models (LLMs) as conversational agents or chatbots has the potential to assist doctors in diagnosing patients, detecting early symptoms of diseases, and providing health advice to patients. This paper focuses on the role of chatbots in healthcare and explores the use of avatars to make AI interactions more appealing to patients. A framework of a general-purpose AI avatar application is demonstrated by using a three-category prompt dictionary and prompt improvement mechanism. A two-phase approach is suggested to fine-tune a general-purpose AI language model and create different AI avatars to discuss medical issues with users. Prompt engineering enhances the chatbot's conversational abilities and personality traits, fostering a more human-like interaction with patients. Ultimately, the injection of personality into the chatbot could potentially increase patient engagement. Future directions for research include investigating ways to improve chatbots' understanding of context and ensuring the accuracy of their outputs through fine-tuning with specialized medical data sets. ",
        "title": "A General-purpose AI Avatar in Healthcare",
        "date": "2024-01-09",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12982",
        "abstract_url": "http://arxiv.org/abs/2401.12982",
        "authors": [
            {
                "last_name": "Taha",
                "first_name": "Kamal"
            },
            {
                "last_name": "Yoo",
                "first_name": "Paul D."
            },
            {
                "last_name": "Yeun",
                "first_name": "Chan"
            },
            {
                "last_name": "Taha",
                "first_name": "Aya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The explosive and widespread growth of data necessitates the use of text classification to extract crucial information from vast amounts of data. Consequently, there has been a surge of research in both classical and deep learning text classification methods. Despite the numerous methods proposed in the literature, there is still a pressing need for a comprehensive and up-to-date survey. Existing survey papers categorize algorithms for text classification into broad classes, which can lead to the misclassification of unrelated algorithms and incorrect assessments of their qualities and behaviors using the same metrics. To address these limitations, our paper introduces a novel methodological taxonomy that classifies algorithms hierarchically into fine-grained classes and specific techniques. The taxonomy includes methodology categories, methodology techniques, and methodology sub-techniques. Our study is the first survey to utilize this methodological taxonomy for classifying algorithms for text classification. Furthermore, our study also conducts empirical evaluation and experimental comparisons and rankings of different algorithms that employ the same specific sub-technique, different sub-techniques within the same technique, different techniques within the same category, and categories ",
        "title": "Text Classification: A Review, Empirical, and Experimental Evaluation",
        "date": "2024-01-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12983",
        "abstract_url": "http://arxiv.org/abs/2401.12983",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Jie"
            },
            {
                "last_name": "Hou",
                "first_name": "Jixin"
            },
            {
                "last_name": "Wu",
                "first_name": "Zihao"
            },
            {
                "last_name": "Shu",
                "first_name": "Peng"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengliang"
            },
            {
                "last_name": "Xiang",
                "first_name": "Yujie"
            },
            {
                "last_name": "Gu",
                "first_name": "Beikang"
            },
            {
                "last_name": "Filla",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Li",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Liu",
                "first_name": "Ning"
            },
            {
                "last_name": "Chen",
                "first_name": "Xianyan"
            },
            {
                "last_name": "Tang",
                "first_name": "Keke"
            },
            {
                "last_name": "Liu",
                "first_name": "Tianming"
            },
            {
                "last_name": "Wang",
                "first_name": "Xianqiao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This study is a pioneering endeavor to investigate the capabilities of Large Language Models (LLMs) in addressing conceptual questions within the domain of mechanical engineering with a focus on mechanics. Our examination involves a manually crafted exam encompassing 126 multiple-choice questions, spanning various aspects of mechanics courses, including Fluid Mechanics, Mechanical Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5), ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against engineering faculties and students with or without mechanical engineering background. The findings reveal GPT-4's superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses. The performances of LLMs were all significantly improved with explanations prompted prior to direct responses, underscoring the crucial role of prompt engineering. Interestingly, GPT-3.5 demonstrates improved performance with prompts covering a broader domain, while GPT-4 excels with prompts focusing on specific subjects. Finally, GPT-4 exhibits notable advancements in mitigating input bias, as evidenced by guessing preferences for humans. This study unveils the substantial potential of LLMs as highly knowledgeable assistants in both mechanical pedagogy and scientific research. ",
        "title": "Assessing Large Language Models in Mechanical Engineering Education: A  Study on Mechanics-Focused Conceptual Understanding",
        "date": "2024-01-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12985",
        "abstract_url": "http://arxiv.org/abs/2401.12985",
        "authors": [
            {
                "last_name": "Lakkaraju",
                "first_name": "Kausik"
            },
            {
                "last_name": "Gupta",
                "first_name": "Aniket"
            },
            {
                "last_name": "Srivastava",
                "first_name": "Biplav"
            },
            {
                "last_name": "Valtorta",
                "first_name": "Marco"
            },
            {
                "last_name": "Wu",
                "first_name": "Dezhi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence (AI) systems that output polarity and emotional intensity when given a piece of text as input. Like other AIs, SASs are also known to have unstable behavior when subjected to changes in data which can make it problematic to trust out of concerns like bias when AI works with humans and data has protected attributes like gender, race, and age. Recently, an approach was introduced to assess SASs in a blackbox setting without training data or code, and rating them for bias using synthetic English data. We augment it by introducing two human-generated chatbot datasets and also consider a round-trip setting of translating the data from one language to the same through an intermediate language. We find that these settings show SASs performance in a more realistic light. Specifically, we find that rating SASs on the chatbot data showed more bias compared to the synthetic data, and round-tripping using Spanish and Danish as intermediate languages reduces the bias (up to 68% reduction) in human-generated data while, in synthetic data, it takes a surprising turn by increasing the bias! Our findings will help researchers and practitioners refine their SAS testing strategies and foster trust as SASs are considered part of more mission-critical applications for global use. ",
        "title": "The Effect of Human v/s Synthetic Test Data and Round-tripping on  Assessment of Sentiment Analysis Systems for Bias",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12986",
        "abstract_url": "http://arxiv.org/abs/2401.12986",
        "authors": [
            {
                "last_name": "Velez",
                "first_name": "Yamil"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "HC"
        ],
        "abstract": "  Public opinion surveys are vital for informing democratic decision-making, but responding to rapidly changing information environments and measuring beliefs within niche communities can be challenging for traditional survey methods. This paper introduces a crowdsourced adaptive survey methodology (CSAS) that unites advances in natural language processing and adaptive algorithms to generate question banks that evolve with user input. The CSAS method converts open-ended text provided by participants into Likert-style items and applies a multi-armed bandit algorithm to determine user-provided questions that should be prioritized in the survey. The method's adaptive nature allows for the exploration of new survey questions, while imposing minimal costs in survey length. Applications in the domains of Latino information environments and issue importance showcase CSAS's ability to identify claims or issues that might otherwise be difficult to track using standard approaches. I conclude by discussing the method's potential for studying topics where participant-generated content might improve our understanding of public opinion. ",
        "title": "Crowdsourced Adaptive Surveys",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12987",
        "abstract_url": "http://arxiv.org/abs/2401.12987",
        "authors": [
            {
                "last_name": "Yun",
                "first_name": "Taeyang"
            },
            {
                "last_name": "Lim",
                "first_name": "Hyunkuk"
            },
            {
                "last_name": "Lee",
                "first_name": "Jeonghwan"
            },
            {
                "last_name": "Song",
                "first_name": "Min"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SD"
        ],
        "abstract": "  Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional experiments. ",
        "title": "TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition  in Conversation",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12988",
        "abstract_url": "http://arxiv.org/abs/2401.12988",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Haoxin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenli"
            },
            {
                "last_name": "Xie",
                "first_name": "Jiaheng"
            },
            {
                "last_name": "Kim",
                "first_name": "Buomsoo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhu"
            },
            {
                "last_name": "Chai",
                "first_name": "Yidong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This study harnesses state-of-the-art AI technology for chronic disease management, specifically in detecting various mental disorders through user-generated textual content. Existing studies typically rely on fully supervised machine learning, which presents challenges such as the labor-intensive manual process of annotating extensive training data for each disease and the need to design specialized deep learning architectures for each problem. To address such challenges, we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering. Specifically, we address two key technical challenges in data-driven chronic disease management: (1) developing personalized prompts to represent each user's uniqueness and (2) incorporating medical knowledge into prompts to provide context for chronic disease detection, instruct learning objectives, and operationalize prediction goals. We evaluate our method using four mental disorders, which are prevalent chronic diseases worldwide, as research cases. On the depression detection task, our method (F1 = 0.975~0.978) significantly outperforms traditional supervised learning paradigms, including feature engineering (F1 = 0.760) and architecture engineering (F1 = 0.756). Meanwhile, our approach demonstrates success in few-shot learning, i.e., requiring only a minimal number of training examples to detect chronic diseases based on user-generated textual content (i.e., only 2, 10, or 100 subjects). Moreover, our method can be generalized to other mental disorder detection tasks, including anorexia, pathological gambling, and self-harm (F1 = 0.919~0.978). ",
        "title": "Few-Shot Learning for Chronic Disease Management: Leveraging Large  Language Models and Multi-Prompt Engineering with Medical Knowledge Injection",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12989",
        "abstract_url": "http://arxiv.org/abs/2401.12989",
        "authors": [
            {
                "last_name": "Belisario",
                "first_name": "Adriano"
            },
            {
                "last_name": "Hale",
                "first_name": "Scott"
            },
            {
                "last_name": "Rocher",
                "first_name": "Luc"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR"
        ],
        "abstract": "  Gun violence is a pressing and growing human rights issue that affects nearly every dimension of the social fabric, from healthcare and education to psychology and the economy. Reliable data on firearm events is paramount to developing more effective public policy and emergency responses. However, the lack of comprehensive databases and the risks of in-person surveys prevent human rights organizations from collecting needed data in most countries. Here, we partner with a Brazilian human rights organization to conduct a systematic evaluation of language models to assist with monitoring real-world firearm events from social media data. We propose a fine-tuned BERT-based model trained on Twitter (now X) texts to distinguish gun violence reports from ordinary Portuguese texts. Our model achieves a high AUC score of 0.97. We then incorporate our model into a web application and test it in a live intervention. We study and interview Brazilian analysts who continuously fact-check social media texts to identify new gun violence events. Qualitative assessments show that our solution helped all analysts use their time more efficiently and expanded their search capacities. Quantitative assessments show that the use of our model was associated with more analysts' interactions with online users reporting gun violence. Taken together, our findings suggest that modern Natural Language Processing techniques can help support the work of human rights organizations. ",
        "title": "Into the crossfire: evaluating the use of a language model to  crowdsource gun violence reports",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12990",
        "abstract_url": "http://arxiv.org/abs/2401.12990",
        "authors": [
            {
                "last_name": "Williams",
                "first_name": "Lowri"
            },
            {
                "last_name": "Anthi",
                "first_name": "Eirini"
            },
            {
                "last_name": "Arman",
                "first_name": "Laura"
            },
            {
                "last_name": "Burnap",
                "first_name": "Pete"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Topic modelling is a text mining technique for identifying salient themes from a number of documents. The output is commonly a set of topics consisting of isolated tokens that often co-occur in such documents. Manual effort is often associated with interpreting a topic's description from such tokens. However, from a human's perspective, such outputs may not adequately provide enough information to infer the meaning of the topics; thus, their interpretability is often inaccurately understood. Although several studies have attempted to automatically extend topic descriptions as a means of enhancing the interpretation of topic models, they rely on external language sources that may become unavailable, must be kept up-to-date to generate relevant results, and present privacy issues when training on or processing data. This paper presents a novel approach towards extending the output of traditional topic modelling methods beyond a list of isolated tokens. This approach removes the dependence on external sources by using the textual data itself by extracting high-scoring keywords and mapping them to the topic model's token outputs. To measure the interpretability of the proposed outputs against those of the traditional topic modelling approach, independent annotators manually scored each output based on their quality and usefulness, as well as the efficiency of the annotation task. The proposed approach demonstrated higher quality and usefulness, as well as higher efficiency in the annotation task, in comparison to the outputs of a traditional topic modelling method, demonstrating an increase in their interpretability. ",
        "title": "Topic Modelling: Going Beyond Token Outputs",
        "date": "2024-01-16",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12991",
        "abstract_url": "http://arxiv.org/abs/2401.12991",
        "authors": [
            {
                "last_name": "Akhukov",
                "first_name": "Mikhail A."
            },
            {
                "last_name": "Es'kin",
                "first_name": "Vasiliy A."
            },
            {
                "last_name": "Smorkalov",
                "first_name": "Mikhail E."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The Mercator projection is sometimes confused with another mapping technique, specifically the central cylindrical projection, which projects the Earth's surface onto a cylinder tangent to the equator, as if a light source is at the Earth's center. Accidentally, this misconception is rather close to a truth. The only operation that the map needs is a free bending in a uniform gravitational field if the map's material is dense and soft enough to produce a catenary profile. The north and south edges of the map should be parallel and placed in the same plane at the appropriate distance. In this case, the bent map been projected onto this plane gives the Mercator projection. This property is rather curious, since it allows to make such a sophisticated one-to-one mapping as the Mercator projection using simple tools available in the workroom. ",
        "title": "Catenary and Mercator projection",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12992",
        "abstract_url": "http://arxiv.org/abs/2401.12992",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Seung-Bin"
            },
            {
                "last_name": "Lee",
                "first_name": "Sang-Hoon"
            },
            {
                "last_name": "Lee",
                "first_name": "Seong-Whan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  Although there has been significant advancement in the field of speech-to-speech translation, conventional models still require language-parallel speech data between the source and target languages for training. In this paper, we introduce TranSentence, a novel speech-to-speech translation without language-parallel speech data. To achieve this, we first adopt a language-agnostic sentence-level speech encoding that captures the semantic information of speech, irrespective of language. We then train our model to generate speech based on the encoded embedding obtained from a language-agnostic sentence-level speech encoder that is pre-trained with various languages. With this method, despite training exclusively on the target language's monolingual data, we can generate target language speech in the inference stage using language-agnostic speech embedding from the source language speech. Furthermore, we extend TranSentence to multilingual speech-to-speech translation. The experimental results demonstrate that TranSentence is superior to other models. ",
        "title": "TranSentence: Speech-to-speech Translation via Language-agnostic  Sentence-level Speech Encoding without Language-parallel Data",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12993",
        "abstract_url": "http://arxiv.org/abs/2401.12993",
        "authors": [
            {
                "last_name": "Mahdavifar",
                "first_name": "Sare"
            },
            {
                "last_name": "Fakhrahmad",
                "first_name": "Seyed Mostafa"
            },
            {
                "last_name": "Ansarifard",
                "first_name": "Elham"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Analyzing authors' sentiments in texts as a technique for identifying text polarity can be practical and useful in various fields, including medicine and dentistry. Currently, due to factors such as patients' limited knowledge about their condition, difficulties in accessing specialist doctors, or fear of illness, particularly in pandemic conditions, there might be a delay between receiving a radiology report and consulting a doctor. In some cases, this delay can pose significant risks to the patient, making timely decision-making crucial. Having an automatic system that can inform patients about the deterioration of their condition by analyzing the text of radiology reports could greatly impact timely decision-making. In this study, a dataset comprising 1,134 cone-beam computed tomography (CBCT) photo reports was collected from the Shiraz University of Medical Sciences. Each case was examined, and an expert labeled a severity level for the patient's condition on each document. After preprocessing all the text data, a deep learning model based on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) network architecture, known as CNN-LSTM, was developed to detect the severity level of the patient's problem based on sentiment analysis in the radiologist's report. The model's performance was evaluated on two datasets, each with two and four classes, in both imbalanced and balanced scenarios. Finally, to demonstrate the effectiveness of our model, we compared its performance with that of other classification models. The results, along with one-way ANOVA and Tukey's test, indicated that our proposed model (CNN-LSTM) performed the best according to precision, recall, and f-measure criteria. This suggests that it can be a reliable model for estimating the severity of oral and dental diseases, thereby assisting patients. ",
        "title": "Estimating the severity of dental and oral problems via sentiment  classification over clinical reports",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12994",
        "abstract_url": "http://arxiv.org/abs/2401.12994",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Jingyu"
            },
            {
                "last_name": "Jiang",
                "first_name": "Yifeng"
            },
            {
                "last_name": "Yuan",
                "first_name": "Bin"
            },
            {
                "last_name": "Li",
                "first_name": "Shulin"
            },
            {
                "last_name": "Song",
                "first_name": "Tianbo"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Clinical patient notes are critical for documenting patient interactions, diagnoses, and treatment plans in medical practice. Ensuring accurate evaluation of these notes is essential for medical education and certification. However, manual evaluation is complex and time-consuming, often resulting in variability and resource-intensive assessments. To tackle these challenges, this research introduces an approach leveraging state-of-the-art Natural Language Processing (NLP) techniques, specifically Masked Language Modeling (MLM) pretraining, and pseudo labeling. Our methodology enhances efficiency and effectiveness, significantly reducing training time without compromising performance. Experimental results showcase improved model performance, indicating a potential transformation in clinical note assessment. ",
        "title": "Automated Scoring of Clinical Patient Notes using Advanced NLP and  Pseudo Labeling",
        "date": "2024-01-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12995",
        "abstract_url": "http://arxiv.org/abs/2401.12995",
        "authors": [
            {
                "last_name": "Kumar",
                "first_name": "Shivani"
            },
            {
                "last_name": "Chakraborty",
                "first_name": "Tanmoy"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Code-mixing, the blending of multiple languages within a single conversation, introduces a distinctive challenge, particularly in the context of response generation. Capturing the intricacies of code-mixing proves to be a formidable task, given the wide-ranging variations influenced by individual speaking styles and cultural backgrounds. In this study, we explore response generation within code-mixed conversations. We introduce a novel approach centered on harnessing the Big Five personality traits acquired in an unsupervised manner from the conversations to bolster the performance of response generation. These inferred personality attributes are seamlessly woven into the fabric of the dialogue context, using a novel fusion mechanism, PA3. It uses an effective two-step attention formulation to fuse the dialogue and personality information. This fusion not only enhances the contextual relevance of generated responses but also elevates the overall performance of the model. Our experimental results, grounded in a dataset comprising of multi-party Hindi-English code-mix conversations, highlight the substantial advantages offered by personality-infused models over their conventional counterparts. This is evident in the increase observed in ROUGE and BLUE scores for the response generation task when the identified personality is seamlessly integrated into the dialogue context. Qualitative assessment for personality identification and response generation aligns well with our quantitative results. ",
        "title": "Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed  Response Generation in Dialogues",
        "date": "2024-01-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12996",
        "abstract_url": "http://arxiv.org/abs/2401.12996",
        "authors": [
            {
                "last_name": "Workman",
                "first_name": "Terri Elizabeth"
            },
            {
                "last_name": "Kupersmith",
                "first_name": "Joel"
            },
            {
                "last_name": "Ma",
                "first_name": "Phillip"
            },
            {
                "last_name": "Spevak",
                "first_name": "Christopher"
            },
            {
                "last_name": "Sandbrink",
                "first_name": "Friedhelm"
            },
            {
                "last_name": "Zeng-Treitler",
                "first_name": "Yan Cheng Qing"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Background: Electronic health records (EHRs) are a data source for opioid research. Opioid use disorder is known to be under-coded as a diagnosis, yet problematic opioid use can be documented in clinical notes.   Objectives: Our goals were 1) to identify problematic opioid use from a full range of clinical notes; and 2) to compare the characteristics of patients identified as having problematic opioid use, exclusively documented in clinical notes, to those having documented ICD opioid use disorder diagnostic codes.   Materials and Methods: We developed and applied a natural language processing (NLP) tool to the clinical notes of a patient cohort (n=222,371) from two Veteran Affairs service regions to identify patients with problematic opioid use. We also used a set of ICD diagnostic codes to identify patients with opioid use disorder from the same cohort. We compared the demographic and clinical characteristics of patients identified only through NLP, to those of patients identified through ICD codes.   Results: NLP exclusively identified 57,331 patients; 6,997 patients had positive ICD code identifications. Patients exclusively identified through NLP were more likely to be women. Those identified through ICD codes were more likely to be male, younger, have concurrent benzodiazepine prescriptions, more comorbidities, more care encounters, and less likely to be married. Patients in the NLP and ICD groups had substantially elevated comorbidity levels compared to patients not documented as experiencing problematic opioid use.   Conclusions: NLP is a feasible approach for identifying problematic opioid use not otherwise recorded by ICD codes. Clinicians may be reluctant to code for opioid use disorder. It is therefore incumbent on the healthcare team to search for documentation of opioid concerns within clinical notes. ",
        "title": "A Comparison of Veterans with Problematic Opioid Use Identified through  Natural Language Processing of Clinical Notes versus Using Diagnostic Codes",
        "date": "2024-01-18",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12997",
        "abstract_url": "http://arxiv.org/abs/2401.12997",
        "authors": [
            {
                "last_name": "Fan",
                "first_name": "Cunhang"
            },
            {
                "last_name": "Chen",
                "first_name": "Yujie"
            },
            {
                "last_name": "Xue",
                "first_name": "Jun"
            },
            {
                "last_name": "Kong",
                "first_name": "Yonghui"
            },
            {
                "last_name": "Tao",
                "first_name": "Jianhua"
            },
            {
                "last_name": "Lv",
                "first_name": "Zhao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive distillation method to distill student models at each grade level, enabling efficient knowledge transfer from teachers to students. The experimental results demonstrate that the model in the pre-distillation stage surpasses the existing state-of-the-art methods. Furthermore, in the progressive distillation stage, the model significantly reduces the model parameters while maintaining a certain level of performance. Specifically, the model parameters of the lower-grade student model are reduced by 56.7\\% compared to the baseline. ",
        "title": "Progressive Distillation Based on Masked Generation Feature Method for  Knowledge Graph Completion",
        "date": "2024-01-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12998",
        "abstract_url": "http://arxiv.org/abs/2401.12998",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xi"
            },
            {
                "last_name": "You",
                "first_name": "MingKe"
            },
            {
                "last_name": "Wang",
                "first_name": "Li"
            },
            {
                "last_name": "Liu",
                "first_name": "WeiZhi"
            },
            {
                "last_name": "Fu",
                "first_name": "Yu"
            },
            {
                "last_name": "Xu",
                "first_name": "Jie"
            },
            {
                "last_name": "Zhang",
                "first_name": "Shaoting"
            },
            {
                "last_name": "Chen",
                "first_name": "Gang"
            },
            {
                "last_name": "Li",
                "first_name": "Kang"
            },
            {
                "last_name": "Li",
                "first_name": "Jian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. This study focused on evaluating and enhancing the clinical capabilities of LLMs in specific domains, using osteoarthritis (OA) management as a case study. A domain specific benchmark framework was developed, which evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM tailored for OA management that integrates retrieval-augmented generation (RAG) and instruction prompts, was developed. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed significant improvements. This study introduces a novel benchmark framework which assesses the domain-specific abilities of LLMs in multiple aspects, highlights the limitations of generalized LLMs in clinical contexts, and demonstrates the potential of tailored approaches for developing domain-specific medical LLMs. ",
        "title": "Evaluating and Enhancing Large Language Models Performance in  Domain-specific Medicine: Osteoarthritis Management with DocOA",
        "date": "2024-01-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.12999",
        "abstract_url": "http://arxiv.org/abs/2401.12999",
        "authors": [
            {
                "last_name": "Shu",
                "first_name": "Runqiu"
            },
            {
                "last_name": "Liu",
                "first_name": "Bowen"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zhaoping"
            },
            {
                "last_name": "Cui",
                "first_name": "Xiaopeng"
            },
            {
                "last_name": "Li",
                "first_name": "Yunting"
            },
            {
                "last_name": "Cui",
                "first_name": "Wei"
            },
            {
                "last_name": "Yung",
                "first_name": "Man-Hong"
            },
            {
                "last_name": "Qiao",
                "first_name": "Nan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD<2) achieves an improvement from 33\\% to 35\\% in our same setup. In particular, a 6\\% improvement is realized in the high-precision region(RMSD<1) on molecules data unseen in DiffDock, which demonstrates the well-generalized of our method. ",
        "title": "Quantum-Inspired Machine Learning for Molecular Docking",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13001",
        "abstract_url": "http://arxiv.org/abs/2401.13001",
        "authors": [
            {
                "last_name": "Wieluch",
                "first_name": "Sabine"
            },
            {
                "last_name": "Schwenker",
                "first_name": "Friedhelm"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "LG"
        ],
        "abstract": "  This paper introduces a process for generating abstract portrait drawings from pictures. Their unique style is created by utilizing single freehand pattern sketches as references to generate unique patterns for shading. The method involves extracting facial and body features from images and transforming them into vector lines. A key aspect of the research is the development of a graph neural network architecture designed to learn sketch stroke representations in vector form, enabling the generation of diverse stroke variations. The combination of these two approaches creates joyful abstract drawings that are realized via a pen plotter. The presented process garnered positive feedback from an audience of approximately 280 participants. ",
        "title": "PatternPortrait: Draw Me Like One of Your Scribbles",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13002",
        "abstract_url": "http://arxiv.org/abs/2401.13002",
        "authors": [
            {
                "last_name": "Todd",
                "first_name": "Philip"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG"
        ],
        "abstract": "  We examine a class of geometric theorems on cyclic 2n-gons. We prove that if we take n disjoint pairs of sides, each pair separated by an even number of polygon sides, then there is a linear combination of the angles between those sides which is constant. We present a formula for the linear combination, which provides a theorem statement in terms of those angles. We describe a program which uses this result to generate new geometry proof problems and their solutions. ",
        "title": "Theorem Discovery Amongst Cyclic Polygons",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13004",
        "abstract_url": "http://arxiv.org/abs/2401.13004",
        "authors": [
            {
                "last_name": "Suppakitpaisarn",
                "first_name": "Vorapong"
            },
            {
                "last_name": "Hao",
                "first_name": "Jin-Kao"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We suggest employing graph sparsification as a pre-processing step for maxcut programs using the QUBO solver. Quantum(-inspired) algorithms are recognized for their potential efficiency in handling quadratic unconstrained binary optimization (QUBO). Given that maxcut is an NP-hard problem and can be readily expressed using QUBO, it stands out as an exemplary case to demonstrate the effectiveness of quantum(-inspired) QUBO approaches. Here, the non-zero count in the QUBO matrix corresponds to the graph's edge count. Given that many quantum(-inspired) solvers operate through cloud services, transmitting data for dense graphs can be costly. By introducing the graph sparsification method, we aim to mitigate these communication costs. Experimental results on classical, quantum-inspired, and quantum solvers indicate that this approach substantially reduces communication overheads and yields an objective value close to the optimal solution. ",
        "title": "Utilizing Graph Sparsification for Pre-processing in Maxcut QUBO Solver",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13006",
        "abstract_url": "http://arxiv.org/abs/2401.13006",
        "authors": [
            {
                "last_name": "Gudavalli",
                "first_name": "Chandrakanth"
            },
            {
                "last_name": "Rosten",
                "first_name": "Erik"
            },
            {
                "last_name": "Nataraj",
                "first_name": "Lakshmanan"
            },
            {
                "last_name": "Chandrasekaran",
                "first_name": "Shivkumar"
            },
            {
                "last_name": "Manjunath",
                "first_name": "B. S."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Content creation and image editing can benefit from flexible user controls. A common intermediate representation for conditional image generation is a semantic map, that has information of objects present in the image. When compared to raw RGB pixels, the modification of semantic map is much easier. One can take a semantic map and easily modify the map to selectively insert, remove, or replace objects in the map. The method proposed in this paper takes in the modified semantic map and alter the original image in accordance to the modified map. The method leverages traditional pre-trained image-to-image translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a limited dataset of reference images associated with the semantic maps. We discuss the qualitative and quantitative performance of our technique to illustrate its capacity and possible applications in the fields of image forgery and image editing. We also demonstrate the effectiveness of the proposed image forgery technique in thwarting the numerous deep learning-based image forensic techniques, highlighting the urgent need to develop robust and generalizable image forensic tools in the fight against the spread of fake media. ",
        "title": "CIMGEN: Controlled Image Manipulation by Finetuning Pretrained  Generative Models on Limited Data",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13009",
        "abstract_url": "http://arxiv.org/abs/2401.13009",
        "authors": [
            {
                "last_name": "Lorbeer",
                "first_name": "Boris"
            },
            {
                "last_name": "Mohsen",
                "first_name": "Mustafa"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Nowadays, the need for causal discovery is ubiquitous. A better understanding of not just the stochastic dependencies between parts of a system, but also the actual cause-effect relations, is essential for all parts of science. Thus, the need for reliable methods to detect causal directions is growing constantly. In the last 50 years, many causal discovery algorithms have emerged, but most of them are applicable only under the assumption that the systems have no feedback loops and that they are causally sufficient, i.e. that there are no unmeasured subsystems that can affect multiple measured variables. This is unfortunate since those restrictions can often not be presumed in practice. Feedback is an integral feature of many processes, and real-world systems are rarely completely isolated and fully measured. Fortunately, in recent years, several techniques, that can cope with cyclic, causally insufficient systems, have been developed. And with multiple methods available, a practical application of those algorithms now requires knowledge of the respective strengths and weaknesses. Here, we focus on the problem of causal discovery for sparse linear models which are allowed to have cycles and hidden confounders. We have prepared a comprehensive and thorough comparative study of four causal discovery techniques: two versions of the LLC method [10] and two variants of the ASP-based algorithm [11]. The evaluation investigates the performance of those techniques for various experiments with multiple interventional setups and different dataset sizes. ",
        "title": "Comparative Study of Causal Discovery Methods for Cyclic Models with  Hidden Confounders",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13011",
        "abstract_url": "http://arxiv.org/abs/2401.13011",
        "authors": [
            {
                "last_name": "Hang",
                "first_name": "Tiankai"
            },
            {
                "last_name": "Gu",
                "first_name": "Shuyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Dong"
            },
            {
                "last_name": "Geng",
                "first_name": "Xin"
            },
            {
                "last_name": "Guo",
                "first_name": "Baining"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper presents a novel generative model, Collaborative Competitive Agents (CCA), which leverages the capabilities of multiple Large Language Models (LLMs) based agents to execute complex tasks. Drawing inspiration from Generative Adversarial Networks (GANs), the CCA system employs two equal-status generator agents and a discriminator agent. The generators independently process user instructions and generate results, while the discriminator evaluates the outputs, and provides feedback for the generator agents to further reflect and improve the generation results. Unlike the previous generative model, our system can obtain the intermediate steps of generation. This allows each generator agent to learn from other successful executions due to its transparency, enabling a collaborative competition that enhances the quality and robustness of the system's results. The primary focus of this study is image editing, demonstrating the CCA's ability to handle intricate instructions robustly. The paper's main contributions include the introduction of a multi-agent-based generative model with controllable intermediate steps and iterative optimization, a detailed examination of agent relationships, and comprehensive experiments on image editing. Code is available at \\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}. ",
        "title": "CCA: Collaborative Competitive Agents for Image Editing",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13014",
        "abstract_url": "http://arxiv.org/abs/2401.13014",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Qi"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  H{\\infty} control of nonlinear continuous-time system depends on the solution of the Hamilton-Jacobi-Isaacs (HJI) equation, which has been proved impossible to obtain a closed-form solution due to the nonlinearity of HJI equation. In order to solve HJI equation, many iterative algorithms were proposed, and most of the algorithms were essentially Newton method when the fixed-point equation was constructed in a Banach space. Newton method is a local optimization method, it has small convergence region and needs the initial guess to be sufficiently close to the solution. Whereas damped Newton method enhances the robustness with respect to initial condition and has larger convergence region. In this paper, a novel reinforcement learning method which is named {\\alpha}-policy iteration ({\\alpha}-PI) is introduced for solving HJI equation. First, by constructing a damped Newton iteration operator equation, a generalized Bellman equation (GBE) is obtained. The GBE is an extension of bellman equation. And then, by iterating on the GBE, an on-policy {\\alpha}-PI reinforcement learning method without using knowledge regarding to the system internal dynamics is proposed. Third, based on the on-policy {\\alpha}-PI reinforcement learning method, we develop an off-policy {\\alpha}-PI reinforcement learning method without requiring any knowledge of the system dynamics. Finally, the neural-network based adaptive critic implementation schemes of on-policy and off-policy {\\alpha}-PI algorithms are derived respectively, and the batch least-squares method is used for calculating the weight parameters of neural networks. The effectiveness of the off-policy {\\alpha}-PI algorithm is verified through computer simulation. ",
        "title": "A Novel Policy Iteration Algorithm for Nonlinear Continuous-Time  H$\\infty$ Control Problem",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13019",
        "abstract_url": "http://arxiv.org/abs/2401.13019",
        "authors": [
            {
                "last_name": "Casaluce",
                "first_name": "Roberto"
            },
            {
                "last_name": "Burattin",
                "first_name": "Andrea"
            },
            {
                "last_name": "Chiaromonte",
                "first_name": "Francesca"
            },
            {
                "last_name": "Lafuente",
                "first_name": "Alberto Lluch"
            },
            {
                "last_name": "Vandin",
                "first_name": "Andrea"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  We propose a novel methodology for validating software product line (PL) models by integrating Statistical Model Checking (SMC) with Process Mining (PM). Our approach focuses on the feature-oriented language QFLan in the PL engineering domain, allowing modeling of PLs with rich cross-tree and quantitative constraints, as well as aspects of dynamic PLs like staged configurations. This richness leads to models with infinite state-space, requiring simulation-based analysis techniques like SMC. For instance, we illustrate with a running example involving infinite state space. SMC involves generating samples of system dynamics to estimate properties such as event probabilities or expected values. On the other hand, PM uses data-driven techniques on execution logs to identify and reason about the underlying execution process. In this paper, we propose, for the first time, applying PM techniques to SMC simulations' byproducts to enhance the utility of SMC analyses. Typically, when SMC results are unexpected, modelers must determine whether they stem from actual system characteristics or model bugs in a black-box manner. We improve on this by using PM to provide a white-box perspective on the observed system dynamics. Samples from SMC are fed into PM tools, producing a compact graphical representation of observed dynamics. The mined PM model is then transformed into a QFLan model, accessible to PL engineers. Using two well-known PL models, we demonstrate the effectiveness and scalability of our methodology in pinpointing issues and suggesting fixes. Additionally, we show its generality by applying it to the security domain. ",
        "title": "White-box validation of quantitative product lines by statistical model  checking and process mining",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13020",
        "abstract_url": "http://arxiv.org/abs/2401.13020",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yixuan"
            },
            {
                "last_name": "Khairy",
                "first_name": "Sami"
            },
            {
                "last_name": "Vilim",
                "first_name": "Richard B."
            },
            {
                "last_name": "Hu",
                "first_name": "Rui"
            },
            {
                "last_name": "Dave",
                "first_name": "Akshay J."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traditional control theory-based methods require tailored engineering for each system and constant fine-tuning. In power plant control, one often needs to obtain a precise representation of the system dynamics and carefully design the control scheme accordingly. Model-free Reinforcement learning (RL) has emerged as a promising solution for control tasks due to its ability to learn from trial-and-error interactions with the environment. It eliminates the need for explicitly modeling the environment's dynamics, which is potentially inaccurate. However, the direct imposition of state constraints in power plant control raises challenges for standard RL methods. To address this, we propose a chance-constrained RL algorithm based on Proximal Policy Optimization for supervisory control. Our method employs Lagrangian relaxation to convert the constrained optimization problem into an unconstrained objective, where trainable Lagrange multipliers enforce the state constraints. Our approach achieves the smallest distance of violation and violation rate in a load-follow maneuver for an advanced Nuclear Power Plant design. ",
        "title": "A Safe Reinforcement Learning Algorithm for Supervisory Control of Power  Plants",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13034",
        "abstract_url": "http://arxiv.org/abs/2401.13034",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zichen"
            },
            {
                "last_name": "Du",
                "first_name": "Chao"
            },
            {
                "last_name": "Lee",
                "first_name": "Wee Sun"
            },
            {
                "last_name": "Lin",
                "first_name": "Min"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a single pass of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods. ",
        "title": "Locality Sensitive Sparse Encoding for Learning World Models Online",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13044",
        "abstract_url": "http://arxiv.org/abs/2401.13044",
        "authors": [
            {
                "last_name": "Bhagat",
                "first_name": "Subhash"
            },
            {
                "last_name": "Pelc",
                "first_name": "Andrzej"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  We consider the fundamental task of network exploration. A network is modeled as a simple connected undirected n-node graph with unlabeled nodes, and all ports at any node of degree d are arbitrarily numbered 0,.....,d-1. Each of two identical mobile agents, initially situated at distinct nodes, has to visit all nodes and stop. Agents execute the same deterministic algorithm and move in synchronous rounds: in each round, an agent can either remain at the same node or move to an adjacent node. Exploration must be collision-free: in every round at most one agent can be at any node. We assume that agents have vision of radius 2: an awake agent situated at a node v can see the subgraph induced by all nodes at a distance at most 2 from v, sees all port numbers in this subgraph, and the agents located at these nodes. Agents do not know the entire graph but they know an upper bound n on its size. The time of an exploration is the number of rounds since the wakeup of the later agent to the termination by both agents. We show a collision-free exploration algorithm working in time polynomial in n, for arbitrary graphs of size larger than 2. Moreover, we show that if agents have only vision of radius 1, then collision-free exploration is impossible, e.g., in any tree of diameter 2. ",
        "title": "Deterministic Collision-Free Exploration of Unknown Anonymous Graphs",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13045",
        "abstract_url": "http://arxiv.org/abs/2401.13045",
        "authors": [
            {
                "last_name": "Edelstein",
                "first_name": "Rachel"
            },
            {
                "last_name": "Gutterman",
                "first_name": "Sterling"
            },
            {
                "last_name": "Newman",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Van Horn",
                "first_name": "John Darrell"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Over the past decade, the intricacies of sports-related concussions among female athletes have become readily apparent. Traditional clinical methods for diagnosing concussions suffer limitations when applied to female athletes, often failing to capture subtle changes in brain structure and function. Advanced neuroinformatics techniques and machine learning models have become invaluable assets in this endeavor. While these technologies have been extensively employed in understanding concussion in male athletes, there remains a significant gap in our comprehension of their effectiveness for female athletes. With its remarkable data analysis capacity, machine learning offers a promising avenue to bridge this deficit. By harnessing the power of machine learning, researchers can link observed phenotypic neuroimaging data to sex-specific biological mechanisms, unraveling the mysteries of concussions in female athletes. Furthermore, embedding methods within machine learning enable examining brain architecture and its alterations beyond the conventional anatomical reference frame. In turn, allows researchers to gain deeper insights into the dynamics of concussions, treatment responses, and recovery processes. To guarantee that female athletes receive the optimal care they deserve, researchers must employ advanced neuroimaging techniques and sophisticated machine-learning models. These tools enable an in-depth investigation of the underlying mechanisms responsible for concussion symptoms stemming from neuronal dysfunction in female athletes. This paper endeavors to address the crucial issue of sex differences in multimodal neuroimaging experimental design and machine learning approaches within female athlete populations, ultimately ensuring that they receive the tailored care they require when facing the challenges of concussions. ",
        "title": "Assessment of Sports Concussion in Female Athletes: A Role for  Neuroinformatics?",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13049",
        "abstract_url": "http://arxiv.org/abs/2401.13049",
        "authors": [
            {
                "last_name": "Imran",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Krebs",
                "first_name": "Jonathan R"
            },
            {
                "last_name": "Gopu",
                "first_name": "Veera Rajasekhar Reddy"
            },
            {
                "last_name": "Fazzone",
                "first_name": "Brian"
            },
            {
                "last_name": "Sivaraman",
                "first_name": "Vishal Balaji"
            },
            {
                "last_name": "Kumar",
                "first_name": "Amarjeet"
            },
            {
                "last_name": "Viscardi",
                "first_name": "Chelsea"
            },
            {
                "last_name": "Heithaus",
                "first_name": "Robert Evans"
            },
            {
                "last_name": "Shickel",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuyin"
            },
            {
                "last_name": "Cooper",
                "first_name": "Michol A"
            },
            {
                "last_name": "Shao",
                "first_name": "Wei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GT",
            "LG"
        ],
        "abstract": "  Advancements in medical imaging and endovascular grafting have facilitated minimally invasive treatments for aortic diseases. Accurate 3D segmentation of the aorta and its branches is crucial for interventions, as inaccurate segmentation can lead to erroneous surgical planning and endograft construction. Previous methods simplified aortic segmentation as a binary image segmentation problem, overlooking the necessity of distinguishing between individual aortic branches. In this paper, we introduce Context Infused Swin-UNet (CIS-UNet), a deep learning model designed for multi-class segmentation of the aorta and thirteen aortic branches. Combining the strengths of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric decoder, skip connections, and a novel Context-aware Shifted Window Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a unique utilization of the patch merging layer, distinct from conventional Swin transformers. It efficiently condenses the feature map, providing a global spatial context and enhancing performance when applied at the bottleneck layer, offering superior computational efficiency and segmentation accuracy compared to the Swin transformers. We trained our model on computed tomography (CT) scans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the state-of-the-art SwinUNetR segmentation model, which is solely based on Swin transformers, by achieving a superior mean Dice coefficient of 0.713 compared to 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm. CIS-UNet's superior 3D aortic segmentation offers improved precision and optimization for planning endovascular treatments. Our dataset and code will be publicly available. ",
        "title": "CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography  Angiography via Context-Aware Shifted Window Self-Attention",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13051",
        "abstract_url": "http://arxiv.org/abs/2401.13051",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Zhaozhi"
            },
            {
                "last_name": "Guan",
                "first_name": "Bochen"
            },
            {
                "last_name": "Jiang",
                "first_name": "Weihao"
            },
            {
                "last_name": "Yi",
                "first_name": "Muyang"
            },
            {
                "last_name": "Ding",
                "first_name": "Yue"
            },
            {
                "last_name": "Lu",
                "first_name": "Hongtao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The Segment Anything Model (SAM) has exhibited outstanding performance in various image segmentation tasks. Despite being trained with over a billion masks, SAM faces challenges in mask prediction quality in numerous scenarios, especially in real-world contexts. In this paper, we introduce a novel prompt-driven adapter into SAM, namely Prompt Adapter Segment Anything Model (PA-SAM), aiming to enhance the segmentation mask quality of the original SAM. By exclusively training the prompt adapter, PA-SAM extracts detailed information from images and optimizes the mask decoder feature at both sparse and dense prompt levels, improving the segmentation performance of SAM to produce high-quality masks. Experimental results demonstrate that our PA-SAM outperforms other SAM-based methods in high-quality, zero-shot, and open-set segmentation. We're making the source code and models available at https://github.com/xzz2/pa-sam. ",
        "title": "PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13053",
        "abstract_url": "http://arxiv.org/abs/2401.13053",
        "authors": [
            {
                "last_name": "Bhaskara",
                "first_name": "Aditya"
            },
            {
                "last_name": "Gollapudi",
                "first_name": "Sreenivas"
            },
            {
                "last_name": "Im",
                "first_name": "Sungjin"
            },
            {
                "last_name": "Kollias",
                "first_name": "Kostas"
            },
            {
                "last_name": "Munagala",
                "first_name": "Kamesh"
            },
            {
                "last_name": "Sankar",
                "first_name": "Govind S."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "DS"
        ],
        "abstract": "  This paper explores the design of a balanced data-sharing marketplace for entities with heterogeneous datasets and machine learning models that they seek to refine using data from other agents. The goal of the marketplace is to encourage participation for data sharing in the presence of such heterogeneity. Our market design approach for data sharing focuses on interim utility balance, where participants contribute and receive equitable utility from refinement of their models. We present such a market model for which we study computational complexity, solution existence, and approximation algorithms for welfare maximization and core stability. We finally support our theoretical insights with simulations on a mean estimation task inspired by road traffic delay estimation. ",
        "title": "Data Exchange Markets via Utility Balancing",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13054",
        "abstract_url": "http://arxiv.org/abs/2401.13054",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Enzhi"
            },
            {
                "last_name": "Fadlallah",
                "first_name": "Bilal"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "DM",
            "LG"
        ],
        "abstract": "  A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe highly complex real-world hypergraphs, which motivates us to introduce frustrated random walks (FRW) to better describe them. We further benchmark our method against DeepWalk, and show that while the latter can achieve comparable results, FRW has a distinct computational advantage in cases where the number of targets is fairly small. For such cases, we show that FRW runs in significantly shorter time than DeepWalk. Finally, we analyze the time complexity of our method, and show that for large and sparse hypergraphs, the complexity is approximately linear, rendering it superior to the DeepWalk alternative. ",
        "title": "Frustrated Random Walks: A Fast Method to Compute Node Distances on  Hypergraphs",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13060",
        "abstract_url": "http://arxiv.org/abs/2401.13060",
        "authors": [
            {
                "last_name": "Elkomy",
                "first_name": "Mohammed Alaa"
            },
            {
                "last_name": "Sarhan",
                "first_name": "Amany"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05% for task A and a partial Average Precision (pAP) of 57.11% for task B. ",
        "title": "TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced  Transformer-based Ensemble Approach for Qur'anic QA",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13062",
        "abstract_url": "http://arxiv.org/abs/2401.13062",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Yaqing"
            },
            {
                "last_name": "Xu",
                "first_name": "Ling"
            },
            {
                "last_name": "Li",
                "first_name": "Chen"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Visual sensing of environmental geometry allows robots to use artificial potential fields to avoid sparse obstacles. Yet robots must further traverse cluttered large obstacles for applications like search and rescue through rubble and planetary exploration across Martain rocks. Recent studies discovered that to traverse cluttered large obstacles, multi-legged insects and insect-inspired robots make strenuous transitions across locomotor modes with major changes in body orientation. When viewed on a potential energy landscape resulting from locomotor-obstacle physical interaction, these are barrier-crossing transitions across landscape basins. This potential energy landscape approach may provide a modeling framework for cluttered large obstacle traversal. Here, we take the next step toward this vision by testing whether force sensing allows the reconstruction of the potential energy landscape. We developed a cockroach-inspired, minimalistic robot capable of sensing obstacle contact forces and torques around its body as it propelled forward against a pair of cluttered grass-like beam obstacles. We performed measurements over many traverses with systematically varied body orientations. Despite the forces and torques not being fully conservative, they well-matched the potential energy landscape gradients and the landscape reconstructed from them well-matched ground truth. In addition, inspired by cockroach observations, we found that robot head oscillation during traversal further improved the accuracies of force sensing and landscape reconstruction. We still need to study how to reconstruct landscape during a single traverse, as in applications, robots have little chance to use multiple traverses to sample the environment systematically and how to find landscape saddles for least-effort transitions to traverse. ",
        "title": "Force sensing to reconstruct potential energy landscapes for cluttered  large obstacle traversal",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13066",
        "abstract_url": "http://arxiv.org/abs/2401.13066",
        "authors": [
            {
                "last_name": "Schubert",
                "first_name": "Lenhart K."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Algorithmic theories of randomness can be related to theories of probabilistic sequence prediction through the notion of a predictor, defined as a function which supplies lower bounds on initial-segment probabilities of infinite sequences. An infinite binary sequence $z$ is called unpredictable iff its initial-segment \"redundancy\" $n+\\log p(z(n))$ remains sufficiently low relative to every effective predictor $p$. A predictor which maximizes the initial-segment redundancy of a sequence is called optimal for that sequence. It turns out that a sequence is random iff it is unpredictable. More generally, a sequence is random relative to an arbitrary computable distribution iff the distribution is itself an optimal predictor for the sequence. Here \"random\" can be taken in the sense of Martin-L\\\"{o}f by using weak criteria of effectiveness, or in the sense of Schnorr by using stronger criteria of effectiveness. Under the weaker criteria of effectiveness it is possible to construct a universal predictor which is optimal for all infinite sequences. This predictor assigns nonvanishing limit probabilities precisely to the recursive sequences. Under the stronger criteria of effectiveness it is possible to establish a law of large numbers for sequences random relative to a computable distribution, which may be useful as a criterion of \"rationality\" for methods of probabilistic prediction. A remarkable feature of effective predictors is the fact that they are expressible in the special form first proposed by Solomonoff. In this form sequence prediction reduces to assigning high probabilities to initial segments with short and/or numerous encodings. This fact provides the link between theories of randomness and Solomonoff's theory of prediction. ",
        "title": "Predictability and Randomness",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13068",
        "abstract_url": "http://arxiv.org/abs/2401.13068",
        "authors": [
            {
                "last_name": "Jarman",
                "first_name": "Scout"
            },
            {
                "last_name": "Hampel-Arias",
                "first_name": "Zigfried"
            },
            {
                "last_name": "Carr",
                "first_name": "Adra"
            },
            {
                "last_name": "Moon",
                "first_name": "Kevin R."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning identification models have shown promise for identifying gas plumes in Longwave IR hyperspectral images of urban scenes, particularly when a large library of gases are being considered. Because many gases have similar spectral signatures, it is important to properly estimate the signal from a detected plume. Typically, a scene's global mean spectrum and covariance matrix are estimated to whiten the plume's signal, which removes the background's signature from the gas signature. However, urban scenes can have many different background materials that are spatially and spectrally heterogeneous. This can lead to poor identification performance when the global background estimate is not representative of a given local background material. We use image segmentation, along with an iterative background estimation algorithm, to create local estimates for the various background materials that reside underneath a gas plume. Our method outperforms global background estimation on a set of simulated and real gas plumes. This method shows promise in increasing deep learning identification confidence, while being simple and easy to tune when considering diverse plumes. ",
        "title": "Local Background Estimation for Improved Gas Plume Identification in  Hyperspectral Images",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13076",
        "abstract_url": "http://arxiv.org/abs/2401.13076",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mingyang"
            },
            {
                "last_name": "Ma",
                "first_name": "Yue"
            },
            {
                "last_name": "Qiu",
                "first_name": "Qinru"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Current techniques in Visual Simultaneous Localization and Mapping (VSLAM) estimate camera displacement by comparing image features of consecutive scenes. These algorithms depend on scene continuity, hence requires frequent camera inputs. However, processing images frequently can lead to significant memory usage and computation overhead. In this study, we introduce SemanticSLAM, an end-to-end visual-inertial odometry system that utilizes semantic features extracted from an RGB-D sensor. This approach enables the creation of a semantic map of the environment and ensures reliable camera localization. SemanticSLAM is scene-agnostic, which means it doesn't require retraining for different environments. It operates effectively in indoor settings, even with infrequent camera input, without prior knowledge. The strength of SemanticSLAM lies in its ability to gradually refine the semantic map and improve pose estimation. This is achieved by a convolutional long-short-term-memory (ConvLSTM) network, trained to correct errors during map construction. Compared to existing VSLAM algorithms, SemanticSLAM improves pose estimation by 17%. The resulting semantic map provides interpretable information about the environment and can be easily applied to various downstream tasks, such as path planning, obstacle avoidance, and robot navigation. The code will be publicly available at https://github.com/Leomingyangli/SemanticSLAM ",
        "title": "SemanticSLAM: Learning based Semantic Map Construction and Robust Camera  Localization",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13078",
        "abstract_url": "http://arxiv.org/abs/2401.13078",
        "authors": [
            {
                "last_name": "Macenski",
                "first_name": "Steve"
            },
            {
                "last_name": "Booker",
                "first_name": "Matthew"
            },
            {
                "last_name": "Wallace",
                "first_name": "Joshua"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper introduces the Smac Planner, an openly available search-based planning framework with multiple algorithm implementations including 2D-A*, Hybrid-A*, and State Lattice planners. This work is motivated by the lack of performant and available feasible planners for mobile and surface robotics research.   This paper contains three main contributions. First, it briefly describes a minimal open-source software framework where search-based planners may be easily added. Further, this paper characterizes new variations on the feasible planners - dubbed Cost-Aware - specific to mobile roboticist's needs. This fills the gap of missing kinematically feasible implementations suitable for academic, extension, and deployed use. Finally, we provide baseline benchmarking against other standard planning frameworks.   Smac Planner has further significance by becoming the standard open-source planning system within ROS 2's Nav2 framework which powers thousands of robots in research and industry. ",
        "title": "Open-Source, Cost-Aware Kinematically Feasible Planning for Mobile and  Surface Robotics",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13079",
        "abstract_url": "http://arxiv.org/abs/2401.13079",
        "authors": [
            {
                "last_name": "Makhortykh",
                "first_name": "Mykola"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The growing application of artificial intelligence (AI) in the field of information retrieval (IR) affects different domains, including cultural heritage. By facilitating organisation and retrieval of large volumes of heritage-related content, AI-driven IR systems inform users about a broad range of historical phenomena, including genocides (e.g. the Holocaust). However, it is currently unclear to what degree IR systems are capable of dealing with multiple ethical challenges associated with the curation of genocide-related information. To address this question, this chapter provides an overview of ethical challenges associated with the human curation of genocide-related information using a three-part framework inspired by Belmont criteria (i.e. curation challenges associated with respect for individuals, beneficence and justice/fairness). Then, the chapter discusses to what degree the above-mentioned challenges are applicable to the ways in which AI-driven IR systems deal with genocide-related information and what can be the potential ways of bridging AI and memory ethics in this context. ",
        "title": "No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of  Information Retrieval of Genocide-Related Information",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13081",
        "abstract_url": "http://arxiv.org/abs/2401.13081",
        "authors": [
            {
                "last_name": "Narayanan",
                "first_name": "Abhishek"
            },
            {
                "last_name": "Musthyala",
                "first_name": "Rushabh"
            },
            {
                "last_name": "Sankar",
                "first_name": "Rahul"
            },
            {
                "last_name": "Nistala",
                "first_name": "Anirudh Prasad"
            },
            {
                "last_name": "Singh",
                "first_name": "Pranav"
            },
            {
                "last_name": "Cirrone",
                "first_name": "Jacopo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Visual Question Answering (VQA) in the medical domain presents a unique, interdisciplinary challenge, combining fields such as Computer Vision, Natural Language Processing, and Knowledge Representation. Despite its importance, research in medical VQA has been scant, only gaining momentum since 2018. Addressing this gap, our research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods. We innovatively augment the SLAKE dataset, enabling our model to respond to a more diverse array of questions, not limited to the immediate content of radiology or pathology images. Our model achieves a top-1 accuracy of 79.55\\% with a less complex architecture, demonstrating comparable performance to current state-of-the-art models. This research not only advances medical VQA but also opens avenues for practical applications in diagnostic settings. ",
        "title": "Free Form Medical Visual Question Answering in Radiology",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13082",
        "abstract_url": "http://arxiv.org/abs/2401.13082",
        "authors": [
            {
                "last_name": "Kannan",
                "first_name": "Shyam Sundar"
            },
            {
                "last_name": "Min",
                "first_name": "Byung-Cheol"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Visual place recognition is a challenging task in the field of computer vision, and autonomous robotics and vehicles, which aims to identify a location or a place from visual inputs. Contemporary methods in visual place recognition employ convolutional neural networks and utilize every region within the image for the place recognition task. However, the presence of dynamic and distracting elements in the image may impact the effectiveness of the place recognition process. Therefore, it is meaningful to focus on task-relevant regions of the image for improved recognition. In this paper, we present PlaceFormer, a novel transformer-based approach for visual place recognition. PlaceFormer employs patch tokens from the transformer to create global image descriptors, which are then used for image retrieval. To re-rank the retrieved images, PlaceFormer merges the patch tokens from the transformer to form multi-scale patches. Utilizing the transformer's self-attention mechanism, it selects patches that correspond to task-relevant areas in an image. These selected patches undergo geometric verification, generating similarity scores across different patch sizes. Subsequently, spatial scores from each patch size are fused to produce a final similarity score. This score is then used to re-rank the images initially retrieved using global image descriptors. Extensive experiments on benchmark datasets demonstrate that PlaceFormer outperforms several state-of-the-art methods in terms of accuracy and computational efficiency, requiring less time and memory. ",
        "title": "PlaceFormer: Transformer-based Visual Place Recognition using  Multi-Scale Patch Selection and Fusion",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13085",
        "abstract_url": "http://arxiv.org/abs/2401.13085",
        "authors": [
            {
                "last_name": "Litake",
                "first_name": "Onkar"
            },
            {
                "last_name": "Yagnik",
                "first_name": "Niraj"
            },
            {
                "last_name": "Labhsetwar",
                "first_name": "Shreyas"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniques surpass LLMs. ",
        "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13086",
        "abstract_url": "http://arxiv.org/abs/2401.13086",
        "authors": [
            {
                "last_name": "Rejeleene",
                "first_name": "Rick"
            },
            {
                "last_name": "Xu",
                "first_name": "Xiaowei"
            },
            {
                "last_name": "Talburt",
                "first_name": "John"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models. ",
        "title": "Towards Trustable Language Models: Investigating Information Quality of  Large Language Models",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13087",
        "abstract_url": "http://arxiv.org/abs/2401.13087",
        "authors": [
            {
                "last_name": "Martell",
                "first_name": "Matthew"
            },
            {
                "last_name": "Terry",
                "first_name": "Nick"
            },
            {
                "last_name": "Sengupta",
                "first_name": "Ribhu"
            },
            {
                "last_name": "Salazar",
                "first_name": "Chris"
            },
            {
                "last_name": "Errett",
                "first_name": "Nicole A."
            },
            {
                "last_name": "Miles",
                "first_name": "Scott B."
            },
            {
                "last_name": "Wartman",
                "first_name": "Joseph"
            },
            {
                "last_name": "Choe",
                "first_name": "Youngjun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Street View Images (SVI) are a common source of valuable data for researchers. Researchers have used SVI data for estimating pedestrian volumes, demographic surveillance, and to better understand built and natural environments in cityscapes. However, the most common source of publicly available SVI data is Google Street View. Google Street View images are collected infrequently, making temporal analysis challenging, especially in low population density areas. Our main contribution is the development of an open-source data pipeline for processing 360-degree video recorded from a car-mounted camera. The video data is used to generate SVIs, which then can be used as an input for temporal analysis. We demonstrate the use of the pipeline by collecting a SVI dataset over a 38-month longitudinal survey of Seattle, WA, USA during the COVID-19 pandemic. The output of our pipeline is validated through statistical analyses of pedestrian traffic in the images. We confirm known results in the literature and provide new insights into outdoor pedestrian traffic patterns. This study demonstrates the feasibility and value of collecting and using SVI for research purposes beyond what is possible with currently available SVI data. Limitations and future improvements on the data pipeline and case study are also discussed. ",
        "title": "Open-source data pipeline for street-view images: a case study on  community mobility during COVID-19 pandemic",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13096",
        "abstract_url": "http://arxiv.org/abs/2401.13096",
        "authors": [
            {
                "last_name": "Kozodoi",
                "first_name": "Nikita"
            },
            {
                "last_name": "Zinovyeva",
                "first_name": "Elizaveta"
            },
            {
                "last_name": "Valentin",
                "first_name": "Simon"
            },
            {
                "last_name": "Pereira",
                "first_name": "Jo\u00e3o"
            },
            {
                "last_name": "Agundez",
                "first_name": "Rodrigo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Demand forecasting is a prominent business use case that allows retailers to optimize inventory planning, logistics, and core business decisions. One of the key challenges in demand forecasting is accounting for relationships and interactions between articles. Most modern forecasting approaches provide independent article-level predictions that do not consider the impact of related articles. Recent research has attempted addressing this challenge using Graph Neural Networks (GNNs) and showed promising results. This paper builds on previous research on GNNs and makes two contributions. First, we integrate a GNN encoder into a state-of-the-art DeepAR model. The combined model produces probabilistic forecasts, which are crucial for decision-making under uncertainty. Second, we propose to build graphs using article attribute similarity, which avoids reliance on a pre-defined graph structure. Experiments on three real-world datasets show that the proposed approach consistently outperforms non-graph benchmarks. We also show that our approach produces article embeddings that encode article similarity and demand dynamics and are useful for other downstream business tasks beyond forecasting. ",
        "title": "Probabilistic Demand Forecasting with Graph Neural Networks",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13097",
        "abstract_url": "http://arxiv.org/abs/2401.13097",
        "authors": [
            {
                "last_name": "Greene",
                "first_name": "Michelle R."
            },
            {
                "last_name": "Josyula",
                "first_name": "Mariam"
            },
            {
                "last_name": "Si",
                "first_name": "Wentao"
            },
            {
                "last_name": "Hart",
                "first_name": "Jennifer A."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Computer-based scene understanding has influenced fields ranging from urban planning to autonomous vehicle performance, yet little is known about how well these technologies work across social differences. We investigate the biases of deep convolutional neural networks (dCNNs) in scene classification, using nearly one million images from global and US sources, including user-submitted home photographs and Airbnb listings. We applied statistical models to quantify the impact of socioeconomic indicators such as family income, Human Development Index (HDI), and demographic factors from public data sources (CIA and US Census) on dCNN performance. Our analyses revealed significant socioeconomic bias, where pretrained dCNNs demonstrated lower classification accuracy, lower classification confidence, and a higher tendency to assign labels that could be offensive when applied to homes (e.g., \"ruin\", \"slum\"), especially in images from homes with lower socioeconomic status (SES). This trend is consistent across two datasets of international images and within the diverse economic and racial landscapes of the United States. This research contributes to understanding biases in computer vision, emphasizing the need for more inclusive and representative training datasets. By mitigating the bias in the computer vision pipelines, we can ensure fairer and more equitable outcomes for applied computer vision, including home valuation and smart home security systems. There is urgency in addressing these biases, which can significantly impact critical decisions in urban development and resource allocation. Our findings also motivate the development of AI systems that better understand and serve diverse communities, moving towards technology that equitably benefits all sectors of society. ",
        "title": "Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in  Deep Learning Systems",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13098",
        "abstract_url": "http://arxiv.org/abs/2401.13098",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Ruixin"
            },
            {
                "last_name": "Spadon",
                "first_name": "Gabriel"
            },
            {
                "last_name": "Bailey",
                "first_name": "Sarah"
            },
            {
                "last_name": "Pelot",
                "first_name": "Ronald"
            },
            {
                "last_name": "Matwin",
                "first_name": "Stan"
            },
            {
                "last_name": "Soares",
                "first_name": "Amilcar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for assessing the invasion threat level given a pair of origin and destination. Accordingly, this paper introduces transformers to gravity models to rebuild the short- and long-term dependencies that make the risk analysis feasible. Thus, we introduce a physics-inspired framework that achieves an 89% segmentation accuracy for existing and non-existing trajectories and an 84.8% accuracy for the number of vessels flowing between key port areas, representing more than 10% improvement over the traditional deep-gravity model. Along these lines, this research contributes to a better understanding of invasive species risk assessment. It allows policymakers, conservationists, and stakeholders to prioritize management actions by identifying high-risk invasion pathways. Besides, our model is versatile and can include new data sources, making it suitable for assessing species invasion risks in a changing global landscape. ",
        "title": "Gravity-Informed Deep Learning Framework for Predicting Ship Traffic  Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13099",
        "abstract_url": "http://arxiv.org/abs/2401.13099",
        "authors": [
            {
                "last_name": "O'Brien",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The SINDy algorithm has been successfully used to identify the governing equations of dynamical systems from time series data. However, SINDy assumes the user has prior knowledge of the variables in the system and of a function library that can act as a basis for the system. In this paper, we demonstrate on real world data how the Augmented SINDy algorithm outperforms SINDy in the presence of system variable uncertainty. We then show SINDy can be further augmented to perform robustly when both kinds of uncertainty are present. ",
        "title": "Sparse identification of nonlinear dynamics in the presence of library  and system uncertainty",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13100",
        "abstract_url": "http://arxiv.org/abs/2401.13100",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shi"
            },
            {
                "last_name": "Ding",
                "first_name": "Zhiyan"
            },
            {
                "last_name": "Li",
                "first_name": "Qin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Bayesian sampling is an important task in statistics and machine learning. Over the past decade, many ensemble-type sampling methods have been proposed. In contrast to the classical Markov chain Monte Carlo methods, these new methods deploy a large number of interactive samples, and the communication between these samples is crucial in speeding up the convergence. To justify the validity of these sampling strategies, the concept of interacting particles naturally calls for the mean-field theory. The theory establishes a correspondence between particle interactions encoded in a set of coupled ODEs/SDEs and a PDE that characterizes the evolution of the underlying distribution. This bridges numerical algorithms with the PDE theory used to show convergence in time. Many mathematical machineries are developed to provide the mean-field analysis, and we showcase two such examples: The coupling method and the compactness argument built upon the martingale strategy. The former has been deployed to show the convergence of ensemble Kalman sampler and ensemble Kalman inversion, and the latter will be shown to be immensely powerful in proving the validity of the Vlasov-Boltzmann simulator. ",
        "title": "Bayesian sampling using interacting particles",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13103",
        "abstract_url": "http://arxiv.org/abs/2401.13103",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "W."
            },
            {
                "last_name": "Oguz",
                "first_name": "S."
            },
            {
                "last_name": "Heinrich",
                "first_name": "M. K."
            },
            {
                "last_name": "Allwright",
                "first_name": "M."
            },
            {
                "last_name": "Wahby",
                "first_name": "M."
            },
            {
                "last_name": "Christensen",
                "first_name": "A. Lyhne"
            },
            {
                "last_name": "Garone",
                "first_name": "E."
            },
            {
                "last_name": "Dorigo",
                "first_name": "M."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  The system architecture controlling a group of robots is generally set before deployment and can be either centralized or decentralized. This dichotomy is highly constraining, because decentralized systems are typically fully self-organized and therefore difficult to design analytically, whereas centralized systems have single points of failure and limited scalability. To address this dichotomy, we present the Self-organizing Nervous System (SoNS), a novel robot swarm architecture based on self-organized hierarchy. The SoNS approach enables robots to autonomously establish, maintain, and reconfigure dynamic multi-level system architectures. For example, a robot swarm consisting of $n$ independent robots could transform into a single $n$-robot SoNS and then into several independent smaller SoNSs, where each SoNS uses a temporary and dynamic hierarchy. Leveraging the SoNS approach, we show that sensing, actuation, and decision-making can be coordinated in a locally centralized way, without sacrificing the benefits of scalability, flexibility, and fault tolerance, for which swarm robotics is usually studied. In several proof-of-concept robot missions -- including binary decision-making and search-and-rescue -- we demonstrate that the capabilities of the SoNS approach greatly advance the state of the art in swarm robotics. The missions are conducted with a real heterogeneous aerial-ground robot swarm, using a custom-developed quadrotor platform. We also demonstrate the scalability of the SoNS approach in swarms of up to 250 robots in a physics-based simulator, and demonstrate several types of system fault tolerance in simulation and reality. ",
        "title": "Self-organizing Nervous Systems for Robot Swarms",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13105",
        "abstract_url": "http://arxiv.org/abs/2401.13105",
        "authors": [
            {
                "last_name": "Powell",
                "first_name": "Jadyn"
            },
            {
                "last_name": "McCafferty-Leroux",
                "first_name": "Alex"
            },
            {
                "last_name": "Hilal",
                "first_name": "Waleed"
            },
            {
                "last_name": "Gadsden",
                "first_name": "Stephen Andrew"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  With the increased energy demands of the 21st century, there is a clear need for developing a more sustainable method of energy generation, distribution, and transmission. The popularity of Smart Grid continues to grow as it presents its benefits, including interconnectivity, improved efficiency, the ability to integrate renewable energy sources, and many more. However, it is not without its challenges. This survey aims to provide an introductory background of smart grids, detail some of the main aspects and current challenges, and review the most recent papers and proposed solutions. It will also highlight the current state of implementation of the smart grid by describing various prototypes, as well as various countries and continents implementation plans and projects. ",
        "title": "Smart Grids: A Comprehensive Survey of Challenges, Industry  Applications, and Future Trends",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13107",
        "abstract_url": "http://arxiv.org/abs/2401.13107",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Ke"
            },
            {
                "last_name": "Li",
                "first_name": "Shizhe"
            },
            {
                "last_name": "Qin",
                "first_name": "Ruwen"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Seniors residing in rural areas often encounter limited accessibility to opportunities, resources, and services. This paper introduces a model proposing that both aging and rural residency are factors contributing to the restricted accessibility faced by rural seniors. Leveraging data from the 2017 National Household Travel Survey, the study examines three hypotheses pertaining to this causal model. Multiple causal pathways emerge in the data analysis, with mobility identified as a mediator in one of them. The study further identifies specific challenges faced by rural seniors, such as the reduced accessibility in reaching medical services and assisting others. These challenges stem primarily from aging and geographic obstacles that not only diminish their willingness to travel but also restrict more in the group from choosing transportation modes with higher mobility. The insights gained from this study serve as a foundation for devising effective methods to enhance transportation accessibility for seniors in rural areas. ",
        "title": "Development of a Causal Model for Improving Rural Seniors'  Accessibility: Data Evidences",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13110",
        "abstract_url": "http://arxiv.org/abs/2401.13110",
        "authors": [
            {
                "last_name": "Mavrepis",
                "first_name": "Philip"
            },
            {
                "last_name": "Makridis",
                "first_name": "Georgios"
            },
            {
                "last_name": "Fatouros",
                "first_name": "Georgios"
            },
            {
                "last_name": "Koukos",
                "first_name": "Vasileios"
            },
            {
                "last_name": "Separdani",
                "first_name": "Maria Margarita"
            },
            {
                "last_name": "Kyriazis",
                "first_name": "Dimosthenis"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  The field of Explainable Artificial Intelligence (XAI) often focuses on users with a strong technical background, making it challenging for non-experts to understand XAI methods. This paper presents \"x-[plAIn]\", a new approach to make XAI more accessible to a wider audience through a custom Large Language Model (LLM), developed using ChatGPT Builder. Our goal was to design a model that can generate clear, concise summaries of various XAI methods, tailored for different audiences, including business professionals and academics. The key feature of our model is its ability to adapt explanations to match each audience group's knowledge level and interests. Our approach still offers timely insights, facilitating the decision-making process by the end users. Results from our use-case studies show that our model is effective in providing easy-to-understand, audience-specific explanations, regardless of the XAI method used. This adaptability improves the accessibility of XAI, bridging the gap between complex AI technologies and their practical applications. Our findings indicate a promising direction for LLMs in making advanced AI concepts more accessible to a diverse range of users. ",
        "title": "XAI for All: Can Large Language Models Simplify Explainable AI?",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13112",
        "abstract_url": "http://arxiv.org/abs/2401.13112",
        "authors": [
            {
                "last_name": "You",
                "first_name": "Lei"
            },
            {
                "last_name": "Cao",
                "first_name": "Lele"
            },
            {
                "last_name": "Nilsson",
                "first_name": "Mattias"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorithm is accompanied by an analysis of its convergence rate. The efficacy of our proposed method is substantiated through a series of illustrative case studies, highlighting its potential in providing deep insights into decision-making models. ",
        "title": "DISCOUNT: Distributional Counterfactual Explanation With Optimal  Transport",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13115",
        "abstract_url": "http://arxiv.org/abs/2401.13115",
        "authors": [
            {
                "last_name": "Tang",
                "first_name": "Wenpin"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hanyang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Diffusion probabilistic models (DPMs) have emerged as a promising technology in generative modeling. The success of DPMs relies on two ingredients: time reversal of Markov diffusion processes and score matching. Most existing work implicitly assumes that score matching is close to perfect, while this assumption is questionable. In view of possibly unguaranteed score matching, we propose a new criterion -- the contraction of backward sampling in the design of DPMs. This leads to a novel class of contractive DPMs (CDPMs), including contractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance preserving (sub-VP) stochastic differential equations (SDEs). The key insight is that the contraction in the backward process narrows score matching errors, as well as discretization error. Thus, the proposed CDPMs are robust to both sources of error. Our proposal is supported by theoretical results, and is corroborated by experiments. Notably, contractive sub-VP shows the best performance among all known SDE-based DPMs on the CIFAR-10 dataset. ",
        "title": "Contractive Diffusion Probabilistic Models",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13127",
        "abstract_url": "http://arxiv.org/abs/2401.13127",
        "authors": [
            {
                "last_name": "Howell",
                "first_name": "Pierce"
            },
            {
                "last_name": "Rudolph",
                "first_name": "Max"
            },
            {
                "last_name": "Torbati",
                "first_name": "Reza"
            },
            {
                "last_name": "Fu",
                "first_name": "Kevin"
            },
            {
                "last_name": "Ravichandar",
                "first_name": "Harish"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "MA"
        ],
        "abstract": "  Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes -- an ability we call adaptive teaming. In this work, we investigate if awareness and communication of robot capabilities can provide such generalization by conducting detailed experiments involving an established multi-robot test bed. We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination. Videos of trained policies can be viewed at: https://sites.google.com/view/cap-comm ",
        "title": "Generalization of Heterogeneous Multi-Robot Policies via Awareness and  Communication of Capabilities",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13128",
        "abstract_url": "http://arxiv.org/abs/2401.13128",
        "authors": [
            {
                "last_name": "Abdelraouf",
                "first_name": "Hassan"
            },
            {
                "last_name": "Feron",
                "first_name": "Eric"
            },
            {
                "last_name": "Shamma",
                "first_name": "Jeff S."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We introduce a new class of quadratic functions based on a hierarchy of linear time-varying (LTV) dynamical systems. These quadratic functions in the higher order space can be also seen as a non-homogeneous polynomial Lyapunov functions for the original system, i.e the first system in the hierarchy. These non-homogeneous polynomials are used to obtain accurate outer approximation for the reachable set given the initial condition and less conservative bounds for the impulse response peak of linear, possibly time-varying systems. In addition, we pose an extension to the presented approach to construct invariant sets that are not necessarily Lyapunov functions. The introduced methods are based on elementary linear systems theory and offer very much flexibility in defining arbitrary polynomial Lyapunov functions and invariant sets for LTV systems. ",
        "title": "Polynomial Lyapunov Functions and Invariant Sets from a New Hierarchy of  Quadratic Lyapunov Functions for LTV Systems",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13129",
        "abstract_url": "http://arxiv.org/abs/2401.13129",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yunyi"
            },
            {
                "last_name": "Shen",
                "first_name": "Yanzhen"
            },
            {
                "last_name": "Deng",
                "first_name": "Yu"
            },
            {
                "last_name": "Popa",
                "first_name": "Lucian"
            },
            {
                "last_name": "Shwartz",
                "first_name": "Larisa"
            },
            {
                "last_name": "Zhai",
                "first_name": "ChengXiang"
            },
            {
                "last_name": "Han",
                "first_name": "Jiawei"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  Accurately typing entity mentions from text segments is a fundamental task for various natural language processing applications. Many previous approaches rely on massive human-annotated data to perform entity typing. Nevertheless, collecting such data in highly specialized science and engineering domains (e.g., software engineering and security) can be time-consuming and costly, without mentioning the domain gaps between training and inference data if the model needs to be applied to confidential datasets. In this paper, we study the task of seed-guided fine-grained entity typing in science and engineering domains, which takes the name and a few seed entities for each entity type as the only supervision and aims to classify new entity mentions into both seen and unseen types (i.e., those without seed entities). To solve this problem, we propose SEType which first enriches the weak supervision by finding more entities for each seen type from an unlabeled corpus using the contextualized representations of pre-trained language models. It then matches the enriched entities to unlabeled text to get pseudo-labeled samples and trains a textual entailment model that can make inferences for both seen and unseen types. Extensive experiments on two datasets covering four domains demonstrate the effectiveness of SEType in comparison with various baselines. ",
        "title": "Seed-Guided Fine-Grained Entity Typing in Science and Engineering  Domains",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13133",
        "abstract_url": "http://arxiv.org/abs/2401.13133",
        "authors": [
            {
                "last_name": "Ahmad",
                "first_name": "Ibrahim Said"
            },
            {
                "last_name": "Aliyu",
                "first_name": "Lukman Jibril"
            },
            {
                "last_name": "Khalid",
                "first_name": "Abubakar Auwal"
            },
            {
                "last_name": "Aliyu",
                "first_name": "Saminu Muhammad"
            },
            {
                "last_name": "Muhammad",
                "first_name": "Shamsuddeen Hassan"
            },
            {
                "last_name": "Abdulmumin",
                "first_name": "Idris"
            },
            {
                "last_name": "Abduljalil",
                "first_name": "Bala Mairiga"
            },
            {
                "last_name": "Bello",
                "first_name": "Bello Shehu"
            },
            {
                "last_name": "Abubakar",
                "first_name": "Amina Imam"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SI"
        ],
        "abstract": "  Numerous successes have been achieved in combating the COVID-19 pandemic, initially using various precautionary measures like lockdowns, social distancing, and the use of face masks. More recently, various vaccinations have been developed to aid in the prevention or reduction of the severity of the COVID-19 infection. Despite the effectiveness of the precautionary measures and the vaccines, there are several controversies that are massively shared on social media platforms like Twitter. In this paper, we explore the use of state-of-the-art transformer-based language models to study people's acceptance of vaccines in Nigeria. We developed a novel dataset by crawling multi-lingual tweets using relevant hashtags and keywords. Our analysis and visualizations revealed that most tweets expressed neutral sentiments about COVID-19 vaccines, with some individuals expressing positive views, and there was no strong preference for specific vaccine types, although Moderna received slightly more positive sentiment. We also found out that fine-tuning a pre-trained LLM with an appropriate dataset can yield competitive results, even if the LLM was not initially pre-trained on the specific language of that dataset. ",
        "title": "Analyzing COVID-19 Vaccination Sentiments in Nigerian Cyberspace:  Insights from a Manually Annotated Twitter Dataset",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13136",
        "abstract_url": "http://arxiv.org/abs/2401.13136",
        "authors": [
            {
                "last_name": "Shen",
                "first_name": "Lingfeng"
            },
            {
                "last_name": "Tan",
                "first_name": "Weiting"
            },
            {
                "last_name": "Chen",
                "first_name": "Sihao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yunmo"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jingyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Haoran"
            },
            {
                "last_name": "Zheng",
                "first_name": "Boyuan"
            },
            {
                "last_name": "Koehn",
                "first_name": "Philipp"
            },
            {
                "last_name": "Khashabi",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction. ",
        "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in  Multilingual Contexts",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13140",
        "abstract_url": "http://arxiv.org/abs/2401.13140",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xiongchao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Guo",
                "first_name": "Xueqi"
            },
            {
                "last_name": "Xie",
                "first_name": "Huidong"
            },
            {
                "last_name": "Liu",
                "first_name": "Qiong"
            },
            {
                "last_name": "Duncan",
                "first_name": "James S."
            },
            {
                "last_name": "Sinusas",
                "first_name": "Albert J."
            },
            {
                "last_name": "Liu",
                "first_name": "Chi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single-Photon Emission Computed Tomography (SPECT) is widely applied for the diagnosis of coronary artery diseases. Low-dose (LD) SPECT aims to minimize radiation exposure but leads to increased image noise. Limited-view (LV) SPECT, such as the latest GE MyoSPECT ES system, enables accelerated scanning and reduces hardware expenses but degrades reconstruction accuracy. Additionally, Computed Tomography (CT) is commonly used to derive attenuation maps ($\\mu$-maps) for attenuation correction (AC) of cardiac SPECT, but it will introduce additional radiation exposure and SPECT-CT misalignments. Although various methods have been developed to solely focus on LD denoising, LV reconstruction, or CT-free AC in SPECT, the solution for simultaneously addressing these tasks remains challenging and under-explored. Furthermore, it is essential to explore the potential of fusing cross-domain and cross-modality information across these interrelated tasks to further enhance the accuracy of each task. Thus, we propose a Dual-Domain Coarse-to-Fine Progressive Network (DuDoCFNet), a multi-task learning method for simultaneous LD denoising, LV reconstruction, and CT-free $\\mu$-map generation of cardiac SPECT. Paired dual-domain networks in DuDoCFNet are cascaded using a multi-layer fusion mechanism for cross-domain and cross-modality feature fusion. Two-stage progressive learning strategies are applied in both projection and image domains to achieve coarse-to-fine estimations of SPECT projections and CT-derived $\\mu$-maps. Our experiments demonstrate DuDoCFNet's superior accuracy in estimating projections, generating $\\mu$-maps, and AC reconstructions compared to existing single- or multi-task learning methods, under various iterations and LD levels. The source code of this work is available at https://github.com/XiongchaoChen/DuDoCFNet-MultiTask. ",
        "title": "Dual-Domain Coarse-to-Fine Progressive Estimation Network for  Simultaneous Denoising, Limited-View Reconstruction, and Attenuation  Correction of Cardiac SPECT",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13142",
        "abstract_url": "http://arxiv.org/abs/2401.13142",
        "authors": [
            {
                "last_name": "Blili-Hamelin",
                "first_name": "Borhane"
            },
            {
                "last_name": "Hancox-Li",
                "first_name": "Leif"
            },
            {
                "last_name": "Smart",
                "first_name": "Andrew"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Dreams of machines that rival human intelligence have shaped the field of AI since its inception. Yet there remains no agreed-upon conception of what human-level AI or artificial general intelligence (AGI) means. We investigate key social, political, and ethical assumptions made by influential conceptions of AGI and human-level AI. We then draw on feminist, STS, and social science scholarship on the political and social character of intelligence in both humans and machines to defend a pluralistic, democratic, and participatory conception of the topic. We argue that framing AGI or human-level AI as a technical or value-neutral topic leads to political, ethical, and epistemic harm. AGI should not be developed without explicit attention to the values they encode, the people they include or exclude, and a view toward epistemic justice. ",
        "title": "Unsocial Intelligence: a Pluralistic, Democratic, and Participatory  Investigation of AGI Discourse",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13146",
        "abstract_url": "http://arxiv.org/abs/2401.13146",
        "authors": [
            {
                "last_name": "Jalal",
                "first_name": "Md Asif"
            },
            {
                "last_name": "Parada",
                "first_name": "Pablo Peso"
            },
            {
                "last_name": "Pavlidis",
                "first_name": "George"
            },
            {
                "last_name": "Moschopoulos",
                "first_name": "Vasileios"
            },
            {
                "last_name": "Saravanan",
                "first_name": "Karthikeyan"
            },
            {
                "last_name": "Kontoulis",
                "first_name": "Chrysovalantis-Giorgos"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jisi"
            },
            {
                "last_name": "Drosou",
                "first_name": "Anastasios"
            },
            {
                "last_name": "Lee",
                "first_name": "Gil Ho"
            },
            {
                "last_name": "Lee",
                "first_name": "Jungin"
            },
            {
                "last_name": "Jung",
                "first_name": "Seokyeong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SD"
        ],
        "abstract": "  Automatic Speech Recognition (ASR) still face challenges when recognizing time-variant rare-phrases. Contextual biasing (CB) modules bias ASR model towards such contextually-relevant phrases. During training, a list of biasing phrases are selected from a large pool of phrases following a sampling strategy. In this work we firstly analyse different sampling strategies to provide insights into the training of CB for ASR with correlation plots between the bias embeddings among various training stages. Secondly, we introduce a neighbourhood attention (NA) that localizes self attention (SA) to the nearest neighbouring frames to further refine the CB output. The results show that this proposed approach provides on average a 25.84% relative WER improvement on LibriSpeech sets and rare-word evaluation compared to the baseline. ",
        "title": "Locality enhanced dynamic biasing and sampling strategies for contextual  ASR",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13147",
        "abstract_url": "http://arxiv.org/abs/2401.13147",
        "authors": [
            {
                "last_name": "Tabassian",
                "first_name": "Mahdi"
            },
            {
                "last_name": "S",
                "first_name": "Somayeh Akbari."
            },
            {
                "last_name": "Queir\u00f3s",
                "first_name": "Sandro"
            },
            {
                "last_name": "D'hooge",
                "first_name": "Jan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This study presents a deep convolutional auto-encoder network for filtering reverberation artifacts, from transthoracic echocardiographic (TTE) image sequences. Given the spatiotemporal nature of these artifacts, the filtering network was built using 3D convolutional layers to suppress the clutter patterns throughout the cardiac cycle. The network was designed by taking advantage of: i) an attention mechanism to focus primarily on cluttered regions and ii) residual learning to preserve fine structures of the image frames. To train the deep network, a diverse set of artifact patterns was simulated and the simulated patterns were superimposed onto artifact-free ultra-realistic synthetic TTE sequences of six ultrasound vendors to generate input of the filtering network. The artifact-free sequences served as ground-truth. Performance of the filtering network was evaluated using unseen synthetic as well as in-vivo artifactual sequences. Satisfactory results obtained using the latter dataset confirmed the good generalization performance of the proposed network which was trained using the synthetic sequences and simulated artifact patterns. Suitability of the clutter-filtered sequences for further processing was assessed by computing segmental strain curves from them. The results showed that the large discrepancy between the strain profiles computed from the cluttered segments and their corresponding segments in the clutter-free images was significantly reduced after filtering the sequences using the proposed network. The trained deep network could process an artifactual TTE sequence in a fraction of a second and can be used for real-time clutter filtering. Moreover, it can improve the precision of the clinical indexes that are computed from the TTE sequences. The source code of the proposed method is available at: https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main. ",
        "title": "Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic  Images Using a 3D Convolutional Auto-Encoder",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13148",
        "abstract_url": "http://arxiv.org/abs/2401.13148",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Liqun"
            },
            {
                "last_name": "Miao",
                "first_name": "Keyan"
            },
            {
                "last_name": "Gatsis",
                "first_name": "Konstantinos"
            },
            {
                "last_name": "Papachristodoulou",
                "first_name": "Antonis"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "RO"
        ],
        "abstract": "  Reinforcement learning (RL) excels in applications such as video games and robotics, but ensuring safety and stability remains challenging when using RL to control real-world systems where using model-free algorithms suffering from low sample efficiency might be prohibitive. This paper first provides safety and stability definitions for the RL system, and then introduces a Neural ordinary differential equations-based Lyapunov-Barrier Actor-Critic (NLBAC) framework that leverages Neural Ordinary Differential Equations (NODEs) to approximate system dynamics and integrates the Control Barrier Function (CBF) and Control Lyapunov Function (CLF) frameworks with the actor-critic method to assist in maintaining the safety and stability for the system. Within this framework, we employ the augmented Lagrangian method to update the RL-based controller parameters. Additionally, we introduce an extra backup controller in situations where CBF constraints for safety and the CLF constraint for stability cannot be satisfied simultaneously. Simulation results demonstrate that the framework leads the system to approach the desired state and allows fewer violations of safety constraints with better sample efficiency compared to other methods. ",
        "title": "NLBAC: A Neural Ordinary Differential Equations-based Framework for  Stable and Safe Reinforcement Learning",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13150",
        "abstract_url": "http://arxiv.org/abs/2401.13150",
        "authors": [
            {
                "last_name": "Cankur",
                "first_name": "Onur"
            },
            {
                "last_name": "Tomar",
                "first_name": "Aditya"
            },
            {
                "last_name": "Nichols",
                "first_name": "Daniel"
            },
            {
                "last_name": "Scully-Allison",
                "first_name": "Connor"
            },
            {
                "last_name": "Isaacs",
                "first_name": "Katherine E."
            },
            {
                "last_name": "Bhatele",
                "first_name": "Abhinav"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "PF"
        ],
        "abstract": "  Developing efficient parallel applications is critical to advancing scientific development but requires significant performance analysis and optimization. Performance analysis tools help developers manage the increasing complexity and scale of performance data, but often rely on the user to manually explore low-level data and are rigid in how the data can be manipulated. We propose a Python-based API, Chopper, which provides high-level and flexible performance analysis for both single and multiple executions of parallel applications. Chopper facilitates performance analysis and reduces developer effort by providing configurable high-level methods for common performance analysis tasks such as calculating load imbalance, hot paths, scalability bottlenecks, correlation between metrics and CCT nodes, and causes of performance variability within a robust and mature Python environment that provides fluid access to lower-level data manipulations. We demonstrate how Chopper allows developers to quickly and succinctly explore performance and identify issues across applications such as AMG, Laghos, LULESH, Quicksilver and Tortuga. ",
        "title": "Automated Programmatic Performance Analysis of Parallel Programs",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13152",
        "abstract_url": "http://arxiv.org/abs/2401.13152",
        "authors": [
            {
                "last_name": "Choi",
                "first_name": "Brian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The fractional discrete nonlinear Schr\\\"odinger equation (fDNLS) is studied on a periodic lattice from the analytic and dynamic perspective by varying the mesh size $h>0$ and the nonlocal L\\'evy index $\\alpha \\in (0,2]$. We show that the discrete system converges to the fractional NLS as $h \\rightarrow 0$ below the energy space by directly estimating the difference between the discrete and continuum solutions in $L^2(\\mathbb{T})$ using the periodic Strichartz estimates. The sharp convergence rate via the finite-difference method is shown to be $O(h^{\\frac{\\alpha}{2+\\alpha}})$ in the energy space. On the other hand for a fixed $h > 0$, the linear stability analysis on a family of continuous wave (CW) solutions reveals a rich dynamical structure of CW waves due to the interplay between nonlinearity, nonlocal dispersion, and discreteness. The gain spectrum is derived to understand the role of $h$ and $\\alpha$ in triggering higher mode excitations. The transition from the quadratic dependence of maximum gain on the amplitude of CW solutions to the linear dependence, due to the lattice structure, is shown analytically and numerically. ",
        "title": "Periodic Fractional Discrete Nonlinear Schr\\\"odinger Equation and  Modulational Instability",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13154",
        "abstract_url": "http://arxiv.org/abs/2401.13154",
        "authors": [
            {
                "last_name": "Xiang",
                "first_name": "Lingfeng"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhen"
            },
            {
                "last_name": "Deng",
                "first_name": "Weishu"
            },
            {
                "last_name": "Lu",
                "first_name": "Hui"
            },
            {
                "last_name": "Rao",
                "first_name": "Jia"
            },
            {
                "last_name": "Yuan",
                "first_name": "Yifan"
            },
            {
                "last_name": "Wang",
                "first_name": "Ren"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS"
        ],
        "abstract": "  With the advent of byte-addressable memory devices, such as CXL memory, persistent memory, and storage-class memory, tiered memory systems have become a reality. Page migration is the de facto method within operating systems for managing tiered memory. It aims to bring hot data whenever possible into fast memory to optimize the performance of data accesses while using slow memory to accommodate data spilled from fast memory. While the existing research has demonstrated the effectiveness of various optimizations on page migration, it falls short of addressing a fundamental question: Is exclusive memory tiering, in which a page is either present in fast memory or slow memory, but not both simultaneously, the optimal strategy for tiered memory management?   We demonstrate that page migration-based exclusive memory tiering suffers significant performance degradation when fast memory is under pressure. In this paper, we propose non-exclusive memory tiering, a page management strategy that retains a copy of pages recently promoted from slow memory to fast memory to mitigate memory thrashing. To enable non-exclusive memory tiering, we develop MATRYOSHKA, a new mechanism that features transactional page migration and page shadowing. MATRYOSHKA removes page migration off the program's critical path and makes migration asynchronous. Evaluations with microbenchmarks and realworld applications show that MATRYOSHKA achieves 6x performance improvement over the state-of-the-art transparent page placement (TPP) approach under memory pressure. We also compare MATRYOSHKA with a recently proposed sampling-based migration approach and demonstrate MATRYOSHKA's strengths and potential weaknesses in various scenarios. Through the evaluations, we discover a serious issue facing all tested approaches, unfortunately including MATRYOSHKA, and call for further research on tiered memory-aware memory allocation. ",
        "title": "MATRYOSHKA: Non-Exclusive Memory Tiering via Transactional Page  Migration",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13156",
        "abstract_url": "http://arxiv.org/abs/2401.13156",
        "authors": [
            {
                "last_name": "Adhikari",
                "first_name": "Bibhas"
            },
            {
                "last_name": "Jha",
                "first_name": "Aryan"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC"
        ],
        "abstract": "  In this paper we develop a classical algorithm of complexity $O(2^n)$ to simulate parametrized quantum circuits (PQCs) of $n$ qubits. The algorithm is developed by finding $2$-sparse unitary matrices of order $2^n$ explicitly corresponding to any single-qubit and two-qubit control gates in an $n$-qubit system. Finally, we determine analytical expression of Hamiltonians for any such gate and consequently a local Hamiltonian decomposition of any PQC is obtained. All results are validated with numerical simulations. ",
        "title": "Local Hamiltonian decomposition and classical simulation of parametrized  quantum circuits",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13157",
        "abstract_url": "http://arxiv.org/abs/2401.13157",
        "authors": [
            {
                "last_name": "Coskunuzer",
                "first_name": "Baris"
            },
            {
                "last_name": "Segovia-Dominguez",
                "first_name": "Ignacio"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuzhou"
            },
            {
                "last_name": "Gel",
                "first_name": "Yulia R."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Learning time-evolving objects such as multivariate time series and dynamic networks requires the development of novel knowledge representation mechanisms and neural network architectures, which allow for capturing implicit time-dependent information contained in the data. Such information is typically not directly observed but plays a key role in the learning task performance. In turn, lack of time dimension in knowledge encoding mechanisms for time-dependent data leads to frequent model updates, poor learning performance, and, as a result, subpar decision-making. Here we propose a new approach to a time-aware knowledge representation mechanism that notably focuses on implicit time-dependent topological information along multiple geometric dimensions. In particular, we propose a new approach, named \\textit{Temporal MultiPersistence} (TMP), which produces multidimensional topological fingerprints of the data by using the existing single parameter topological summaries. The main idea behind TMP is to merge the two newest directions in topological representation learning, that is, multi-persistence which simultaneously describes data shape evolution along multiple key parameters, and zigzag persistence to enable us to extract the most salient data shape information over time. We derive theoretical guarantees of TMP vectorizations and show its utility, in application to forecasting on benchmark traffic flow, Ethereum blockchain, and electrocardiogram datasets, demonstrating the competitive performance, especially, in scenarios of limited data records. In addition, our TMP method improves the computational efficiency of the state-of-the-art multipersistence summaries up to 59.5 times. ",
        "title": "Time-Aware Knowledge Representations of Dynamic Objects with  Multidimensional Persistence",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13160",
        "abstract_url": "http://arxiv.org/abs/2401.13160",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Ke"
            },
            {
                "last_name": "Jiang",
                "first_name": "Heinrich"
            },
            {
                "last_name": "Rostamizadeh",
                "first_name": "Afshin"
            },
            {
                "last_name": "Chakrabarti",
                "first_name": "Ayan"
            },
            {
                "last_name": "DeSalvo",
                "first_name": "Giulia"
            },
            {
                "last_name": "Kagy",
                "first_name": "Jean-Fran\u00e7ois"
            },
            {
                "last_name": "Karydas",
                "first_name": "Lazaros"
            },
            {
                "last_name": "Citovsky",
                "first_name": "Gui"
            },
            {
                "last_name": "Kumar",
                "first_name": "Sanjiv"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial $\\tau$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50% reduction in pre-training iterations and 40% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor results in significantly improved downstream benchmark performance. ",
        "title": "SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced  Token Detection",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13161",
        "abstract_url": "http://arxiv.org/abs/2401.13161",
        "authors": [
            {
                "last_name": "Ayres",
                "first_name": "Luciano Carvalho"
            },
            {
                "last_name": "Borsoi",
                "first_name": "Ricardo Augusto"
            },
            {
                "last_name": "Bermudez",
                "first_name": "Jos\u00e9 Carlos Moreira"
            },
            {
                "last_name": "de Almeida",
                "first_name": "S\u00e9rgio Jos\u00e9 Melo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In hyperspectral sparse unmixing, a successful approach employs spectral bundles to address the variability of the endmembers in the spatial domain. However, the regularization penalties usually employed aggregate substantial computational complexity, and the solutions are very noise-sensitive. We generalize a multiscale spatial regularization approach to solve the unmixing problem by incorporating group sparsity-inducing mixed norms. Then, we propose a noise-robust method that can take advantage of the bundle structure to deal with endmember variability while ensuring inter- and intra-class sparsity in abundance estimation with reasonable computational cost. We also present a general heuristic to select the \\emph{most representative} abundance estimation over multiple runs of the unmixing process, yielding a solution that is robust and highly reproducible. Experiments illustrate the robustness and consistency of the results when compared to related methods. ",
        "title": "A Generalized Multiscale Bundle-Based Hyperspectral Sparse Unmixing  Algorithm",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13165",
        "abstract_url": "http://arxiv.org/abs/2401.13165",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Sourojit"
            },
            {
                "last_name": "Chatterjee",
                "first_name": "Srishti"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This chapter focuses on gender-related errors in machine translation (MT) in the context of low-resource languages. We begin by explaining what low-resource languages are, examining the inseparable social and computational factors that create such linguistic hierarchies. We demonstrate through a case study of our mother tongue Bengali, a global language spoken by almost 300 million people but still classified as low-resource, how gender is assumed and inferred in translations to and from the high(est)-resource English when no such information is provided in source texts. We discuss the postcolonial and societal impacts of such errors leading to linguistic erasure and representational harms, and conclude by discussing potential solutions towards uplifting languages by providing them more agency in MT conversations. ",
        "title": "Misgendering and Assuming Gender in Machine Translation when Working  with Low-Resource Languages",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13169",
        "abstract_url": "http://arxiv.org/abs/2401.13169",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinchen"
            },
            {
                "last_name": "Hu",
                "first_name": "Ruida"
            },
            {
                "last_name": "Gao",
                "first_name": "Cuiyun"
            },
            {
                "last_name": "Wen",
                "first_name": "Xin-Cheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Yujia"
            },
            {
                "last_name": "Liao",
                "first_name": "Qing"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "SE"
        ],
        "abstract": "  Open-Source Software (OSS) vulnerabilities bring great challenges to the software security and pose potential risks to our society. Enormous efforts have been devoted into automated vulnerability detection, among which deep learning (DL)-based approaches have proven to be the most effective. However, the current labeled data present the following limitations: (1) \\textbf{Tangled Patches}: Developers may submit code changes unrelated to vulnerability fixes within patches, leading to tangled patches. (2) \\textbf{Lacking Inter-procedural Vulnerabilities}: The existing vulnerability datasets typically contain function-level and file-level vulnerabilities, ignoring the relations between functions, thus rendering the approaches unable to detect the inter-procedural vulnerabilities. (3) \\textbf{Outdated Patches}: The existing datasets usually contain outdated patches, which may bias the model during training.   To address the above limitations, in this paper, we propose an automated data collection framework and construct the first repository-level high-quality vulnerability dataset named \\textbf{ReposVul}. The proposed framework mainly contains three modules: (1) A vulnerability untangling module, aiming at distinguishing vulnerability-fixing related code changes from tangled patches, in which the Large Language Models (LLMs) and static analysis tools are jointly employed. (2) A multi-granularity dependency extraction module, aiming at capturing the inter-procedural call relationships of vulnerabilities, in which we construct multiple-granularity information for each vulnerability patch, including repository-level, file-level, function-level, and line-level. (3) A trace-based filtering module, aiming at filtering the outdated patches, which leverages the file path trace-based filter and commit time trace-based filter to construct an up-to-date dataset. ",
        "title": "A Repository-Level Dataset For Detecting, Classifying and Repairing  Software Vulnerabilities",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13170",
        "abstract_url": "http://arxiv.org/abs/2401.13170",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zongxia"
            },
            {
                "last_name": "Mondal",
                "first_name": "Ishani"
            },
            {
                "last_name": "Liang",
                "first_name": "Yijun"
            },
            {
                "last_name": "Nghiem",
                "first_name": "Huy"
            },
            {
                "last_name": "Boyd-Graber",
                "first_name": "Jordan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evaluate answer correctness in accordance with adopted expert AE rules that are more aligned with human judgments. ",
        "title": "CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert  Judgments For Open-Domain Question Answering",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13171",
        "abstract_url": "http://arxiv.org/abs/2401.13171",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Tailin"
            },
            {
                "last_name": "Maruyama",
                "first_name": "Takashi"
            },
            {
                "last_name": "Wei",
                "first_name": "Long"
            },
            {
                "last_name": "Zhang",
                "first_name": "Tao"
            },
            {
                "last_name": "Du",
                "first_name": "Yilun"
            },
            {
                "last_name": "Iaccarino",
                "first_name": "Gianluca"
            },
            {
                "last_name": "Leskovec",
                "first_name": "Jure"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CE"
        ],
        "abstract": "  Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learned diffusion model at test time, our method allows us to design initial states and boundary shapes that are more complex than those in the training data. Our method outperforms state-of-the-art neural inverse design method by an average of 41.5% in prediction MAE and 14.3% in design objective for the N-body dataset and discovers formation flying to minimize drag in the multi-airfoil design task. Project website and code can be found at https://github.com/AI4Science-WestlakeU/cindm. ",
        "title": "Compositional Generative Inverse Design",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13172",
        "abstract_url": "http://arxiv.org/abs/2401.13172",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Haotian"
            },
            {
                "last_name": "Wang",
                "first_name": "Fanyi"
            },
            {
                "last_name": "Wang",
                "first_name": "Yaonong"
            },
            {
                "last_name": "Hu",
                "first_name": "Laifeng"
            },
            {
                "last_name": "Xu",
                "first_name": "Jingwei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhiwang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the field of autonomous driving, online high-definition (HD) map reconstruction is crucial for planning tasks. Recent research has developed several high-performance HD map reconstruction models to meet this necessity. However, the point sequences within the instance vectors may be jittery or jagged due to prediction bias, which can impact subsequent tasks. Therefore, this paper proposes the Anti-disturbance Map reconstruction framework (ADMap). To mitigate point-order jitter, the framework consists of three modules: Multi-Scale Perception Neck, Instance Interactive Attention (IIA), and Vector Direction Difference Loss (VDDL). By exploring the point-order relationships between and within instances in a cascading manner, the model can monitor the point-order prediction process more effectively. ADMap achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive results demonstrate its ability to produce stable and reliable map elements in complex and changing driving scenarios. Code and more demos are available at https://github.com/hht1996ok/ADMap. ",
        "title": "ADMap: Anti-disturbance framework for reconstructing online vectorized  HD map",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13174",
        "abstract_url": "http://arxiv.org/abs/2401.13174",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Dong"
            },
            {
                "last_name": "Dong",
                "first_name": "Pingcheng"
            },
            {
                "last_name": "Hu",
                "first_name": "Xinting"
            },
            {
                "last_name": "Chen",
                "first_name": "Long"
            },
            {
                "last_name": "Cheng",
                "first_name": "Kwang-Ting"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, it has been revealed that small semantic segmentation (SS) models exhibit a tendency to make errors in maintaining boundary region completeness and preserving target region connectivity, despite their effective segmentation of the main object regions. To address these errors, we propose a targeted boundary and relation distillation (BRD) strategy using knowledge distillation from large teacher models to small student models. Specifically, the boundary distillation extracts explicit object boundaries from the hierarchical feature maps of the backbone network, subsequently enhancing the student model's mask quality in boundary regions. Concurrently, the relation distillation transfers implicit relations from the teacher model to the student model using pixel-level self-relation as a bridge, ensuring that the student's mask has strong target region connectivity. The proposed BRD is designed concretely for SS and is characterized by simplicity and efficiency. Through experimental evaluations on multiple SS datasets, including Pascal VOC 2012, Cityscapes, ADE20K, and COCO-Stuff 10K, we demonstrated that BRD significantly surpasses the current methods without increasing the inference costs, generating crisp region boundaries and smooth connecting regions that are challenging for small models. ",
        "title": "Boundary and Relation Distillation for Semantic Segmentation",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13177",
        "abstract_url": "http://arxiv.org/abs/2401.13177",
        "authors": [
            {
                "last_name": "Taraghi",
                "first_name": "Mina"
            },
            {
                "last_name": "Dorcelus",
                "first_name": "Gianolli"
            },
            {
                "last_name": "Foundjem",
                "first_name": "Armstrong"
            },
            {
                "last_name": "Tambon",
                "first_name": "Florian"
            },
            {
                "last_name": "Khomh",
                "first_name": "Foutse"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CY",
            "LG"
        ],
        "abstract": "  The ubiquity of large-scale Pre-Trained Models (PTMs) is on the rise, sparking interest in model hubs, and dedicated platforms for hosting PTMs. Despite this trend, a comprehensive exploration of the challenges that users encounter and how the community leverages PTMs remains lacking. To address this gap, we conducted an extensive mixed-methods empirical study by focusing on discussion forums and the model hub of HuggingFace, the largest public model hub. Based on our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community. We then conduct a quantitative study to track model-type trends and model documentation evolution over time. Our findings highlight prevalent challenges such as limited guidance for beginner users, struggles with model output comprehensibility in training or inference, and a lack of model understanding. We also identified interesting trends among models where some models maintain high upload rates despite a decline in topics related to them. Additionally, we found that despite the introduction of model documentation tools, its quantity has not increased over time, leading to difficulties in model comprehension and selection among users. Our study sheds light on new challenges in reusing PTMs that were not reported before and we provide recommendations for various stakeholders involved in PTM reuse. ",
        "title": "Deep Learning Model Reuse in the HuggingFace Community: Challenges,  Benefit and Trends",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13178",
        "abstract_url": "http://arxiv.org/abs/2401.13178",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Chang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junlei"
            },
            {
                "last_name": "Zhu",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Yang",
                "first_name": "Cheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Yujiu"
            },
            {
                "last_name": "Jin",
                "first_name": "Yaohui"
            },
            {
                "last_name": "Lan",
                "first_name": "Zhenzhong"
            },
            {
                "last_name": "Kong",
                "first_name": "Lingpeng"
            },
            {
                "last_name": "He",
                "first_name": "Junxian"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a significant step towards demystifying agent behaviors and accelerating the development of stronger LLM agents. ",
        "title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13182",
        "abstract_url": "http://arxiv.org/abs/2401.13182",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Zelong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This letter proposes a market-clearing-based locational marginal carbon emission (LMCE) metric to assess the marginal carbon emission effect of nodal load demand. Unlike the prevalent carbon emission flow (CEF) method that relies on a hypothetical power-flow tracking process, the proposed LMCE metric depends on a novel sensitivity analysis of market-clearing results, capable of revealing both energy-dependent and network-dependent impacts on emissions. Additionally, we introduce a locational average carbon emission (LACE) metric, derived from LMCE, to effectively measure the general emission effect. It offers insights into demand-side carbon emission effects, such as a negative LMCE and LACE indicating emission reduction even as load increases. It can also prevent excessive demand-side emission allocations. Overall, the proposed method provides a clear perspective for the ongoing decarbonization policies. ",
        "title": "A Market-Clearing-based Sensitivity Model for Locational Marginal and  Average Carbon Emission",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13185",
        "abstract_url": "http://arxiv.org/abs/2401.13185",
        "authors": [
            {
                "last_name": "Engstr\u00f8m",
                "first_name": "Ole-Christian Galbo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DS"
        ],
        "abstract": "  Cross-validation is a widely used technique for assessing the performance of predictive models on unseen data. Many predictive models, such as Kernel-Based Partial Least-Squares (PLS) models, require the computation of $\\mathbf{X}^{\\mathbf{T}}\\mathbf{X}$ and $\\mathbf{X}^{\\mathbf{T}}\\mathbf{Y}$ using only training set samples from the input and output matrices, $\\mathbf{X}$ and $\\mathbf{Y}$, respectively. In this work, we present three algorithms that efficiently compute these matrices. The first one allows no column-wise preprocessing. The second one allows column-wise centering around the training set means. The third one allows column-wise centering and column-wise scaling around the training set means and standard deviations. Demonstrating correctness and superior computational complexity, they offer significant cross-validation speedup compared with straight-forward cross-validation and previous work on fast cross-validation - all without data leakage. Their suitability for parallelization is highlighted with an open-source Python implementation combining our algorithms with Improved Kernel PLS. ",
        "title": "Shortcutting Cross-Validation: Efficiently Deriving Column-Wise Centered  and Scaled Training Set $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and  $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ Without Full Recomputation of Matrix  Products or Statistical Moments",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13190",
        "abstract_url": "http://arxiv.org/abs/2401.13190",
        "authors": [
            {
                "last_name": "Seo",
                "first_name": "Joohwan"
            },
            {
                "last_name": "Prakash",
                "first_name": "Nikhil Potu Surya"
            },
            {
                "last_name": "Choi",
                "first_name": "Jongeun"
            },
            {
                "last_name": "Horowitz",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper, a comparison analysis between geometric impedance controls (GICs) derived from two different potential functions on SE(3) for robotic manipulators is presented. The first potential function is defined on the Lie group, utilizing the Frobenius norm of the configuration error matrix. The second potential function is defined utilizing the Lie algebra, i.e., log-map of the configuration error. Using a differential geometric approach, the detailed derivation of the distance metric and potential function on SE(3) is introduced. The GIC laws are respectively derived from the two potential functions, followed by extensive comparison analyses. In the qualitative analysis, the properties of the error function and control laws are analyzed, while the performances of the controllers are quantitatively compared using numerical simulation. ",
        "title": "A Comparison Between Lie Group- and Lie Algebra- Based Potential  Functions for Geometric Impedance Control",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13191",
        "abstract_url": "http://arxiv.org/abs/2401.13191",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuanming"
            },
            {
                "last_name": "Kim",
                "first_name": "Gwantae"
            },
            {
                "last_name": "Kwak",
                "first_name": "Jeong-gi"
            },
            {
                "last_name": "Ku",
                "first_name": "Bon-hwa"
            },
            {
                "last_name": "Ko",
                "first_name": "Hanseok"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, deep learning-based facial landmark detection for in-the-wild faces has achieved significant improvement. However, there are still challenges in face landmark detection in other domains (e.g. cartoon, caricature, etc). This is due to the scarcity of extensively annotated training data. To tackle this concern, we design a two-stage training approach that effectively leverages limited datasets and the pre-trained diffusion model to obtain aligned pairs of landmarks and face in multiple domains. In the first stage, we train a landmark-conditioned face generation model on a large dataset of real faces. In the second stage, we fine-tune the above model on a small dataset of image-landmark pairs with text prompts for controlling the domain. Our new designs enable our method to generate high-quality synthetic paired datasets from multiple domains while preserving the alignment between landmarks and facial features. Finally, we fine-tuned a pre-trained face landmark detection model on the synthetic dataset to achieve multi-domain face landmark detection. Our qualitative and quantitative results demonstrate that our method outperforms existing methods on multi-domain face landmark detection. ",
        "title": "Towards Multi-domain Face Landmark Detection with Synthetic Data from  Diffusion model",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13192",
        "abstract_url": "http://arxiv.org/abs/2401.13192",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Zhelin"
            },
            {
                "last_name": "Mrad",
                "first_name": "Rami"
            },
            {
                "last_name": "Jiao",
                "first_name": "Runxian"
            },
            {
                "last_name": "Huang",
                "first_name": "Guan"
            },
            {
                "last_name": "Shan",
                "first_name": "Jun"
            },
            {
                "last_name": "Chu",
                "first_name": "Shibing"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuanping"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the conventional substitution or experience-based discovery. ",
        "title": "Generative Design of Crystal Structures by Point Cloud Representations  and Diffusion Model",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13193",
        "abstract_url": "http://arxiv.org/abs/2401.13193",
        "authors": [
            {
                "last_name": "Kang",
                "first_name": "Minsoo"
            },
            {
                "last_name": "Kang",
                "first_name": "Minkoo"
            },
            {
                "last_name": "Kim",
                "first_name": "Suhyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, our idea is to mitigate the problem of over-reliance on strong filters by substituting highly activated features. To this end, we present a novel method called Catch-up Mix, which provides learning opportunities to a wide range of filters during training, focusing on filters that may lag behind. By mixing activation maps with relatively lower norms, Catch-up Mix promotes the development of more diverse representations and reduces reliance on a small subset of filters. Experimental results demonstrate the superiority of our method in various vision classification datasets, providing enhanced robustness. ",
        "title": "Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13196",
        "abstract_url": "http://arxiv.org/abs/2401.13196",
        "authors": [
            {
                "last_name": "Shakeri",
                "first_name": "Rezgar"
            },
            {
                "last_name": "Ghaffari",
                "first_name": "Leila"
            },
            {
                "last_name": "Stengel",
                "first_name": "Karen"
            },
            {
                "last_name": "Thompson",
                "first_name": "Jeremy L."
            },
            {
                "last_name": "Brown",
                "first_name": "Jed"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  A backward stable numerical calculation of a function with condition number $\\kappa$ will have a relative accuracy of $\\kappa\\epsilon_{\\text{machine}}$. Standard formulations and software implementations of finite-strain elastic materials models make use of the deformation gradient $\\boldsymbol F = I + \\partial \\boldsymbol u/\\partial \\boldsymbol X$ and Cauchy-Green tensors. These formulations are not numerically stable, leading to loss of several digits of accuracy when used in the small strain regime, and often precluding the use of single precision floating point arithmetic. We trace the source of this instability to specific points of numerical cancellation, interpretable as ill-conditioned steps. We show how to compute various strain measures in a stable way and how to transform common constitutive models to their stable representations, formulated in either initial or current configuration. The stable formulations all provide accuracy of order $\\epsilon_{\\text{machine}}$. In many cases, the stable formulations have elegant representations in terms of appropriate strain measures and offer geometric intuition that is lacking in their standard representation. We show that algorithmic differentiation can stably compute stresses so long as the strain energy is expressed stably, and give principles for stable computation that can be applied to inelastic materials. ",
        "title": "Stable numerics for finite-strain elasticity",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13197",
        "abstract_url": "http://arxiv.org/abs/2401.13197",
        "authors": [
            {
                "last_name": "Vyas",
                "first_name": "Tejas"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Mohsena"
            },
            {
                "last_name": "Xiao",
                "first_name": "Xiaojiao"
            },
            {
                "last_name": "Claeys",
                "first_name": "Mathias"
            },
            {
                "last_name": "Ong",
                "first_name": "G\u00e9raldine"
            },
            {
                "last_name": "Wang",
                "first_name": "Guanghui"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Mitral Transcatheter Edge-to-Edge Repair (mTEER) is a medical procedure utilized for the treatment of mitral valve disorders. However, predicting the outcome of the procedure poses a significant challenge. This paper makes the first attempt to harness classical machine learning (ML) and deep learning (DL) techniques for predicting mitral valve mTEER surgery outcomes. To achieve this, we compiled a dataset from 467 patients, encompassing labeled echocardiogram videos and patient reports containing Transesophageal Echocardiography (TEE) measurements detailing Mitral Valve Repair (MVR) treatment outcomes. Leveraging this dataset, we conducted a benchmark evaluation of six ML algorithms and two DL models. The results underscore the potential of ML and DL in predicting mTEER surgery outcomes, providing insight for future investigation and advancements in this domain. ",
        "title": "Predicting Mitral Valve mTEER Surgery Outcomes Using Machine Learning  and Deep Learning Techniques",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13199",
        "abstract_url": "http://arxiv.org/abs/2401.13199",
        "authors": [
            {
                "last_name": "Jayatilaka",
                "first_name": "Asangi"
            },
            {
                "last_name": "Arachchilage",
                "first_name": "Nalin Asanka Gamagedara"
            },
            {
                "last_name": "Babar",
                "first_name": "Muhammad Ali"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "CY",
            "HC"
        ],
        "abstract": "  Despite technical and non-technical countermeasures, humans continue to be tricked by phishing emails. How users make email response decisions is a missing piece in the puzzle to identifying why people still fall for phishing emails. We conducted an empirical study using a think-aloud method to investigate how people make 'response decisions' while reading emails. The grounded theory analysis of the in-depth qualitative data has enabled us to identify different elements of email users' decision-making that influence their email response decisions. Furthermore, we developed a theoretical model that explains how people could be driven to respond to emails based on the identified elements of users' email decision-making processes and the relationships uncovered from the data. The findings provide deeper insights into phishing email susceptibility due to people's email response decision-making behavior. We also discuss the implications of our findings for designers and researchers working in anti-phishing training, education, and awareness interventions ",
        "title": "Why People Still Fall for Phishing Emails: An Empirical Investigation  into How Users Make Email Response Decisions",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13200",
        "abstract_url": "http://arxiv.org/abs/2401.13200",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xikun"
            },
            {
                "last_name": "Song",
                "first_name": "Dongjin"
            },
            {
                "last_name": "Chen",
                "first_name": "Yixin"
            },
            {
                "last_name": "Tao",
                "first_name": "Dacheng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\\mathcal{O}(nd^L)$ to $\\mathcal{O}(n)$~\\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \\textit{Topology-aware Embeddings} (TEs), which compress ego-subgraphs into compact vectors (i.e., TEs) to reduce the memory consumption. Based on this framework, we discover a unique \\textit{pseudo-training effect} in continual learning on expanding graphs and this effect motivates us to develop a novel \\textit{coverage maximization sampling} strategy that can enhance the performance with a tight memory budget. Thorough empirical studies demonstrate that, by tackling the memory explosion problem and incorporating topological information into memory replay, PDGNNs with TEM significantly outperform state-of-the-art techniques, especially in the challenging class-incremental setting. ",
        "title": "Topology-aware Embedding Memory for Learning on Expanding Graphs",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13201",
        "abstract_url": "http://arxiv.org/abs/2401.13201",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Shan"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongfei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instruction, a simple approach that leverages the essence ability of LLMs to continue writing, avoiding complex and diverse instruction design. Secondly, we proposed DirectReID, which effectively employs the latent image feature vectors of images outputted by LLMs in ReID tasks. The experimental results demonstrate the superiority of our method. We will open-source the code on GitHub. ",
        "title": "MLLMReID: Multimodal Large Language Model-based Person Re-identification",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13202",
        "abstract_url": "http://arxiv.org/abs/2401.13202",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jiakun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenyi"
            },
            {
                "last_name": "Poor",
                "first_name": "H. Vincent"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In practical communication systems, knowledge of channel models is often absent, and consequently, transceivers need be designed based on empirical data. In this work, we study data-driven approaches to reliably choosing decoding metrics and code rates that facilitate reliable communication over unknown discrete memoryless channels (DMCs). Our analysis is inspired by the PAC learning theory and does not rely on any assumptions on the statistical characteristics of DMCs. We show that a naive plug-in algorithm for choosing decoding metrics is likely to fail for finite training sets. We propose an alternative algorithm called the virtual sample algorithm and establish a non-asymptotic lower bound on its performance. The virtual sample algorithm is then used as a building block for constructing a learning algorithm that chooses a decoding metric and a code rate using which a transmitter and a receiver can reliably communicate at a rate arbitrarily close to the channel mutual information. Therefore, we conclude that DMCs are PAC learnable. ",
        "title": "PAC Learnability for Reliable Communication over Discrete Memoryless  Channels",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13203",
        "abstract_url": "http://arxiv.org/abs/2401.13203",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yunfan"
            },
            {
                "last_name": "Huang",
                "first_name": "Hong"
            },
            {
                "last_name": "Xiong",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Shen",
                "first_name": "Zhiqi"
            },
            {
                "last_name": "Lin",
                "first_name": "Guosheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            },
            {
                "last_name": "Vun",
                "first_name": "Nicholas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Controllable 3D indoor scene synthesis stands at the forefront of technological progress, offering various applications like gaming, film, and augmented/virtual reality. The capability to stylize and de-couple objects within these scenarios is a crucial factor, providing an advanced level of control throughout the editing process. This control extends not just to manipulating geometric attributes like translation and scaling but also includes managing appearances, such as stylization. Current methods for scene stylization are limited to applying styles to the entire scene, without the ability to separate and customize individual objects. Addressing the intricacies of this challenge, we introduce a unique pipeline designed for synthesis 3D indoor scenes. Our approach involves strategically placing objects within the scene, utilizing information from professionally designed bounding boxes. Significantly, our pipeline prioritizes maintaining style consistency across multiple objects within the scene, ensuring a cohesive and visually appealing result aligned with the desired aesthetic. The core strength of our pipeline lies in its ability to generate 3D scenes that are not only visually impressive but also exhibit features like photorealism, multi-view consistency, and diversity. These scenes are crafted in response to various natural language prompts, demonstrating the versatility and adaptability of our model. ",
        "title": "Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13205",
        "abstract_url": "http://arxiv.org/abs/2401.13205",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Junlin"
            },
            {
                "last_name": "Lyu",
                "first_name": "Xinchen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Adversarial examples are one critical security threat to various visual applications, where injected human-imperceptible perturbations can confuse the output.Generating transferable adversarial examples in the black-box setting is crucial but challenging in practice. Existing input-diversity-based methods adopt different image transformations, but may be inefficient due to insufficient input diversity and an identical perturbation step size. Motivated by the fact that different image regions have distinctive weights in classification, this paper proposes a black-box adversarial generative framework by jointly designing enhanced input diversity and adaptive step sizes. We design local mixup to randomly mix a group of transformed adversarial images, strengthening the input diversity. For precise adversarial generation, we project the perturbation into the $tanh$ space to relax the boundary constraint. Moreover, the step sizes of different regions can be dynamically adjusted by integrating a second-order momentum.Extensive experiments on ImageNet validate that our framework can achieve superior transferability compared to state-of-the-art baselines. ",
        "title": "Boosting the Transferability of Adversarial Examples via Local Mixup and  Adaptive Step Size",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13206",
        "abstract_url": "http://arxiv.org/abs/2401.13206",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Hyun-Suk"
            },
            {
                "last_name": "Kim",
                "first_name": "Do-Yup"
            },
            {
                "last_name": "Min",
                "first_name": "Kyungsik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper presents a groundbreaking self-improving interference management framework tailored for wireless communications, integrating deep learning with uncertainty quantification to enhance overall system performance. Our approach addresses the computational challenges inherent in traditional optimization-based algorithms by harnessing deep learning models to predict optimal interference management solutions. A significant breakthrough of our framework is its acknowledgment of the limitations inherent in data-driven models, particularly in scenarios not adequately represented by the training dataset. To overcome these challenges, we propose a method for uncertainty quantification, accompanied by a qualifying criterion, to assess the trustworthiness of model predictions. This framework strategically alternates between model-generated solutions and traditional algorithms, guided by a criterion that assesses the prediction credibility based on quantified uncertainties. Experimental results validate the framework's efficacy, demonstrating its superiority over traditional deep learning models, notably in scenarios underrepresented in the training dataset. This work marks a pioneering endeavor in harnessing self-improving deep learning for interference management, through the lens of uncertainty quantification. ",
        "title": "Self-Improving Interference Management Based on Deep Learning With  Uncertainty Quantification",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13209",
        "abstract_url": "http://arxiv.org/abs/2401.13209",
        "authors": [
            {
                "last_name": "Kaufmann",
                "first_name": "Julian M."
            },
            {
                "last_name": "Zahr",
                "first_name": "Matthew J."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In high-order and high-dimensional finite elements, ill-conditioned nodal distributions are often computationally cost-prohibitive. As a result, uniform distributions quickly fall apart. For tensor-product like elements, Gauss-Legendre-Lobatto (GLL) nodal distributions are often used as a substitute. Besides these, other efficient nodal distributions are difficult to create due to a desired symmetry within elements and conformity with neighboring elements. In this paper, we provide a general framework to construct symmetric, well-conditioned, cross-element compatible nodal distributions which can be used for high-order and high-dimensional finite elements. Starting from the inherent symmetries in any potential element, the framework is used to build up nodal groups in a structured and efficient manner utilizing the natural coordinates of each element, while ensuring nodes stay within the elements. By constructing constrained symmetry groups, the vertices, edges, and faces, of all elements are required to conform to their respective lower-dimensional distributions. Optimizing over these groups yields the desired optimized nodal distributions. We demonstrate the strength of this framework by creating and comparing optimized nodal distributions with GLL distributions (in elements such as the line, quadrilateral, and hexahedron), and its robustness by generating optimized nodal distributions for otherwise difficult elements (such as the triangle, tetrahedron, and triangular prism). ",
        "title": "Symmetric, Optimization-based, Cross-element Compatible Nodal  Distributions for High-order Finite Elements",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13210",
        "abstract_url": "http://arxiv.org/abs/2401.13210",
        "authors": [
            {
                "last_name": "Chang",
                "first_name": "Wenjing"
            },
            {
                "last_name": "Liu",
                "first_name": "Kay"
            },
            {
                "last_name": "Ding",
                "first_name": "Kaize"
            },
            {
                "last_name": "Yu",
                "first_name": "Philip S."
            },
            {
                "last_name": "Yu",
                "first_name": "Jianjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SI"
        ],
        "abstract": "  In the web era, graph machine learning has been widely used on ubiquitous graph-structured data. As a pivotal component for bolstering web security and enhancing the robustness of graph-based applications, the significance of graph anomaly detection is continually increasing. While Graph Neural Networks (GNNs) have demonstrated efficacy in supervised and semi-supervised graph anomaly detection, their performance is contingent upon the availability of sufficient ground truth labels. The labor-intensive nature of identifying anomalies from complex graph structures poses a significant challenge in real-world applications. Despite that, the indirect supervision signals from other tasks (e.g., node classification) are relatively abundant. In this paper, we propose a novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE. Firstly, by coupling node classification tasks, MITIGATE obtains the capability to detect out-of-distribution nodes without known anomalies. Secondly, MITIGATE quantifies the informativeness of nodes by the confidence difference across tasks, allowing samples with conflicting predictions to provide informative yet not excessively challenging information for subsequent training. Finally, to enhance the likelihood of selecting representative nodes that are distant from known patterns, MITIGATE adopts a masked aggregation mechanism for distance measurement, considering both inherent features of nodes and current labeled status. Empirical studies on four datasets demonstrate that MITIGATE significantly outperforms the state-of-the-art methods for anomaly detection. Our code is publicly available at: https://github.com/AhaChang/MITIGATE. ",
        "title": "Multitask Active Learning for Graph Anomaly Detection",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13212",
        "abstract_url": "http://arxiv.org/abs/2401.13212",
        "authors": [
            {
                "last_name": "Shen",
                "first_name": "Lulan"
            },
            {
                "last_name": "Edalati",
                "first_name": "Ali"
            },
            {
                "last_name": "Meyer",
                "first_name": "Brett"
            },
            {
                "last_name": "Gross",
                "first_name": "Warren"
            },
            {
                "last_name": "Clark",
                "first_name": "James J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method is based on modification of the training set and making use of the duality between network weights and layer inputs. We call this input space training. The method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct incorrect training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The technique can be straightforwardly applied to refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. The adversarial correction technique also results in enhanced robustness to adversarial attacks. ",
        "title": "AdCorDA: Classifier Refinement via Adversarial Correction and Domain  Adaptation",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13213",
        "abstract_url": "http://arxiv.org/abs/2401.13213",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Miao"
            },
            {
                "last_name": "fryer",
                "first_name": "Zee"
            },
            {
                "last_name": "Colman",
                "first_name": "Ben"
            },
            {
                "last_name": "Shahriyari",
                "first_name": "Ali"
            },
            {
                "last_name": "Bharaj",
                "first_name": "Gaurav"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Machine learning model bias can arise from dataset composition: sensitive features correlated to the learning target disturb the model decision rule and lead to performance differences along the features. Existing de-biasing work captures prominent and delicate image features which are traceable in model latent space, like colors of digits or background of animals. However, using the latent space is not sufficient to understand all dataset feature correlations. In this work, we propose a framework to extract feature clusters in a dataset based on image descriptions, allowing us to capture both subtle and coarse features of the images. The feature co-occurrence pattern is formulated and correlation is measured, utilizing a human-in-the-loop for examination. The analyzed features and correlations are human-interpretable, so we name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive correlations in a dataset, we demonstrate that downstream model bias can be mitigated by adjusting image sampling weights, without requiring a sensitive group label supervision. Experiments show that our method discovers novel biases on multiple classification tasks for two benchmark image datasets, and the intervention outperforms state-of-the-art unsupervised bias mitigation methods. ",
        "title": "Common-Sense Bias Discovery and Mitigation for Classification Tasks",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13214",
        "abstract_url": "http://arxiv.org/abs/2401.13214",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Xiaolin"
            },
            {
                "last_name": "Cheng",
                "first_name": "Junkai"
            },
            {
                "last_name": "Li",
                "first_name": "Aihua"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuhua"
            },
            {
                "last_name": "Lin",
                "first_name": "Zhilong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Recently, methods based on deep learning have been successfully applied to ship detection for synthetic aperture radar (SAR) images. Despite the development of numerous ship detection methodologies, detecting small and coastal ships remains a significant challenge due to the limited features and clutter in coastal environments. For that, a novel adaptive multi-hierarchical attention module (AMAM) is proposed to learn multi-scale features and adaptively aggregate salient features from various feature layers, even in complex environments. Specifically, we first fuse information from adjacent feature layers to enhance the detection of smaller targets, thereby achieving multi-scale feature enhancement. Then, to filter out the adverse effects of complex backgrounds, we dissect the previously fused multi-level features on the channel, individually excavate the salient regions, and adaptively amalgamate features originating from different channels. Thirdly, we present a novel adaptive multi-hierarchical attention network (AMANet) by embedding the AMAM between the backbone network and the feature pyramid network (FPN). Besides, the AMAM can be readily inserted between different frameworks to improve object detection. Lastly, extensive experiments on two large-scale SAR ship detection datasets demonstrate that our AMANet method is superior to state-of-the-art methods. ",
        "title": "AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical  Attention Network",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13216",
        "abstract_url": "http://arxiv.org/abs/2401.13216",
        "authors": [
            {
                "last_name": "Yuan",
                "first_name": "Honglin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Federated Learning (FL), a distributed learning paradigm that scales on-device learning collaboratively, has emerged as a promising approach for decentralized AI applications. Local optimization methods such as Federated Averaging (FedAvg) are the most prominent methods for FL applications. Despite their simplicity and popularity, the theoretical understanding of local optimization methods is far from clear. This dissertation aims to advance the theoretical foundation of local methods in the following three directions.   First, we establish sharp bounds for FedAvg, the most popular algorithm in Federated Learning. We demonstrate how FedAvg may suffer from a notion we call iterate bias, and how an additional third-order smoothness assumption may mitigate this effect and lead to better convergence rates. We explain this phenomenon from a Stochastic Differential Equation (SDE) perspective.   Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc), the first principled acceleration of FedAvg, which provably improves the convergence rate and communication efficiency. Our technique uses on a potential-based perturbed iterate analysis, a novel stability analysis of generalized accelerated SGD, and a strategic tradeoff between acceleration and stability.   Third, we study the Federated Composite Optimization problem, which extends the classic smooth setting by incorporating a shared non-smooth regularizer. We show that direct extensions of FedAvg may suffer from the \"curse of primal averaging,\" resulting in slow convergence. As a solution, we propose a new primal-dual algorithm, Federated Dual Averaging, which overcomes the curse of primal averaging by employing a novel inter-client dual averaging procedure. ",
        "title": "On Principled Local Optimization Methods for Federated Learning",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13218",
        "abstract_url": "http://arxiv.org/abs/2401.13218",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xinliang Frederick"
            },
            {
                "last_name": "Blum",
                "first_name": "Carter"
            },
            {
                "last_name": "Choji",
                "first_name": "Temma"
            },
            {
                "last_name": "Shah",
                "first_name": "Shalin"
            },
            {
                "last_name": "Vempala",
                "first_name": "Alakananda"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Structural extraction of events within discourse is critical since it avails a deeper understanding of communication patterns and behavior trends. Event argument extraction (EAE), at the core of event-centric understanding, is the task of identifying role-specific text spans (i.e., arguments) for a given event. Document-level EAE (DocEAE) focuses on arguments that are scattered across an entire document. In this work, we explore the capabilities of open source Large Language Models (LLMs), i.e., Flan-UL2, for the DocEAE task. To this end, we propose ULTRA, a hierarchical framework that extracts event arguments more cost-effectively -- the method needs as few as 50 annotations and doesn't require hitting costly API endpoints. Further, it alleviates the positional bias issue intrinsic to LLMs. ULTRA first sequentially reads text chunks of a document to generate a candidate argument set, upon which ULTRA learns to drop non-pertinent candidates through self-refinement. We further introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument span. ULTRA outperforms strong baselines, which include strong supervised models and ChatGPT, by 9.8% when evaluated by the exact match (EM) metric. ",
        "title": "ULTRA: Unleash LLMs' Potential for Event Argument Extraction through  Hierarchical Modeling and Pair-wise Refinement",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13219",
        "abstract_url": "http://arxiv.org/abs/2401.13219",
        "authors": [
            {
                "last_name": "Aakur",
                "first_name": "Sathyanarayanan"
            },
            {
                "last_name": "Laguduva",
                "first_name": "Vishalini R."
            },
            {
                "last_name": "Ramamurthy",
                "first_name": "Priyadharsini"
            },
            {
                "last_name": "Ramachandran",
                "first_name": "Akhilesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A species' genetic code or genome encodes valuable evolutionary, biological, and phylogenetic information that aids in species recognition, taxonomic classification, and understanding genetic predispositions like drug resistance and virulence. However, the vast number of potential species poses significant challenges in developing a general-purpose whole genome classification tool. Traditional bioinformatics tools have made notable progress but lack scalability and are computationally expensive. Machine learning-based frameworks show promise but must address the issue of large classification vocabularies with long-tail distributions. In this study, we propose addressing this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a taxonomy-aware embedding space for reasoning and classification. This embedding space captures compositional and phylogenetic relationships of species, enabling predictions in extensive search spaces. We evaluate TEPI using two rigorous zero-shot settings and demonstrate its generalization capabilities qualitatively on curated, large-scale, publicly sourced data. ",
        "title": "TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled  Zero-shot Genome Classification",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13220",
        "abstract_url": "http://arxiv.org/abs/2401.13220",
        "authors": [
            {
                "last_name": "Na",
                "first_name": "Saiyang"
            },
            {
                "last_name": "Guo",
                "first_name": "Yuzhi"
            },
            {
                "last_name": "Jiang",
                "first_name": "Feng"
            },
            {
                "last_name": "Ma",
                "first_name": "Hehuan"
            },
            {
                "last_name": "Huang",
                "first_name": "Junzhou"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In the rapidly evolving field of AI research, foundational models like BERT and GPT have significantly advanced language and vision tasks. The advent of pretrain-prompting models such as ChatGPT and Segmentation Anything Model (SAM) has further revolutionized image segmentation. However, their applications in specialized areas, particularly in nuclei segmentation within medical imaging, reveal a key challenge: the generation of high-quality, informative prompts is as crucial as applying state-of-the-art (SOTA) fine-tuning techniques on foundation models. To address this, we introduce Segment Any Cell (SAC), an innovative framework that enhances SAM specifically for nuclei segmentation. SAC integrates a Low-Rank Adaptation (LoRA) within the attention layer of the Transformer to improve the fine-tuning process, outperforming existing SOTA methods. It also introduces an innovative auto-prompt generator that produces effective prompts to guide segmentation, a critical factor in handling the complexities of nuclei segmentation in biomedical imaging. Our extensive experiments demonstrate the superiority of SAC in nuclei segmentation tasks, proving its effectiveness as a tool for pathologists and researchers. Our contributions include a novel prompt generation strategy, automated adaptability for diverse segmentation tasks, the innovative application of Low-Rank Attention Adaptation in SAM, and a versatile framework for semantic segmentation challenges. ",
        "title": "Segment Any Cell: A SAM-based Auto-prompting Fine-tuning Framework for  Nuclei Segmentation",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13221",
        "abstract_url": "http://arxiv.org/abs/2401.13221",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Yimin"
            },
            {
                "last_name": "Gao",
                "first_name": "Nanxi"
            },
            {
                "last_name": "Shan",
                "first_name": "Zhongyun"
            },
            {
                "last_name": "Chao",
                "first_name": "Fei"
            },
            {
                "last_name": "Ji",
                "first_name": "Rongrong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In contrast to traditional image restoration methods, all-in-one image restoration techniques are gaining increased attention for their ability to restore images affected by diverse and unknown corruption types and levels. However, contemporary all-in-one image restoration methods omit task-wise difficulties and employ the same networks to reconstruct images afflicted by diverse degradations. This practice leads to an underestimation of the task correlations and suboptimal allocation of computational resources. To elucidate task-wise complexities, we introduce a novel concept positing that intricate image degradation can be represented in terms of elementary degradation. Building upon this foundation, we propose an innovative approach, termed the Unified-Width Adaptive Dynamic Network (U-WADN), consisting of two pivotal components: a Width Adaptive Backbone (WAB) and a Width Selector (WS). The WAB incorporates several nested sub-networks with varying widths, which facilitates the selection of the most apt computations tailored to each task, thereby striking a balance between accuracy and computational efficiency during runtime. For different inputs, the WS automatically selects the most appropriate sub-network width, taking into account both task-specific and sample-specific complexities. Extensive experiments across a variety of image restoration tasks demonstrate that the proposed U-WADN achieves better performance while simultaneously reducing up to 32.3\\% of FLOPs and providing approximately 15.7\\% real-time acceleration. The code has been made available at \\url{https://github.com/xuyimin0926/U-WADN}. ",
        "title": "Unified-Width Adaptive Dynamic Network for All-In-One Image Restoration",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13223",
        "abstract_url": "http://arxiv.org/abs/2401.13223",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Fengbin"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Feng",
                "first_name": "Fuli"
            },
            {
                "last_name": "Wang",
                "first_name": "Chao"
            },
            {
                "last_name": "Li",
                "first_name": "Moxin"
            },
            {
                "last_name": "Chua",
                "first_name": "Tat-Seng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM model can outperform all baseline models, including the previous best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks. We hope our work can serve as a pioneering example of specializing smaller language models for specific tasks. ",
        "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over  Tabular and Textual Data",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13227",
        "abstract_url": "http://arxiv.org/abs/2401.13227",
        "authors": [
            {
                "last_name": "Bi",
                "first_name": "Baolong"
            },
            {
                "last_name": "Liu",
                "first_name": "Shenghua"
            },
            {
                "last_name": "Wang",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Mei",
                "first_name": "Lingrui"
            },
            {
                "last_name": "Chen",
                "first_name": "Xueqi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG",
            "SI"
        ],
        "abstract": "  Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselines, highlighting its remarkable performance in link prediction tasks on large-scale graphs. ",
        "title": "Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large  Language Models",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13229",
        "abstract_url": "http://arxiv.org/abs/2401.13229",
        "authors": [
            {
                "last_name": "Alcoforado",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Ferraz",
                "first_name": "Thomas Palmeira"
            },
            {
                "last_name": "Okamura",
                "first_name": "Lucas Hideki"
            },
            {
                "last_name": "Fama",
                "first_name": "Israel Campos"
            },
            {
                "last_name": "Lavado",
                "first_name": "Arnold Moya"
            },
            {
                "last_name": "Bueno",
                "first_name": "B\u00e1rbara Dias"
            },
            {
                "last_name": "Veloso",
                "first_name": "Bruno"
            },
            {
                "last_name": "Costa",
                "first_name": "Anna Helena Reali"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To address these issues, this paper contributes an automatic and informed data selection architecture to build a small dataset for few-shot learning. Our proposal minimizes the quantity and maximizes diversity of data selected for human annotation, while improving model performance. ",
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to  Optimize Human Annotation and Few-Shot Learning",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13231",
        "abstract_url": "http://arxiv.org/abs/2401.13231",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Suning"
            },
            {
                "last_name": "Chen",
                "first_name": "Boyuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Huazhe"
            },
            {
                "last_name": "Sitzmann",
                "first_name": "Vincent"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG"
        ],
        "abstract": "  Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine algorithm on DittoGym and demonstrate robots that learn to change their morphology several times within a sequence, uniquely enabled by our RL algorithm. More results are available at https://dittogym.github.io. ",
        "title": "DittoGym: Learning to Control Soft Shape-Shifting Robots",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13232",
        "abstract_url": "http://arxiv.org/abs/2401.13232",
        "authors": [
            {
                "last_name": "Muramatsu",
                "first_name": "Jun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper investigates the general distributed lossless/lossy source coding formulated by Jana and Blahut. Their multi-letter rate-distortion region, an alternative to the region derived by Yang and Qin, is characterized by entropy functions for arbitrary general correlated sources. Achievability is shown by constructing a code based on constrained-random number generators. ",
        "title": "Distributed Source Coding Using Constrained-Random-Number Generators",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13236",
        "abstract_url": "http://arxiv.org/abs/2401.13236",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yuchang"
            },
            {
                "last_name": "Kountouris",
                "first_name": "Marios"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Federated learning (FL) has attracted vivid attention as a privacy-preserving distributed learning framework. In this work, we focus on cross-silo FL, where clients become the model owners after training and are only concerned about the model's generalization performance on their local data. Due to the data heterogeneity issue, asking all the clients to join a single FL training process may result in model performance degradation. To investigate the effectiveness of collaboration, we first derive a generalization bound for each client when collaborating with others or when training independently. We show that the generalization performance of a client can be improved only by collaborating with other clients that have more training data and similar data distribution. Our analysis allows us to formulate a client utility maximization problem by partitioning clients into multiple collaborating groups. A hierarchical clustering-based collaborative training (HCCT) scheme is then proposed, which does not need to fix in advance the number of groups. We further analyze the convergence of HCCT for general non-convex loss functions which unveils the effect of data similarity among clients. Extensive simulations show that HCCT achieves better generalization performance than baseline schemes, whereas it degenerates to independent training and conventional FL in specific scenarios. ",
        "title": "How to Collaborate: Towards Maximizing the Generalization Performance in  Cross-Silo Federated Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13237",
        "abstract_url": "http://arxiv.org/abs/2401.13237",
        "authors": [
            {
                "last_name": "Sasaki",
                "first_name": "Toi"
            },
            {
                "last_name": "Miyahara",
                "first_name": "Hideyuki"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Natural gradient (NG) is an information-geometric optimization method that plays a crucial role, especially in the estimation of parameters for machine learning models like neural networks. To apply NG to quantum systems, the quantum natural gradient (QNG) was introduced and utilized for noisy intermediate-scale devices. Additionally, a mathematically equivalent approach to QNG, known as the stochastic reconfiguration method, has been implemented to enhance the performance of quantum Monte Carlo methods. It is worth noting that these methods are based on the symmetric logarithmic derivative (SLD) metric, which is one of the monotone metrics. So far, monotonicity has been believed to be a guiding principle to construct a geometry in physics. In this paper, we propose generalized QNG by removing the condition of monotonicity. Initially, we demonstrate that monotonicity is a crucial condition for conventional QNG to be optimal. Subsequently, we provide analytical and numerical evidence showing that non-monotone QNG outperforms conventional QNG based on the SLD metric in terms of convergence speed. ",
        "title": "Quantum natural gradient without monotonicity",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13238",
        "abstract_url": "http://arxiv.org/abs/2401.13238",
        "authors": [
            {
                "last_name": "Ray",
                "first_name": "Gourab"
            },
            {
                "last_name": "Sen",
                "first_name": "Arnab"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  We study the minimal spanning arborescence which is the directed analogue of the minimal spanning tree, with a particular focus on its infinite volume limit and its geometric properties. We prove that in a certain large class of transient trees, the infinite volume limit exists almost surely. We also prove that for nonamenable, unimodular graphs, the limit is almost surely one-ended assuming a certain sufficient condition that guarantees the existence of the limit.   This object cannot be studied using well-known algorithms, such as Kruskal's or Prim's algorithm, to sample the minimal spanning tree which has been instrumental in getting analogous results about them (Lyons, Peres, and Schramm). Instead, we use a recursive algorithm due to Chu, Liu, Edmonds, and Bock, which leads to a novel stochastic process which we call the \\emph{loop contracting random walk}. This is similar to the well-known and widely studied loop erased random walk, except instead of erasing loops we contract them. The full algorithm bears similarities with the celebrated Wilson's algorithm to generate uniform spanning trees and can be seen as a certain limit of the original Wilson's algorithm. ",
        "title": "Minimal spanning arborescence",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13239",
        "abstract_url": "http://arxiv.org/abs/2401.13239",
        "authors": [
            {
                "last_name": "Kagrecha",
                "first_name": "Anmol"
            },
            {
                "last_name": "Marklund",
                "first_name": "Henrik"
            },
            {
                "last_name": "Van Roy",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Jeon",
                "first_name": "Hong Jun"
            },
            {
                "last_name": "Zeckhauser",
                "first_name": "Richard"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "HC"
        ],
        "abstract": "  Common crowdsourcing systems average estimates of a latent quantity of interest provided by many crowdworkers to produce a group estimate. We develop a new approach -- just-predict-others -- that leverages self-supervised learning and a novel aggregation scheme. This approach adapts weights assigned to crowdworkers based on estimates they provided for previous quantities. When skills vary across crowdworkers or their estimates correlate, the weighted sum offers a more accurate group estimate than the average. Existing algorithms such as expectation maximization can, at least in principle, produce similarly accurate group estimates. However, their computational requirements become onerous when complex models, such as neural networks, are required to express relationships among crowdworkers. Just-predict-others accommodates such complexity as well as many other practical challenges. We analyze the efficacy of just-predict-others through theoretical and computational studies. Among other things, we establish asymptotic optimality as the number of engagements per crowdworker grows. ",
        "title": "Adaptive Crowdsourcing Via Self-Supervised Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13244",
        "abstract_url": "http://arxiv.org/abs/2401.13244",
        "authors": [
            {
                "last_name": "Nagy",
                "first_name": "Shaan"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinwoo"
            },
            {
                "last_name": "D'Antoni",
                "first_name": "Loris"
            },
            {
                "last_name": "Reps",
                "first_name": "Thomas"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  Unrealizability logic (UL) was proposed by Kim et al. as the first Hoare-style proof system for proving properties that hold for an infinite set of programs (defined by a regular tree grammar). The goal of our work is to automate reasoning and proof generation for UL. A key ingredient in UL is the notion of nonterminal summaries-inductive facts that characterize recursive nonterminals in the grammar that defines the set of programs. They are analogous to procedure summaries in Hoare logic. The goal of automating UL led us to reformulate the inference rules-in particular, introducing a unified rule for nonterminal summaries, called the rule of adaptation, which draws inspiration from how procedure summaries are handled in Hoare logic. In the same way that verification conditions can be used to synthesize loop invariants for Hoare logic proofs, our reformulation of UL reduces the problem of synthesizing a nonterminal summary to a Syntax-Guided Synthesis problem. We implement Wuldo, the first checker and synthesizer for UL. Wuldo can express proofs beyond the reach of existing tools, including proofs that establish how infinitely many programs behave on infinitely many inputs, and in some cases Wuldo can even synthesize the needed nonterminal summaries. ",
        "title": "Automating Unrealizability Logic: Hoare-style Proof Synthesis for  Infinite Sets of Programs",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13245",
        "abstract_url": "http://arxiv.org/abs/2401.13245",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Qirui"
            },
            {
                "last_name": "Lu",
                "first_name": "Min"
            },
            {
                "last_name": "Lanir",
                "first_name": "Joel"
            },
            {
                "last_name": "Lischinski",
                "first_name": "Dani"
            },
            {
                "last_name": "Cohen-Or",
                "first_name": "Daniel"
            },
            {
                "last_name": "Huang",
                "first_name": "Hui"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Information graphics are pivotal in effective information dissemination and storytelling. However, creating such graphics is extremely challenging for non-professionals, since the design process requires multifaceted skills and comprehensive knowledge. Thus, despite the many available authoring tools, a significant gap remains in enabling non-experts to produce compelling information graphics seamlessly, especially from scratch. Recent breakthroughs show that Large Language Models (LLMs), especially when tool-augmented, can autonomously engage with external tools, making them promising candidates for enabling innovative graphic design applications. In this work, we propose a LLM-centric interface with the agent GraphiMind for automatic generation, recommendation, and composition of information graphics design resources, based on user intent expressed through natural language. Our GraphiMind integrates a Textual Conversational Interface, powered by tool-augmented LLM, with a traditional Graphical Manipulation Interface, streamlining the entire design process from raw resource curation to composition and refinement. Extensive evaluations highlight our tool's proficiency in simplifying the design process, opening avenues for its use by non-professional users. Moreover, we spotlight the potential of LLMs in reshaping the domain of information graphics design, offering a blend of automation, versatility, and user-centric interactivity. ",
        "title": "GraphiMind: LLM-centric Interface for Information Graphics Design",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13246",
        "abstract_url": "http://arxiv.org/abs/2401.13246",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Guoxin"
            },
            {
                "last_name": "Tang",
                "first_name": "Kexin"
            },
            {
                "last_name": "Yang",
                "first_name": "Chao"
            },
            {
                "last_name": "Ye",
                "first_name": "Fuying"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            },
            {
                "last_name": "Qian",
                "first_name": "Yiming"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Elucidating the reasoning process with structured explanations from question to answer is fundamentally crucial, as it significantly enhances the interpretability and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricate structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Meanwhile, existing reinforcement learning (RL)-based methods overlook the structured relationships, impeding RL's potential in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between states. We also introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that SEER significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9% over RL-based methods on EntailmentBank, a 4.4% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance. ",
        "title": "SEER: Facilitating Structured Reasoning and Explanation via  Reinforcement Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13247",
        "abstract_url": "http://arxiv.org/abs/2401.13247",
        "authors": [
            {
                "last_name": "Moon",
                "first_name": "Erina Seh-Young"
            },
            {
                "last_name": "Guha",
                "first_name": "Shion"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Homelessness is a humanitarian challenge affecting an estimated 1.6 billion people worldwide. In the face of rising homeless populations in developed nations and a strain on social services, government agencies are increasingly adopting data-driven models to determine one's risk of experiencing homelessness and assigning scarce resources to those in need. We conducted a systematic literature review of 57 papers to understand the evolution of these decision-making algorithms. We investigated trends in computational methods, predictor variables, and target outcomes used to develop the models using a human-centered lens and found that only 9 papers (15.7%) investigated model fairness and bias. We uncovered tensions between explainability and ecological validity wherein predictive risk models (53.4%) focused on reductive explainability while resource allocation models (25.9%) were dependent on unrealistic assumptions and simulated data that are not useful in practice. Further, we discuss research challenges and opportunities for developing human-centered algorithms in this area. ",
        "title": "A Human-Centered Review of Algorithms in Homelessness Research",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13248",
        "abstract_url": "http://arxiv.org/abs/2401.13248",
        "authors": [
            {
                "last_name": "Efstratiou",
                "first_name": "Alexandros"
            },
            {
                "last_name": "Efstratiou",
                "first_name": "Marina"
            },
            {
                "last_name": "Yudhoatmojo",
                "first_name": "Satrio"
            },
            {
                "last_name": "Blackburn",
                "first_name": "Jeremy"
            },
            {
                "last_name": "De Cristofaro",
                "first_name": "Emiliano"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "SI"
        ],
        "abstract": "  The COVID-19 pandemic brought about an extraordinary rate of scientific papers on the topic that were discussed among the general public, although often in biased or misinformed ways. In this paper, we present a mixed-methods analysis aimed at examining whether public discussions were commensurate with the scientific consensus on several COVID-19 issues. We estimate scientific consensus based on samples of abstracts from preprint servers and compare against the volume of public discussions on Twitter mentioning these papers. We find that anti-consensus posts and users, though overall less numerous than pro-consensus ones, are vastly over-represented on Twitter, thus producing a false consensus effect. This transpires with favorable papers being disproportionately amplified, along with an influx of new anti-consensus user sign-ups. Finally, our content analysis highlights that anti-consensus users misrepresent scientific findings or question scientists' integrity in their efforts to substantiate their claims. ",
        "title": "\"Here's Your Evidence\": False Consensus in Public Twitter Discussions of  COVID-19 Science",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13249",
        "abstract_url": "http://arxiv.org/abs/2401.13249",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Wangjin"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhengdong"
            },
            {
                "last_name": "Chu",
                "first_name": "Chenhui"
            },
            {
                "last_name": "Li",
                "first_name": "Sheng"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yi"
            },
            {
                "last_name": "Kawahara",
                "first_name": "Tatsuya"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  Automatic Mean Opinion Score (MOS) prediction is employed to evaluate the quality of synthetic speech. This study extends the application of predicted MOS to the task of Fake Audio Detection (FAD), as we expect that MOS can be used to assess how close synthesized speech is to the natural human voice. We propose MOS-FAD, where MOS can be leveraged at two key points in FAD: training data selection and model fusion. In training data selection, we demonstrate that MOS enables effective filtering of samples from unbalanced datasets. In the model fusion, our results demonstrate that incorporating MOS as a gating mechanism in FAD model fusion enhances overall performance. ",
        "title": "MOS-FAD: Improving Fake Audio Detection Via Automatic Mean Opinion Score  Prediction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13254",
        "abstract_url": "http://arxiv.org/abs/2401.13254",
        "authors": [
            {
                "last_name": "Carf\u00ec",
                "first_name": "Alessandro"
            },
            {
                "last_name": "Alameh",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Belcamino",
                "first_name": "Valerio"
            },
            {
                "last_name": "Mastrogiovanni",
                "first_name": "Fulvio"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The flexibility and range of motion in human hands play a crucial role in human interaction with the environment and have been studied across different fields. Researchers explored various technological solutions for gathering information from the hands. These solutions include tracking hand motion through cameras or wearable sensors and using wearable sensors to measure the position and pressure of contact points. Data gloves can collect both types of information by utilizing inertial measurement units, flex sensors, magnetic trackers for motion tracking, and force resistors or touch sensors for contact measurement. Although there are commercially available data gloves, researchers often create custom data gloves to achieve the desired flexibility and control over the hardware. However, the existing literature lacks standardization and the reuse of previously designed data gloves. As a result, many gloves with unclear characteristics exist, which makes replication challenging and negatively impacts the reproducibility of studies. This work proposes a modular, open hardware and software architecture for creating customized data gloves based on IMU technology. We also provide an architecture implementation along with an experimental protocol to evaluate device performance. ",
        "title": "A modular architecture for IMU-based data gloves",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13255",
        "abstract_url": "http://arxiv.org/abs/2401.13255",
        "authors": [
            {
                "last_name": "Tuy\u00e9ras",
                "first_name": "R\u00e9my"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  This paper redefines the foundations of asymmetric cryptography's homomorphic cryptosystems through the application of the Yoneda Lemma. It explicitly illustrates that widely adopted systems, including ElGamal, RSA, Benaloh, Regev's LWE, and NTRUEncrypt, directly derive from the principles of the Yoneda Lemma. This synthesis gives rise to a holistic homomorphic encryption framework named the Yoneda Encryption Scheme. Within this scheme, encryption is elucidated through the bijective maps of the Yoneda Lemma Isomorphism, and decryption seamlessly follows from the naturality of these maps. This unification suggests a conjecture for a unified model theory framework, providing a basis for reasoning about both homomorphic and fully homomorphic encryption (FHE) schemes. As a practical demonstration, the paper introduces an FHE scheme capable of processing arbitrary finite sequences of encrypted multiplications and additions without the need for additional tweaking techniques, such as squashing or bootstrapping. This not only underscores the practical implications of the proposed theoretical advancements but also introduces new possibilities for leveraging model theory and forcing techniques in cryptography to facilitate the design of FHE schemes. ",
        "title": "Constructing a fully homomorphic encryption scheme with the Yoneda Lemma",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13256",
        "abstract_url": "http://arxiv.org/abs/2401.13256",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Hongru"
            },
            {
                "last_name": "Huang",
                "first_name": "Wenyu"
            },
            {
                "last_name": "Deng",
                "first_name": "Yang"
            },
            {
                "last_name": "Wang",
                "first_name": "Rui"
            },
            {
                "last_name": "Wang",
                "first_name": "Zezhong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yufei"
            },
            {
                "last_name": "Mi",
                "first_name": "Fei"
            },
            {
                "last_name": "Pan",
                "first_name": "Jeff Z."
            },
            {
                "last_name": "Wong",
                "first_name": "Kam-Fai"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems. ",
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for  Personalized Dialogue Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13260",
        "abstract_url": "http://arxiv.org/abs/2401.13260",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Jiajun"
            },
            {
                "last_name": "Shi",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Li",
                "first_name": "Xingfeng"
            },
            {
                "last_name": "Toda",
                "first_name": "Tomoki"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "MM",
            "SD"
        ],
        "abstract": "  The prevalent approach in speech emotion recognition (SER) involves integrating both audio and textual information to comprehensively identify the speaker's emotion, with the text generally obtained through automatic speech recognition (ASR). An essential issue of this approach is that ASR errors from the text modality can worsen the performance of SER. Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses. However, this approach has limited improvement potential because it does not address the coherence of semantic information in the text. Additionally, the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging. Therefore, in this paper, we incorporate two auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to enhance the semantic coherence of ASR text, and further introduce a novel multi-modal fusion (MF) method to learn shared representations across modalities. We refer to our method as MF-AED-AEC. Experimental results indicate that MF-AED-AEC significantly outperforms the baseline model by a margin of 4.1\\%. ",
        "title": "MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion,  ASR Error Detection, and ASR Error Correction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13262",
        "abstract_url": "http://arxiv.org/abs/2401.13262",
        "authors": [
            {
                "last_name": "Damle",
                "first_name": "Sankarshan"
            },
            {
                "last_name": "Padala",
                "first_name": "Manisha"
            },
            {
                "last_name": "Gujar",
                "first_name": "Sujit"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "CR"
        ],
        "abstract": "  Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user transactions to include in blocks and determine their payments (i.e., transaction fees). Increasing demand and scarce block resources have led to high user transaction fees. As these blockchains are a public resource, it may be preferable to reduce these transaction fees. To this end, we introduce Transaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG payments collected from such TFM as rebates to minimize transaction fees. Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows the non-triviality of applying RM in TFMs. More concretely, we prove that it is impossible to reduce transaction fees when (i) transactions that are not confirmed do not receive rebates and (ii) the miner can strategically manipulate the mechanism. Driven by this, we propose \\emph{Robust} TFRM (\\textsf{R-TFRM}): a mechanism that compromises on an honest miner's individual rationality to guarantee strictly positive rebates to the users. We then introduce \\emph{robust} and \\emph{rational} TFRM (\\textsf{R}$^2$\\textsf{-TFRM}) that uses trusted on-chain randomness that additionally guarantees miner's individual rationality (in expectation) and strictly positive rebates. Our results show that TFRMs provide a promising new direction for reducing transaction fees in public blockchains. ",
        "title": "Designing Redistribution Mechanisms for Reducing Transaction Fees in  Blockchains",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13264",
        "abstract_url": "http://arxiv.org/abs/2401.13264",
        "authors": [
            {
                "last_name": "Zeng",
                "first_name": "Ziru"
            },
            {
                "last_name": "Ding",
                "first_name": "Yue"
            },
            {
                "last_name": "Lu",
                "first_name": "Hongtao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently,the detection transformer has gained substantial attention for its inherent minimal post-processing requirement.However,this paradigm relies on abundant training data,yet in the context of the cross-domain adaptation,insufficient labels in the target domain exacerbate issues of class imbalance and model performance degradation.To address these challenges, we propose a novel class-aware cross domain detection transformer based on the adversarial learning and mean-teacher framework.First,considering the inconsistencies between the classification and regression tasks,we introduce an IoU-aware prediction branch and exploit the consistency of classification and location scores to filter and reweight pseudo labels.Second, we devise a dynamic category threshold refinement to adaptively manage model confidence.Third,to alleviate the class imbalance,an instance-level class-aware contrastive learning module is presented to encourage the generation of discriminative features for each class,particularly benefiting minority classes.Experimental results across diverse domain-adaptive scenarios validate our method's effectiveness in improving performance and alleviating class imbalance issues,which outperforms the state-of-the-art transformer based methods. ",
        "title": "Enhancing cross-domain detection: adaptive class-aware contrastive  transformer",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13266",
        "abstract_url": "http://arxiv.org/abs/2401.13266",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Mengming"
            },
            {
                "last_name": "Fang",
                "first_name": "Wenji"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qijun"
            },
            {
                "last_name": "Xie",
                "first_name": "Zhiyao"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  The development of architecture specifications is an initial and fundamental stage of the integrated circuit (IC) design process. Traditionally, architecture specifications are crafted by experienced chip architects, a process that is not only time-consuming but also error-prone. Mistakes in these specifications may significantly affect subsequent stages of chip design. Despite the presence of advanced electronic design automation (EDA) tools, effective solutions to these specification-related challenges remain scarce. Since writing architecture specifications is naturally a natural language processing (NLP) task, this paper pioneers the automation of architecture specification development with the advanced capabilities of large language models (LLMs). Leveraging our definition and dataset, we explore the application of LLMs in two key aspects of architecture specification development: (1) Generating architecture specifications, which includes both writing specifications from scratch and converting RTL code into detailed specifications. (2) Reviewing existing architecture specifications. We got promising results indicating that LLMs may revolutionize how these critical specification documents are developed in IC design nowadays. By reducing the effort required, LLMs open up new possibilities for efficiency and accuracy in this crucial aspect of chip design. ",
        "title": "SpecLLM: Exploring Generation and Review of VLSI Design Specification  with Large Language Model",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13267",
        "abstract_url": "http://arxiv.org/abs/2401.13267",
        "authors": [
            {
                "last_name": "Ye",
                "first_name": "Shuchang"
            },
            {
                "last_name": "Meng",
                "first_name": "Mingyuan"
            },
            {
                "last_name": "Li",
                "first_name": "Mingjian"
            },
            {
                "last_name": "Feng",
                "first_name": "Dagan"
            },
            {
                "last_name": "Kim",
                "first_name": "Jinman"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  With increasing reliance on medical imaging in clinical practices, automated report generation from medical images is in great demand. Existing report generation methods typically adopt an encoder-decoder deep learning framework to build a uni-directional image-to-report mapping. However, such a framework ignores the bi-directional mutual associations between images and reports, thus incurring difficulties in associating the intrinsic medical meanings between them. Recent generative representation learning methods have demonstrated the benefits of dual-modal learning from both image and text modalities. However, these methods exhibit two major drawbacks for medical report generation: 1) they tend to capture morphological information and have difficulties in capturing subtle pathological semantic information, and 2) they predict masked text rely on both unmasked images and text, inevitably degrading performance when inference is based solely on images. In this study, we propose a new report generation framework with dual-modal dynamic traceback learning (DTrace) to overcome the two identified drawbacks and enable dual-modal learning for medical report generation. To achieve this, our DTrace introduces a traceback mechanism to control the semantic validity of generated content via self-assessment. Further, our DTrace introduces a dynamic learning strategy to adapt to various proportions of image and text input, enabling report generation without reliance on textual input during inference. Extensive experiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that our DTrace outperforms state-of-the-art medical report generation methods. ",
        "title": "Dual-modal Dynamic Traceback Learning for Medical Report Generation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13268",
        "abstract_url": "http://arxiv.org/abs/2401.13268",
        "authors": [
            {
                "last_name": "del-Pino-L\u00f3pez",
                "first_name": "Juan Carlos"
            },
            {
                "last_name": "Cruz-Romero",
                "first_name": "Pedro"
            },
            {
                "last_name": "S\u00e1nchez-D\u00edaz",
                "first_name": "Luis Carlos"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Loss allocation of the three different components (conductor, sheaths and armor) of solidly bonded three-core separated lead-sheathed armored cables, frequently employed in offshore wind farms, is challenging due to the lack of accurate enough analytical expressions in the IEC standard. Also, loss allocation through experimental tests leads to inaccurate results since it is based on questionable assumptions. This paper improves both the IEC formulae and experimental methods by means of different analytical corrections in the conductor and sheath loss expressions. To this aim, an ad hoc application interface (Virtual Lab) based on 3D numerical simulations (finite element method) has been developed. This tool virtualizes and automates different test setups to emulate, in few seconds, the most employed experimental procedures in this type of cable. The analytical corrections have been derived from an in-depth analysis of a first set of 368 cables, ranging from 30 to 275 kV. The new loss expressions were successfully applied to a second set of 645 armored cables of quite diverse features (voltages from 10 to 275 kV, sections and dimensional parameters), hence bringing a general framework for any kind of three-core armored cable. ",
        "title": "Loss Allocation in Submarine Armored Three-core HVAC Power Cables",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13270",
        "abstract_url": "http://arxiv.org/abs/2401.13270",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Pengcheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Yanxiang"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yang"
            },
            {
                "last_name": "Jia",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhao"
            },
            {
                "last_name": "Wang",
                "first_name": "Ronggang"
            },
            {
                "last_name": "Hong",
                "first_name": "Richang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representation is fed into the pretrained network to finally realize the audio-guided colorization. The whole process is trained in a self-supervised manner without human annotation. In addition, an audiovisual colorization dataset is established for training and testing. Experiments demonstrate that audio guidance can effectively improve the performance of automatic colorization, especially for some scenes that are difficult to understand only from visual modality. ",
        "title": "Audio-Infused Automatic Image Colorization by Exploiting Audio Scene  Semantics",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13274",
        "abstract_url": "http://arxiv.org/abs/2401.13274",
        "authors": [
            {
                "last_name": "Bao",
                "first_name": "Weizhu"
            },
            {
                "last_name": "Li",
                "first_name": "Yifei"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We propose an energy-stable parametric finite element method (PFEM) for the planar Willmore flow and establish its unconditional energy stability of the full discretization scheme. The key lies in the introduction of two novel geometric identities to describe the planar Willmore flow: the first one involves the coupling of the outward unit normal vector $\\boldsymbol{n}$ and the normal velocity $V$, and the second one concerns the time derivative of the mean curvature $\\kappa$. Based on them, we derive a set of new geometric partial differential equations for the planar Willmore flow, leading to our new fully-discretized and unconditionally energy-stable PFEM. Our stability analysis is also based on the two new geometric identities. Extensive numerical experiments are provided to illustrate its efficiency and validate its unconditional energy stability. ",
        "title": "An energy-stable parametric finite element method for the planar  Willmore flow",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13275",
        "abstract_url": "http://arxiv.org/abs/2401.13275",
        "authors": [
            {
                "last_name": "Cheng",
                "first_name": "Qinyuan"
            },
            {
                "last_name": "Sun",
                "first_name": "Tianxiang"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiangyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenwei"
            },
            {
                "last_name": "Yin",
                "first_name": "Zhangyue"
            },
            {
                "last_name": "Li",
                "first_name": "Shimin"
            },
            {
                "last_name": "Li",
                "first_name": "Linyang"
            },
            {
                "last_name": "Chen",
                "first_name": "Kai"
            },
            {
                "last_name": "Qiu",
                "first_name": "Xipeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question \"Can AI assistants know what they don't know and express them through natural language?\" To answer this question, we construct a model-specific \"I don't know\" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding Idk dataset and observe whether it can refuse to answer its unknown questions after alignment. Experimental results show that after alignment with Idk datasets, the assistant can refuse to answer most its unknown questions. For questions they attempt to answer, the accuracy is significantly higher than before the alignment. ",
        "title": "Can AI Assistants Know What They Don't Know?",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13280",
        "abstract_url": "http://arxiv.org/abs/2401.13280",
        "authors": [
            {
                "last_name": "Chiu",
                "first_name": "Ming-Chang"
            },
            {
                "last_name": "Wang",
                "first_name": "Yingfei"
            },
            {
                "last_name": "Kuo",
                "first_name": "Yen-Ju"
            },
            {
                "last_name": "Chen",
                "first_name": "Pin-Yu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CE"
        ],
        "abstract": "  Skin tone as a demographic bias and inconsistent human labeling poses challenges in dermatology AI. We take another angle to investigate color contrast's impact, beyond skin tones, on malignancy detection in skin disease datasets: We hypothesize that in addition to skin tones, the color difference between the lesion area and skin also plays a role in malignancy detection performance of dermatology AI models. To study this, we first propose a robust labeling method to quantify color contrast scores of each image and validate our method by showing small labeling variations. More importantly, applying our method to \\textit{the only} diverse-skin tone and pathologically-confirmed skin disease dataset DDI, yields \\textbf{DDI-CoCo Dataset}, and we observe a performance gap between the high and low color difference groups. This disparity remains consistent across various state-of-the-art (SoTA) image classification models, which supports our hypothesis. Furthermore, we study the interaction between skin tone and color difference effects and suggest that color difference can be an additional reason behind model performance bias between skin tones. Our work provides a complementary angle to dermatology AI for improving skin disease detection. ",
        "title": "DDI-CoCo: A Dataset For Understanding The Effect Of Color Contrast In  Machine-Assisted Skin Disease Detection",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13282",
        "abstract_url": "http://arxiv.org/abs/2401.13282",
        "authors": [
            {
                "last_name": "Farooq",
                "first_name": "Junaid"
            },
            {
                "last_name": "Rafiq",
                "first_name": "Danish"
            },
            {
                "last_name": "Vlachas",
                "first_name": "Pantelis R."
            },
            {
                "last_name": "Bazaz",
                "first_name": "Mohammad Abid"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Forecasting complex system dynamics, particularly for long-term predictions, is persistently hindered by error accumulation and computational burdens. This study presents RefreshNet, a multiscale framework developed to overcome these challenges, delivering an unprecedented balance between computational efficiency and predictive accuracy. RefreshNet incorporates convolutional autoencoders to identify a reduced order latent space capturing essential features of the dynamics, and strategically employs multiple recurrent neural network (RNN) blocks operating at varying temporal resolutions within the latent space, thus allowing the capture of latent dynamics at multiple temporal scales. The unique \"refreshing\" mechanism in RefreshNet allows coarser blocks to reset inputs of finer blocks, effectively controlling and alleviating error accumulation. This design demonstrates superiority over existing techniques regarding computational efficiency and predictive accuracy, especially in long-term forecasting. The framework is validated using three benchmark applications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and Kuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms state-of-the-art methods in long-term forecasting accuracy and speed, marking a significant advancement in modeling complex systems and opening new avenues in understanding and predicting their behavior. ",
        "title": "RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13285",
        "abstract_url": "http://arxiv.org/abs/2401.13285",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Shengjing"
            },
            {
                "last_name": "Han",
                "first_name": "Yinan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiuping"
            },
            {
                "last_name": "Zhao",
                "first_name": "Xiantong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Single Object Tracking in LiDAR point cloud is one of the most essential parts of environmental perception, in which small objects are inevitable in real-world scenarios and will bring a significant barrier to the accurate location. However, the existing methods concentrate more on exploring universal architectures for common categories and overlook the challenges that small objects have long been thorny due to the relative deficiency of foreground points and a low tolerance for disturbances. To this end, we propose a Siamese network-based method for small object tracking in the LiDAR point cloud, which is composed of the target-awareness prototype mining (TAPM) module and the regional grid subdivision (RGS) module. The TAPM module adopts the reconstruction mechanism of the masked decoder to learn the prototype in the feature space, aiming to highlight the presence of foreground points that will facilitate the subsequent location of small objects. Through the above prototype is capable of accentuating the small object of interest, the positioning deviation in feature maps still leads to high tracking errors. To alleviate this issue, the RGS module is proposed to recover the fine-grained features of the search region based on ViT and pixel shuffle layers. In addition, apart from the normal settings, we elaborately design a scaling experiment to evaluate the robustness of the different trackers on small objects. Extensive experiments on KITTI and nuScenes demonstrate that our method can effectively improve the tracking performance of small targets without affecting normal-sized objects. ",
        "title": "Small Object Tracking in LiDAR Point Cloud: Learning the  Target-awareness Prototype and Fine-grained Search Region",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13296",
        "abstract_url": "http://arxiv.org/abs/2401.13296",
        "authors": [
            {
                "last_name": "Tores",
                "first_name": "Julie"
            },
            {
                "last_name": "Sassatelli",
                "first_name": "Lucile"
            },
            {
                "last_name": "Wu",
                "first_name": "Hui-Yin"
            },
            {
                "last_name": "Bergman",
                "first_name": "Clement"
            },
            {
                "last_name": "Andolfi",
                "first_name": "Lea"
            },
            {
                "last_name": "Ecrement",
                "first_name": "Victor"
            },
            {
                "last_name": "Precioso",
                "first_name": "Frederic"
            },
            {
                "last_name": "Devars",
                "first_name": "Thierry"
            },
            {
                "last_name": "Guaresi",
                "first_name": "Magali"
            },
            {
                "last_name": "Julliard",
                "first_name": "Virginie"
            },
            {
                "last_name": "Lecossais",
                "first_name": "Sarah"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community. ",
        "title": "Visual Objectification in Films: Towards a New AI Task for Video  Interpretation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13298",
        "abstract_url": "http://arxiv.org/abs/2401.13298",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Hongzhan"
            },
            {
                "last_name": "Luo",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Gao",
                "first_name": "Wei"
            },
            {
                "last_name": "Ma",
                "first_name": "Jing"
            },
            {
                "last_name": "Wang",
                "first_name": "Bo"
            },
            {
                "last_name": "Yang",
                "first_name": "Ruichao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfulness rationales and the intrinsic multimodal information within memes. In this way, our model is empowered to perform dialectical reasoning over intricate and implicit harm-indicative patterns, utilizing multimodal explanations originating from both harmless and harmful arguments. Extensive experiments on three public meme datasets demonstrate that our harmful meme detection approach achieves much better performance than state-of-the-art methods and exhibits a superior capacity for explaining the meme harmfulness of the model predictions. ",
        "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate  between Large Language Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13301",
        "abstract_url": "http://arxiv.org/abs/2401.13301",
        "authors": [
            {
                "last_name": "Mato-Abad",
                "first_name": "V"
            },
            {
                "last_name": "Labiano-Fontcuberta",
                "first_name": "A"
            },
            {
                "last_name": "Rodriguez-Yanez",
                "first_name": "S"
            },
            {
                "last_name": "Garcia-Vazquez",
                "first_name": "R"
            },
            {
                "last_name": "Munteanu",
                "first_name": "CR"
            },
            {
                "last_name": "Andrade-Garda",
                "first_name": "J"
            },
            {
                "last_name": "Domingo-Santos",
                "first_name": "A"
            },
            {
                "last_name": "Sanchez-Seco",
                "first_name": "V Galan"
            },
            {
                "last_name": "Aladro",
                "first_name": "Y"
            },
            {
                "last_name": "Martinez-Gines",
                "first_name": "ML"
            },
            {
                "last_name": "Ayuso",
                "first_name": "L"
            },
            {
                "last_name": "Benito-Leon",
                "first_name": "J"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Background and purpose: The unanticipated detection by magnetic resonance imaging (MRI) in the brain of asymptomatic subjects of white matter lesions suggestive of multiple sclerosis (MS) has been named radiologically isolated syndrome (RIS). As the difference between early MS [i.e. clinically isolated syndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to improve detection of the subclinical form without interfering with MRI as there are radiological diagnostic criteria for that. Our objective was to use machine-learning classification methods to identify morphometric measures that help to discriminate patients with RIS from those with CIS.   Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers (cortical thickness, cortical and subcortical grey matter volume, and white matter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS for single-subject level classification.   Results: The best proposed models to predict the diagnosis of CIS and RIS were based on the Naive Bayes, Bagging and Multilayer Perceptron classifiers using only three features: the left rostral middle frontal gyrus volume and the fractional anisotropy values in the right amygdala and right lingual gyrus. The Naive Bayes obtained the highest accuracy [overall classification, 0.765; area under the receiver operating characteristic (AUROC), 0.782].   Conclusions: A machine-learning approach applied to multimodal MRI data may differentiate between the earliest clinical expressions of MS (CIS and RIS) with an accuracy of 78%.   Keywords: Bagging; Multilayer Perceptron; Naive Bayes classifier; clinically isolated syndrome; diffusion tensor imaging; machine-learning; magnetic resonance imaging; multiple sclerosis; radiologically isolated syndrome. ",
        "title": "Classification of Radiologically Isolated Syndrome and Clinically  Isolated Syndrome with Machine-Learning Techniques",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13302",
        "abstract_url": "http://arxiv.org/abs/2401.13302",
        "authors": [
            {
                "last_name": "M\u00f6ller",
                "first_name": "Ralf"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this report we explore the application of the Lagrange-Newton method to the SAM (smoothing-and-mapping) problem in mobile robotics. In Lagrange-Newton SAM, the angular component of each pose vector is expressed by orientation vectors and treated through Lagrange constraints. This is different from the typical Gauss-Newton approach where variations need to be mapped back and forth between Euclidean space and a manifold suitable for rotational components. We derive equations for five different types of measurements between robot poses: translation, distance, and rotation from odometry in the plane, as well as home-vector angle and compass angle from visual homing. We demonstrate the feasibility of the Lagrange-Newton approach for a simple example related to a cleaning robot scenario. ",
        "title": "A Lagrange-Newton Approach to Smoothing-and-Mapping",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13303",
        "abstract_url": "http://arxiv.org/abs/2401.13303",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Peiqin"
            },
            {
                "last_name": "Ji",
                "first_name": "Shaoxiong"
            },
            {
                "last_name": "Tiedemann",
                "first_name": "J\u00f6rg"
            },
            {
                "last_name": "Martins",
                "first_name": "Andr\u00e9 F. T."
            },
            {
                "last_name": "Sch\u00fctze",
                "first_name": "Hinrich"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at https://huggingface.co/MaLA-LM ",
        "title": "MaLA-500: Massive Language Adaptation of Large Language Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13306",
        "abstract_url": "http://arxiv.org/abs/2401.13306",
        "authors": [
            {
                "last_name": "Henze",
                "first_name": "Martin"
            },
            {
                "last_name": "Ortmann",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Vogt",
                "first_name": "Thomas"
            },
            {
                "last_name": "Ugus",
                "first_name": "Osman"
            },
            {
                "last_name": "Hermann",
                "first_name": "Kai"
            },
            {
                "last_name": "Nohr",
                "first_name": "Svenja"
            },
            {
                "last_name": "Lu",
                "first_name": "Zeren"
            },
            {
                "last_name": "Michaelides",
                "first_name": "Sotiris"
            },
            {
                "last_name": "Massonet",
                "first_name": "Angela"
            },
            {
                "last_name": "Schmitt",
                "first_name": "Robert H."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  To meet the requirements of modern production, industrial communication increasingly shifts from wired fieldbus to wireless 5G communication. Besides tremendous benefits, this shift introduces severe novel risks, ranging from limited reliability over new security vulnerabilities to a lack of accountability. To address these risks, we present approaches to (i) prevent attacks through authentication and redundant communication, (ii) detect anomalies and jamming, and (iii) respond to detected attacks through device exclusion and accountability measures. ",
        "title": "POSTER: Towards Secure 5G Infrastructures for Production Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13307",
        "abstract_url": "http://arxiv.org/abs/2401.13307",
        "authors": [
            {
                "last_name": "Tian",
                "first_name": "Yunjie"
            },
            {
                "last_name": "Ma",
                "first_name": "Tianren"
            },
            {
                "last_name": "Xie",
                "first_name": "Lingxi"
            },
            {
                "last_name": "Qiu",
                "first_name": "Jihao"
            },
            {
                "last_name": "Tang",
                "first_name": "Xi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuan"
            },
            {
                "last_name": "Jiao",
                "first_name": "Jianbin"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            },
            {
                "last_name": "Ye",
                "first_name": "Qixiang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this study, we establish a baseline for a new task named multimodal multi-round referring and grounding (MRG), opening up a promising direction for instance-level multimodal dialogues. We present a new benchmark and an efficient vision-language model for this purpose. The new benchmark, named CB-300K, spans challenges including multi-round dialogue, complex spatial relationships among multiple instances, and consistent reasoning, which are beyond those shown in existing benchmarks. The proposed model, named ChatterBox, utilizes a two-branch architecture to collaboratively handle vision and language tasks. By tokenizing instance regions, the language branch acquires the ability to perceive referential information. Meanwhile, ChatterBox feeds a query embedding in the vision branch to a token receiver for visual grounding. A two-stage optimization strategy is devised, making use of both CB-300K and auxiliary external data to improve the model's stability and capacity for instance-level understanding. Experiments show that ChatterBox outperforms existing models in MRG both quantitatively and qualitatively, paving a new path towards multimodal dialogue scenarios with complicated and precise interactions. Code, data, and model are available at: https://github.com/sunsmarterjie/ChatterBox. ",
        "title": "ChatterBox: Multi-round Multimodal Referring and Grounding",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13310",
        "abstract_url": "http://arxiv.org/abs/2401.13310",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jolly"
            },
            {
                "last_name": "Dessole",
                "first_name": "Monica"
            },
            {
                "last_name": "Varbanescu",
                "first_name": "Ana Lucia"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  The world's largest particle accelerator, located at CERN, produces petabytes of data that need to be analysed efficiently, to study the fundamental structures of our universe. ROOT is an open-source C++ data analysis framework, developed for this purpose. Its high-level data analysis interface, RDataFrame, currently only supports CPU parallelism. Given the increasing heterogeneity in computing facilities, it becomes crucial to efficiently support GPGPUs to take advantage of the available resources. SYCL allows for a single-source implementation, which enables support for different architectures. In this paper, we describe a CUDA implementation and the migration process to SYCL, focusing on a core high energy physics operation in RDataFrame -- histogramming. We detail the challenges that we faced when integrating SYCL into a large and complex code base. Furthermore, we perform an extensive comparative performance analysis of two SYCL compilers, AdaptiveCpp and DPC++, and the reference CUDA implementation. We highlight the performance bottlenecks that we encountered, and the methodology used to detect these. Based on our findings, we provide actionable insights for developers of SYCL applications. ",
        "title": "Lessons Learned Migrating CUDA to SYCL: A HEP Case Study with ROOT  RDataFrame",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13311",
        "abstract_url": "http://arxiv.org/abs/2401.13311",
        "authors": [
            {
                "last_name": "Wadhawan",
                "first_name": "Rohan"
            },
            {
                "last_name": "Bansal",
                "first_name": "Hritik"
            },
            {
                "last_name": "Chang",
                "first_name": "Kai-Wei"
            },
            {
                "last_name": "Peng",
                "first_name": "Nanyun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In addition to human evaluations, we also employed automatic evaluation metrics using GPT-4, uncovering similar trends in performance disparities. We also perform a fine-grained evaluation across diverse visual contexts and provide qualitative analysis which provides a robust framework for future advancements in the LMM design. https://con-textual.github.io/ ",
        "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in  Large Multimodal Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13312",
        "abstract_url": "http://arxiv.org/abs/2401.13312",
        "authors": [
            {
                "last_name": "del-Pino-L\u00f3pez",
                "first_name": "Juan Carlos"
            },
            {
                "last_name": "Cruz-Romero",
                "first_name": "Pedro"
            },
            {
                "last_name": "Bravo-Rodr\u00edguez",
                "first_name": "Juan Carlos"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The great expansion in offshore power plants is raising the concern regarding the cumulative effect of the electromagnetic field emissions caused by submarine power cables. In this sense, owners are required to predict these emissions during the permitting and consenting process of new power plants. This is a challenging task, especially in the case of HVAC three-core armored cables due to their complex geometry. Customarily, 2D approaches based on the finite element method (FEM) have been employed for evaluating the magnetic field emissions caused by these cables. However, inaccurate results are obtained since the phase conductors and armor twisting is omitted. This work develops, for the first time in the literature, an in-depth analysis of the magnetic field caused by this type of cable through an ultra-shortened 3D-FEM model, which is also faced to experimental measurements taken on an actual 132 kV, 800 mm2 three-core armored cable. Relevant conclusions are derived regarding the impact of the cable design on the magnetic field emissions, including material properties, as well as single and double-layer armors, presenting the proposed model not only as a valuable tool for predicting purposes, but also for optimizing cable design in terms of magnetic field emissions. ",
        "title": "Evaluation of the power frequency magnetic field generated by three-core  armored cables through 3D finite element simulations",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13313",
        "abstract_url": "http://arxiv.org/abs/2401.13313",
        "authors": [
            {
                "last_name": "Tanaka",
                "first_name": "Ryota"
            },
            {
                "last_name": "Iki",
                "first_name": "Taichi"
            },
            {
                "last_name": "Nishida",
                "first_name": "Kyosuke"
            },
            {
                "last_name": "Saito",
                "first_name": "Kuniko"
            },
            {
                "last_name": "Suzuki",
                "first_name": "Jun"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training. ",
        "title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document  Understanding with Instructions",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13314",
        "abstract_url": "http://arxiv.org/abs/2401.13314",
        "authors": [
            {
                "last_name": "Abbas-Turki",
                "first_name": "Lokman"
            },
            {
                "last_name": "Cr\u00e9pey",
                "first_name": "St\u00e9phane"
            },
            {
                "last_name": "Li",
                "first_name": "Botao"
            },
            {
                "last_name": "Saadeddine",
                "first_name": "Bouazza"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Motivated by the equations of cross valuation adjustments (XVAs) in the realistic case where capital is deemed fungible as a source of funding for variation margin, we introduce a simulation/regression scheme for a class of anticipated BSDEs, where the coefficient entails a conditional expected shortfall of the martingale part of the solution. The scheme is explicit in time and uses neural network least-squares and quantile regressions for the embedded conditional expectations and expected shortfall computations. An a posteriori Monte Carlo validation procedure allows assessing the regression error of the scheme at each time step. The superiority of this scheme with respect to Picard iterations is illustrated in a high-dimensional and hybrid market/default risks XVA use-case. ",
        "title": "An Explicit Scheme for Pathwise XVA Computations",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13315",
        "abstract_url": "http://arxiv.org/abs/2401.13315",
        "authors": [
            {
                "last_name": "Haugland",
                "first_name": "Mathias Ramm"
            },
            {
                "last_name": "Qadir",
                "first_name": "Hemin Ali"
            },
            {
                "last_name": "Balasingham",
                "first_name": "Ilangko"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  To cope with the growing prevalence of colorectal cancer (CRC), screening programs for polyp detection and removal have proven their usefulness. Colonoscopy is considered the best-performing procedure for CRC screening. To ease the examination, deep learning based methods for automatic polyp detection have been developed for conventional white-light imaging (WLI). Compared with WLI, narrow-band imaging (NBI) can improve polyp classification during colonoscopy but requires special equipment. We propose a CycleGAN-based framework to convert images captured with regular WLI to synthetic NBI (SNBI) as a pre-processing method for improving object detection on WLI when NBI is unavailable. This paper first shows that better results for polyp detection can be achieved on NBI compared to a relatively similar dataset of WLI. Secondly, experimental results demonstrate that our proposed modality translation can achieve improved polyp detection on SNBI images generated from WLI compared to the original WLI. This is because our WLI-to-SNBI translation model can enhance the observation of polyp surface patterns in the generated SNBI images. ",
        "title": "Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band  Imaging",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13320",
        "abstract_url": "http://arxiv.org/abs/2401.13320",
        "authors": [
            {
                "last_name": "Pastor-Galindo",
                "first_name": "Javier"
            },
            {
                "last_name": "Sandlin",
                "first_name": "H\u00f4ng-\u00c2n"
            },
            {
                "last_name": "M\u00e1rmol",
                "first_name": "F\u00e9lix G\u00f3mez"
            },
            {
                "last_name": "Bovet",
                "first_name": "G\u00e9r\u00f4me"
            },
            {
                "last_name": "P\u00e9rez",
                "first_name": "Gregorio Mart\u00ednez"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "IR"
        ],
        "abstract": "  The dark web has become notorious for its association with illicit activities and there is a growing need for systems to automate the monitoring of this space. This paper proposes an end-to-end scalable architecture for the early identification of new Tor sites and the daily analysis of their content. The solution is built using an Open Source Big Data stack for data serving with Kubernetes, Kafka, Kubeflow, and MinIO, continuously discovering onion addresses in different sources (threat intelligence, code repositories, web-Tor gateways, and Tor repositories), downloading the HTML from Tor and deduplicating the content using MinHash LSH, and categorizing with the BERTopic modeling (SBERT embedding, UMAP dimensionality reduction, HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days, the system identified 80,049 onion services and characterized 90% of them, addressing the challenge of Tor volatility. A disproportionate amount of repeated content is found, with only 6.1% unique sites. From the HTML files of the dark sites, 31 different low-topics are extracted, manually labeled, and grouped into 11 high-level topics. The five most popular included sexual and violent content, repositories, search engines, carding, cryptocurrencies, and marketplaces. During the experiments, we identified 14 sites with 13,946 clones that shared a suspiciously similar mirroring rate per day, suggesting an extensive common phishing network. Among the related works, this study is the most representative characterization of onion services based on topics to date. ",
        "title": "A Big Data Architecture for Early Identification and Categorization of  Dark Web Sites",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13322",
        "abstract_url": "http://arxiv.org/abs/2401.13322",
        "authors": [
            {
                "last_name": "Sommariva",
                "first_name": "Alvise"
            },
            {
                "last_name": "Vianello",
                "first_name": "Marco"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In a recent paper almost sure unisolvence of RBF interpolation at random points with no polynomial addition was proved, for Thin-Plate Splines and Radial Powers with noninteger exponent. The proving technique left unsolved the case of odd exponents. In this short note we prove almost sure polynomial-free unisolvence in such instances, by a deeper analysis of the interpolation matrix determinant and fundamental properties of analytic functions. ",
        "title": "Polynomial-free unisolvence of polyharmonic splines with odd exponent by  random sampling",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13325",
        "abstract_url": "http://arxiv.org/abs/2401.13325",
        "authors": [
            {
                "last_name": "Tu",
                "first_name": "Yuanpeng"
            },
            {
                "last_name": "Zhong",
                "first_name": "Zhun"
            },
            {
                "last_name": "Li",
                "first_name": "Yuxi"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hengshuang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Generalized category discovery (GCD) aims at addressing a more realistic and challenging setting of semi-supervised learning, where only part of the category labels are assigned to certain training samples. Previous methods generally employ naive contrastive learning or unsupervised clustering scheme for all the samples. Nevertheless, they usually ignore the inherent critical information within the historical predictions of the model being trained. Specifically, we empirically reveal that a significant number of salient unlabeled samples yield consistent historical predictions corresponding to their ground truth category. From this observation, we propose a Memory Consistency guided Divide-and-conquer Learning framework (MCDL). In this framework, we introduce two memory banks to record historical prediction of unlabeled data, which are exploited to measure the credibility of each sample in terms of its prediction consistency. With the guidance of credibility, we can design a divide-and-conquer learning strategy to fully utilize the discriminative information of unlabeled data while alleviating the negative influence of noisy labels. Extensive experimental results on multiple benchmarks demonstrate the generality and superiority of our method, where our method outperforms state-of-the-art models by a large margin on both seen and unseen classes of the generic image recognition and challenging semantic shift settings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars). ",
        "title": "Memory Consistency Guided Divide-and-Conquer Learning for Generalized  Category Discovery",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13327",
        "abstract_url": "http://arxiv.org/abs/2401.13327",
        "authors": [
            {
                "last_name": "Lange",
                "first_name": "Lucas"
            },
            {
                "last_name": "Wenzlitschke",
                "first_name": "Nils"
            },
            {
                "last_name": "Rahm",
                "first_name": "Erhard"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Smartwatch health sensor data is increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprises sensitive personal information and is resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress. Our method involves the generation of synthetic sequence data through Generative Adversarial Networks (GANs), coupled with the implementation of Differential Privacy (DP) safeguards for protecting patient information during model training. To ensure the integrity of our synthetic data, we employ a range of quality assessments and monitor the plausibility between synthetic and original data. To test the usefulness, we create private machine learning models on a commonly used, albeit small, stress detection dataset, exploring strategies for enhancing the existing data foundation with our synthetic data. Through our GAN-based augmentation methods, we observe improvements in model performance, both in non-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with limited availability of real training samples. ",
        "title": "Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable  Stress Detection",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13328",
        "abstract_url": "http://arxiv.org/abs/2401.13328",
        "authors": [
            {
                "last_name": "Boja\u0144czyk",
                "first_name": "Miko\u0142aj"
            },
            {
                "last_name": "Ohlmann",
                "first_name": "Pierre"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We propose to study transformations on graphs, and more generally structures, by looking at how the cut-rank (as introduced by Oum) of subsets is affected when going from the input structure to the output structure. We consider transformations in which the underlying sets are the same for both the input and output, and so the cut-ranks of subsets can be easily compared. The purpose of this paper is to give a characterisation of logically defined transductions that is expressed in purely structural terms, without referring to logic: transformations which decrease the cut-rank, in the asymptotic sense, are exactly those that can be defined in monadic second-order logic. This characterisation assumes that the transduction has inputs of bounded treewidth; we also show that the characterisation fails in the absence of any assumptions. ",
        "title": "Rank-decreasing transductions",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13329",
        "abstract_url": "http://arxiv.org/abs/2401.13329",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Dezhao"
            },
            {
                "last_name": "Huang",
                "first_name": "Jiabo"
            },
            {
                "last_name": "Gong",
                "first_name": "Shaogang"
            },
            {
                "last_name": "Jin",
                "first_name": "Hailin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yang"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video Moment Retrieval (VMR) requires precise modelling of fine-grained moment-text associations to capture intricate visual-language relationships. Due to the lack of a diverse and generalisable VMR dataset to facilitate learning scalable moment-text associations, existing methods resort to joint training on both source and target domain videos for cross-domain applications. Meanwhile, recent developments in vision-language multimodal models pre-trained on large-scale image-text and/or video-text pairs are only based on coarse associations (weakly labelled). They are inadequate to provide fine-grained moment-text correlations required for cross-domain VMR. In this work, we solve the problem of unseen cross-domain VMR, where certain visual and textual concepts do not overlap across domains, by only utilising target domain sentences (text prompts) without accessing their videos. To that end, we explore generative video diffusion for fine-grained editing of source videos controlled by the target sentences, enabling us to simulate target domain videos. We address two problems in video editing for optimising unseen domain VMR: (1) generation of high-quality simulation videos of different moments with subtle distinctions, (2) selection of simulation videos that complement existing source training videos without introducing harmful noise or unnecessary repetitions. On the first problem, we formulate a two-stage video diffusion generation controlled simultaneously by (1) the original video structure of a source video, (2) subject specifics, and (3) a target sentence prompt. This ensures fine-grained variations between video moments. On the second problem, we introduce a hybrid selection mechanism that combines two quantitative metrics for noise filtering and one qualitative metric for leveraging VMR prediction on simulation video selection. ",
        "title": "Generative Video Diffusion for Unseen Cross-Domain Video Moment  Retrieval",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13330",
        "abstract_url": "http://arxiv.org/abs/2401.13330",
        "authors": [
            {
                "last_name": "Gambella",
                "first_name": "Matteo"
            },
            {
                "last_name": "Pomponi",
                "first_name": "Jary"
            },
            {
                "last_name": "Scardapane",
                "first_name": "Simone"
            },
            {
                "last_name": "Roveri",
                "first_name": "Manuel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN ",
        "title": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit  Neural Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13334",
        "abstract_url": "http://arxiv.org/abs/2401.13334",
        "authors": [
            {
                "last_name": "Chakraborty",
                "first_name": "Tanmay"
            },
            {
                "last_name": "Seifert",
                "first_name": "Christin"
            },
            {
                "last_name": "Wirth",
                "first_name": "Christian"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable optimization techniques for real-world applications. ",
        "title": "Explainable Bayesian Optimization",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13335",
        "abstract_url": "http://arxiv.org/abs/2401.13335",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Zehua"
            },
            {
                "last_name": "Li",
                "first_name": "Zimeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Jingyuan"
            },
            {
                "last_name": "He",
                "first_name": "Yue"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called \\textit{n}FBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, \\textit{n}FBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, \\textit{n}FBST is a general framework that can be extended based on the measures selected, such as Grad-\\textit{n}FBST, LRP-\\textit{n}FBST, DeepLIFT-\\textit{n}FBST, LIME-\\textit{n}FBST. A range of experiments on both simulated and real data are conducted to show the advantages of our method. ",
        "title": "Full Bayesian Significance Testing for Neural Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13341",
        "abstract_url": "http://arxiv.org/abs/2401.13341",
        "authors": [
            {
                "last_name": "Lesar",
                "first_name": "\u017diga"
            },
            {
                "last_name": "Bohak",
                "first_name": "Ciril"
            },
            {
                "last_name": "Marolt",
                "first_name": "Matija"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  Depth perception in volumetric visualization plays a crucial role in the understanding and interpretation of volumetric data. Numerous visualization techniques, many of which rely on physically based optical effects, promise to improve depth perception but often do so without considering camera movement or the content of the volume. As a result, the findings from previous studies may not be directly applicable to crowded volumes, where a large number of contained structures disrupts spatial perception. Crowded volumes therefore require special analysis and visualization tools with sparsification capabilities. Interactivity is an integral part of visualizing and exploring crowded spaces, but has received little attention in previous studies. To address this gap, we conducted a study to assess the impact of different rendering techniques on depth perception in crowded volumes, with a particular focus on the effects of camera movement. The results show that depth perception considering camera motion depends much more on the content of the volume than on the chosen visualization technique. Furthermore, we found that traditional rendering techniques, which have often performed poorly in previous studies, showed comparable performance to physically based methods in our study. ",
        "title": "Evaluation of depth perception in crowded volumes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13343",
        "abstract_url": "http://arxiv.org/abs/2401.13343",
        "authors": [
            {
                "last_name": "del R\u00edo",
                "first_name": "Tereso"
            },
            {
                "last_name": "England",
                "first_name": "Matthew"
            }
        ],
        "primary_category": "SC",
        "categories": [
            "SC",
            "LG"
        ],
        "abstract": "  Symbolic Computation algorithms and their implementation in computer algebra systems often contain choices which do not affect the correctness of the output but can significantly impact the resources required: such choices can benefit from having them made separately for each problem via a machine learning model. This study reports lessons on such use of machine learning in symbolic computation, in particular on the importance of analysing datasets prior to machine learning and on the different machine learning paradigms that may be utilised. We present results for a particular case study, the selection of variable ordering for cylindrical algebraic decomposition, but expect that the lessons learned are applicable to other decisions in symbolic computation.   We utilise an existing dataset of examples derived from applications which was found to be imbalanced with respect to the variable ordering decision. We introduce an augmentation technique for polynomial systems problems that allows us to balance and further augment the dataset, improving the machine learning results by 28\\% and 38\\% on average, respectively. We then demonstrate how the existing machine learning methodology used for the problem $-$ classification $-$ might be recast into the regression paradigm. While this does not have a radical change on the performance, it does widen the scope in which the methodology can be applied to make choices. ",
        "title": "Lessons on Datasets and Paradigms in Machine Learning for Symbolic  Computation: A Case Study on CAD",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13345",
        "abstract_url": "http://arxiv.org/abs/2401.13345",
        "authors": [
            {
                "last_name": "Banerjee",
                "first_name": "Apoorva"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Traffic lights also known as stop-lights are signaling devices placed at road crossings which control the competing flow of traffic and avoid collisions. The traffic light controller uses a worldwide color code (red, yellow and green). A traffic light controller can be implemented by using a microcontroller, Field Programmable Gate Array or Application Specific Integrated Circuits. Use of Field Programmable Gate Array is beneficial for a number of reasons viz number of Input/Output ports, performance compared to that of a microcontroller and also it is less expensive as compared to Application Specific Integrated Circuits. In this paper, an efficient Traffic Light controller is designed using Moore finite state machine. The circuit description is done in Verilog and the design is tested and simulated on FPGA board Spartan-3e. ",
        "title": "Intelligent Traffic Light Controller using Verilog and Xilinx Spartan-3e",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13346",
        "abstract_url": "http://arxiv.org/abs/2401.13346",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Peizheng"
            },
            {
                "last_name": "Mavromatis",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Khan",
                "first_name": "Aftab"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative robots, and edge-intelligence-enabled devices. This paper provides a guide to the implemented and prospective artificial intelligence (AI) capabilities of UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are presented in detail: 1) An automated streetlight monitoring for detecting issues and triggering maintenance alerts; 2) A Digital twin of building environments providing enhanced air quality sensing with reduced cost; 3) A large-scale Federated Learning framework for reducing communication overhead; and 4) An intrusion detection for containerised applications identifying malicious activities. Additionally, the potential of UMBRELLA is outlined for future smart city and multi-robot crowdsensing applications enhanced by semantic communications and multi-agent planning. Finally, to realise the above use-cases we discuss the need for a tailored MLOps platform to automate UMBRELLA model pipelines and establish trust. ",
        "title": "Past, Present, Future: A Comprehensive Exploration of AI Use Cases in  the UMBRELLA IoT Testbed",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13351",
        "abstract_url": "http://arxiv.org/abs/2401.13351",
        "authors": [
            {
                "last_name": "Vicente-L\u00f3pez",
                "first_name": "Eduardo"
            },
            {
                "last_name": "de Campos",
                "first_name": "Luis M."
            },
            {
                "last_name": "Fern\u00e1ndez-Luna",
                "first_name": "Juan M."
            },
            {
                "last_name": "Huete",
                "first_name": "Juan F."
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Personalization generally improves the performance of queries but in a few cases it may also harms it. If we are able to predict and therefore to disable personalization for those situations, the overall performance will be higher and users will be more satisfied with personalized systems. We use some state-of-the-art pre-retrieval query performance predictors and propose some others including the user profile information for the previous purpose. We study the correlations among these predictors and the difference between the personalized and the original queries. We also use classification and regression techniques to improve the results and finally reach a bit more than one third of the maximum ideal performance. We think this is a good starting point within this research line, which certainly needs more effort and improvements. ",
        "title": "Predicting IR Personalization Performance using Pre-retrieval Query  Predictors",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13352",
        "abstract_url": "http://arxiv.org/abs/2401.13352",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yangsen"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. ",
        "title": "EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable  Endoscopic Tissues Reconstruction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13354",
        "abstract_url": "http://arxiv.org/abs/2401.13354",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Tianxia"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhuofu"
            },
            {
                "last_name": "Wei",
                "first_name": "Xingda"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Rong"
            },
            {
                "last_name": "Chen",
                "first_name": "Haibo"
            }
        ],
        "primary_category": "OS",
        "categories": [
            "OS",
            "NI"
        ],
        "abstract": "  GPU remoting is a promising technique for supporting AI applications. Networking plays a key role in enabling remoting. However, for efficient remoting, the network requirements in terms of latency and bandwidth are unknown. In this paper, we take a GPU-centric approach to derive the minimum latency and bandwidth requirements for GPU remoting, while ensuring no (or little) performance degradation for AI applications. Our study including theoretical model demonstrates that, with careful remoting design, unmodified AI applications can run on the remoting setup using commodity networking hardware without any overhead or even with better performance, with low network demands. ",
        "title": "Characterizing Network Requirements for GPU API Remoting in AI  Applications",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13355",
        "abstract_url": "http://arxiv.org/abs/2401.13355",
        "authors": [
            {
                "last_name": "Bundschuh",
                "first_name": "Jonas"
            },
            {
                "last_name": "Sp\u00e4ck-Leigsnering",
                "first_name": "Yvonne"
            },
            {
                "last_name": "De Gersem",
                "first_name": "Herbert"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In conventional finite element simulations, foil windings with a thin foil and many turns require many mesh elements. This renders models quickly computationally infeasible. With the use of homogenization approaches, the finite element mesh does not need to resolve the small-scale structure of the foil winding domain. Present homogenization approaches take resistive and inductive effects into account. With an increase of the operation frequency of foil windings, however, capacitive effects between adjacent turns in the foil winding become relevant. This paper presents an extension to the standard foil winding model that covers the capacitive behavior of foil windings. ",
        "title": "Considering Capacitive Effects in Foil Winding Homogenization",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13357",
        "abstract_url": "http://arxiv.org/abs/2401.13357",
        "authors": [
            {
                "last_name": "Cai",
                "first_name": "Qi"
            },
            {
                "last_name": "Li",
                "first_name": "Xinrui"
            },
            {
                "last_name": "Wu",
                "first_name": "Yuanxin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  How to efficiently and accurately handle image matching outliers is a critical issue in two-view relative estimation. The prevailing RANSAC method necessitates that the minimal point pairs be inliers. This paper introduces a linear relative pose estimation algorithm for n $( n \\geq 6$) point pairs, which is founded on the recent pose-only imaging geometry to filter out outliers by proper reweighting. The proposed algorithm is able to handle planar degenerate scenes, and enhance robustness and accuracy in the presence of a substantial ratio of outliers. Specifically, we embed the linear global translation (LiGT) constraint into the strategies of iteratively reweighted least-squares (IRLS) and RANSAC so as to realize robust outlier removal. Simulations and real tests of the Strecha dataset show that the proposed algorithm achieves relative rotation accuracy improvement of 2 $\\sim$ 10 times in face of as large as 80% outliers. ",
        "title": "Linear Relative Pose Estimation Founded on Pose-only Imaging Geometry",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13358",
        "abstract_url": "http://arxiv.org/abs/2401.13358",
        "authors": [
            {
                "last_name": "Storvik",
                "first_name": "Erlend"
            },
            {
                "last_name": "Riethm\u00fcller",
                "first_name": "Cedric"
            },
            {
                "last_name": "Both",
                "first_name": "Jakub Wiktor"
            },
            {
                "last_name": "Radu",
                "first_name": "Florin Adrian"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents a study of solution strategies for the Cahn-Hilliard-Biot equations, a complex mathematical model for understanding flow in deformable porous media with changing solid phases. Solving the Cahn-Hilliard-Biot system poses significant challenges due to its coupled, nonlinear and non-convex nature. We explore various solution algorithms, comparing monolithic and splitting strategies, focusing on both their computational efficiency and robustness. ",
        "title": "Sequential solution strategies for the Cahn-Hilliard-Biot model",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13359",
        "abstract_url": "http://arxiv.org/abs/2401.13359",
        "authors": [
            {
                "last_name": "Kutner",
                "first_name": "David C."
            },
            {
                "last_name": "Stewart",
                "first_name": "Iain A."
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC",
            "DM",
            "NI"
        ],
        "abstract": "  The Reconfigurable Routing Problem (RRP) in hybrid networks is, in short, the problem of finding settings for optical switches augmenting a static network so as to achieve optimal delivery of some given workload. The problem has previously been studied in various scenarios with both tractable and NP-hardness results obtained. However, the data center and interconnection networks to which the problem is most relevant are almost always such that the static network is highly structured whereas all previous results assume that the static network can be arbitrary (which makes existing computational hardness results less technologically relevant and also easier to obtain). In this paper, and for the first time, we prove various intractability results for RRP where the underlying static network is highly structured, for example consisting of a hypercube, and also extend some existing tractability results. ",
        "title": "Reconfigurable routing in data center networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13360",
        "abstract_url": "http://arxiv.org/abs/2401.13360",
        "authors": [
            {
                "last_name": "Wei",
                "first_name": "Qi"
            },
            {
                "last_name": "Feng",
                "first_name": "Lei"
            },
            {
                "last_name": "Wang",
                "first_name": "Haobo"
            },
            {
                "last_name": "An",
                "first_name": "Bo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strategy based on two weight-based data samplers. By training on the mixture of two class-discriminative mini-batches, the model mitigates the effect of the imbalanced training set while avoiding sparse representations that are easily caused by sampling strategies. Extensive experiments and analyses demonstrate the effectiveness of ITEM. Our code is available at this url \\href{https://github.com/1998v7/ITEM}{ITEM}. ",
        "title": "Debiased Sample Selection for Combating Noisy Labels",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13361",
        "abstract_url": "http://arxiv.org/abs/2401.13361",
        "authors": [
            {
                "last_name": "Hout",
                "first_name": "Karel J. in 't"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this note we consider the approximation of the Greeks Delta and Gamma of American-style options through the numerical solution of time-dependent partial differential complementarity problems (PDCPs). This approach is very attractive as it can yield accurate approximations to these Greeks at essentially no additional computational cost during the numerical solution of the PDCP for the pertinent option value function. For the temporal discretization, the Crank-Nicolson method is arguably the most popular method in computational finance. It is well-known, however, that this method can have an undesirable convergence behaviour in the approximation of the Greeks Delta and Gamma for American-style options, even when backward Euler damping (Rannacher smoothing) is employed.   In this note we study for the temporal discretization an interesting family of diagonally implicit Runge-Kutta (DIRK) methods together with the two-stage Lobatto IIIC method. Through ample numerical experiments for one- and two-asset American-style options, it is shown that these methods can yield a regular second-order convergence behaviour for the option value as well as for the Greeks Delta and Gamma. A mutual comparison reveals that the DIRK method with suitably chosen parameter $\\theta$ is preferable. ",
        "title": "A note on the numerical approximation of Greeks for American-style  options",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13362",
        "abstract_url": "http://arxiv.org/abs/2401.13362",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Rojas",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Approaching robotic cloth manipulation using reinforcement learning based on visual feedback is appealing as robot perception and control can be learned simultaneously. However, major challenges result due to the intricate dynamics of cloth and the high dimensionality of the corresponding states, what shadows the practicality of the idea. To tackle these issues, we propose TraKDis, a novel Transformer-based Knowledge Distillation approach that decomposes the visual reinforcement learning problem into two distinct stages. In the first stage, a privileged agent is trained, which possesses complete knowledge of the cloth state information. This privileged agent acts as a teacher, providing valuable guidance and training signals for subsequent stages. The second stage involves a knowledge distillation procedure, where the knowledge acquired by the privileged agent is transferred to a vision-based agent by leveraging pre-trained state estimation and weight initialization. TraKDis demonstrates better performance when compared to state-of-the-art RL techniques, showing a higher performance of 21.9%, 13.8%, and 8.3% in cloth folding tasks in simulation. Furthermore, to validate robustness, we evaluate the agent in a noisy environment; the results indicate its ability to handle and adapt to environmental uncertainties effectively. Real robot experiments are also conducted to showcase the efficiency of our method in real-world scenarios. ",
        "title": "TraKDis: A Transformer-based Knowledge Distillation Approach for Visual  Reinforcement Learning with Application to Cloth Manipulation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13363",
        "abstract_url": "http://arxiv.org/abs/2401.13363",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Zhe"
            },
            {
                "last_name": "Wei",
                "first_name": "Kun"
            },
            {
                "last_name": "Yang",
                "first_name": "Xu"
            },
            {
                "last_name": "Deng",
                "first_name": "Cheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human dance generation (HDG) aims to synthesize realistic videos from images and sequences of driving poses. Despite great success, existing methods are limited to generating videos of a single person with specific backgrounds, while the generalizability for real-world scenarios with multiple persons and complex backgrounds remains unclear. To systematically measure the generalizability of HDG models, we introduce a new task, dataset, and evaluation protocol of compositional human dance generation (cHDG). Evaluating the state-of-the-art methods on cHDG, we empirically find that they fail to generalize to real-world scenarios. To tackle the issue, we propose a novel zero-shot framework, dubbed MultiDance-Zero, that can synthesize videos consistent with arbitrary multiple persons and background while precisely following the driving poses. Specifically, in contrast to straightforward DDIM or null-text inversion, we first present a pose-aware inversion method to obtain the noisy latent code and initialization text embeddings, which can accurately reconstruct the composed reference image. Since directly generating videos from them will lead to severe appearance inconsistency, we propose a compositional augmentation strategy to generate augmented images and utilize them to optimize a set of generalizable text embeddings. In addition, consistency-guided sampling is elaborated to encourage the background and keypoints of the estimated clean image at each reverse step to be close to those of the reference image, further improving the temporal consistency of generated videos. Extensive qualitative and quantitative results demonstrate the effectiveness and superiority of our approach. ",
        "title": "Do You Guys Want to Dance: Zero-Shot Compositional Human Dance  Generation with Multiple Persons",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13365",
        "abstract_url": "http://arxiv.org/abs/2401.13365",
        "authors": [
            {
                "last_name": "Karras",
                "first_name": "Oliver"
            },
            {
                "last_name": "G\u00f6pfert",
                "first_name": "Jan"
            },
            {
                "last_name": "Kuckertz",
                "first_name": "Patrick"
            },
            {
                "last_name": "Pelser",
                "first_name": "Tristan"
            },
            {
                "last_name": "Auer",
                "first_name": "S\u00f6ren"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  Engineering sciences, such as energy system research, play an important role in developing solutions to technical, environmental, economic, and social challenges of our modern society. In this context, the transformation of energy systems into climate-neutral systems is one of the key strategies for mitigating climate change. For the transformation of energy systems, engineers model, simulate and analyze scenarios and transformation pathways to initiate debates about possible transformation strategies. For these debates and research in general, all steps of the research process must be traceable to guarantee the trustworthiness of published results, avoid redundancies, and ensure their social acceptance. However, the analysis of energy systems is an interdisciplinary field as the investigations of large, complex energy systems often require the use of different software applications and large amounts of heterogeneous data. Engineers must therefore communicate, understand, and (re)use heterogeneous scientific knowledge and data. Although the importance of FAIR scientific knowledge and data in the engineering sciences and energy system research is increasing, little research has been conducted on this topic. When it comes to publishing scientific knowledge and data from publications, software, and datasets (such as models, scenarios, and simulations) openly available and transparent, energy system research lags behind other research domains. According to Schmitt et al. and Nie{\\ss}e et al., engineers need technical support in the form of infrastructures, services, and terminologies to improve communication, understanding, and (re)use of scientific knowledge and data. ",
        "title": "Organizing Scientific Knowledge From Energy System Research Using the  Open Research Knowledge Graph",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13366",
        "abstract_url": "http://arxiv.org/abs/2401.13366",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Jikun"
            },
            {
                "last_name": "Mavromatis",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Li",
                "first_name": "Peizheng"
            },
            {
                "last_name": "Carnelli",
                "first_name": "Pietro"
            },
            {
                "last_name": "Khan",
                "first_name": "Aftab"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable global model training despite limiting client resources and statistical data heterogeneity. This improves robustness and scalability for real-world FL deployments. ",
        "title": "Mitigating System Bias in Resource Constrained Asynchronous Federated  Learning Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13369",
        "abstract_url": "http://arxiv.org/abs/2401.13369",
        "authors": [
            {
                "last_name": "Dolgorukov",
                "first_name": "Vitaliy"
            },
            {
                "last_name": "Galimullin",
                "first_name": "Rustam"
            },
            {
                "last_name": "Gladyshev",
                "first_name": "Maksim"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Logics for resource-bounded agents have been getting more and more attention in recent years since they provide us with more realistic tools for modelling and reasoning about multi-agent systems. While many existing approaches are based on the idea of agents as imperfect reasoners, who must spend their resources to perform logical inference, this is not the only way to introduce resource constraints into logical settings. In this paper we study agents as perfect reasoners, who may purchase a new piece of information from a trustworthy source. For this purpose we propose dynamic epistemic logic for semi-public queries for resource-bounded agents. In this logic (groups of) agents can perform a query (ask a question) about whether some formula is true and receive a correct answer. These queries are called semi-public, because the very fact of the query is public, while the answer is private. We also assume that every query has a cost and every agent has a budget constraint. Finally, our framework allows us to reason about group queries, in which agents may share resources to obtain a new piece of information together. We demonstrate that our logic is complete, decidable and has an efficient model checking procedure. ",
        "title": "Dynamic Epistemic Logic of Resource Bounded Information Mining Agents",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13382",
        "abstract_url": "http://arxiv.org/abs/2401.13382",
        "authors": [
            {
                "last_name": "Das",
                "first_name": "Anupam"
            },
            {
                "last_name": "De",
                "first_name": "Abhishek"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "FL"
        ],
        "abstract": "  Right-linear (or left-linear) grammars are a well-known class of context-free grammars computing just the regular languages. They may naturally be written as expressions with (least) fixed points but with products restricted to letters as left arguments, giving an alternative to the syntax of regular expressions. In this work, we investigate the resulting logical theory of this syntax. Namely, we propose a theory of right-linear algebras (RLA) over of this syntax and a cyclic proof system CRLA for reasoning about them.   We show that CRLA is sound and complete for the intended model of regular languages. From here we recover the same completeness result for RLA by extracting inductive invariants from cyclic proofs, rendering the model of regular languages the free right-linear algebra.   Finally, we extend system CRLA by greatest fixed points, nuCRLA, naturally modelled by languages of omega-words thanks to right-linearity. We show a similar soundness and completeness result of (the guarded fragment of) nuCRLA for the model of omega-regular languages, employing game theoretic techniques. ",
        "title": "A proof theory of right-linear (omega-)grammars via cyclic proofs",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13384",
        "abstract_url": "http://arxiv.org/abs/2401.13384",
        "authors": [
            {
                "last_name": "Caragiannis",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Kalantzis",
                "first_name": "Georgios"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We consider the fundamental problem of designing a truthful single-item auction with the challenging objective of extracting a large fraction of the highest agent valuation as revenue. Following a recent trend in algorithm design, we assume that the agent valuations belong to a known interval, and a (possibly erroneous) prediction for the highest valuation is available. Then, auction design aims for high consistency and robustness, meaning that, for appropriate pairs of values $\\gamma$ and $\\rho$, the extracted revenue should be at least a $\\gamma$- or $\\rho$-fraction of the highest valuation when the prediction is correct for the input instance or not. We characterize all pairs of parameters $\\gamma$ and $\\rho$ so that a randomized $\\gamma$-consistent and $\\rho$-robust auction exists. Furthermore, for the setting in which robustness can be a function of the prediction error, we give sufficient and necessary conditions for the existence of robust auctions and present randomized auctions that extract a revenue that is only a polylogarithmic (in terms of the prediction error) factor away from the highest agent valuation. ",
        "title": "Randomized learning-augmented auctions with revenue guarantees",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13386",
        "abstract_url": "http://arxiv.org/abs/2401.13386",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Dong"
            },
            {
                "last_name": "Li",
                "first_name": "Yong"
            },
            {
                "last_name": "Denzler",
                "first_name": "Joachim"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face recognition technology has been deployed in various real-life applications. The most sophisticated deep learning-based face recognition systems rely on training millions of face images through complex deep neural networks to achieve high accuracy. It is quite common for clients to upload face images to the service provider in order to access the model inference. However, the face image is a type of sensitive biometric attribute tied to the identity information of each user. Directly exposing the raw face image to the service provider poses a threat to the user's privacy. Current privacy-preserving approaches to face recognition focus on either concealing visual information on model input or protecting model output face embedding. The noticeable drop in recognition accuracy is a pitfall for most methods. This paper proposes a hybrid frequency-color fusion approach to reduce the input dimensionality of face recognition in the frequency domain. Moreover, sparse color information is also introduced to alleviate significant accuracy degradation after adding differential privacy noise. Besides, an identity-specific embedding mapping scheme is applied to protect original face embedding by enlarging the distance among identities. Lastly, secure multiparty computation is implemented for safely computing the embedding distance during model inference. The proposed method performs well on multiple widely used verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy than the state-of-the-art in the 1:N verification scenario. ",
        "title": "Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13387",
        "abstract_url": "http://arxiv.org/abs/2401.13387",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ping"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The year 1948 witnessed the historic moment of the birth of classic information theory (CIT). Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function $H(U)$, channel capacity $C=\\max_{p(x)}I(X;Y)$ and rate-distortion function $R(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d(x,\\hat{x})\\leq D} I(X;\\hat{X})$. Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case. Especially, for band-limited Gaussian channel, we obtain a new channel capacity formula, $C_s=B\\log\\left[S^4\\left(1+\\frac{P}{N_0B}\\right)\\right]$ with the synonymous length $S$. ",
        "title": "A Mathematical Theory of Semantic Communication",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13390",
        "abstract_url": "http://arxiv.org/abs/2401.13390",
        "authors": [
            {
                "last_name": "Kiefer",
                "first_name": "Stefan"
            },
            {
                "last_name": "Mayr",
                "first_name": "Richard"
            },
            {
                "last_name": "Shirmohammadi",
                "first_name": "Mahsa"
            },
            {
                "last_name": "Totzke",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO",
            "GT"
        ],
        "abstract": "  We study concurrent stochastic reachability games played on finite graphs. Two players, Max and Min, seek respectively to maximize and minimize the probability of reaching a set of target states. We prove that Max has a memoryless strategy that is optimal from all states that have an optimal strategy. Our construction provides an alternative proof of this result by Bordais, Bouyer and Le Roux, and strengthens it, as we allow Max's action sets to be countably infinite. ",
        "title": "Memoryless Strategies in Stochastic Reachability Games",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13391",
        "abstract_url": "http://arxiv.org/abs/2401.13391",
        "authors": [
            {
                "last_name": "Goethals",
                "first_name": "Sofie"
            },
            {
                "last_name": "Calders",
                "first_name": "Toon"
            },
            {
                "last_name": "Martens",
                "first_name": "David"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a paradigm shift: initially, we should focus on generating the most precise ranking for each subgroup. Following this, individuals should be chosen from these rankings to meet both fairness standards and practical considerations. ",
        "title": "Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely  on between-group metrics",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13394",
        "abstract_url": "http://arxiv.org/abs/2401.13394",
        "authors": [
            {
                "last_name": "Jia",
                "first_name": "Xue"
            },
            {
                "last_name": "Yue",
                "first_name": "Qin"
            },
            {
                "last_name": "Sun",
                "first_name": "Huan"
            },
            {
                "last_name": "Sui",
                "first_name": "Junzhen"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we provide conditions that hulls of generalized Reed-Solomon (GRS) codes are also GRS codes from algebraic geometry codes. If the conditions are not satisfied, we provide a method of linear algebra to find the bases of hulls of GRS codes and give formulas to compute their dimensions. Besides, we explain that the conditions are too good to be improved by some examples. Moreover, we show self-orthogonal and self-dual GRS codes. ",
        "title": "Determining hulls of generalized Reed-Solomon codes from algebraic  geometry codes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13398",
        "abstract_url": "http://arxiv.org/abs/2401.13398",
        "authors": [
            {
                "last_name": "Turki",
                "first_name": "Houcemeddine"
            },
            {
                "last_name": "Etori",
                "first_name": "Naome A."
            },
            {
                "last_name": "Taieb",
                "first_name": "Mohamed Ali Hadj"
            },
            {
                "last_name": "Omotayo",
                "first_name": "Abdul-Hakeem"
            },
            {
                "last_name": "Emezue",
                "first_name": "Chris Chinenye"
            },
            {
                "last_name": "Aouicha",
                "first_name": "Mohamed Ben"
            },
            {
                "last_name": "Awokoya",
                "first_name": "Ayodele"
            },
            {
                "last_name": "Lawan",
                "first_name": "Falalu Ibrahim"
            },
            {
                "last_name": "Nixdorf",
                "first_name": "Doreen"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French. By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages. Nevertheless, linguistic variances result in lower detection rates for certain languages. Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category. Uncommon stopwords add depth to text but their classification as stopwords depends on context. Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method. This research enhances NLP for African languages and underscores the importance of text categorization in stopword extraction. ",
        "title": "Text Categorization Can Enhance Domain-Agnostic Stopword Extraction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13399",
        "abstract_url": "http://arxiv.org/abs/2401.13399",
        "authors": [
            {
                "last_name": "Bluhm",
                "first_name": "Marcel"
            },
            {
                "last_name": "Vasiljevi\u0107",
                "first_name": "Adrian Cachinero"
            },
            {
                "last_name": "Derivaux",
                "first_name": "S\u00e9bastien"
            },
            {
                "last_name": "Jessen",
                "first_name": "S\u00f8ren Terp H\u00f8rl\u00fcck"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Stablecoins have turned out to be the \"killer\" use case of the growing digital asset space. However, risk management frameworks, including regulatory ones, have been largely absent. In this paper, we address the critical question of measuring and managing risk in stablecoin protocols, which operate on public blockchain infrastructure. The on-chain environment makes it possible to monitor risk and automate its management via transparent smart-contracts in real-time. We propose two risk metrics covering capitalization and liquidity of stablecoin protocols. We then explore in a case-study type analysis how our risk management framework can be applied to DAI, the biggest decentralized stablecoin by market capitalisation to-date, governed by MakerDAO. Based on our findings, we recommend that the protocol explores implementing automatic capital buffer adjustments and dynamic maturity gap matching. Our analysis demonstrates the practical benefits for scalable (prudential) risk management stemming from real-time availability of high-quality, granular, tamper-resistant on-chain data in the digital asset space. We name this approach Crypto Asset-Liability Management (CALM). ",
        "title": "Real-time Risk Metrics for Programmatic Stablecoin Crypto  Asset-Liability Management (CALM)",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13400",
        "abstract_url": "http://arxiv.org/abs/2401.13400",
        "authors": [
            {
                "last_name": "Estevan",
                "first_name": "Asier"
            },
            {
                "last_name": "Min\u00e3na",
                "first_name": "Juan-Jos\u00e9"
            },
            {
                "last_name": "Valero",
                "first_name": "Oscar"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The celebrated Kleene fixed point theorem is crucial in the mathematical modelling of recursive specifications in Denotational Semantics. In this paper we discuss whether the hypothesis of the aforementioned result can be weakened. An affirmative answer to the aforesaid inquiry is provided so that a characterization of those properties that a self-mapping must satisfy in order to guarantee that its set of fixed points is non-empty when no notion of completeness are assumed to be satisfied by the partially ordered set. Moreover, the case in which the partially ordered set is coming from a quasi-metric space is treated in depth. Finally, an application of the exposed theory is obtained. Concretely, a mathematical method to discuss the asymptotic complexity of those algorithms whose running time of computing fulfills a recurrence equation is presented. Moreover, the aforesaid method retrieves the fixed point based methods that appear in the literature for asymptotic complexity analysis of algorithms. However, our new method improves the aforesaid methods because it imposes fewer requirements than those that have been assumed in the literature and, in addition, it allows to state simultaneously upper and lower asymptotic bounds for the running time computing. ",
        "title": "On fixed point theory in partially ordered sets and an application to  asymptotic complexity of algorithms",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13403",
        "abstract_url": "http://arxiv.org/abs/2401.13403",
        "authors": [
            {
                "last_name": "Olisah",
                "first_name": "Chollette C."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite the advancement in computational modeling towards brain tumor segmentation, of which several models have been developed, it is evident from the computational complexity of existing models which are still at an all-time high, that performance and efficiency under clinical application scenarios are limited. Therefore, this paper proposes a shallow encoder and decoder network named SEDNet for brain tumor segmentation. The proposed network is adapted from the U-Net structure. Though brain tumors do not assume complex structures like the task the traditional U-Net was designed for, their variance in appearance, shape, and ambiguity of boundaries makes it a compelling complex task to solve. SEDNet architecture design is inspired by the localized nature of brain tumors in brain images, thus consists of sufficient hierarchical convolutional blocks in the encoding pathway capable of learning the intrinsic features of brain tumors in brain slices, and a decoding pathway with selective skip path sufficient for capturing miniature local-level spatial features alongside the global-level features of brain tumor. SEDNet with the integration of the proposed preprocessing algorithm and optimization function on the BraTS2020 set reserved for testing achieves impressive dice and Hausdorff scores of 0.9308, 0.9451, 0.9026, and 0.7040, 1.2866, 0.7762 for non-enhancing tumor core (NTC), peritumoral edema (ED), and enhancing tumor (ET), respectively. Furthermore, through transfer learning with initialized SEDNet pre-trained weights, termed SEDNetX, a performance increase is observed. The dice and Hausdorff scores recorded are 0.9336, 0.9478, 0.9061, 0.6983, 1.2691, and 0.7711 for NTC, ED, and ET, respectively. With about 1.3 million parameters and impressive performance in comparison to the state-of-the-art, SEDNet(X) is shown to be computationally efficient for real-time clinical diagnosis. ",
        "title": "SEDNet: Shallow Encoder-Decoder Network for Brain Tumor Segmentation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13405",
        "abstract_url": "http://arxiv.org/abs/2401.13405",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Dongmyoung"
            },
            {
                "last_name": "Chen",
                "first_name": "Wei"
            },
            {
                "last_name": "Rojas",
                "first_name": "Nicolas"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation. In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset). Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically. RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects. The object to grasp is then determined by the confidence score of the segmentation algorithm. Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively. Supplementary material is available at https://sites.google.com/view/synthetic-dataset-generation. ",
        "title": "Synthetic data enables faster annotation and robust segmentation for  multi-object grasping in clutter",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13407",
        "abstract_url": "http://arxiv.org/abs/2401.13407",
        "authors": [
            {
                "last_name": "Borg",
                "first_name": "Markus"
            },
            {
                "last_name": "Pruvost",
                "first_name": "Ilyana"
            },
            {
                "last_name": "Mones",
                "first_name": "Enys"
            },
            {
                "last_name": "Tornhill",
                "first_name": "Adam"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Understanding and effectively managing Technical Debt (TD) remains a vital challenge in software engineering. While many studies on code-level TD have been published, few illustrate the business impact of low-quality source code. In this study, we combine two publicly available datasets to study the association between code quality on the one hand, and defect count and implementation time on the other hand. We introduce a value-creation model, derived from regression analyses, to explore relative changes from a baseline. Our results show that the associations vary across different intervals of code quality. Furthermore, the value model suggests strong non-linearities at the extremes of the code quality spectrum. Most importantly, the model suggests amplified returns on investment in the upper end. We discuss the findings within the context of the \"broken windows\" theory and recommend organizations to diligently prevent the introduction of code smells in files with high churn. Finally, we argue that the value-creation model can be used to initiate discussions regarding the return on investment in refactoring efforts. ",
        "title": "Increasing, not Diminishing: Investigating the Returns of Highly  Maintainable Code",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13408",
        "abstract_url": "http://arxiv.org/abs/2401.13408",
        "authors": [
            {
                "last_name": "Alvarez",
                "first_name": "Jose M."
            },
            {
                "last_name": "Ruggieri",
                "first_name": "Salvatore"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC"
        ],
        "abstract": "  Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal properties of faithfulness and consistency. We illustrate our framework through a series of decision-making examples and discuss relevant fairness applications. The goal of this work is to position perception as a parameter of interest, useful for extending the standard, single interpretation ADM problem formulation. ",
        "title": "Causal Perception",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13410",
        "abstract_url": "http://arxiv.org/abs/2401.13410",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Shuyi"
            },
            {
                "last_name": "Liu",
                "first_name": "Bing"
            },
            {
                "last_name": "Zuccon",
                "first_name": "Guido"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "IR",
            "LG"
        ],
        "abstract": "  Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \\textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation.   In this paper, we study an effective and efficient unlearning method that can remove a client's contribution without compromising the overall ranker effectiveness and without needing to retrain the global ranker from scratch. A key challenge is how to measure whether the model has unlearned the contributions from the client $c^*$ that has requested removal. For this, we instruct $c^*$ to perform a poisoning attack (add noise to this client updates) and then we measure whether the impact of the attack is lessened when the unlearning process has taken place. Through experiments on four datasets, we demonstrate the effectiveness and efficiency of the unlearning strategy under different combinations of parameter settings. ",
        "title": "How to Forget Clients in Federated Online Learning to Rank?",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13414",
        "abstract_url": "http://arxiv.org/abs/2401.13414",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Xingyu"
            },
            {
                "last_name": "Li",
                "first_name": "Zhan"
            },
            {
                "last_name": "Chen",
                "first_name": "Shi"
            },
            {
                "last_name": "Demachi",
                "first_name": "Kazuyuki"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training. ",
        "title": "GTAutoAct: An Automatic Datasets Generation Framework Based on Game  Engine Redevelopment for Action Recognition",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13416",
        "abstract_url": "http://arxiv.org/abs/2401.13416",
        "authors": [
            {
                "last_name": "Rife",
                "first_name": "Jason"
            },
            {
                "last_name": "McDermott",
                "first_name": "Matthew"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper quantifies an error source that limits the accuracy of lidar scan matching, particularly for voxel-based methods. Lidar scan matching, which is used in dead reckoning (also known as lidar odometry) and mapping, computes the rotation and translation that best align a pair of point clouds. Perspective errors occur when a scene is viewed from different angles, with different surfaces becoming visible or occluded from each viewpoint. To explain perspective anomalies observed in data, this paper models perspective errors for two objects representative of urban landscapes: a cylindrical column and a dual-wall corner. For each object, we provide an analytical model of the perspective error for voxel-based lidar scan matching. We then analyze how perspective errors accumulate as a lidar-equipped vehicle moves past these objects. ",
        "title": "Characterizing Perspective Error in Voxel-Based Lidar Scan Matching",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13418",
        "abstract_url": "http://arxiv.org/abs/2401.13418",
        "authors": [
            {
                "last_name": "Marcialis",
                "first_name": "Gian Luca"
            },
            {
                "last_name": "Mastinu",
                "first_name": "Paolo"
            },
            {
                "last_name": "Roli",
                "first_name": "Fabio"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Serial, or sequential, fusion of multiple biometric matchers has been not thoroughly investigated so far. However, this approach exhibits some advantages with respect to the widely adopted parallel approaches. In this paper, we propose a novel theoretical framework for the assessment of performance of such systems, based on a previous work of the authors. Benefits in terms of performance are theoretically evaluated, as well as estimation errors in the model parameters computation. Model is analyzed from the viewpoint of its pros and cons, by mean of preliminary experiments performed on NIST Biometric Score Set 1. ",
        "title": "Serial fusion of multi-modal biometric systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13420",
        "abstract_url": "http://arxiv.org/abs/2401.13420",
        "authors": [
            {
                "last_name": "L\u00f3pez-Mart\u00ednez",
                "first_name": "Javier"
            },
            {
                "last_name": "Blanco-Claraco",
                "first_name": "Jos\u00e9 Luis"
            },
            {
                "last_name": "P\u00e9rez-Alonso",
                "first_name": "Jos\u00e9"
            },
            {
                "last_name": "Callej\u00f3n-Ferre",
                "first_name": "\u00c1ngel Jes\u00fas"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  In Mediterranean countries of Southern Europe, the climatic conditions are usually favourable to cultivate greenhouse vegetables but not always for workers. The aim of this study was to design a network of weather stations capable of gathering data of environmental parameters related to the wellbeing of workers in greenhouses in south-eastern Spain. The unevenness of the thermal environment was studied both vertically as well as horizontally following guideline ISO 7726. The results indicate that the greenhouse should be considered a heterogeneous environment, implying that, for an evaluation of the environmental conditions related to thermal stress of the workers inside the greenhouse, measurements should be taken at different points within the greenhouse at three heights (ankle, abdomen, and head). ",
        "title": "Distributed network for measuring climatic parameters in heterogeneous  environments: Application in a greenhouse",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13421",
        "abstract_url": "http://arxiv.org/abs/2401.13421",
        "authors": [
            {
                "last_name": "Daskin",
                "first_name": "Ammar"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "LG"
        ],
        "abstract": "  The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.   In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model parameters, but instead sends the operator as a quantum state, the clients are not required to share the model. This allows for the creation of asynchronous learning models. In addition, the model as a quantum state is fed into client-side chips directly; therefore, it does not require measurements on the upcoming quantum state to obtain model parameters in order to compute gradients. This can provide efficiency over the models where parameter vector is sent via classical or quantum channels and local gradients are obtained through the obtained values of these parameters. ",
        "title": "Federated learning with distributed fixed design quantum chips and  quantum channels",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13428",
        "abstract_url": "http://arxiv.org/abs/2401.13428",
        "authors": [
            {
                "last_name": "Buckwar",
                "first_name": "Evelyn"
            },
            {
                "last_name": "Meddah",
                "first_name": "Amira"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we focus on numerical approximations of Piecewise Diffusion Markov Processes (PDifMPs), particularly when the explicit flow maps are unavailable. Our approach is based on the thinning method for modelling the jump mechanism and combines the Euler-Maruyama scheme to approximate the underlying flow dynamics. For the proposed approximation schemes, we study both the mean-square and weak convergence. Weak convergence of the algorithms is established by a martingale problem formulation. Moreover, we employ these results to simulate the migration patterns exhibited by moving glioma cells at the microscopic level. Further, we develop and implement a splitting method for this PDifMP model and employ both the Thinned Euler-Maruyama and the splitting scheme in our simulation example, allowing us to compare both methods. ",
        "title": "Numerical Approximations and Convergence Analysis of Piecewise Diffusion  Markov Processes, with Application to Glioma Cell Migration",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13429",
        "abstract_url": "http://arxiv.org/abs/2401.13429",
        "authors": [
            {
                "last_name": "Elimelech",
                "first_name": "Dor"
            },
            {
                "last_name": "Huleihel",
                "first_name": "Wasim"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "LG"
        ],
        "abstract": "  In this paper, we investigate the problem of deciding whether two standard normal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and $\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\\mathsf{X}$ and a randomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with correlation $\\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices, and furthermore allow for partial correlations between these two. ",
        "title": "Detection of Correlated Random Vectors",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13432",
        "abstract_url": "http://arxiv.org/abs/2401.13432",
        "authors": [
            {
                "last_name": "Nie",
                "first_name": "Lang"
            },
            {
                "last_name": "Lin",
                "first_name": "Chunyu"
            },
            {
                "last_name": "Liao",
                "first_name": "Kang"
            },
            {
                "last_name": "Liu",
                "first_name": "Shuaicheng"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation cost, we develop a semi-supervised learning scheme to improve warping quality by exploiting unlabeled data. It is formulated through dual transformation between the searched control points of unlabeled data and its graphic augmentation, yielding an implicit correction consistency constraint. Finally, we collect massive unlabeled data to exhibit the benefit of our semi-supervised scheme in rotation correction. Extensive experiments demonstrate the superiority and universality of CoupledTPS over the existing state-of-the-art (SoTA) solutions for rotation correction and beyond. The code and data will be available at https://github.com/nie-lang/CoupledTPS. ",
        "title": "Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction  and Beyond",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13434",
        "abstract_url": "http://arxiv.org/abs/2401.13434",
        "authors": [
            {
                "last_name": "Jaenich",
                "first_name": "Thomas"
            },
            {
                "last_name": "McDonald",
                "first_name": "Graham"
            },
            {
                "last_name": "Ounis",
                "first_name": "Iadh"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query. To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model. However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage. Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking. For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions. With this in mind, it is beneficial to predict the amount of exposure that a group of documents is likely to receive in the results of the first stage retrieval process, in order to ensure that there are a sufficient number of documents included from each of the groups. In this paper, we introduce the novel task of query exposure prediction (QEP). Specifically, we propose the first approach for predicting the distribution of exposure that groups of documents will receive for a given query. Our new approach, called GEP, uses lexical information from individual groups of documents to estimate the exposure the groups will receive in a ranking. Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our proposed GEP approach results in exposure predictions that are up to 40 % more accurate than the predictions of adapted existing query performance prediction and resource allocation approaches. ",
        "title": "Query Exposure Prediction for Groups of Documents in Rankings",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13438",
        "abstract_url": "http://arxiv.org/abs/2401.13438",
        "authors": [
            {
                "last_name": "Van Mulders",
                "first_name": "Jarne"
            },
            {
                "last_name": "Cox",
                "first_name": "Bert"
            },
            {
                "last_name": "Deutschmann",
                "first_name": "Benjamin J. B."
            },
            {
                "last_name": "Callebaut",
                "first_name": "Gilles"
            },
            {
                "last_name": "de Strycker",
                "first_name": "Lieven"
            },
            {
                "last_name": "Van der Perre",
                "first_name": "Liesbet"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Keeping the batteries on the shelf: this is the holy grail for low-cost Internet of Things (IoT) nodes. In this paper we study the potential of radio frequency (RF)-based wireless power transfer implementing coherent beamforming with many antennas to realize this ambitious target. We optimize the deployment of the antennas to charge electronic shelf labels (ESLs), considering actual regulatory constraints. The results confirm the feasibility to create power spots that are sufficient to keep the high density of battery-less devices operational. ",
        "title": "Keeping Energy-Neutral Devices Operational: a Coherent Massive  Beamforming Approach",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13439",
        "abstract_url": "http://arxiv.org/abs/2401.13439",
        "authors": [
            {
                "last_name": "Walker",
                "first_name": "Kyle L."
            },
            {
                "last_name": "Della Santina",
                "first_name": "Cosimo"
            },
            {
                "last_name": "Giorgio-Serchi",
                "first_name": "Francesco"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Inspired by the octopus and other animals living in water, soft robots should naturally lend themselves to underwater operations, as supported by encouraging validations in deep water scenarios. This work deals with equipping soft arms with the intelligence necessary to move precisely in wave-dominated environments, such as shallow waters where marine renewable devices are located. This scenario is substantially more challenging than calm deep water since, at low operational depths, hydrodynamic wave disturbances can represent a significant impediment. We propose a control strategy based on Nonlinear Model Predictive Control that can account for wave disturbances explicitly, optimising control actions by considering an estimate of oncoming hydrodynamic loads. The proposed strategy is validated through a set of tasks covering set-point regulation, trajectory tracking and mechanical failure compensation, all under a broad range of varying significant wave heights and peak spectral periods. The proposed control methodology displays positional error reductions as large as 84% with respect to a baseline controller, proving the effectiveness of the method. These initial findings present a first step in the development and deployment of soft manipulators for performing tasks in hazardous water environments. ",
        "title": "Model Predictive Wave Disturbance Rejection for Underwater Soft Robotic  Manipulators",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13441",
        "abstract_url": "http://arxiv.org/abs/2401.13441",
        "authors": [
            {
                "last_name": "St\u00f6lzle",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Baberwal",
                "first_name": "Sonal Santosh"
            },
            {
                "last_name": "Rus",
                "first_name": "Daniela"
            },
            {
                "last_name": "Coyle",
                "first_name": "Shirley"
            },
            {
                "last_name": "Della Santina",
                "first_name": "Cosimo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings. Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans. This work presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots. We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels. A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a new Cartesian impedance controller. We specifically focus here on planar soft robot-based architected metamaterials, which require the development of a novel control architecture to deal with the peculiar nonlinearities - e.g., non-affinity in control. We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation. We observe that the user reaches the proximity of the setpoint in 66% of steps and that for successful steps, the average response time is 21.5s. We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness. ",
        "title": "Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance  Control",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13442",
        "abstract_url": "http://arxiv.org/abs/2401.13442",
        "authors": [
            {
                "last_name": "Fang",
                "first_name": "Yiming"
            },
            {
                "last_name": "Chen",
                "first_name": "Li"
            },
            {
                "last_name": "Chen",
                "first_name": "Yunfei"
            },
            {
                "last_name": "Yin",
                "first_name": "Huarui"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Efficient implementation of massive multiple-input-multiple-output (MIMO) transceivers is essential for the next-generation wireless networks. To reduce the high computational complexity of the massive MIMO transceiver, in this paper, we propose a new massive MIMO architecture using finite-precision arithmetic. First, we conduct the rounding error analysis and derive the lower bound of the achievable rate for single-input-multiple-output (SIMO) using maximal ratio combining (MRC) and multiple-input-single-output (MISO) systems using maximal ratio transmission (MRT) with finite-precision arithmetic. Then, considering the multi-user scenario, the rounding error analysis of zero-forcing (ZF) detection and precoding is derived by using the normal equations (NE) method. The corresponding lower bounds of the achievable sum rate are also derived and asymptotic analyses are presented. Built upon insights from these analyses and lower bounds, we propose a mixed-precision architecture for massive MIMO systems to offset performance gaps due to finite-precision arithmetic. The corresponding analysis of rounding errors and computational costs is obtained. Simulation results validate the derived bounds and underscore the superiority of the proposed mixed-precision architecture to the conventional structure. ",
        "title": "Finite-Precision Arithmetic Transceiver for Massive MIMO Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13444",
        "abstract_url": "http://arxiv.org/abs/2401.13444",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Dehao"
            },
            {
                "last_name": "Huang",
                "first_name": "Feng"
            },
            {
                "last_name": "Huang",
                "first_name": "Yongfeng"
            },
            {
                "last_name": "Jiang",
                "first_name": "Minghu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs with fewer parameters. In some instances, even ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4. Furthermore, the results indicate a minimal invocation frequency of CGPE on LLMs, suggesting reduced computational overhead. For organizations and individuals facing constraints in computational resources, our research offers significant practical value. ",
        "title": "Clue-Guided Path Exploration: An Efficient Knowledge Base  Question-Answering Framework with Low Computational Resource Consumption",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13447",
        "abstract_url": "http://arxiv.org/abs/2401.13447",
        "authors": [
            {
                "last_name": "Dabelow",
                "first_name": "Lennart"
            },
            {
                "last_name": "Ueda",
                "first_name": "Masahito"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "SC"
        ],
        "abstract": "  Machine-learning methods are gradually being adopted in a great variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics. A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations. Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function). Thus far, these rules have usually needed to be discovered and subsequently programmed by humans. Focusing on the paradigmatic example of solving linear equations in symbolic form, we demonstrate how the process of finding elementary transformation rules and step-by-step solutions can be automated using reinforcement learning with deep neural networks. ",
        "title": "Symbolic Equation Solving via Reinforcement Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13448",
        "abstract_url": "http://arxiv.org/abs/2401.13448",
        "authors": [
            {
                "last_name": "Zheng",
                "first_name": "Ruiqi"
            },
            {
                "last_name": "Qu",
                "first_name": "Liang"
            },
            {
                "last_name": "Chen",
                "first_name": "Tong"
            },
            {
                "last_name": "Cui",
                "first_name": "Lizhen"
            },
            {
                "last_name": "Shi",
                "first_name": "Yuhui"
            },
            {
                "last_name": "Yin",
                "first_name": "Hongzhi"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance. To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration. It first generates a desensitized public reference data pool with transformation and probability data generation methods. For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function. Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors. During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data. Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data. ",
        "title": "Decentralized Collaborative Learning with Adaptive Reference Data for  On-Device POI Recommendation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13451",
        "abstract_url": "http://arxiv.org/abs/2401.13451",
        "authors": [
            {
                "last_name": "del-Pino-L\u00f3pez",
                "first_name": "Juan Carlos"
            },
            {
                "last_name": "Cruz-Romero",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Recently, large offshore wind power plants have been installed far from the shore, using long HVAC three-core armored cables to export power. Its high capacitance may contribute to the appearance of unwanted phenomena, such as overvoltages or resonances at low frequencies. To adequately assess these problems, detailed and reliable cable models are required to develop time-domain/frequency-domain analyses on this type of cables. This paper presents, for the first time in the literature, an assessment on the performance of 3D finite element method-based (3D-FEM) models for developing frequency-domain analyses on three-core armored cables, confronting simulation results with experimental measurements found in the literature for three real cables. To this aim, a simplified ultra-shortened 3D-FEM model is proposed to reduce the simulation time during frequency sweeps, through which relevant aspects never analyzed before with frequency-domain 3D-FEM simulations are addressed, such as total losses, induced sheath current, magnetic field around the power cable, positive and zero sequence harmonic impedances, as well as resonant frequencies. Also, a time-domain example derived from the frequency-domain analysis is provided. Remarkable results are obtained when comparing computed values and measurements, presenting the simplified ultra-shortened 3DFEM model as a valuable tool for the frequency-domain analysis of these cables. ",
        "title": "Experimental validation of ultra-shortened 3D finite element models for  frequency-domain analyses of three-core armored cables",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13460",
        "abstract_url": "http://arxiv.org/abs/2401.13460",
        "authors": [
            {
                "last_name": "Samvelyan",
                "first_name": "Mikayel"
            },
            {
                "last_name": "Paglieri",
                "first_name": "Davide"
            },
            {
                "last_name": "Jiang",
                "first_name": "Minqi"
            },
            {
                "last_name": "Parker-Holder",
                "first_name": "Jack"
            },
            {
                "last_name": "Rockt\u00e4schel",
                "first_name": "Tim"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "MA"
        ],
        "abstract": "  In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning. Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which \"masters\" the game through 45 days of training on a large-scale distributed infrastructure. We expose key shortcomings in TiZero's tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems. ",
        "title": "Multi-Agent Diagnostics for Robustness via Illuminated Diversity",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13462",
        "abstract_url": "http://arxiv.org/abs/2401.13462",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Shoujie"
            },
            {
                "last_name": "Yu",
                "first_name": "Ran"
            },
            {
                "last_name": "Wu",
                "first_name": "Tong"
            },
            {
                "last_name": "Zhong",
                "first_name": "JunWen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xiao-Ping"
            },
            {
                "last_name": "Ding",
                "first_name": "Wenbo"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Intelligent robot is the ultimate goal in the robotics field. Existing works leverage learning-based or optimization-based methods to accomplish human-defined tasks. However, the challenge of enabling robots to explore various environments autonomously remains unresolved. In this work, we propose a framework named GExp, which enables robots to explore and learn autonomously without human intervention. To achieve this goal, we devise modules including self-exploration, knowledge-base-building, and close-loop feedback based on foundation models. Inspired by the way that infants interact with the world, GExp encourages robots to understand and explore the environment with a series of self-generated tasks. During the process of exploration, the robot will acquire skills from beneficial experiences that are useful in the future. GExp provides robots with the ability to solve complex tasks through self-exploration. GExp work is independent of prior interactive knowledge and human intervention, allowing it to adapt directly to different scenarios, unlike previous studies that provided in-context examples as few-shot learning. In addition, we propose a workflow of deploying the real-world robot system with self-learned skills as an embodied assistant. ",
        "title": "Growing from Exploration: A self-exploring framework for robots based on  foundation models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13463",
        "abstract_url": "http://arxiv.org/abs/2401.13463",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Chyi-Jiunn"
            },
            {
                "last_name": "Lin",
                "first_name": "Guan-Ting"
            },
            {
                "last_name": "Chuang",
                "first_name": "Yung-Sung"
            },
            {
                "last_name": "Wu",
                "first_name": "Wei-Lun"
            },
            {
                "last_name": "Li",
                "first_name": "Shang-Wen"
            },
            {
                "last_name": "Mohamed",
                "first_name": "Abdelrahman"
            },
            {
                "last_name": "Lee",
                "first_name": "Hung-yi"
            },
            {
                "last_name": "Lee",
                "first_name": "Lin-shan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "IR",
            "SD"
        ],
        "abstract": "  Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robust to speech recognition errors. ",
        "title": "SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken  Question Answering",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13464",
        "abstract_url": "http://arxiv.org/abs/2401.13464",
        "authors": [
            {
                "last_name": "del Moral",
                "first_name": "David Lopez"
            },
            {
                "last_name": "Barrado",
                "first_name": "Andres"
            },
            {
                "last_name": "Sanz",
                "first_name": "Marina"
            },
            {
                "last_name": "Lazaro",
                "first_name": "Antonio"
            },
            {
                "last_name": "Zumel",
                "first_name": "Pablo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The mismatching phenomenon is one of the main issues in photovoltaic (PV) applications. It could reduce the generated power of a string when a PV panel has different performances from the other PV panels connected to the same string. Distributed Maximum Power Point Tracking (DMPPT) architectures are one of the most promising solutions to overcome the drawbacks associated with mismatching phenomena in PV applications. In this kind of architectures, a DC-DC module integrated converter (MIC) manages each PV panel, isolating it from the rest of the PV panels, for harvesting the maximum available power from the Sun. Due to the high number of DCDC converters used in a grid-tied PV installation, the most desired MIC requirements are high efficiency, low cost and the capability of voltage step-up and step-down. This paper proposes the Buck-Boost Modified Forward (BBMSF) converter as a good candidate to be applied in DMPPT architectures. A complete analysis of the BBMSF converter is carried out, including the steady-state analysis as well as the small signal analysis in continuous conduction mode. The main advantages of the BBMSF converter are its step-up and step-down voltage transfer function; a higher simplicity, since it only includes a single controlled switch; the soft switching characteristics in all the diodes and MOSFET, reaching in some cases ZVS and ZCS, and yielding high efficiencies; the use of an autotransformer, with better performances than a typical Forward transformer; and the good dynamic performance, like the Forward converter ones. The theoretical analyses are validated through the experimental results in a 225 W BBMSF prototype designed and built under the requirements of a 100 kW grid-tied PV installation, achieving an efficiency up to 93.6%. ",
        "title": "Analysis and implementation of the Buck-Boost Modified Series Forward  converter applied to photovoltaic systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13472",
        "abstract_url": "http://arxiv.org/abs/2401.13472",
        "authors": [
            {
                "last_name": "Ibrahim",
                "first_name": "Mihaela Croitor"
            },
            {
                "last_name": "Ravikumar",
                "first_name": "Nishant"
            },
            {
                "last_name": "Curd",
                "first_name": "Alistair"
            },
            {
                "last_name": "Leng",
                "first_name": "Joanna"
            },
            {
                "last_name": "Umney",
                "first_name": "Oliver"
            },
            {
                "last_name": "Peckham",
                "first_name": "Michelle"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Z-disks are complex structures that delineate repeating sarcomeres in striated muscle. They play significant roles in cardiomyocytes such as providing mechanical stability for the contracting sarcomere, cell signalling and autophagy. Changes in Z-disk architecture have been associated with impaired cardiac function. Hence, there is a strong need to create tools to segment Z-disks from microscopy images, that overcome traditional limitations such as variability in image brightness and staining technique. In this study, we apply deep learning based segmentation models to extract Z-disks in images of striated muscle tissue. We leverage a novel Airyscan confocal dataset, which comprises high resolution images of Z-disks of healthy heart tissue, stained with Affimers for specific Z-disk proteins. We employed an interactive labelling tool, Ilastik to obtain ground truth segmentation masks and use the resulting data set to train and evaluate the performance of several state-of-the-art segmentation networks. On the test set, UNet++ achieves best segmentation performance for Z-disks in cardiomyocytes, with an average Dice score of 0.91 and outperforms other established segmentation methods including UNet, FPN, DeepLabv3+ and pix2pix. However, pix2pix demonstrates improved generalisation, when tested on an additional dataset of cardiomyocytes with a titin mutation. This is the first study to demonstrate that automated machine learning-based segmentation approaches may be used effectively to segment Z-disks in confocal microscopy images. Automated segmentation approaches and predicted segmentation masks could be used to derive morphological features of Z-disks (e.g. width and orientation), and subsequently, to quantify disease-related changes to cardiac microstructure. ",
        "title": "Segmenting Cardiac Muscle Z-disks with Deep Neural Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13478",
        "abstract_url": "http://arxiv.org/abs/2401.13478",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Siwei"
            },
            {
                "last_name": "Li",
                "first_name": "Yizhi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Kang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ge"
            },
            {
                "last_name": "Liang",
                "first_name": "Yiming"
            },
            {
                "last_name": "Ma",
                "first_name": "Kaijing"
            },
            {
                "last_name": "Xiao",
                "first_name": "Chenghao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Haoran"
            },
            {
                "last_name": "Yang",
                "first_name": "Bohao"
            },
            {
                "last_name": "Chen",
                "first_name": "Wenhu"
            },
            {
                "last_name": "Huang",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Moubayed",
                "first_name": "Noura Al"
            },
            {
                "last_name": "Fu",
                "first_name": "Jie"
            },
            {
                "last_name": "Lin",
                "first_name": "Chenghua"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CL",
            "CV",
            "MM"
        ],
        "abstract": "  Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders. All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR. ",
        "title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13480",
        "abstract_url": "http://arxiv.org/abs/2401.13480",
        "authors": [
            {
                "last_name": "Ashkinaze",
                "first_name": "Joshua"
            },
            {
                "last_name": "Gilbert",
                "first_name": "Eric"
            },
            {
                "last_name": "Budak",
                "first_name": "Ceren"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "CY",
            "HC"
        ],
        "abstract": "  Many studies explore how people 'come into' misinformation exposure. But much less is known about how people 'come out of' misinformation exposure. Do people organically sever ties to misinformation spreaders? And what predicts doing so? Over six months, we tracked the frequency and predictors of ~1M followers unfollowing ~5K health misinformation spreaders on Twitter. We found that misinformation ties are persistent. Monthly unfollowing rates are just 0.52%. Users are also 31% more likely to unfollow non-misinformation spreaders than they are to unfollow misinformation spreaders. Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology. First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later. Second, liberals are more likely to unfollow than conservatives. Overall, we observe strong persistence of misinformation ties. The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place. ",
        "title": "The Dynamics of (Not) Unfollowing Misinformation Spreaders",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13481",
        "abstract_url": "http://arxiv.org/abs/2401.13481",
        "authors": [
            {
                "last_name": "Ashkinaze",
                "first_name": "Joshua"
            },
            {
                "last_name": "Mendelsohn",
                "first_name": "Julia"
            },
            {
                "last_name": "Qiwei",
                "first_name": "Li"
            },
            {
                "last_name": "Budak",
                "first_name": "Ceren"
            },
            {
                "last_name": "Gilbert",
                "first_name": "Eric"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CL",
            "HC"
        ],
        "abstract": "  Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI, and that participants were more likely to knowingly adopt AI ideas when the task was difficult. Our findings suggest that introducing AI ideas into society may increase collective diversity but not individual creativity. ",
        "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human  Ideas: Evidence From a Large, Dynamic Experiment",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13483",
        "abstract_url": "http://arxiv.org/abs/2401.13483",
        "authors": [
            {
                "last_name": "Halla",
                "first_name": "Martin"
            },
            {
                "last_name": "Kachanovska",
                "first_name": "Maryna"
            },
            {
                "last_name": "Wess",
                "first_name": "Markus"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We consider the scalar anisotropic wave equation. Recently a convergence analysis for radial perfectly matched layers (PML) in the frequency domain was reported and in the present article we continue this approach into the time domain. First we explain why there is a good hope that radial complex scalings can overcome the instabilities of PML methods caused by anisotropic materials. Next we discuss some sensitive details, which seem like a paradox at the first glance: if the absorbing layer and the inhomogeneities are sufficiently separated, then the solution is indeed stable. However, for more general data the problem becomes unstable. In numerical computations we observe instabilities regardless of the position of the inhomogeneities, although the instabilities arise only for fine enough discretizations. As a remedy we propose a complex frequency shifted scaling and discretizations by Hardy space infinite elements or truncation-free PMLs. We show numerical experiments which confirm the stability and convergence of these methods. ",
        "title": "Radial perfectly matched layers and infinite elements for the  anisotropic wave equation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13486",
        "abstract_url": "http://arxiv.org/abs/2401.13486",
        "authors": [
            {
                "last_name": "Es'kin",
                "first_name": "Vasiliy A."
            },
            {
                "last_name": "Davydov",
                "first_name": "Danil V."
            },
            {
                "last_name": "Gur'eva",
                "first_name": "Julia V."
            },
            {
                "last_name": "Malkhanov",
                "first_name": "Alexey O."
            },
            {
                "last_name": "Smorkalov",
                "first_name": "Mikhail E."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented. Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs). In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations. Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters. ",
        "title": "Separable Physics-Informed Neural Networks for the solution of  elasticity problems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13488",
        "abstract_url": "http://arxiv.org/abs/2401.13488",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Shenshen"
            },
            {
                "last_name": "Luo",
                "first_name": "Jian"
            },
            {
                "last_name": "Guo",
                "first_name": "Dong"
            },
            {
                "last_name": "Gao",
                "first_name": "Kai"
            },
            {
                "last_name": "Yang",
                "first_name": "Yang Richard"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Data plane verification (DPV) analyzes routing tables and detects routing abnormalities and policy violations during network operation and planning. Thus, it has become an important tool to harden the networking infrastructure and the computing systems building on top. Substantial advancements have been made in the last decade and state-of-the-art DPV systems can achieve sub-us verification for an update of a single forwarding rule.   In this paper, we introduce fast inverse model transformation (FIMT), the first theoretical framework to systematically model and analyze centralized DPV systems. FIMT reveals the algebraic structure in the model update process, a key step in fast DPV systems. Thus, it can systematically analyze the correctness of several DPV systems, using algebraic properties. The theory also guides the design and implementation of NeoFlash, a refactored version of Flash with new optimization techniques. Evaluations show that NeoFlash outperforms existing state-of-the-art centralized DPV systems in various datasets and reveal insights to key techniques towards fast DPV. ",
        "title": "Fast Inverse Model Transformation: Algebraic Framework for Fast Data  Plane Verification",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13490",
        "abstract_url": "http://arxiv.org/abs/2401.13490",
        "authors": [
            {
                "last_name": "Nazarovets",
                "first_name": "Serhii"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL"
        ],
        "abstract": "  University rankings, despite facing criticism, continue to maintain their popularity. In the 2023 Scopus Ranking of Ukrainian Universities, certain institutions stood out due to their high h-index, despite modest publication and citation numbers. This phenomenon can be attributed to influential research topics or involvement in international collaborative research. However, these results may also be due to the authors' own efforts to increase the number of citations of their publications in order to improve their h-index. To investigate this, the publications from the top 30 universities in the ranking were analysed, revealing humpback rank-citation curves for two universities. These humpbacks indicate unusual trends in the citation data, especially considering the high percentage of self-citations and FWCI of analysed papers. While quantitative analysis has limitations, the combination of humped rank-citation curves, self-citations, FWCI, and previous research findings raises concerns about the possible causes of these anomalies in the citation data of the analysed universities. The method presented in this paper can aid ranking compilers and citation databases managers in identifying potential instances of citation data anomalies, emphasizing the importance of expert assessment for accurate conclusions. ",
        "title": "Visualization of rank-citation curves for fast detection of h-index  anomalies in university metrics",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13493",
        "abstract_url": "http://arxiv.org/abs/2401.13493",
        "authors": [
            {
                "last_name": "Cichocki",
                "first_name": "Max"
            },
            {
                "last_name": "Reitbauer",
                "first_name": "Eva"
            },
            {
                "last_name": "Theurl",
                "first_name": "Fabian"
            },
            {
                "last_name": "Schmied",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This preprint presents the current status of research into the development and application of an autonomous, self-driving compost turner. The aim is to overcome challenges in the composting industry, such as adverse working conditions, by automating the composting process. The preprint provides a comprehensive overview of the overall concept of the self-driving compost turner, including the hardware architecture with sensors, navigation module and control module. In addition, the methodical development of the architecture of concepts, models and their subsequent software integration in ROS using model-based systems engineering is described. The validation and verification of the overall system is carried out in an industrial environment using three scenarios. The capabilities of the compost turner are demonstrated by autonomously following predefined trajectories in the composting plant and performing the required composting tasks. The results show that the autonomous compost turner is capable of performing the required activities. In addition, the compost turner has intelligent processing capabilities for compost data as well as its transmission, visualization and storage in a cloud server. It is important to note that this work is a preprint that represents the current state of research. The authors aim to publish the full paper in a peer-reviewed journal in the near future. ",
        "title": "Towards an Autonomous Compost Turner: Current State of Research",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13494",
        "abstract_url": "http://arxiv.org/abs/2401.13494",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Fukai"
            },
            {
                "last_name": "Liu",
                "first_name": "Ziyang"
            },
            {
                "last_name": "Lin",
                "first_name": "Guochang"
            },
            {
                "last_name": "Chen",
                "first_name": "Junqing"
            },
            {
                "last_name": "Shi",
                "first_name": "Zuoqiang"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper, we propose Neumann Series Neural Operator (NSNO) to learn the solution operator of Helmholtz equation from inhomogeneity coefficients and source terms to solutions. Helmholtz equation is a crucial partial differential equation (PDE) with applications in various scientific and engineering fields. However, efficient solver of Helmholtz equation is still a big challenge especially in the case of high wavenumber. Recently, deep learning has shown great potential in solving PDEs especially in learning solution operators. Inspired by Neumann series in Helmholtz equation, we design a novel network architecture in which U-Net is embedded inside to capture the multiscale feature. Extensive experiments show that the proposed NSNO significantly outperforms the state-of-the-art FNO with at least 60\\% lower relative $L^2$-error, especially in the large wavenumber case, and has 50\\% lower computational cost and less data requirement. Moreover, NSNO can be used as the surrogate model in inverse scattering problems. Numerical tests show that NSNO is able to give comparable results with traditional finite difference forward solver while the computational cost is reduced tremendously. ",
        "title": "NSNO: Neumann Series Neural Operator for Solving Helmholtz Equations in  Inhomogeneous Medium",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13496",
        "abstract_url": "http://arxiv.org/abs/2401.13496",
        "authors": [
            {
                "last_name": "Sarpe",
                "first_name": "Julian"
            },
            {
                "last_name": "Klaedtke",
                "first_name": "Andreas"
            },
            {
                "last_name": "De Gersem",
                "first_name": "Herbert"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents a transient forward harmonic adjoint sensitivity analysis (TFHA), which is a combination of a transient forward circuit analysis with a harmonic balance based adjoint sensitivity analysis. TFHA provides sensitivities of quantities of interest from time-periodic problems w.r.t. many design parameters, as used in the design process of power-electronics devices. The TFHA shows advantages in applications where the harmonic balance based adjoint sensitivity analysis or finite difference approaches for sensitivity analysis perform poorly. In contrast to existing methods, the TFHA can be used in combination with arbitrary forward solvers, i.e. general transient solvers. ",
        "title": "Transient Forward Harmonic Adjoint Sensitivity Analysis",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13498",
        "abstract_url": "http://arxiv.org/abs/2401.13498",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Hounsu"
            },
            {
                "last_name": "Choi",
                "first_name": "Soonbeom"
            },
            {
                "last_name": "Nam",
                "first_name": "Juhan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work. ",
        "title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific  Input Representation and Diffusion Outpainting",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13499",
        "abstract_url": "http://arxiv.org/abs/2401.13499",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Maofa"
            },
            {
                "last_name": "Yan",
                "first_name": "Bingchen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Few-shot image classification has emerged as a key challenge in the field of computer vision, highlighting the capability to rapidly adapt to new tasks with minimal labeled data. Existing methods predominantly rely on image-level features or local descriptors, often overlooking the holistic context surrounding these descriptors. In this work, we introduce a novel approach termed \"Local Descriptor with Contextual Augmentation (LDCA)\". Specifically, this method bridges the gap between local and global understanding uniquely by leveraging an adaptive global contextual enhancement module. This module incorporates a visual transformer, endowing local descriptors with contextual awareness capabilities, ranging from broad global perspectives to intricate surrounding nuances. By doing so, LDCA transcends traditional descriptor-based approaches, ensuring each local feature is interpreted within its larger visual narrative. Extensive experiments underscore the efficacy of our method, showing a maximal absolute improvement of 20\\% over the next-best on fine-grained classification datasets, thus demonstrating significant advancements in few-shot classification tasks. ",
        "title": "LDCA: Local Descriptors with Contextual Augmentation for Few-Shot  Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13502",
        "abstract_url": "http://arxiv.org/abs/2401.13502",
        "authors": [
            {
                "last_name": "Abboud",
                "first_name": "Amir"
            },
            {
                "last_name": "Fischer",
                "first_name": "Nick"
            },
            {
                "last_name": "Shechter",
                "first_name": "Yarin"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Detecting if a graph contains a $k$-Clique is one of the most fundamental problems in computer science. The asymptotically fastest algorithm runs in time $O(n^{\\omega k/3})$, where $\\omega$ is the exponent of Boolean matrix multiplication. To date, this is the only technique capable of beating the trivial $O(n^k)$ bound by a polynomial factor. Due to this technique's various limitations, much effort has gone into designing \"combinatorial\" algorithms that improve over exhaustive search via other techniques.   The first contribution of this work is a faster combinatorial algorithm for $k$-Clique, improving Vassilevska's bound of $O(n^{k}/\\log^{k-1}{n})$ by two log factors. Technically, our main result is a new reduction from $k$-Clique to Triangle detection that exploits the same divide-and-conquer at the core of recent combinatorial algorithms by Chan (SODA'15) and Yu (ICALP'15).   Our second contribution is exploiting combinatorial techniques to improve the state-of-the-art (even of non-combinatorial algorithms) for generalizations of the $k$-Clique problem. In particular, we give the first $o(n^k)$ algorithm for $k$-clique in hypergraphs and an $O(n^3/\\log^{2.25}{n} + t)$ algorithm for listing $t$ triangles in a graph. ",
        "title": "Faster Combinatorial k-Clique Algorithms",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13503",
        "abstract_url": "http://arxiv.org/abs/2401.13503",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hai-Xin"
            },
            {
                "last_name": "Huang",
                "first_name": "Dong"
            },
            {
                "last_name": "Ling",
                "first_name": "Hua-Bao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Guang-Yu"
            },
            {
                "last_name": "Sun",
                "first_name": "Wei-jun"
            },
            {
                "last_name": "Wen",
                "first_name": "Zi-hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we present a novel deep image clustering approach termed PICI, which enforces the partial information discrimination and the cross-level interaction in a joint learning framework. In particular, we leverage a Transformer encoder as the backbone, through which the masked image modeling with two paralleled augmented views is formulated. After deriving the class tokens from the masked images by the Transformer encoder, three partial information learning modules are further incorporated, including the PISD module for training the auto-encoder via masked image reconstruction, the PICD module for employing two levels of contrastive learning, and the CLI module for mutual interaction between the instance-level and cluster-level subspaces. Extensive experiments have been conducted on six real-world image datasets, which demononstrate the superior clustering performance of the proposed PICI approach over the state-of-the-art deep clustering approaches. The source code is available at https://github.com/Regan-Zhang/PICI. ",
        "title": "Learning Representations for Clustering via Partial Information  Discrimination and Cross-Level Interaction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13504",
        "abstract_url": "http://arxiv.org/abs/2401.13504",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Jizhe"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In recent years, particularly since the early 2020s, Large Language Models (LLMs) have emerged as the most powerful AI tools in addressing a diverse range of challenges, from natural language processing to complex problem-solving in various domains. In the field of tamper detection, LLMs are capable of identifying basic tampering activities.To assess the capabilities of LLMs in more specialized domains, we have collected five different LLMs developed by various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This diverse range of models allows for a comprehensive evaluation of their performance in detecting sophisticated tampering instances.We devised two domains of detection: AI-Generated Content (AIGC) detection and manipulation detection. AIGC detection aims to test the ability to distinguish whether an image is real or AI-generated. Manipulation detection, on the other hand, focuses on identifying tampered images. According to our experiments, most LLMs can identify composite pictures that are inconsistent with logic, and only more powerful LLMs can distinguish logical, but visible signs of tampering to the human eye. All of the LLMs can't identify carefully forged images and very realistic images generated by AI. In the area of tamper detection, LLMs still have a long way to go, particularly in reliably identifying highly sophisticated forgeries and AI-generated images that closely mimic reality. ",
        "title": "Research about the Ability of LLM in the Tamper-Detection Area",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13505",
        "abstract_url": "http://arxiv.org/abs/2401.13505",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Chuan"
            },
            {
                "last_name": "Mu",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Zuo",
                "first_name": "Xinxin"
            },
            {
                "last_name": "Dai",
                "first_name": "Peng"
            },
            {
                "last_name": "Yan",
                "first_name": "Youliang"
            },
            {
                "last_name": "Lu",
                "first_name": "Juwei"
            },
            {
                "last_name": "Cheng",
                "first_name": "Li"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-arts in style reeanactment, content preservation, and generalization across various applications and settings. Project Page: https://yxmu.foo/GenMoStyle ",
        "title": "Generative Human Motion Stylization in Latent Space",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13509",
        "abstract_url": "http://arxiv.org/abs/2401.13509",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Chuting"
            },
            {
                "last_name": "Li",
                "first_name": "Hang"
            },
            {
                "last_name": "Mourad",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Koopman",
                "first_name": "Bevan"
            },
            {
                "last_name": "Zuccon",
                "first_name": "Guido"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever. ",
        "title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient  and Effective Retrieval",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13511",
        "abstract_url": "http://arxiv.org/abs/2401.13511",
        "authors": [
            {
                "last_name": "Lucassen",
                "first_name": "Ruben T."
            },
            {
                "last_name": "Blokx",
                "first_name": "Willeke A. M."
            },
            {
                "last_name": "Veta",
                "first_name": "Mitko"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions. Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue. Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed. In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning. To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&E stained WSIs. For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sections in a 2D histogram. On an independent test set, the model achieved a mean Dice score of 0.981$\\pm$0.033 for tissue segmentation and a mean Dice score of 0.912$\\pm$0.090 for pen marking segmentation. The mean absolute difference between the number of annotated and separated cross-sections was 0.075$\\pm$0.350. Our results demonstrate that the proposed model can accurately segment H&E stained tissue cross-sections and pen markings in WSIs while being robust to many common slide and scanning artifacts. The model with trained model parameters and post-processing method are made publicly available as a Python package called SlideSegmenter. ",
        "title": "Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13512",
        "abstract_url": "http://arxiv.org/abs/2401.13512",
        "authors": [
            {
                "last_name": "Falis",
                "first_name": "Mat\u00fa\u0161"
            },
            {
                "last_name": "Gema",
                "first_name": "Aryo Pradipta"
            },
            {
                "last_name": "Dong",
                "first_name": "Hang"
            },
            {
                "last_name": "Daines",
                "first_name": "Luke"
            },
            {
                "last_name": "Basetti",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Holder",
                "first_name": "Michael"
            },
            {
                "last_name": "Penfold",
                "first_name": "Rose S"
            },
            {
                "last_name": "Birch",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Alex",
                "first_name": "Beatrice"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Objective: To investigate GPT-3.5 in generating and coding medical documents with ICD-10 codes for data augmentation on low-resources labels.   Materials and Methods: Employing GPT-3.5 we generated and coded 9,606 discharge summaries based on lists of ICD-10 code descriptions of patients with infrequent (generation) codes within the MIMIC-IV dataset. Combined with the baseline training set, this formed an augmented training set. Neural coding models were trained on baseline and augmented data and evaluated on a MIMIC-IV test set. We report micro- and macro-F1 scores on the full codeset, generation codes, and their families. Weak Hierarchical Confusion Matrices were employed to determine within-family and outside-of-family coding errors in the latter codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided self-generated data and real MIMIC-IV data. Clinical professionals evaluated the clinical acceptability of the generated documents.   Results: Augmentation slightly hinders the overall performance of the models but improves performance for the generation candidate codes and their families, including one unseen in the baseline training data. Augmented models display lower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the prompted descriptions, but performs poorly on real data. Evaluators note the correctness of generated concepts while suffering in variety, supporting information, and narrative.   Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding. Augmentation positively affects generation code families but mainly benefits codes with existing examples. Augmentation reduces out-of-family errors. Discharge summaries generated by GPT-3.5 state prompted concepts correctly but lack variety, and authenticity in narratives. They are unsuitable for clinical practice. ",
        "title": "Can GPT-3.5 Generate and Code Discharge Summaries?",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13516",
        "abstract_url": "http://arxiv.org/abs/2401.13516",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Juan"
            },
            {
                "last_name": "Liao",
                "first_name": "Xin"
            },
            {
                "last_name": "Gao",
                "first_name": "Difei"
            },
            {
                "last_name": "Tsutsui",
                "first_name": "Satoshi"
            },
            {
                "last_name": "Wang",
                "first_name": "Qian"
            },
            {
                "last_name": "Qin",
                "first_name": "Zheng"
            },
            {
                "last_name": "Shou",
                "first_name": "Mike Zheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR"
        ],
        "abstract": "  Deepfake videos are becoming increasingly realistic, showing subtle tampering traces on facial areasthat vary between frames. Consequently, many existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region. To address thislimitation, we propose Delocate, a novel Deepfake detection model that can both recognize andlocalize unknown domain Deepfake videos. Ourmethod consists of two stages named recoveringand localization. In the recovering stage, the modelrandomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, resulting in a relatively good recovery effect for realfaces and a poor recovery effect for fake faces. Inthe localization stage, the output of the recoveryphase and the forgery ground truth mask serve assupervision to guide the forgery localization process. This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions. Ourextensive experiments on four widely used benchmark datasets demonstrate that Delocate not onlyexcels in localizing tampered areas but also enhances cross-domain detection performance. ",
        "title": "Delocate: Detection and Localization for Deepfake Videos with  Randomly-Located Tampered Traces",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13518",
        "abstract_url": "http://arxiv.org/abs/2401.13518",
        "authors": [
            {
                "last_name": "Van Der Donckt",
                "first_name": "Jonas"
            },
            {
                "last_name": "Vandenbussche",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Van Der Donckt",
                "first_name": "Jeroen"
            },
            {
                "last_name": "Chen",
                "first_name": "Stephanie"
            },
            {
                "last_name": "Stojchevska",
                "first_name": "Marija"
            },
            {
                "last_name": "De Brouwer",
                "first_name": "Mathias"
            },
            {
                "last_name": "Steenwinckel",
                "first_name": "Bram"
            },
            {
                "last_name": "Paemeleire",
                "first_name": "Koen"
            },
            {
                "last_name": "Ongenae",
                "first_name": "Femke"
            },
            {
                "last_name": "Van Hoecke",
                "first_name": "Sofie"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Chronic disease management and follow-up are vital for realizing sustained patient well-being and optimal health outcomes. Recent advancements in wearable sensing technologies, particularly wrist-worn devices, offer promising solutions for longitudinal patient follow-up by shifting from subjective, intermittent self-reporting to objective, continuous monitoring. However, collecting and analyzing wearable data presents unique challenges, such as data entry errors, non-wear periods, missing wearable data, and wearable artifacts. We therefore present an in-depth exploration of data analysis challenges tied to wrist-worn wearables and ambulatory label acquisition, using two real-world datasets (i.e., mBrain21 and ETRI lifelog2020). We introduce novel practical countermeasures, including participant compliance visualizations, interaction-triggered questionnaires to assess personal bias, and an optimized wearable non-wear detection pipeline. Further, we propose a visual analytics approach to validate processing pipelines using scalable tools such as tsflex and Plotly-Resampler. Lastly, we investigate the impact of missing wearable data on \"window-of-interest\" analysis methodologies. Prioritizing transparency and reproducibility, we offer open access to our detailed code examples, facilitating adaptation in future wearable research. In conclusion, our contributions provide actionable approaches for wearable data collection and analysis in chronic disease management. ",
        "title": "Addressing Data Quality Challenges in Observational Ambulatory Studies:  Analysis, Methodologies and Practical Solutions for Wrist-worn Wearable  Monitoring",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13524",
        "abstract_url": "http://arxiv.org/abs/2401.13524",
        "authors": [
            {
                "last_name": "Allouche",
                "first_name": "Jean-Paul"
            },
            {
                "last_name": "Shallit",
                "first_name": "Jeffrey"
            },
            {
                "last_name": "Stipulanti",
                "first_name": "Manon"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "FL"
        ],
        "abstract": "  Generating series are crucial in enumerative combinatorics, analytic combinatorics, and combinatorics on words. Though it might seem at first view that generating Dirichlet series are less used in these fields than ordinary and exponential generating series, there are many notable papers where they play a fundamental role, as can be seen in particular in the work of Flajolet and several of his co-authors. In this paper, we study Dirichlet series of integers with missing digits or blocks of digits in some integer base $b$, i.e., where the summation ranges over the integers whose expansions form some language strictly included in the set of all words on the alphabet $\\{0, 1, \\dots, b-1\\}$ that do not begin with a $0$. We show how to unify and extend results proved by Nathanson in 2021 and by K\\\"ohler and Spilker in 2009. En route, we encounter several sequences from Sloane's On-Line Encyclopedia of Integer Sequences, as well as some famous $q$-automatic sequences or $q$-regular sequences. ",
        "title": "Combinatorics on words and generating Dirichlet series of automatic  sequences",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13531",
        "abstract_url": "http://arxiv.org/abs/2401.13531",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zengbin"
            },
            {
                "last_name": "Hou",
                "first_name": "Saihui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Man"
            },
            {
                "last_name": "Liu",
                "first_name": "Xu"
            },
            {
                "last_name": "Cao",
                "first_name": "Chunshui"
            },
            {
                "last_name": "Huang",
                "first_name": "Yongzhen"
            },
            {
                "last_name": "Li",
                "first_name": "Peipei"
            },
            {
                "last_name": "Xu",
                "first_name": "Shibiao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait. ",
        "title": "QAGait: Revisit Gait Recognition from a Quality Perspective",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13535",
        "abstract_url": "http://arxiv.org/abs/2401.13535",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Xiao",
                "first_name": "Han"
            },
            {
                "last_name": "Fang",
                "first_name": "Qizhi"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  The flow game with public arcs is a cooperative revenue game derived from a flow network. In this game, each player possesses an arc, while certain arcs, known as public arcs, are not owned by any specific player and are accessible to any coalition. The aim of this game is to maximize the flow that can be routed in the network through strategic coalition formation. By exploring its connection to the maximum partially disjoint path problem, we investigate the approximate core and nucleon of the flow game with public arcs. The approximate core is an extension of the core that allows for some deviation in group rationality, while the nucleon is a multiplicative analogue of the nucleolus. In this paper, we provide two complete characterizations for the optimal approximate core and show that the nucleon can be computed in polynomial time. ",
        "title": "On the Approximate Core and Nucleon of Flow Games",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13539",
        "abstract_url": "http://arxiv.org/abs/2401.13539",
        "authors": [
            {
                "last_name": "Schneider",
                "first_name": "Daniel"
            },
            {
                "last_name": "Reich",
                "first_name": "Jan"
            },
            {
                "last_name": "Adler",
                "first_name": "Rasmus"
            },
            {
                "last_name": "Liggesmeyer",
                "first_name": "Peter"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Cyber Physical Systems (CPS) enable new kinds of applications as well as significant improvements of existing ones in numerous different application domains. A major trait of upcoming CPS is an increasing degree of automation up to the point of autonomy, as there is a huge potential for economic success as well as for ecologic and societal improvements. However, to unlock the full potential of such (cooperative and automated) CPS, we first need to overcome several significant engineering challenges, where safety assurance is a particularly important one. Unfortunately, established safety assurance methods and standards do not live up to this task, as they have been designed with closed and less complex systems in mind. This paper structures safety assurance challenges of cooperative automated CPS, provides an overview on our vision of dynamic risk management and describes already existing building blocks. ",
        "title": "Dynamic Risk Management in Cyber Physical Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13540",
        "abstract_url": "http://arxiv.org/abs/2401.13540",
        "authors": [
            {
                "last_name": "Lilge",
                "first_name": "Sven"
            },
            {
                "last_name": "Barfoot",
                "first_name": "Timothy D."
            },
            {
                "last_name": "Burgner-Kahrs",
                "first_name": "Jessica"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In contrast to conventional robots, accurately modeling the kinematics and statics of continuum robots is challenging due to partially unknown material properties, parasitic effects, or unknown forces acting on the continuous body. Consequentially, state estimation approaches that utilize additional sensor information to predict the shape of continuum robots have garnered significant interest. This paper presents a novel approach to state estimation for systems with multiple coupled continuum robots, which allows estimating the shape and strain variables of multiple continuum robots in an arbitrary coupled topology. Simulations and experiments demonstrate the capabilities and versatility of the proposed method, while achieving accurate and continuous estimates for the state of such systems, resulting in average end-effector errors of 3.3 mm and 5.02{\\deg} depending on the sensor setup. It is further shown, that the approach offers fast computation times of below 10 ms, enabling its utilization in quasi-static real-time scenarios with average update rates of 100-200 Hz. An open-source C++ implementation of the proposed state estimation method is made publicly available to the community. ",
        "title": "State Estimation for Continuum Multi-Robot Systems on SE(3)",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13544",
        "abstract_url": "http://arxiv.org/abs/2401.13544",
        "authors": [
            {
                "last_name": "Marcinkevi\u010ds",
                "first_name": "Ri\u010dards"
            },
            {
                "last_name": "Laguna",
                "first_name": "Sonia"
            },
            {
                "last_name": "Vandenhirtz",
                "first_name": "Moritz"
            },
            {
                "last_name": "Vogt",
                "first_name": "Julia E."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of the proposed techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes can be as intervenable and more performant than CBMs. ",
        "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13545",
        "abstract_url": "http://arxiv.org/abs/2401.13545",
        "authors": [
            {
                "last_name": "Adibhatla",
                "first_name": "Hiranmai Sri"
            },
            {
                "last_name": "Baswani",
                "first_name": "Pavan"
            },
            {
                "last_name": "Shrivastava",
                "first_name": "Manish"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language. We harness the strength of LLMs by integrating supervised learning within them. The goal of this combined strategy is to boost the performance of LLMs in extraction tasks like NER while simultaneously addressing hallucination issues often observed in LLM-generated content. A novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models. Our models and dataset are available to the community for future research * . ",
        "title": "Fine-grained Contract NER using instruction based model",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13546",
        "abstract_url": "http://arxiv.org/abs/2401.13546",
        "authors": [
            {
                "last_name": "del Moral",
                "first_name": "David Lopez"
            },
            {
                "last_name": "Barrado",
                "first_name": "Andres"
            },
            {
                "last_name": "Sanz",
                "first_name": "Marina"
            },
            {
                "last_name": "Lazaro",
                "first_name": "Antonio"
            },
            {
                "last_name": "Zumel",
                "first_name": "Pablo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Grid-tied photovoltaic (PV) installations with Distributed Maximum Power Point Tracking (DMPPT) architectures include a DC-DC Module Integrated Converter (MIC) for managing each PV panel, isolating it from the others, reducing the mismatching effect and maximizing the harvested power. In this paper, the Autotransformer Forward converter with type-Zeta resonant reset (AFZ) is proposed as a DMPPT architecture MIC candidate. The main characteristics of the AFZ converter are the high versatility due to its voltage step-up and step-down capability; the use of an optimized autotransformer with only two windings, reducing the complexity and power losses of this component; the good dynamic performances, like the Forward converter ones; the low number of components and the simplicity and high feasibility associated to the use of just one active switch. Besides, soft switching transitions are achieved thanks to the autotransformer type-Zeta resonant reset. The steady-state theoretical analysis, considering the effect of the autotransformer leakage inductance, is presented. The converter is also studied in the frequency domain, obtaining the small-signal transfer functions. A design procedure based on the requirements of a 100 kW grid-tied photovoltaic installation is described, yielding in a 225 W prototype with efficiencies up to 95.6 %. Experimental results validate the theoretical analysis. ",
        "title": "Analysis, design, and implementation of the AFZ converter applied to  photovoltaic systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13548",
        "abstract_url": "http://arxiv.org/abs/2401.13548",
        "authors": [
            {
                "last_name": "Monir",
                "first_name": "Nasser-Eddine"
            },
            {
                "last_name": "Magron",
                "first_name": "Paul"
            },
            {
                "last_name": "Serizel",
                "first_name": "Romain"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  In the intricate acoustic landscapes where speech intelligibility is challenged by noise and reverberation, multichannel speech enhancement emerges as a promising solution for individuals with hearing loss. Such algorithms are commonly evaluated at the utterance level. However, this approach overlooks the granular acoustic nuances revealed by phoneme-specific analysis, potentially obscuring key insights into their performance. This paper presents an in-depth phoneme-scale evaluation of 3 state-of-the-art multichannel speech enhancement algorithms. These algorithms -- FasNet, MVDR, and Tango -- are extensively evaluated across different noise conditions and spatial setups, employing realistic acoustic simulations with measured room impulse responses, and leveraging diversity offered by multiple microphones in a binaural hearing setup. The study emphasizes the fine-grained phoneme-level analysis, revealing that while some phonemes like plosives are heavily impacted by environmental acoustics and challenging to deal with by the algorithms, others like nasals and sibilants see substantial improvements after enhancement. These investigations demonstrate important improvements in phoneme clarity in noisy conditions, with insights that could drive the development of more personalized and phoneme-aware hearing aid technologies. ",
        "title": "A Phoneme-Scale Assessment of Multichannel Speech Enhancement Algorithms",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13551",
        "abstract_url": "http://arxiv.org/abs/2401.13551",
        "authors": [
            {
                "last_name": "Nie",
                "first_name": "Yongwei"
            },
            {
                "last_name": "Huang",
                "first_name": "Hao"
            },
            {
                "last_name": "Long",
                "first_name": "Chengjiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qing"
            },
            {
                "last_name": "Maji",
                "first_name": "Pradipta"
            },
            {
                "last_name": "Cai",
                "first_name": "Hongmin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches. ",
        "title": "Interleaving One-Class and Weakly-Supervised Models with Adaptive  Thresholding for Unsupervised Video Anomaly Detection",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13552",
        "abstract_url": "http://arxiv.org/abs/2401.13552",
        "authors": [
            {
                "last_name": "Bahavarnia",
                "first_name": "MirSaleh"
            },
            {
                "last_name": "Ji",
                "first_name": "Junyi"
            },
            {
                "last_name": "Taha",
                "first_name": "Ahmad F."
            },
            {
                "last_name": "Work",
                "first_name": "and Daniel B."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The main objective of the connected and automated vehicle (CAV) platoon control problem is to regulate CAVs' position while ensuring stability and accounting for vehicle dynamics. Although this problem has been studied in the literature, existing research has some limitations. This paper presents two new theoretical results that address these limitations: (i) the synthesis of unrealistic high-gain control parameters due to the lack of a systematic way to incorporate the lower and upper bounds on the control parameters, and (ii) the performance sensitivity to the communication delay due to inaccurate Taylor series approximation. To be more precise, taking advantage of the wellknown Pade approximation, this paper proposes a constrained CAV platoon controller synthesis that (i) systematically incorporates the lower and upper bounds on the control parameters, and (ii) significantly improves the performance sensitivity to the communication delay. The effectiveness of the presented results is verified through conducting extensive numerical simulations. The proposed controller effectively attenuates the stop-and-go disturbance -- a single cycle of deceleration followed by acceleration -- amplification throughout the mixed platoon (consisting of CAVs and human-driven vehicles). Modern transportation systems will benefit from the proposed CAV controls in terms of effective disturbance attenuation as it will potentially reduce collisions. ",
        "title": "On the Constrained CAV Platoon Control Problem",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13554",
        "abstract_url": "http://arxiv.org/abs/2401.13554",
        "authors": [
            {
                "last_name": "Brookes",
                "first_name": "Otto"
            },
            {
                "last_name": "Mirmehdi",
                "first_name": "Majid"
            },
            {
                "last_name": "Stephens",
                "first_name": "Colleen"
            },
            {
                "last_name": "Angedakin",
                "first_name": "Samuel"
            },
            {
                "last_name": "Corogenes",
                "first_name": "Katherine"
            },
            {
                "last_name": "Dowd",
                "first_name": "Dervla"
            },
            {
                "last_name": "Dieguez",
                "first_name": "Paula"
            },
            {
                "last_name": "Hicks",
                "first_name": "Thurston C."
            },
            {
                "last_name": "Jones",
                "first_name": "Sorrel"
            },
            {
                "last_name": "Lee",
                "first_name": "Kevin"
            },
            {
                "last_name": "Leinert",
                "first_name": "Vera"
            },
            {
                "last_name": "Lapuente",
                "first_name": "Juan"
            },
            {
                "last_name": "McCarthy",
                "first_name": "Maureen S."
            },
            {
                "last_name": "Meier",
                "first_name": "Amelia"
            },
            {
                "last_name": "Murai",
                "first_name": "Mizuki"
            },
            {
                "last_name": "Normand",
                "first_name": "Emmanuelle"
            },
            {
                "last_name": "Vergnes",
                "first_name": "Virginie"
            },
            {
                "last_name": "Wessling",
                "first_name": "Erin G."
            },
            {
                "last_name": "Wittig",
                "first_name": "Roman M."
            },
            {
                "last_name": "Langergraber",
                "first_name": "Kevin"
            },
            {
                "last_name": "Maldonado",
                "first_name": "Nuria"
            },
            {
                "last_name": "Yang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Zuberbuhler",
                "first_name": "Klaus"
            },
            {
                "last_name": "Boesch",
                "first_name": "Christophe"
            },
            {
                "last_name": "Arandjelovic",
                "first_name": "Mimi"
            },
            {
                "last_name": "Kuhl",
                "first_name": "Hjalmar"
            },
            {
                "last_name": "Burghardt",
                "first_name": "Tilo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 18 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts. ",
        "title": "PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour  Recognition",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13555",
        "abstract_url": "http://arxiv.org/abs/2401.13555",
        "authors": [
            {
                "last_name": "Laszkiewicz",
                "first_name": "Mike"
            },
            {
                "last_name": "Daunhawer",
                "first_name": "Imant"
            },
            {
                "last_name": "Vogt",
                "first_name": "Julia E."
            },
            {
                "last_name": "Fischer",
                "first_name": "Asja"
            },
            {
                "last_name": "Lederer",
                "first_name": "Johannes"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\\unicode{x2013}$inspired by their supervised fairness counterparts$\\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results. ",
        "title": "Benchmarking the Fairness of Image Upsampling Methods",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13556",
        "abstract_url": "http://arxiv.org/abs/2401.13556",
        "authors": [
            {
                "last_name": "Ochoa",
                "first_name": "Diego"
            },
            {
                "last_name": "Lazaro",
                "first_name": "Antonio"
            },
            {
                "last_name": "Zumel",
                "first_name": "Pablo"
            },
            {
                "last_name": "Fernandez",
                "first_name": "Cristina"
            },
            {
                "last_name": "Sanz",
                "first_name": "Marina"
            },
            {
                "last_name": "Rodriguez",
                "first_name": "Jorge"
            },
            {
                "last_name": "Barrado",
                "first_name": "Andres"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In railway applications, it is common to use an LC filter connected between the catenary and the input port of the main converter of the auxiliary and traction systems. In addition, in the auxiliary systems, there is a converter operating as a battery charger, which requires a very low ripple in the output current and output voltage, so a postfilter may be placed at the output port of the converter. This article proposes a step-by-step methodology to extend the injected-absorbed-current (IAC) method in order to obtain transfer functions that consider the effects of the input filter, output postfilter, and some feedforward compensations. The proposed methodology allows reusing the characteristic coefficients of the DC-DC converter model derived from the existing IAC method. One of the advantages of the proposed methodology is that the transfer functions obtained in this article are valid for cases where both, one or none of the filters, are implemented. Finally, for the experimental validation of the proposed methodology, the phase-shifted full-bridge converter was selected as a convenient example. Furthermore, the experimental measurements have been performed on two prototypes. ",
        "title": "Extension of the Injected-Absorbed-Current Method applied to DC-DC  Converters with Input Filter, Output Post-filter and Feedforward  Compensations",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13558",
        "abstract_url": "http://arxiv.org/abs/2401.13558",
        "authors": [
            {
                "last_name": "Alleman",
                "first_name": "Matteo"
            },
            {
                "last_name": "Lindsey",
                "first_name": "Jack W"
            },
            {
                "last_name": "Fusi",
                "first_name": "Stefano"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of ReLU, which leads feature neurons to specialize for different regions of input space. By contrast, feature neurons in Tanh networks tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks. ",
        "title": "Task structure and nonlinearity jointly determine learned  representational geometry",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13561",
        "abstract_url": "http://arxiv.org/abs/2401.13561",
        "authors": [
            {
                "last_name": "Chu",
                "first_name": "Zhongda"
            },
            {
                "last_name": "Wu",
                "first_name": "Jingyi"
            },
            {
                "last_name": "Teng",
                "first_name": "Fei"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  With the growing penetration of Inverter-Based Resources (IBRs) in power systems, stability service markets have emerged to incentivize technologies that ensure power system stability and reliability. Among the various challenges faced in power system operation and stability, a prominent issue raised from the increasing integration of large-scale IBRs is the significant reduction of the Short-Circuit Current (SCC) level in the system, which poses a considerable threat to system voltage stability and protection. Thus, a proper market mechanism to incentivize the provision of SCC as a stability service is desired. However, the pricing of this service within the future stability market has not yet been fully developed, due to the nonconvex nature of SCC constraints and the locational property of SCC. To address these problems, this work aims to explore, for the first time, a pricing model for SCC service by incorporating a linearized SCC constraint into the Unit Commitment (UC) problem, to achieve the desired SCC level and extract the shadow price for SCC through different pricing methods. ",
        "title": "Pricing of Short Circuit Current in High IBR-Penetrated System",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13564",
        "abstract_url": "http://arxiv.org/abs/2401.13564",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Jun"
            },
            {
                "last_name": "Yang",
                "first_name": "Gang"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanwei"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiangyun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper studies an extremely large-scale reconfigurable intelligent surface (XL-RIS) empowered covert communication system in the near-field region. Alice covertly transmits messages to Bob with the assistance of the XL-RIS, while evading detection by Willie. To enhance the covert communication performance, we maximize the achievable covert rate by jointly optimizing the hybrid analog and digital beamformers at Alice, as well as the reflection coefficient matrix at the XL-RIS. An alternating optimization algorithm is proposed to solve the joint beamforming design problem. For the hybrid beamformer design, a semi-closed-form solution for fully digital beamformer is first obtained by a weighted minimum mean-square error based algorithm, then the baseband digital and analog beamformers at Alice are designed by approximating the fully digital beamformer via manifold optimization. For the XL-RIS's reflection coefficient matrix design, a low-complexity alternating direction method of multipliers based algorithm is proposed to address the challenge of large-scale variables and unit-modulus constraints. Numerical results unveil that i) the near-field communications can achieve a higher covert rate than the far-field covert communications in general, and still realize covert transmission even if Willie is located at the same direction as Bob and closer to the XL-RIS; ii) the proposed algorithm can enhance the covert rate significantly compared to the benchmark schemes; iii) the proposed algorithm leads to a beam diffraction pattern that can bypass Willie and achieve high-rate covert transmission to Bob. ",
        "title": "RIS Empowered Near-Field Covert Communications",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13565",
        "abstract_url": "http://arxiv.org/abs/2401.13565",
        "authors": [
            {
                "last_name": "Zolkepli",
                "first_name": "Husein"
            },
            {
                "last_name": "Razak",
                "first_name": "Aisyah"
            },
            {
                "last_name": "Adha",
                "first_name": "Kamarul"
            },
            {
                "last_name": "Nazhan",
                "first_name": "Ariff"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.   Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.   Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (Malay grammar) test set, particularly when fine-tuned with instructions.   All models released at https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c ",
        "title": "Large Malaysian Language Model Based on Mistral for Enhanced Local  Language Understanding",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13566",
        "abstract_url": "http://arxiv.org/abs/2401.13566",
        "authors": [
            {
                "last_name": "Boratto",
                "first_name": "Ludovico"
            },
            {
                "last_name": "Cerniglia",
                "first_name": "Giulia"
            },
            {
                "last_name": "Marras",
                "first_name": "Mirko"
            },
            {
                "last_name": "Perniciano",
                "first_name": "Alessandra"
            },
            {
                "last_name": "Pes",
                "first_name": "Barbara"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure. ",
        "title": "A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in  Recommendation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13568",
        "abstract_url": "http://arxiv.org/abs/2401.13568",
        "authors": [
            {
                "last_name": "Pace",
                "first_name": "Anna"
            },
            {
                "last_name": "Grioli",
                "first_name": "Giorgio"
            },
            {
                "last_name": "Ghezzi",
                "first_name": "Alice"
            },
            {
                "last_name": "Bicchi",
                "first_name": "Antonio"
            },
            {
                "last_name": "Catalano",
                "first_name": "Manuel G."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Biped robots usually adopt feet with a rigid structure that simplifies walking on flat grounds and yet hinders ground adaptation in unstructured environments, thus jeopardizing stability. We recently explored in the SoftFoot the idea of adapting a robotic foot to ground irregularities along the sagittal plane. Building on the previous results, we propose in this paper a novel robotic foot able to adapt both in the sagittal and frontal planes, similarly to the human foot. It features five parallel modules with intrinsic longitudinal adaptability that can be combined in many possible designs through optional rigid or elastic connections. By following a methodological design approach, we narrow down the design space to five candidate foot designs and implement them on a modular system. Prototypes are tested experimentally via controlled application of force, through a robotic arm, onto a sensorized plate endowed with different obstacles. Their performance is compared, using also a rigid foot and the previous SoftFoot as a baseline. Analysis of footprint stability shows that the introduction of the transverse arch, by elastically connecting the five parallel modules, is advantageous for obstacle negotiation, especially when obstacles are located under the forefoot. In addition to biped robots' locomotion, this finding might also benefit lower-limb prostheses design. ",
        "title": "Investigating the Performance of Soft Robotic Adaptive Feet with  Longitudinal and Transverse Arches",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13569",
        "abstract_url": "http://arxiv.org/abs/2401.13569",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xi"
            },
            {
                "last_name": "Hatasaka",
                "first_name": "Bryan"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhengyan"
            },
            {
                "last_name": "Tope",
                "first_name": "Sayali"
            },
            {
                "last_name": "Karkhanis",
                "first_name": "Mohit"
            },
            {
                "last_name": "Noh",
                "first_name": "Seungbeom"
            },
            {
                "last_name": "Sium",
                "first_name": "Farhan"
            },
            {
                "last_name": "Mural",
                "first_name": "Ravi V."
            },
            {
                "last_name": "Kim",
                "first_name": "Hanseup"
            },
            {
                "last_name": "Mastrangelo",
                "first_name": "Carlos"
            },
            {
                "last_name": "Zang",
                "first_name": "Ling"
            },
            {
                "last_name": "Schnable",
                "first_name": "James"
            },
            {
                "last_name": "Ji",
                "first_name": "Mingyue"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "DC"
        ],
        "abstract": "  With the rapid development of cloud and edge computing, Internet of Things (IoT) applications have been deployed in various aspects of human life. In this paper, we design and implement a holistic LoRa-based IoT system with LoRa communication capabilities, named SPARC-LoRa, which consists of field sensor nodes and a gateway connected to the Internet. SPARC-LoRa has the following important features. First, the proposed wireless network of SPARC-LoRa is even-driven and using off-the-shelf microcontroller and LoRa communication modules with a customized PCB design to integrate all the hardware. This enables SPARC-LoRa to achieve low power consumption, long range communication, and low cost. With a new connection-based upper layer protocol design, the scalability and communication reliability of SPARC-loRa can be achieved. Second, an open source software including sensor nodes and servers is designed based on Docker container with cloud storage, computing, and LTE functionalities. In order to achieve reliable wireless communication under extreme conditions, a relay module is designed and applied to SPARC-LoRa to forward the data from sensor nodes to the gateway node. The system design and implementation is completely open source and hosted on the DigitalOcean Droplet Cloud. Hence, the proposed system enables further research and applications in both academia and industry. The proposed system has been tested in real fields under different and extreme environmental conditions in Salt Lake City, Utah and the University of Nebraska-Lincoln. The experimental results validate the features of SPARC-LoRa including low power, reliability, and cloud services provided by SPARC-LoRa. ",
        "title": "SPARC-LoRa: A Scalable, Power-efficient, Affordable, Reliable, and Cloud  Service-enabled LoRa Networking System for Agriculture Applications",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13570",
        "abstract_url": "http://arxiv.org/abs/2401.13570",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Yanyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Lili"
            },
            {
                "last_name": "Zhai",
                "first_name": "Xiaoya"
            },
            {
                "last_name": "Chen",
                "first_name": "Kai"
            },
            {
                "last_name": "Wu",
                "first_name": "Wenming"
            },
            {
                "last_name": "Zhao",
                "first_name": "Yunkai"
            },
            {
                "last_name": "Liu",
                "first_name": "Ligang"
            },
            {
                "last_name": "Fu",
                "first_name": "Xiao-Ming"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE",
            "LG"
        ],
        "abstract": "  Mechanical metamaterial is a synthetic material that can possess extraordinary physical characteristics, such as abnormal elasticity, stiffness, and stability, by carefully designing its internal structure. To make metamaterials contain delicate local structures with unique mechanical properties, it is a potential method to represent them through high-resolution voxels. However, it brings a substantial computational burden. To this end, this paper proposes a fast inverse design method, whose core is an advanced deep generative AI algorithm, to generate voxel-based mechanical metamaterials. Specifically, we use the self-conditioned diffusion model, capable of generating a microstructure with a resolution of $128^3$ to approach the specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid reverse design tool facilitates the exploration of extreme metamaterials, the sequence interpolation in metamaterials, and the generation of diverse microstructures for multi-scale design. This flexible and adaptive generative tool is of great value in structural engineering or other mechanical systems and can stimulate more subsequent research. ",
        "title": "Guided Diffusion for Fast Inverse Design of Density-based Mechanical  Metamaterials",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13573",
        "abstract_url": "http://arxiv.org/abs/2401.13573",
        "authors": [
            {
                "last_name": "Fidalgo-D\u00edaz",
                "first_name": "Adri\u00e1n"
            },
            {
                "last_name": "Mart\u00ednez-Pe\u00f1as",
                "first_name": "Umberto"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The problem of straggler mitigation in distributed matrix multiplication (DMM) is considered for a large number of worker nodes and a fixed small finite field. Polynomial codes and matdot codes are generalized by making use of algebraic function fields (i.e., algebraic functions over an algebraic curve) over a finite field. The construction of optimal solutions is translated to a combinatorial problem on the Weierstrass semigroups of the corresponding algebraic curves. Optimal or almost optimal solutions are provided. These have the same computational complexity per worker as classical polynomial and matdot codes, and their recovery thresholds are almost optimal in the asymptotic regime (growing number of workers and a fixed finite field). ",
        "title": "Distributed matrix multiplication with straggler tolerance using  algebraic function fields",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13575",
        "abstract_url": "http://arxiv.org/abs/2401.13575",
        "authors": [
            {
                "last_name": "Horvath",
                "first_name": "Peter"
            },
            {
                "last_name": "Chmielewski",
                "first_name": "Lukasz"
            },
            {
                "last_name": "Weissbart",
                "first_name": "Leo"
            },
            {
                "last_name": "Batina",
                "first_name": "Lejla"
            },
            {
                "last_name": "Yarom",
                "first_name": "Yuval"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  Neural networks have become popular due to their versatility and state-of-the-art results in many applications, such as image classification, natural language processing, speech recognition, forecasting, etc. These applications are also used in resource-constrained environments such as embedded devices. In this work, the susceptibility of neural network implementations to reverse engineering is explored on the NVIDIA Jetson Nano microcomputer via side-channel analysis. To this end, an architecture extraction attack is presented. In the attack, 15 popular convolutional neural network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is analyzed during the inference operation of the neural networks. The results of the analysis show that neural network architectures are easily distinguishable using deep learning-based side-channel analysis. ",
        "title": "CNN architecture extraction on edge GPU",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13578",
        "abstract_url": "http://arxiv.org/abs/2401.13578",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Zhengyao"
            },
            {
                "last_name": "Li",
                "first_name": "Yongqiang"
            },
            {
                "last_name": "Yuan",
                "first_name": "Danni"
            },
            {
                "last_name": "Liu",
                "first_name": "Li"
            },
            {
                "last_name": "Wei",
                "first_name": "Shaokui"
            },
            {
                "last_name": "Wu",
                "first_name": "Baoyuan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works. ",
        "title": "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13581",
        "abstract_url": "http://arxiv.org/abs/2401.13581",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Haixin"
            },
            {
                "last_name": "Huang",
                "first_name": "Dong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Previous contrastive deep clustering methods mostly focus on instance-level information while overlooking the member relationship within groups/clusters, which may significantly undermine their representation learning and clustering capability. Recently, some group-contrastive methods have been developed, which, however, typically rely on the samples of the entire dataset to obtain pseudo labels and lack the ability to efficiently update the group assignments in a batch-wise manner. To tackle these critical issues, we present a novel end-to-end deep clustering framework with dynamic grouping and prototype aggregation, termed as DigPro. Specifically, the proposed dynamic grouping extends contrastive learning from instance-level to group-level, which is effective and efficient for timely updating groups. Meanwhile, we perform contrastive learning on prototypes in a spherical feature space, termed as prototype aggregation, which aims to maximize the inter-cluster distance. Notably, with an expectation-maximization framework, DigPro simultaneously takes advantage of compact intra-cluster connections, well-separated clusters, and efficient group updating during the self-supervised training. Extensive experiments on six image benchmarks demonstrate the superior performance of our approach over the state-of-the-art. Code is available at https://github.com/Regan-Zhang/DigPro. ",
        "title": "Towards Efficient and Effective Deep Clustering with Dynamic Grouping  and Prototype Aggregation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13584",
        "abstract_url": "http://arxiv.org/abs/2401.13584",
        "authors": [
            {
                "last_name": "Alamleh",
                "first_name": "Hosam"
            },
            {
                "last_name": "Gogarty",
                "first_name": "Michael"
            },
            {
                "last_name": "Ruddell",
                "first_name": "David"
            },
            {
                "last_name": "AlQahtani",
                "first_name": "Ali Abdullah S."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  This study presents an in-depth analysis of the security landscape in Bluetooth Low Energy (BLE) tracking systems, with a particular emphasis on Apple AirTags and Samsung SmartTags, including their cryptographic frameworks. Our investigation traverses a wide spectrum of attack vectors such as physical tampering, firmware exploitation, signal spoofing, eavesdropping, jamming, app security flaws, Bluetooth security weaknesses, location spoofing, threats to owner devices, and cloud-related vulnerabilities. Moreover, we delve into the security implications of the cryptographic methods utilized in these systems. Our findings reveal that while BLE trackers like AirTags and SmartTags offer substantial utility, they also pose significant security risks. Notably, Apple's approach, which prioritizes user privacy by removing intermediaries, inadvertently leads to device authentication challenges, evidenced by successful AirTag spoofing instances. Conversely, Samsung SmartTags, designed to thwart beacon spoofing, raise critical concerns about cloud security and user privacy. Our analysis also highlights the constraints faced by these devices due to their design focus on battery life conservation, particularly the absence of secure boot processes, which leaves them susceptible to OS modification and a range of potential attacks. The paper concludes with insights into the anticipated evolution of these tracking systems. We predict that future enhancements will likely focus on bolstering security features, especially as these devices become increasingly integrated into the broader IoT ecosystem and face evolving privacy regulations. This shift is imperative to address the intricate balance between functionality and security in next-generation BLE tracking systems. ",
        "title": "Securing the Invisible Thread: A Comprehensive Analysis of BLE Tracker  Security in Apple AirTags and Samsung SmartTags",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13585",
        "abstract_url": "http://arxiv.org/abs/2401.13585",
        "authors": [
            {
                "last_name": "Aldana-L\u00f3pez",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Arag\u00fc\u00e9s",
                "first_name": "Rosario"
            },
            {
                "last_name": "Sag\u00fc\u00e9s",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In robotic systems, perception latency is a term that refers to the computing time measured from the data acquisition to the moment in which perception output is ready to be used to compute control commands. There is a compromise between perception latency, precision for the overall robotic system, and computational resource usage referred to here as the latency-precision trade-off. In this work, we analyze a robot model given by a linear system, a zero-order hold controller, and measurements taken by several perception mode possibilities with different noise levels. We show that the analysis of this system is reduced to studying an equivalent switching system. Our goal is to schedule perception modes such that stability is attained while optimizing a cost function that models the latency-precision trade-off. Our solution framework comprises three main tools: the construction of perception scheduling policy candidates, admissibility verification for policy candidates, and optimal strategies based on admissible policies. ",
        "title": "Latency vs precision: Stability preserving perception scheduling",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13586",
        "abstract_url": "http://arxiv.org/abs/2401.13586",
        "authors": [
            {
                "last_name": "Huerta-Enochian",
                "first_name": "Mathew"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW. ",
        "title": "Prompt Weight Experiments for LLM Instruction Fine-Tuning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13587",
        "abstract_url": "http://arxiv.org/abs/2401.13587",
        "authors": [
            {
                "last_name": "Tandler",
                "first_name": "Daniel"
            },
            {
                "last_name": "Gauger",
                "first_name": "Marc"
            },
            {
                "last_name": "Tan",
                "first_name": "Ahmet Serdar"
            },
            {
                "last_name": "D\u00f6rner",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Brink",
                "first_name": "Stephan ten"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The challenging propagation environment, combined with the hardware limitations of mmWave systems, gives rise to the need for accurate initial access beam alignment strategies with low latency and high achievable beamforming gain. Much of the recent work in this area either focuses on one-sided beam alignment, or, joint beam alignment methods where both sides of the link perform a sequence of fixed channel probing steps. Codebook-based non-adaptive beam alignment schemes have the potential to allow multiple user equipment (UE) to perform initial access beam alignment in parallel whereas adaptive schemes are favourable in achievable beamforming gain. This work introduces a novel deep learning based joint beam alignment scheme that aims to combine the benefits of adaptive, codebook-free beam alignment at the UE side with the advantages of a codebook-sweep based scheme at the base station. The proposed end-to-end trainable scheme is compatible with current cellular standard signaling and can be readily integrated into the standard without requiring significant changes to it. Extensive simulations demonstrate superior performance of the proposed approach over purely codebook-based ones. ",
        "title": "Deep Learning Based Adaptive Joint mmWave Beam Alignment",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13588",
        "abstract_url": "http://arxiv.org/abs/2401.13588",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Darren"
            },
            {
                "last_name": "Ding",
                "first_name": "Cheng"
            },
            {
                "last_name": "Bold",
                "first_name": "Delgersuren"
            },
            {
                "last_name": "Bouvier",
                "first_name": "Monique"
            },
            {
                "last_name": "Lu",
                "first_name": "Jiaying"
            },
            {
                "last_name": "Shickel",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Jabaley",
                "first_name": "Craig S."
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenhui"
            },
            {
                "last_name": "Park",
                "first_name": "Soojin"
            },
            {
                "last_name": "Young",
                "first_name": "Michael J."
            },
            {
                "last_name": "Wainwright",
                "first_name": "Mark S."
            },
            {
                "last_name": "Clermont",
                "first_name": "Gilles"
            },
            {
                "last_name": "Rashidi",
                "first_name": "Parisa"
            },
            {
                "last_name": "Rosenthal",
                "first_name": "Eric S."
            },
            {
                "last_name": "Dimisko",
                "first_name": "Laurie"
            },
            {
                "last_name": "Xiao",
                "first_name": "Ran"
            },
            {
                "last_name": "Yoon",
                "first_name": "Joo Heung"
            },
            {
                "last_name": "Yang",
                "first_name": "Carl"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an in-depth analysis. Results: GPT-4 showed overall superior performance compared to other LLMs. In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed. The GPT family models have demonstrated considerable efficiency, evidenced by their cost-effectiveness and time-saving capabilities. Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized. This framework goes beyond singular performance aspects. With expert annotations, this methodology not only validates LLMs' capabilities in processing complex medical data but also establishes a benchmark for future LLM evaluations across specialized domains. ",
        "title": "Evaluation of General Large Language Models in Contextually Assessing  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record  Notes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13594",
        "abstract_url": "http://arxiv.org/abs/2401.13594",
        "authors": [
            {
                "last_name": "Pham",
                "first_name": "Hai X."
            },
            {
                "last_name": "Hadji",
                "first_name": "Isma"
            },
            {
                "last_name": "Xu",
                "first_name": "Xinnuo"
            },
            {
                "last_name": "Degutyte",
                "first_name": "Ziedune"
            },
            {
                "last_name": "Rainey",
                "first_name": "Jay"
            },
            {
                "last_name": "Kazakos",
                "first_name": "Evangelos"
            },
            {
                "last_name": "Fazly",
                "first_name": "Afsaneh"
            },
            {
                "last_name": "Tzimiropoulos",
                "first_name": "Georgios"
            },
            {
                "last_name": "Martinez",
                "first_name": "Brais"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller. 2) semantic coverage is the key indicator for downstream QA performance. Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model. In contrast, the higher semantic coverage provided by our method is critical for QA performance. ",
        "title": "Graph Guided Question Answer Generation for Procedural  Question-Answering",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13596",
        "abstract_url": "http://arxiv.org/abs/2401.13596",
        "authors": [
            {
                "last_name": "Aldana-L\u00f3pez",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Arag\u00fc\u00e9s",
                "first_name": "Rosario"
            },
            {
                "last_name": "Sag\u00fc\u00e9s",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Target tracking is a popular problem with many potential applications. There has been a lot of effort on improving the quality of the detection of targets using cameras through different techniques. In general, with higher computational effort applied, i.e., a longer perception-latency, a better detection accuracy is obtained. However, it is not always useful to apply the longest perception-latency allowed, particularly when the environment doesn't require to and when the computational resources are shared between other tasks. In this work, we propose a new Perception-LATency aware Estimator (PLATE), which uses different perception configurations in different moments of time in order to optimize a certain performance measure. This measure takes into account a perception-latency and accuracy trade-off aiming for a good compromise between quality and resource usage. Compared to other heuristic frame-skipping techniques, PLATE comes with a formal complexity and optimality analysis. The advantages of PLATE are verified by several experiments including an evaluation over a standard benchmark with real data and using state of the art deep learning object detection methods for the perception stage. ",
        "title": "PLATE: A perception-latency aware estimator,",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13597",
        "abstract_url": "http://arxiv.org/abs/2401.13597",
        "authors": [
            {
                "last_name": "Eckhardt",
                "first_name": "Timo"
            },
            {
                "last_name": "Pym",
                "first_name": "David J."
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  In proof-theoretic semantics, meaning is based on inference. It may seen as the mathematical expression of the inferentialist interpretation of logic. Much recent work has focused on base-extension semantics, in which the validity of formulas is given by an inductive definition generated by provability in a `base' of atomic rules. Base-extension semantics for classical and intuitionistic propositional logic have been explored by several authors. In this paper, we develop base-extension semantics for the classical propositional modal systems K, KT , K4, and S4, with $\\square$ as the primary modal operator. We establish appropriate soundness and completeness theorems and establish the duality between $\\square$ and a natural presentation of $\\lozenge$. We also show that our semantics is in its current form not complete with respect to euclidean modal logics. Our formulation makes essential use of relational structures on bases. ",
        "title": "Base-extension Semantics for Modal Logic",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13598",
        "abstract_url": "http://arxiv.org/abs/2401.13598",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Qi"
            },
            {
                "last_name": "Huang",
                "first_name": "Kun"
            },
            {
                "last_name": "Yang",
                "first_name": "Xiaocui"
            },
            {
                "last_name": "Tong",
                "first_name": "Rong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kun"
            },
            {
                "last_name": "Poria",
                "first_name": "Soujanya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines. ",
        "title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for  Zero-shot Document-level Relation Triplet Extraction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13601",
        "abstract_url": "http://arxiv.org/abs/2401.13601",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Duzhen"
            },
            {
                "last_name": "Yu",
                "first_name": "Yahan"
            },
            {
                "last_name": "Li",
                "first_name": "Chenxing"
            },
            {
                "last_name": "Dong",
                "first_name": "Jiahua"
            },
            {
                "last_name": "Su",
                "first_name": "Dan"
            },
            {
                "last_name": "Chu",
                "first_name": "Chenhui"
            },
            {
                "last_name": "Yu",
                "first_name": "Dong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain. ",
        "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13602",
        "abstract_url": "http://arxiv.org/abs/2401.13602",
        "authors": [
            {
                "last_name": "Aldana-L\u00f3pez",
                "first_name": "Rodrigo"
            },
            {
                "last_name": "Arag\u00fc\u00e9s",
                "first_name": "Rosario"
            },
            {
                "last_name": "Sag\u00fc\u00e9s",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This work is devoted to the problem of distributed target tracking when a team of robots detect the target through a variable perception-latency mechanism. A reference for the robots to track is constructed in terms of a desired formation around the estimation of the target position. However, it is noted that due to the perception-latency, classical estimation techniques have smoothness issues which prevent asymptotic stability for the formation control. We propose a near-optimal smooth-output estimator which circumvents this issue. Moreover, local estimations are fused using novel dynamic consensus techniques. The advantages of the proposal as well as a comparison with a non-smooth optimal alternative are discussed through simulation examples. ",
        "title": "Perception-latency aware distributed target tracking",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13604",
        "abstract_url": "http://arxiv.org/abs/2401.13604",
        "authors": [
            {
                "last_name": "D\u00f6tterl",
                "first_name": "Jeremias"
            },
            {
                "last_name": "Bruns",
                "first_name": "Ralf"
            },
            {
                "last_name": "Dunkel",
                "first_name": "J\u00fcrgen"
            },
            {
                "last_name": "Ossowski",
                "first_name": "Sascha"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements. Experiments with real smartphone data demonstrate the benefits of stream-based agent perception. ",
        "title": "Stream-based perception for cognitive agents in mobile ecosystems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13605",
        "abstract_url": "http://arxiv.org/abs/2401.13605",
        "authors": [
            {
                "last_name": "Kieslich",
                "first_name": "Kimon"
            },
            {
                "last_name": "L\u00fcnich",
                "first_name": "Marco"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but are criticised for inheriting biases and violating fundamental human rights. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI. As a possible counterweight, public opinion can have a decisive influence on policymakers to establish boundaries and conditions under which AI systems should be used -- if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems. ",
        "title": "Regulating AI-Based Remote Biometric Identification. Investigating the  Public Demand for Bans, Audits, and Public Database Registrations",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13606",
        "abstract_url": "http://arxiv.org/abs/2401.13606",
        "authors": [
            {
                "last_name": "Moya-Lasheras",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Sagues",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  There is great interest in minimizing the impact forces of reluctance actuators during commutations, in order to reduce contact bouncing, acoustic noise and mechanical wear. In this regard, a run-to-run control algorithm is proposed to decrease the contact velocity, by exploiting the repetitive operations of these devices. The complete control is presented, with special focus on the optimization method and the input definition. The search method is based on Bayesian optimization, and several additions are introduced for its application in run-to-run control, e.g. the removal of stored points and the definition of a new acquisition function. Additionally, methods for the input parametrization and dimension reduction are presented. For analysis, Monte Carlo simulations are performed using a dynamic model of a commercial solenoid valve, comparing the proposed search method with two alternatives. Furthermore, the control strategy is validated through experimental testing, using several devices from the same ensemble of solenoid valves. ",
        "title": "Run-to-Run Control With Bayesian Optimization for Soft Landing of  Short-Stroke Reluctance Actuators",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13609",
        "abstract_url": "http://arxiv.org/abs/2401.13609",
        "authors": [
            {
                "last_name": "Abu-Rasheed",
                "first_name": "Hasan"
            },
            {
                "last_name": "Dornh\u00f6fer",
                "first_name": "Mareike"
            },
            {
                "last_name": "Weber",
                "first_name": "Christian"
            },
            {
                "last_name": "Kismih\u00f3k",
                "first_name": "G\u00e1bor"
            },
            {
                "last_name": "Buchmann",
                "first_name": "Ulrike"
            },
            {
                "last_name": "Fathi",
                "first_name": "Madjid"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality. ",
        "title": "Building Contextual Knowledge Graphs for Personalized Learning  Recommendations using Text Mining and Semantic Graph Completion",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13610",
        "abstract_url": "http://arxiv.org/abs/2401.13610",
        "authors": [
            {
                "last_name": "Aranda",
                "first_name": "Miguel"
            },
            {
                "last_name": "Mezouar",
                "first_name": "Youcef"
            },
            {
                "last_name": "L\u00f3pez-Nicol\u00e1s",
                "first_name": "Gonzalo"
            },
            {
                "last_name": "Sag\u00fc\u00e9s",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We present a novel vision-based control method to make a group of ground mobile robots achieve a specified formation shape with unspecified size. Our approach uses multiple aerial control units equipped with downward-facing cameras, each observing a partial subset of the multirobot team. The units compute the control commands from the ground robots' image projections, using neither calibration nor scene scale information, and transmit them to the robots. The control strategy relies on the calculation of image similarity transformations, and we show it to be asymptotically stable if the overlaps between the subsets of controlled robots satisfy certain conditions. The presence of the supervisory units, which coordinate their motions to guarantee a correct control performance, gives rise to a hybrid system topology. All in all, the proposed system provides relevant practical advantages in simplicity and flexibility. Within the problem of controlling a team shape, our contribution lies in addressing several simultaneous challenges: the controller needs only partial information of the robotic group, does not use distance measurements or global reference frames, is designed for unicycle agents, and can accommodate topology changes. We present illustrative simulation results. ",
        "title": "Scale-free vision-based aerial control of a ground formation with hybrid  topology",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13611",
        "abstract_url": "http://arxiv.org/abs/2401.13611",
        "authors": [
            {
                "last_name": "Mogridge",
                "first_name": "Rhiannon"
            },
            {
                "last_name": "Close",
                "first_name": "George"
            },
            {
                "last_name": "Sutherland",
                "first_name": "Robert"
            },
            {
                "last_name": "Hain",
                "first_name": "Thomas"
            },
            {
                "last_name": "Barker",
                "first_name": "Jon"
            },
            {
                "last_name": "Goetze",
                "first_name": "Stefan"
            },
            {
                "last_name": "Ragni",
                "first_name": "Anton"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7. ",
        "title": "Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired  Users using Intermediate ASR Features and Human Memory Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13612",
        "abstract_url": "http://arxiv.org/abs/2401.13612",
        "authors": [
            {
                "last_name": "Aragues",
                "first_name": "Rosario"
            },
            {
                "last_name": "Dimarogonas",
                "first_name": "Dimos V."
            },
            {
                "last_name": "Guallar",
                "first_name": "Pablo"
            },
            {
                "last_name": "Sagues",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We consider a scenario of cooperative task servicing, with a team of heterogeneous robots with different maximum speeds and communication radii, in charge of keeping the network intermittently connected. We abstract the task locations into a $1D$ cycle graph that is traversed by the communicating robots, and we discuss intermittent communication strategies so that each task location is periodically visited, with a worst--case revisiting time. Robots move forward and backward along the cycle graph, exchanging data with their previous and next neighbors when they meet, and updating their region boundaries. Asymptotically, each robot is in charge of a region of the cycle graph, depending on its capabilities. The method is distributed, and robots only exchange data when they meet. ",
        "title": "Intermittent Connectivity Maintenance With Heterogeneous Robots",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13613",
        "abstract_url": "http://arxiv.org/abs/2401.13613",
        "authors": [
            {
                "last_name": "Lahajal",
                "first_name": "Naresh Kumar"
            },
            {
                "last_name": "S",
                "first_name": "Harini"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications ",
        "title": "Enhancing Image Retrieval : A Comprehensive Study on Photo Search using  the CLIP Mode",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13614",
        "abstract_url": "http://arxiv.org/abs/2401.13614",
        "authors": [
            {
                "last_name": "Palacios-Gas\u00f3s",
                "first_name": "Jos\u00e9 Manuel"
            },
            {
                "last_name": "Tardioli",
                "first_name": "Danilo"
            },
            {
                "last_name": "Montijano",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Sag\u00fc\u00e9s",
                "first_name": "Carlos"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper we tackle the problem of persistently covering a complex non-convex environment with a team of robots. We consider scenarios where the coverage quality of the environment deteriorates with time, requiring to constantly revisit every point. As a first step, our solution finds a partition of the environment where the amount of work for each robot, weighted by the importance of each point, is equal. This is achieved using a power diagram and finding an equitable partition through a provably correct distributed control law on the power weights. Compared to other existing partitioning methods, our solution considers a continuous environment formulation with non-convex obstacles. In the second step, each robot computes a graph that gathers sweep-like paths and covers its entire partition. At each planning time, the coverage error at the graph vertices is assigned as weights of the corresponding edges. Then, our solution is capable of efficiently finding the optimal open coverage path through the graph with respect to the coverage error per distance traversed. Simulation and experimental results are presented to support our proposal. ",
        "title": "Equitable Persistent Coverage of Non-Convex Environments with  Graph-Based Planning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13616",
        "abstract_url": "http://arxiv.org/abs/2401.13616",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xi"
            },
            {
                "last_name": "Wu",
                "first_name": "Xiaolin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, DNN models for lossless image coding have surpassed their traditional counterparts in compression performance, reducing the bit rate by about ten percent for natural color images. But even with these advances, mathematically lossless image compression (MLLIC) ratios for natural images still fall short of the bandwidth and cost-effectiveness requirements of most practical imaging and vision systems at present and beyond. To break the bottleneck of MLLIC in compression performance, we question the necessity of MLLIC, as almost all digital sensors inherently introduce acquisition noises, making mathematically lossless compression counterproductive. Therefore, in contrast to MLLIC, we propose a new paradigm of joint denoising and compression called functionally lossless image compression (FLLIC), which performs lossless compression of optimally denoised images (the optimality may be task-specific). Although not literally lossless with respect to the noisy input, FLLIC aims to achieve the best possible reconstruction of the latent noise-free original image. Extensive experiments show that FLLIC achieves state-of-the-art performance in joint denoising and compression of noisy images and does so at a lower computational cost. ",
        "title": "FLLIC: Functionally Lossless Image Compression",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13621",
        "abstract_url": "http://arxiv.org/abs/2401.13621",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Xinghao"
            },
            {
                "last_name": "He",
                "first_name": "Junliang"
            },
            {
                "last_name": "Wang",
                "first_name": "Pengyu"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yunhua"
            },
            {
                "last_name": "Sun",
                "first_name": "Tianxiang"
            },
            {
                "last_name": "Qiu",
                "first_name": "Xipeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in comparison to contrastive-learning-based methods. Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance. Our code is available at https://github.com/xinghaow99/DenoSent. ",
        "title": "DenoSent: A Denoising Objective for Self-Supervised Sentence  Representation Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13622",
        "abstract_url": "http://arxiv.org/abs/2401.13622",
        "authors": [
            {
                "last_name": "Palacios-Gas\u00f3s",
                "first_name": "Jos\u00e9 Manuel"
            },
            {
                "last_name": "Montijano",
                "first_name": "Eduardo"
            },
            {
                "last_name": "Sag\u00fc\u00e9s",
                "first_name": "Carlos"
            },
            {
                "last_name": "Llorente",
                "first_name": "Sergio"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  In this paper we propose a periodic solution to the problem of persistently covering a finite set of interest points with a group of autonomous mobile agents. These agents visit periodically the points and spend some time carrying out the coverage task, which we call coverage time. Since this periodic persistent coverage problem is NP-hard, we split it into three subproblems to counteract its complexity. In the first place, we plan individual closed paths for the agents to cover all the points. Second, we formulate a quadratically constrained linear program to find the optimal coverage times and actions that satisfy the coverage objective. Finally, we join together the individual plans of the agents in a periodic team plan by obtaining a schedule that guarantees collision avoidance. To this end, we solve a mixed integer linear program that minimizes the time in which two or more agents move at the same time. Eventually, we apply the proposed solution to an induction hob with mobile inductors for a domestic heating application and show its performance with experiments on a real prototype. ",
        "title": "Cooperative Periodic Coverage With Collision Avoidance",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13623",
        "abstract_url": "http://arxiv.org/abs/2401.13623",
        "authors": [
            {
                "last_name": "Farias",
                "first_name": "Roselane Silva"
            },
            {
                "last_name": "Ahmed",
                "first_name": "Iftekhar"
            },
            {
                "last_name": "de Almeida",
                "first_name": "Eduardo Santana"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities. In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end. Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked. As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great? We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world. We use the data collected from these activities to derive a comprehensive set of attributes that are considered important. As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes. Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills. This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice. ",
        "title": "What Makes a Great Software Quality Assurance Engineer?",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13624",
        "abstract_url": "http://arxiv.org/abs/2401.13624",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Zhongjie"
            },
            {
                "last_name": "Liu",
                "first_name": "Fanghui"
            },
            {
                "last_name": "Cao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Suykens",
                "first_name": "Johan A. K."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations. However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \\textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint. Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) Linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough. iii) For regression, our results demonstrate that there also exist infinitely many overfitted DNNs with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error. Overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable. We hope our analysis will give a better understanding of the mathematical foundations of robustness in DNNs from an approximation view. ",
        "title": "Can overfitted deep neural networks in adversarial training generalize?  -- An approximation viewpoint",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13627",
        "abstract_url": "http://arxiv.org/abs/2401.13627",
        "authors": [
            {
                "last_name": "Yu",
                "first_name": "Fanghua"
            },
            {
                "last_name": "Gu",
                "first_name": "Jinjin"
            },
            {
                "last_name": "Li",
                "first_name": "Zheyuan"
            },
            {
                "last_name": "Hu",
                "first_name": "Jinfan"
            },
            {
                "last_name": "Kong",
                "first_name": "Xiangtao"
            },
            {
                "last_name": "Wang",
                "first_name": "Xintao"
            },
            {
                "last_name": "He",
                "first_name": "Jingwen"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            },
            {
                "last_name": "Dong",
                "first_name": "Chao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts. ",
        "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic  Image Restoration In the Wild",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13630",
        "abstract_url": "http://arxiv.org/abs/2401.13630",
        "authors": [
            {
                "last_name": "Vieira",
                "first_name": "Emanuel"
            },
            {
                "last_name": "Almeida",
                "first_name": "Jo\u00e3o"
            },
            {
                "last_name": "Ferreira",
                "first_name": "Joaquim"
            },
            {
                "last_name": "Bartolomeu",
                "first_name": "Paulo C."
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Cooperative driving is an emerging paradigm to enhance the safety and efficiency of autonomous vehicles. To ensure successful cooperation, road users must reach a consensus for making collective decisions, while recording vehicular data to analyze and address failures related to such agreements. This data has the potential to provide valuable insights into various vehicular events, while also potentially improving accountability measures. Furthermore, vehicles may benefit from the ability to negotiate and trade services among themselves, adding value to the cooperative driving framework. However, the majority of proposed systems aiming to ensure data security, consensus, or service trading, lack efficient and thoroughly validated mechanisms that consider the distinctive characteristics of vehicular networks. These limitations are amplified by a dependency on the centralized support provided by the infrastructure. Furthermore, corresponding mechanisms must diligently address security concerns, especially regarding potential malicious or misbehaving nodes, while also considering inherent constraints of the wireless medium. We introduce the Verifiable Event Extension (VEE), an applicational extension designed for Intelligent Transportation System (ITS) messages. The VEE operates seamlessly with any existing standardized vehicular communications protocol, addressing crucial aspects of data security, consensus, and trading with minimal overhead. To achieve this, we employ blockchain techniques, Byzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based mechanics. To assess our proposal's feasibility and lightweight nature, we employed a hardware-in-the-loop setup for analysis. Experimental results demonstrate the viability and efficiency of the VEE extension in overcoming the challenges posed by the distributed and opportunistic nature of wireless vehicular communications. ",
        "title": "Enabling Seamless Data Security, Consensus, and Trading in Vehicular  Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13631",
        "abstract_url": "http://arxiv.org/abs/2401.13631",
        "authors": [
            {
                "last_name": "Debnath",
                "first_name": "Rubi"
            },
            {
                "last_name": "Hortig",
                "first_name": "Philipp"
            },
            {
                "last_name": "Zhao",
                "first_name": "Luxi"
            },
            {
                "last_name": "Steinhorst",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Different scheduling mechanisms in Time Sensitive Networking (TSN) can be integrated together to design and support complex architectures with enhanced capabilities for mixed critical networks. Integrating Frame Preemption (FP) with Credit-Based Shaper (CBS) and Gate Control List (GCL) opens up different modes and configuration choices resulting in a complex evaluation of several possibilities and their impact on the Quality of Service (QoS). In this paper, we implement and quantify the integration of preemptive CBS with GCL by incorporating FP into the architecture. Our experiments show that the end-to-end delay of Audio Video Bridging (AVB) flows shaped by CBS reduces significantly (up to 40\\%) when AVB flows are set to preemptable class. We further show that the jitter of Time Triggered (TT) traffic remains unaffected in \"with Hold/Release\" mode. Furthermore, we propose to introduce Guardband (GB) in the \"without Hold/Release\" to reduce the jitter of the TT flow. We compare all the different integration modes, starting with CBS with GCL, extending it further to FP. We evaluate all feasible combinations in both synthetic and realistic scenarios and offer recommendations for practical configuration methods. ",
        "title": "Quantifying the Impact of Frame Preemption on Combined TSN Shapers",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13639",
        "abstract_url": "http://arxiv.org/abs/2401.13639",
        "authors": [
            {
                "last_name": "Xiao",
                "first_name": "Dong"
            },
            {
                "last_name": "Ma",
                "first_name": "Yueji"
            },
            {
                "last_name": "Shi",
                "first_name": "Zuoqiang"
            },
            {
                "last_name": "Xin",
                "first_name": "Shiqing"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenping"
            },
            {
                "last_name": "Deng",
                "first_name": "Bailin"
            },
            {
                "last_name": "Wang",
                "first_name": "Bin"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR"
        ],
        "abstract": "  We propose to explore the properties of raw point clouds through the \\emph{winding clearness}, a concept we first introduce for assessing the clarity of the interior/exterior relationships represented by the winding number field of the point cloud. In geometric modeling, the winding number is a powerful tool for distinguishing the interior and exterior of a given surface $\\partial \\Omega$, and it has been previously used for point normal orientation and surface reconstruction. In this work, we introduce a novel approach to assess and optimize the quality of point clouds based on the winding clearness. We observe that point clouds with reduced noise tend to exhibit improved winding clearness. Accordingly, we propose an objective function that quantifies the error in winding clearness, solely utilizing the positions of the point clouds. Moreover, we demonstrate that the winding clearness error is differentiable and can serve as a loss function in optimization-based and learning-based point cloud processing. In the optimization-based method, the loss function is directly back-propagated to update the point positions, resulting in an overall improvement of the point cloud. In the learning-based method, we incorporate the winding clearness as a geometric constraint in the diffusion-based 3D generative model. Experimental results demonstrate the effectiveness of optimizing the winding clearness in enhancing the quality of the point clouds. Our method exhibits superior performance in handling noisy point clouds with thin structures, highlighting the benefits of the global perspective enabled by the winding number. ",
        "title": "Winding Clearness for Differentiable Point Cloud Optimization",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13641",
        "abstract_url": "http://arxiv.org/abs/2401.13641",
        "authors": [
            {
                "last_name": "DeAndres-Tame",
                "first_name": "Ivan"
            },
            {
                "last_name": "Tolosana",
                "first_name": "Ruben"
            },
            {
                "last_name": "Vera-Rodriguez",
                "first_name": "Ruben"
            },
            {
                "last_name": "Morales",
                "first_name": "Aythami"
            },
            {
                "last_name": "Fierrez",
                "first_name": "Julian"
            },
            {
                "last_name": "Ortega-Garcia",
                "first_name": "Javier"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CY",
            "LG"
        ],
        "abstract": "  Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).   The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field. The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability. For reproducibility reasons, we release all the code in GitHub. ",
        "title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition,  Soft Biometrics, and Explainability",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13643",
        "abstract_url": "http://arxiv.org/abs/2401.13643",
        "authors": [
            {
                "last_name": "Lee",
                "first_name": "Christine P"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences. ",
        "title": "Design, Development, and Deployment of Context-Adaptive AI Systems for  Enhanced End-User Adoption",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13645",
        "abstract_url": "http://arxiv.org/abs/2401.13645",
        "authors": [
            {
                "last_name": "Mayer",
                "first_name": "Florian"
            },
            {
                "last_name": "Brandner",
                "first_name": "Julian"
            },
            {
                "last_name": "Philippsen",
                "first_name": "Michael"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL"
        ],
        "abstract": "  It is well known that to accelerate stencil codes on CPUs or GPUs and to exploit hardware caches and their lines optimizers must find spatial and temporal locality of array accesses to harvest data-reuse opportunities. On FPGAs there is the burden that there are no built-in caches (or only pre-built hardware descriptions for cache blocks that are inefficient for stencil codes). But this paper demonstrates that this lack is also a chance as polyhedral methods can be used to generate stencil-specific cache-structures of the right sizes on the FPGA and to fill and flush them efficiently with wide bursts during stencil execution. The paper shows how to derive the appropriate directives and code restructurings from stencil codes so that the FPGA compiler generates fast stencil hardware. Switching on our optimization improves the runtime of a set of 10 stencils by between 43x and 156x. ",
        "title": "Employing polyhedral methods to optimize stencils on FPGAs with  stencil-specific caches, data reuse, and wide data bursts",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13649",
        "abstract_url": "http://arxiv.org/abs/2401.13649",
        "authors": [
            {
                "last_name": "Koh",
                "first_name": "Jing Yu"
            },
            {
                "last_name": "Lo",
                "first_name": "Robert"
            },
            {
                "last_name": "Jang",
                "first_name": "Lawrence"
            },
            {
                "last_name": "Duvvur",
                "first_name": "Vikram"
            },
            {
                "last_name": "Lim",
                "first_name": "Ming Chong"
            },
            {
                "last_name": "Huang",
                "first_name": "Po-Yu"
            },
            {
                "last_name": "Neubig",
                "first_name": "Graham"
            },
            {
                "last_name": "Zhou",
                "first_name": "Shuyan"
            },
            {
                "last_name": "Salakhutdinov",
                "first_name": "Ruslan"
            },
            {
                "last_name": "Fried",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "CV"
        ],
        "abstract": "  Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa. ",
        "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web  Tasks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13650",
        "abstract_url": "http://arxiv.org/abs/2401.13650",
        "authors": [
            {
                "last_name": "Rakic",
                "first_name": "Marianne"
            },
            {
                "last_name": "Wong",
                "first_name": "Hallee E."
            },
            {
                "last_name": "Ortiz",
                "first_name": "Jose Javier Gonzalez"
            },
            {
                "last_name": "Cimini",
                "first_name": "Beth"
            },
            {
                "last_name": "Guttag",
                "first_name": "John"
            },
            {
                "last_name": "Dalca",
                "first_name": "Adrian V."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain. ",
        "title": "Tyche: Stochastic In-Context Learning for Medical Image Segmentation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13652",
        "abstract_url": "http://arxiv.org/abs/2401.13652",
        "authors": [
            {
                "last_name": "Della Santa",
                "first_name": "Francesco"
            },
            {
                "last_name": "Pieraccini",
                "first_name": "Sandra"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users. ",
        "title": "Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity  Detectors",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13653",
        "abstract_url": "http://arxiv.org/abs/2401.13653",
        "authors": [
            {
                "last_name": "Meel",
                "first_name": "Shreya"
            },
            {
                "last_name": "Ulukus",
                "first_name": "Sennur"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "CR",
            "NI"
        ],
        "abstract": "  Verifying user attributes to provide fine-grained access control to databases is fundamental to an attribute-based authentication system. In such systems, either a single (central) authority verifies all attributes, or multiple independent authorities verify individual attributes distributedly to allow a user to access records stored on the servers. While a \\emph{central} setup is more communication cost efficient, it causes privacy breach of \\emph{all} user attributes to a central authority. Recently, Jafarpisheh et al. studied an information theoretic formulation of the \\emph{distributed} multi-authority setup with $N$ non-colluding authorities, $N$ attributes and $K$ possible values for each attribute, called an $(N,K)$ distributed attribute-based private access control (DAPAC) system, where each server learns only one attribute value that it verifies, and remains oblivious to the remaining $N-1$ attributes. We show that off-loading a subset of attributes to a central server for verification improves the achievable rate from $\\frac{1}{2K}$ in Jafarpisheh et al. to $\\frac{1}{K+1}$ in this paper, thus \\emph{almost doubling the rate} for relatively large $K$, while sacrificing the privacy of a few possibly non-sensitive attributes. ",
        "title": "HetDAPAC: Distributed Attribute-Based Private Access Control with  Heterogeneous Attributes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13656",
        "abstract_url": "http://arxiv.org/abs/2401.13656",
        "authors": [
            {
                "last_name": "Colacrai",
                "first_name": "Ernesto"
            },
            {
                "last_name": "Cinus",
                "first_name": "Federico"
            },
            {
                "last_name": "Morales",
                "first_name": "Gianmarco De Francisci"
            },
            {
                "last_name": "Starnini",
                "first_name": "Michele"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "CY"
        ],
        "abstract": "  The prevalent perspective in quantitative research on opinion dynamics flattens the landscape of the online political discourse into a traditional left--right dichotomy. While this approach helps simplify the analysis and modeling effort, it also neglects the intrinsic multidimensional richness of ideologies. In this study, we analyze social interactions on Reddit, under the lens of a multi-dimensional ideological framework: the political compass. We examine over 8 million comments posted on the subreddits /r/PoliticalCompass and /r/PoliticalCompassMemes during 2020--2022. By leveraging their self-declarations, we disentangle the ideological dimensions of users into economic (left--right) and social (libertarian--authoritarian) axes. In addition, we characterize users by their demographic attributes (age, gender, and affluence).   We find significant homophily for interactions along the social axis of the political compass and demographic attributes. Compared to a null model, interactions among individuals of similar ideology surpass expectations by 6%. In contrast, we uncover a significant heterophily along the economic axis: left/right interactions exceed expectations by 10%. Furthermore, heterophilic interactions are characterized by a higher language toxicity than homophilic interactions, which hints at a conflictual discourse between every opposite ideology. Our results help reconcile apparent contradictions in recent literature, which found a superposition of homophilic and heterophilic interactions in online political discussions. By disentangling such interactions into the economic and social axes we pave the way for a deeper understanding of opinion dynamics on social media. ",
        "title": "Navigating Multidimensional Ideologies with Reddit's Political Compass:  Economic Conflict and Social Affinity",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13660",
        "abstract_url": "http://arxiv.org/abs/2401.13660",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Junxiong"
            },
            {
                "last_name": "Gangavarapu",
                "first_name": "Tushaar"
            },
            {
                "last_name": "Yan",
                "first_name": "Jing Nathan"
            },
            {
                "last_name": "Rush",
                "first_name": "Alexander M"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling. ",
        "title": "MambaByte: Token-free Selective State Space Model",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13662",
        "abstract_url": "http://arxiv.org/abs/2401.13662",
        "authors": [
            {
                "last_name": "Lehmann",
                "first_name": "Matthias"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax. ",
        "title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:  Theory, Algorithms and Implementations",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13666",
        "abstract_url": "http://arxiv.org/abs/2401.13666",
        "authors": [
            {
                "last_name": "Kabulov",
                "first_name": "Anvar"
            },
            {
                "last_name": "Babadzhanov",
                "first_name": "Alimdzhan"
            },
            {
                "last_name": "Saymanov",
                "first_name": "Islambek"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we propose to consider various models of pattern recognition. At the same time, it is proposed to consider models in the form of two operators: a recognizing operator and a decision rule. Algebraic operations are introduced on recognizing operators, and based on the application of these operators, a family of recognizing algorithms is created. An upper estimate is constructed for the model, which guarantees the completeness of the extension. ",
        "title": "Algebraic methods for solving recognition problems with non-crossing  classes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13667",
        "abstract_url": "http://arxiv.org/abs/2401.13667",
        "authors": [
            {
                "last_name": "Mujahid",
                "first_name": "Suhaib"
            },
            {
                "last_name": "Costa",
                "first_name": "Diego Elias"
            },
            {
                "last_name": "Castelluccio",
                "first_name": "Marco"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases. Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases. In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels. We also list the challenges that need to be considered when approaching this problem. ",
        "title": "Predicting the Impact of Crashes Across Release Channels",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13671",
        "abstract_url": "http://arxiv.org/abs/2401.13671",
        "authors": [
            {
                "last_name": "Ramaharo",
                "first_name": "Franck"
            },
            {
                "last_name": "Randriamifidy",
                "first_name": "Fitiavana"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The aim of this note is to identify the factors influencing renewable energy consumption in Madagascar. We tested 12 features covering macroeconomic, financial, social, and environmental aspects, including economic growth, domestic investment, foreign direct investment, financial development, industrial development, inflation, income distribution, trade openness, exchange rate, tourism development, environmental quality, and urbanization. To assess their significance, we assumed a linear relationship between renewable energy consumption and these features over the 1990-2021 period. Next, we applied different machine learning feature selection algorithms classified as filter-based (relative importance for linear regression, correlation method), embedded (LASSO), and wrapper-based (best subset regression, stepwise regression, recursive feature elimination, iterative predictor weighting partial least squares, Boruta, simulated annealing, and genetic algorithms) methods. Our analysis revealed that the five most influential drivers stem from macroeconomic aspects. We found that domestic investment, foreign direct investment, and inflation positively contribute to the adoption of renewable energy sources. On the other hand, industrial development and trade openness negatively affect renewable energy consumption in Madagascar. ",
        "title": "Determinants of renewable energy consumption in Madagascar: Evidence  from feature selection algorithms",
        "date": "2023-10-27",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13672",
        "abstract_url": "http://arxiv.org/abs/2401.13672",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Yu"
            },
            {
                "last_name": "Sun",
                "first_name": "Jianxin"
            },
            {
                "last_name": "Yu",
                "first_name": "Hongfeng"
            },
            {
                "last_name": "Bai",
                "first_name": "Geng"
            },
            {
                "last_name": "Ge",
                "first_name": "Yufeng"
            },
            {
                "last_name": "Luck",
                "first_name": "Joe"
            },
            {
                "last_name": "Awada",
                "first_name": "Tala"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "IR"
        ],
        "abstract": "  Modern agriculture faces grand challenges to meet increased demands for food, fuel, feed, and fiber with population growth under the constraints of climate change and dwindling natural resources. Data innovation is urgently required to secure and improve the productivity, sustainability, and resilience of our agroecosystems. As various sensors and Internet of Things (IoT) instrumentation become more available, affordable, reliable, and stable, it has become possible to conduct data collection, integration, and analysis at multiple temporal and spatial scales, in real-time, and with high resolutions. At the same time, the sheer amount of data poses a great challenge to data storage and analysis, and the \\textit{de facto} data management and analysis practices adopted by scientists have become increasingly inefficient. Additionally, the data generated from different disciplines, such as genomics, phenomics, environment, agronomy, and socioeconomic, can be highly heterogeneous. That is, datasets across disciplines often do not share the same ontology, modality, or format. All of the above make it necessary to design a new data management infrastructure that implements the principles of Findable, Accessible, Interoperable, and Reusable (FAIR). In this paper, we propose Agriculture Data Management and Analytics (ADMA), which satisfies the FAIR principles. Our new data management infrastructure is intelligent by supporting semantic data management across disciplines, interactive by providing various data management/analysis portals such as web GUI, command line, and API, scalable by utilizing the power of high-performance computing (HPC), extensible by allowing users to load their own data analysis tools, trackable by keeping track of different operations on each file, and open by using a rich set of mature open source technologies. ",
        "title": "Transforming Agriculture with Intelligent Data Management and Insights",
        "date": "2023-11-07",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13677",
        "abstract_url": "http://arxiv.org/abs/2401.13677",
        "authors": [
            {
                "last_name": "Koschmider",
                "first_name": "Agnes"
            },
            {
                "last_name": "Aleknonyt\u0117-Resch",
                "first_name": "Milda"
            },
            {
                "last_name": "Fonger",
                "first_name": "Frederik"
            },
            {
                "last_name": "Imenkamp",
                "first_name": "Christian"
            },
            {
                "last_name": "Lepsien",
                "first_name": "Arvid"
            },
            {
                "last_name": "Apaydin",
                "first_name": "Kaan"
            },
            {
                "last_name": "Harms",
                "first_name": "Maximilian"
            },
            {
                "last_name": "Janssen",
                "first_name": "Dominik"
            },
            {
                "last_name": "Langhammer",
                "first_name": "Dominic"
            },
            {
                "last_name": "Ziolkowski",
                "first_name": "Tobias"
            },
            {
                "last_name": "Zisgen",
                "first_name": "Yorck"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB",
            "LG"
        ],
        "abstract": "  The application of process mining for unstructured data might significantly elevate novel insights into disciplines where unstructured data is a common data format. To efficiently analyze unstructured data by process mining and to convey confidence into the analysis result, requires bridging multiple challenges. The purpose of this paper is to discuss these challenges, present initial solutions and describe future research directions. We hope that this article lays the foundations for future collaboration on this topic. ",
        "title": "Process Mining for Unstructured Data: Challenges and Research Directions",
        "date": "2023-11-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13680",
        "abstract_url": "http://arxiv.org/abs/2401.13680",
        "authors": [
            {
                "last_name": "Goglachev",
                "first_name": "Andrey"
            }
        ],
        "primary_category": "DB",
        "categories": [
            "DB"
        ],
        "abstract": "  This article presents the PaSTiLa algorithm for automated labeling of large time series on a cluster with GPUs. The method automatically selects snippet length values based on the new proposed criterion and allows to search for patterns with high performance. Experiments showed high accuracy of pattern search and the advantage of the method compared to analogues. ",
        "title": "A parallel algorithm for automated labeling of large time series",
        "date": "2023-12-11",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13689",
        "abstract_url": "http://arxiv.org/abs/2401.13689",
        "authors": [
            {
                "last_name": "Saffarini",
                "first_name": "Ali"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  The use of artificial intelligence models has recently grown common; we may use them to write lines of code for us, summarize readings, draft emails, or even illustrate images. But when it comes to important decisions we need to make, such as choosing between job offers or implementing certain economic policies, our level of confidence and trust in AI falls. This raises an intriguing point of exploration which I tackle in this paper - What would need to happen for people to trust artificial intelligence for important decisions? In this paper, I elaborate on how trust in AI for high-stake decisions would be accomplished if the technology was anthropomorphized because its anthropomorphism would overcome psychological barriers that are necessary to overcome for us to trust AI for important decisions. ",
        "title": "Trusting AI in High-stake Decision Making",
        "date": "2023-12-30",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13691",
        "abstract_url": "http://arxiv.org/abs/2401.13691",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Abel C. H."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "NI"
        ],
        "abstract": "  In recent years, the elliptic curve Qu-Vanstone (ECQV) implicit certificate scheme has found application in security credential management systems (SCMS) and secure vehicle-to-everything (V2X) communication to issue pseudonymous certificates. However, the vulnerability of elliptic-curve cryptography (ECC) to polynomial-time attacks posed by quantum computing raises concerns. In order to enhance resistance against quantum computing threats, various post-quantum cryptography methods have been adopted as standard (e.g. Dilithium) or candidate standard methods (e.g. McEliece cryptography), but state of the art has proven to be challenging to implement implicit certificates using lattice-based cryptography methods. Therefore, this study proposes a post-quantum cryptography McEliece-Chen (PQCMC) based on an efficient random invertible matrix generation method to issue pseudonymous certificates with less computation time. The study provides mathematical models to validate the key expansion process for implicit certificates. Furthermore, comprehensive security evaluations and discussions are conducted to demonstrate that distinct implicit certificates can be linked to the same end entity. In experiments, a comparison is conducted between the certificate length and computation time to evaluate the performance of the proposed PQCMC. This study demonstrates the viability of the implicit certificate scheme based on PQC as a means of countering quantum computing threats. ",
        "title": "PQCMC: Post-Quantum Cryptography McEliece-Chen Implicit Certificate  Scheme",
        "date": "2024-01-03",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13692",
        "abstract_url": "http://arxiv.org/abs/2401.13692",
        "authors": [
            {
                "last_name": "Qin",
                "first_name": "Likun"
            },
            {
                "last_name": "Qiu",
                "first_name": "Tianshuo"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The emergence and evolution of Local Differential Privacy (LDP) and its various adaptations play a pivotal role in tackling privacy issues related to the vast amounts of data generated by intelligent devices, which are crucial for data-informed decision-making in the realm of crowdsensing. Utilizing these extensive datasets can provide critical insights but also introduces substantial privacy concerns for the individuals involved. LDP, noted for its decentralized framework, excels in providing strong privacy protection for individual users during the stages of data collection and processing. The core principle of LDP lies in its technique of altering each user's data locally at the client end before it is sent to the server, thus preventing privacy violations at both stages. There are many LDP variances in the privacy research community aimed to improve the utility-privacy tradeoff. On the other hand, one of the major applications of the privacy-preserving mechanisms is machine learning. In this paper, we firstly delves into a comprehensive analysis of LDP and its variances, focusing on their various models, the diverse range of its adaptations, and the underlying structure of privacy mechanisms; then we discuss the state-of-art privacy mechanisms applications in machine learning. ",
        "title": "Local Privacy-preserving Mechanisms and Applications in Machine Learning",
        "date": "2024-01-08",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13693",
        "abstract_url": "http://arxiv.org/abs/2401.13693",
        "authors": [
            {
                "last_name": "Balderas",
                "first_name": "Hugo Jair Escalante"
            },
            {
                "last_name": "Guyon",
                "first_name": "Isabelle"
            },
            {
                "last_name": "Howard",
                "first_name": "Addison"
            },
            {
                "last_name": "Reade",
                "first_name": "Walter"
            },
            {
                "last_name": "Treguer",
                "first_name": "Sebastien"
            }
        ],
        "primary_category": "OH",
        "categories": [
            "OH",
            "HC"
        ],
        "abstract": "  Challenges can be seen as a type of game that motivates participants to solve serious tasks. As a result, competition organizers must develop effective game rules. However, these rules have multiple objectives beyond making the game enjoyable for participants. These objectives may include solving real-world problems, advancing scientific or technical areas, making scientific discoveries, and educating the public. In many ways, creating a challenge is similar to launching a product. It requires the same level of excitement and rigorous testing, and the goal is to attract ''customers'' in the form of participants. The process begins with a solid plan, such as a competition proposal that will eventually be submitted to an international conference and subjected to peer review. Although peer review does not guarantee quality, it does force organizers to consider the impact of their challenge, identify potential oversights, and generally improve its quality. This chapter provides guidelines for creating a strong plan for a challenge. The material draws on the preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and Tailor 3 , as well as the NeurIPS proposal template, which some of the authors contributed to. ",
        "title": "Challenge design roadmap",
        "date": "2024-01-15",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13695",
        "abstract_url": "http://arxiv.org/abs/2401.13695",
        "authors": [
            {
                "last_name": "Choi",
                "first_name": "Yongjin"
            },
            {
                "last_name": "Kumar",
                "first_name": "Krishna"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granular flow by representing the system as a graph and predicts the evolution of the graph at the next time step, given the current state. The differentiable GNS shows optimization capabilities beyond the training data. We demonstrate the effectiveness of our method for inverse estimation across single and multi-parameter optimization problems, including evaluating material properties and boundary conditions for a target runout distance and designing baffle locations to limit a landslide runout. Our proposed differentiable GNS framework offers an orders of magnitude faster solution to these inverse problems than the conventional finite difference approach to gradient-based optimization. ",
        "title": "Inverse analysis of granular flows using differentiable graph neural  network simulator",
        "date": "2024-01-17",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13697",
        "abstract_url": "http://arxiv.org/abs/2401.13697",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Xianbing"
            },
            {
                "last_name": "Poria",
                "first_name": "Soujanya"
            },
            {
                "last_name": "Li",
                "first_name": "Xuejiao"
            },
            {
                "last_name": "Chen",
                "first_name": "Yixin"
            },
            {
                "last_name": "Tang",
                "first_name": "Buzhou"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CL"
        ],
        "abstract": "  Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missing modality inference module to generate virtual modaliites and replace missing modalities. We also design a semantic matching learning module to align semantic spaces generated and missing modalities. Under the prompt of complete modality, our model captures the semantics of missing modalities by leveraging the aligned cross-modal semantic space. Experiments demonstrate the superiority of our approach on three multimodal sentiment analysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD. ",
        "title": "Toward Robust Multimodal Learning using Multimodal Foundational Models",
        "date": "2024-01-19",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13699",
        "abstract_url": "http://arxiv.org/abs/2401.13699",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Jiayuan"
            },
            {
                "last_name": "Shi",
                "first_name": "You"
            },
            {
                "last_name": "Yi",
                "first_name": "Changyan"
            },
            {
                "last_name": "Du",
                "first_name": "Hongyang"
            },
            {
                "last_name": "Kang",
                "first_name": "Jiawen"
            },
            {
                "last_name": "Niyato",
                "first_name": "Dusit"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify valuable while diverse data. This survey particularly focuses on the implementation of GAI-driven HDT in IoT-healthcare. We start by introducing the background of IoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the fundamental techniques and present the overall framework of GAI-driven HDT. After that, we explore the realization of GAI-driven HDT in detail, including GAI-enabled data acquisition, communication, data management, digital modeling, and data analysis. Besides, we discuss typical IoT-healthcare applications that can be revolutionized by GAI-driven HDT, namely personalized health monitoring and diagnosis, personalized prescription, and personalized rehabilitation. Finally, we conclude this survey by highlighting some future research directions. ",
        "title": "Generative AI-Driven Human Digital Twin in IoT-Healthcare: A  Comprehensive Survey",
        "date": "2024-01-21",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13700",
        "abstract_url": "http://arxiv.org/abs/2401.13700",
        "authors": [
            {
                "last_name": "Marinkovi\u0107",
                "first_name": "Vesna"
            },
            {
                "last_name": "\u0160ukilovi\u0107",
                "first_name": "Tijana"
            },
            {
                "last_name": "Mari\u0107",
                "first_name": "Filip"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Although there are several systems that successfully generate construction steps for ruler and compass construction problems, none of them provides readable synthetic correctness proofs for generated constructions. In the present work, we demonstrate how our triangle construction solver ArgoTriCS can cooperate with automated theorem provers for first order logic and coherent logic so that it generates construction correctness proofs, that are both human-readable and formal (can be checked by interactive theorem provers such as Coq or Isabelle/HOL). These proofs currently rely on many high-level lemmas and our goal is to have them all formally shown from the basic axioms of geometry. ",
        "title": "Towards Automated Readable Proofs of Ruler and Compass Constructions",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13702",
        "abstract_url": "http://arxiv.org/abs/2401.13702",
        "authors": [
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Zolt\u00e1n"
            },
            {
                "last_name": "Vujic",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "MS",
            "SC",
            "SE"
        ],
        "abstract": "  The well known JGEX program became open source a few years ago, but seemingly, further development of the program can only be done without the original authors. In our project, we are looking at whether it is possible to continue such a large project as a newcomer without the involvement of the original authors. Is there a way to internationalize, fix bugs, improve the code base, add new features? In other words, to save a relic found in the attic and polish it into a useful everyday tool. ",
        "title": "Open Source Prover in the Attic",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13703",
        "abstract_url": "http://arxiv.org/abs/2401.13703",
        "authors": [
            {
                "last_name": "Hota",
                "first_name": "Amela"
            },
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Zolt\u00e1n"
            },
            {
                "last_name": "Vujic",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "CG",
        "categories": [
            "CG",
            "SC"
        ],
        "abstract": "  In this article, we solve some of the geometry problems of the N\\'aboj 2023 competition with the help of a computer, using examples that the software tool GeoGebra Discovery can calculate. In each case, the calculation requires symbolic computations. We analyze the difficulty of feeding the problem into the machine and set further goals to make the problems of this type of contests even more tractable in the future. ",
        "title": "Solving Some Geometry Problems of the N\\'aboj 2023 Contest with  Automated Deduction in GeoGebra Discovery",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13704",
        "abstract_url": "http://arxiv.org/abs/2401.13704",
        "authors": [
            {
                "last_name": "Ganglmayr",
                "first_name": "Ines"
            },
            {
                "last_name": "Kov\u00e1cs",
                "first_name": "Zolt\u00e1n"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CG",
            "SC"
        ],
        "abstract": "  We give an insight into Java Geometry Expert (JGEX) in use in a school context, focusing on the Austrian school system. JGEX can offer great support in some classroom situations, especially for solving mathematical competition tasks. Also, we discuss some limitations of the program. ",
        "title": "Using Java Geometry Expert as Guide in the Preparations for Math  Contests",
        "date": "2024-01-22",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13708",
        "abstract_url": "http://arxiv.org/abs/2401.13708",
        "authors": [
            {
                "last_name": "Skrodzki",
                "first_name": "Martin"
            },
            {
                "last_name": "van Geffen",
                "first_name": "Hunter"
            },
            {
                "last_name": "Chaves-de-Plaza",
                "first_name": "Nicolas F."
            },
            {
                "last_name": "H\u00f6llt",
                "first_name": "Thomas"
            },
            {
                "last_name": "Eisemann",
                "first_name": "Elmar"
            },
            {
                "last_name": "Hildebrandt",
                "first_name": "Klaus"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We compare our approach with existing methods and demonstrate that it computes embeddings of similar quality in significantly less time. Implementation and scripts for the experiments can be found at https://graphics.tudelft.nl/accelerating-hyperbolic-tsne. ",
        "title": "Accelerating hyperbolic t-SNE",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13712",
        "abstract_url": "http://arxiv.org/abs/2401.13712",
        "authors": [
            {
                "last_name": "Ntetsikas",
                "first_name": "Nikolaos"
            },
            {
                "last_name": "Kyriakoudi",
                "first_name": "Styliana"
            },
            {
                "last_name": "Kirmizis",
                "first_name": "Antonis"
            },
            {
                "last_name": "Unluturk",
                "first_name": "Bige Deniz"
            },
            {
                "last_name": "Pitsillides",
                "first_name": "Andreas"
            },
            {
                "last_name": "Akyildiz",
                "first_name": "Ian F."
            },
            {
                "last_name": "Lestas",
                "first_name": "Marios"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "IT"
        ],
        "abstract": "  Although continuous advances in theoretical modelling of Molecular Communications (MC) are observed, there is still an insuperable gap between theory and experimental testbeds, especially at the microscale. In this paper, the development of the first testbed incorporating engineered yeast cells is reported. Different from the existing literature, eukaryotic yeast cells are considered for both the sender and the receiver, with {\\alpha}-factor molecules facilitating the information transfer. The use of such cells is motivated mainly by the well understood biological mechanism of yeast mating, together with their genetic amenability. In addition, recent advances in yeast biosensing establish yeast as a suitable detector and a neat interface to in-body sensor networks. The system under consideration is presented first, and the mathematical models of the underlying biological processes leading to an end-to-end (E2E) system are given. The experimental setup is then described and used to obtain experimental results which validate the developed mathematical models. Beyond that, the ability of the system to effectively generate output pulses in response to repeated stimuli is demonstrated, reporting one event per two hours. However, fast RNA fluctuations indicate cell responses in less than three minutes, demonstrating the potential for much higher rates in the future. ",
        "title": "Engineering Yeast Cells to Facilitate Information Exchange",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13713",
        "abstract_url": "http://arxiv.org/abs/2401.13713",
        "authors": [
            {
                "last_name": "Segovia-Dominguez",
                "first_name": "Ignacio"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuzhou"
            },
            {
                "last_name": "Akcora",
                "first_name": "Cuneyt G."
            },
            {
                "last_name": "Zhen",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Kantarcioglu",
                "first_name": "Murat"
            },
            {
                "last_name": "Gel",
                "first_name": "Yulia R."
            },
            {
                "last_name": "Coskunuzer",
                "first_name": "Baris"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CG"
        ],
        "abstract": "  Topological data analysis (TDA) is gaining prominence across a wide spectrum of machine learning tasks that spans from manifold learning to graph classification. A pivotal technique within TDA is persistent homology (PH), which furnishes an exclusive topological imprint of data by tracing the evolution of latent structures as a scale parameter changes. Present PH tools are confined to analyzing data through a single filter parameter. However, many scenarios necessitate the consideration of multiple relevant parameters to attain finer insights into the data. We address this issue by introducing the Effective Multidimensional Persistence (EMP) framework. This framework empowers the exploration of data by simultaneously varying multiple scale parameters. The framework integrates descriptor functions into the analysis process, yielding a highly expressive data summary. It seamlessly integrates established single PH summaries into multidimensional counterparts like EMP Landscapes, Silhouettes, Images, and Surfaces. These summaries represent data's multidimensional aspects as matrices and arrays, aligning effectively with diverse ML models. We provide theoretical guarantees and stability proofs for EMP summaries. We demonstrate EMP's utility in graph classification tasks, showing its effectiveness. Results reveal that EMP enhances various single PH descriptors, outperforming cutting-edge methods on multiple benchmark datasets. ",
        "title": "EMP: Effective Multidimensional Persistence for Graph Representation  Learning",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13714",
        "abstract_url": "http://arxiv.org/abs/2401.13714",
        "authors": [
            {
                "last_name": "Tao",
                "first_name": "Wei"
            },
            {
                "last_name": "He",
                "first_name": "Shenglin"
            },
            {
                "last_name": "Lu",
                "first_name": "Kai"
            },
            {
                "last_name": "Qu",
                "first_name": "Xiaoyang"
            },
            {
                "last_name": "Li",
                "first_name": "Guokuan"
            },
            {
                "last_name": "Wan",
                "first_name": "Jiguang"
            },
            {
                "last_name": "Wang",
                "first_name": "Jianzong"
            },
            {
                "last_name": "Xiao",
                "first_name": "Jing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Deploying neural networks on microcontroller units (MCUs) presents substantial challenges due to their constrained computation and memory resources. Previous researches have explored patch-based inference as a strategy to conserve memory without sacrificing model accuracy. However, this technique suffers from severe redundant computation overhead, leading to a substantial increase in execution latency. A feasible solution to address this issue is mixed-precision quantization, but it faces the challenges of accuracy degradation and a time-consuming search time. In this paper, we propose QuantMCU, a novel patch-based inference method that utilizes value-driven mixed-precision quantization to reduce redundant computation. We first utilize value-driven patch classification (VDPC) to maintain the model accuracy. VDPC classifies patches into two classes based on whether they contain outlier values. For patches containing outlier values, we apply 8-bit quantization to the feature maps on the dataflow branches that follow. In addition, for patches without outlier values, we utilize value-driven quantization search (VDQS) on the feature maps of their following dataflow branches to reduce search time. Specifically, VDQS introduces a novel quantization search metric that takes into account both computation and accuracy, and it employs entropy as an accuracy representation to avoid additional training. VDQS also adopts an iterative approach to determine the bitwidth of each feature map to further accelerate the search process. Experimental results on real-world MCU devices show that QuantMCU can reduce computation by 2.2x on average while maintaining comparable model accuracy compared to the state-of-the-art patch-based inference methods. ",
        "title": "Value-Driven Mixed-Precision Quantization for Patch-Based Inference on  Microcontrollers",
        "date": "2024-01-23",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13715",
        "abstract_url": "http://arxiv.org/abs/2401.13715",
        "authors": [
            {
                "last_name": "Shi",
                "first_name": "Ge"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "ET"
        ],
        "abstract": "  In this paper, we investigate the secrecy performance of a single-input single-output visible light communication (VLC) channel in the presence of an eavesdropper. The studied VLC system comprises distributed light-emitting diodes (LEDs) and multiple randomly located users (UEs) within an indoor environment. A sum secrecy rate maximization problem is formulated to enhance confidential transmission by selecting the optimal LED for each UE. To address the non-convex and non-continuous nature of this problem, we propose a tabu search-based algorithm that prevents entrapment in local optima by organizing the trial vectors from previous iterations. Furthermore, we develop three straightforward LED selection strategies that reduce computational complexity by employing fixed criteria to choose one LED for each UE. We also examine the convergence and complexity analysis of the proposed algorithm and strategies. The results demonstrate that the secrecy performance of our proposed algorithm is very close to the global optimal value and surpasses that of the developed strategies. ",
        "title": "A tabu search-based LED selection approach safeguarding visible light  communication systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13716",
        "abstract_url": "http://arxiv.org/abs/2401.13716",
        "authors": [
            {
                "last_name": "Vallevik",
                "first_name": "Vibeke Binz"
            },
            {
                "last_name": "Babic",
                "first_name": "Aleksandar"
            },
            {
                "last_name": "Marshall",
                "first_name": "Serena Elizabeth"
            },
            {
                "last_name": "Elvatun",
                "first_name": "Severin"
            },
            {
                "last_name": "Br\u00f8gger",
                "first_name": "Helga"
            },
            {
                "last_name": "Alagaratnam",
                "first_name": "Sharmini"
            },
            {
                "last_name": "Edwin",
                "first_name": "Bj\u00f8rn"
            },
            {
                "last_name": "Veeraragavan",
                "first_name": "Narasimha Raghavan"
            },
            {
                "last_name": "Befring",
                "first_name": "Anne Kjersti"
            },
            {
                "last_name": "Nyg\u00e5rd",
                "first_name": "Jan Franz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Registry. We present a conceptual framework for quality assurance of SD for AI applications in healthcare that aligns diverging taxonomies, expands on common quality dimensions to include the dimensions of Fairness and Carbon footprint, and proposes stages necessary to support real-life applications. Building trust in synthetic data by increasing transparency and reducing the safety risk will accelerate the development and uptake of trustworthy AI tools for the benefit of patients. Despite the growing emphasis on algorithmic fairness and carbon footprint, these metrics were scarce in the literature review. The overwhelming focus was on statistical similarity using distance metrics while sequential logic detection was scarce. A consensus-backed framework that includes all relevant quality dimensions can provide assurance for safe and responsible real-life applications of SD. ",
        "title": "Can I trust my fake data -- A comprehensive quality assessment framework  for synthetic tabular data in healthcare",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13719",
        "abstract_url": "http://arxiv.org/abs/2401.13719",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Yuanqing"
            },
            {
                "last_name": "Chen",
                "first_name": "Huilong"
            },
            {
                "last_name": "Wang",
                "first_name": "Yinggui"
            },
            {
                "last_name": "Wang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Face recognition (FR) has been applied to nearly every aspect of daily life, but it is always accompanied by the underlying risk of leaking private information. At present, almost all attack models against FR rely heavily on the presence of a classification layer. However, in practice, the FR model can obtain complex features of the input via the model backbone, and then compare it with the target for inference, which does not explicitly involve the outputs of the classification layer adopting logit or other losses. In this work, we advocate a novel inference attack composed of two stages for practical FR models without a classification layer. The first stage is the membership inference attack. Specifically, We analyze the distances between the intermediate features and batch normalization (BN) parameters. The results indicate that this distance is a critical metric for membership inference. We thus design a simple but effective attack model that can determine whether a face image is from the training dataset or not. The second stage is the model inversion attack, where sensitive private data is reconstructed using a pre-trained generative adversarial network (GAN) guided by the attack model in the first stage. To the best of our knowledge, the proposed attack model is the very first in the literature developed for FR models without a classification layer. We illustrate the application of the proposed attack model in the establishment of privacy-preserving FR techniques. ",
        "title": "Inference Attacks Against Face Recognition Model without Classification  Layers",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13721",
        "abstract_url": "http://arxiv.org/abs/2401.13721",
        "authors": [
            {
                "last_name": "Nejjar",
                "first_name": "Ismail"
            },
            {
                "last_name": "Frusque",
                "first_name": "Gaetan"
            },
            {
                "last_name": "Forest",
                "first_name": "Florent"
            },
            {
                "last_name": "Fink",
                "first_name": "Olga"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evidential Learning framework, which outputs both predictions and uncertainties for each input sample. We propose aligning the parameters of higher-order evidential distributions between the source and target domains using traditional alignment methods at the feature or posterior level. Additionally, we propose to augment the feature space representation by mixing source samples with pseudo-labeled target samples based on label similarity. This cross-domain mixing strategy produces more realistic samples than random mixing and introduces higher uncertainty, facilitating further alignment. We demonstrate the effectiveness of our approach on four benchmarks for UDAR, on which we outperform existing methods. ",
        "title": "Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in  Regression",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13722",
        "abstract_url": "http://arxiv.org/abs/2401.13722",
        "authors": [
            {
                "last_name": "Asif",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Mishra",
                "first_name": "Sudhakar"
            },
            {
                "last_name": "Sonker",
                "first_name": "Ankush"
            },
            {
                "last_name": "Gupta",
                "first_name": "Sanidhya"
            },
            {
                "last_name": "Maurya",
                "first_name": "Somesh Kumar"
            },
            {
                "last_name": "Tiwary",
                "first_name": "Uma Shanker"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  This research project aims to tackle the growing mental health challenges in today's digital age. It employs a modified pre-trained BERT model to detect depressive text within social media and users' web browsing data, achieving an impressive 93% test accuracy. Simultaneously, the project aims to incorporate physiological signals from wearable devices, such as smartwatches and EEG sensors, to provide long-term tracking and prognosis of mood disorders and emotional states. This comprehensive approach holds promise for enhancing early detection of depression and advancing overall mental health outcomes. ",
        "title": "Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion  Monitoring",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13726",
        "abstract_url": "http://arxiv.org/abs/2401.13726",
        "authors": [
            {
                "last_name": "Gero",
                "first_name": "Katy Ilonka"
            },
            {
                "last_name": "Swoopes",
                "first_name": "Chelse"
            },
            {
                "last_name": "Gu",
                "first_name": "Ziwei"
            },
            {
                "last_name": "Kummerfeld",
                "first_name": "Jonathan K."
            },
            {
                "last_name": "Glassman",
                "first_name": "Elena L."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "LG"
        ],
        "abstract": "  Large language models (LLMs) are capable of generating multiple responses to a single prompt, yet little effort has been expended to help end-users or system designers make use of this capability. In this paper, we explore how to present many LLM responses at once. We design five features, which include both pre-existing and novel methods for computing similarities and differences across textual documents, as well as how to render their outputs. We report on a controlled user study (n=24) and eight case studies evaluating these features and how they support users in different tasks. We find that the features support a wide variety of sensemaking tasks and even make tasks previously considered to be too difficult by our participants now tractable. Finally, we present design guidelines to inform future explorations of new LLM interfaces. ",
        "title": "Supporting Sensemaking of Large Language Model Outputs at Scale",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13743",
        "abstract_url": "http://arxiv.org/abs/2401.13743",
        "authors": [
            {
                "last_name": "Karacora",
                "first_name": "Yasemin"
            },
            {
                "last_name": "Umra",
                "first_name": "Adam"
            },
            {
                "last_name": "Sezgin",
                "first_name": "Aydin"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The transition to Terahertz (THz) frequencies, providing an ultra-wide bandwidth, is a key driver for future wireless communication networks. However, the specific properties of the THz channel, such as severe path loss and vulnerability to blockage, pose a significant challenge in balancing data rate and reliability. This work considers reconfigurable intelligent surface (RIS)-aided THz communication, where the effective exploitation of a strong, but intermittent line-of-sight (LOS) path versus a reliable, yet weaker RIS-path is studied. We introduce a mixed-criticality superposition coding scheme that addresses this tradeoff from a data significance perspective. The results show that the proposed scheme enables reliable transmission for a portion of high-criticality data without significantly impacting the overall achievable sum rate and queuing delay. Additionally, we gain insights into how the LOS blockage probability and the channel gain of the RIS-link influence the rate performance of our scheme. ",
        "title": "Intermittency versus Path Loss in RIS-aided THz Communication: A Data  Significance Approach",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13744",
        "abstract_url": "http://arxiv.org/abs/2401.13744",
        "authors": [
            {
                "last_name": "Cresswell",
                "first_name": "Jesse C."
            },
            {
                "last_name": "Sui",
                "first_name": "Yi"
            },
            {
                "last_name": "Kumar",
                "first_name": "Bhargava"
            },
            {
                "last_name": "Vouitsis",
                "first_name": "No\u00ebl"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "HC"
        ],
        "abstract": "  In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams. ",
        "title": "Conformal Prediction Sets Improve Human Decision Making",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13747",
        "abstract_url": "http://arxiv.org/abs/2401.13747",
        "authors": [
            {
                "last_name": "Dereniowski",
                "first_name": "Dariusz"
            },
            {
                "last_name": "Wrosz",
                "first_name": "Izajasz"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DM"
        ],
        "abstract": "  We consider the following generalization of binary search in sorted arrays to tree domains. In each step of the search, an algorithm is querying a vertex $q$, and as a reply, it receives an answer, which either states that $q$ is the desired target, or it gives the neighbor of $q$ that is closer to the target than $q$. A further generalization assumes that a vertex-weight function $\\omega$ gives the query costs, i.e., the cost of querying $q$ is $\\omega(q)$. The goal is to find an adaptive search strategy requiring the minimum cost in the worst case. This problem is NP-complete for general weight functions and one of the challenging open questions is whether there exists a polynomial-time constant factor approximation algorithm for an arbitrary tree? In this work, we prove that there exist a constant-factor approximation algorithm for trees with a monotonic cost function, i.e., when the tree has a vertex $v$ such that the weights of the subsequent vertices on the path from $v$ to any leaf give a monotonic (non-increasing or non-decreasing) sequence $S$. This gives a constant factor approximation algorithm for trees with cost functions such that each such sequence $S$ has a fixed number of monotonic segments. Finally, we combine several earlier results to show that the problem is NP-complete when the number of monotonic segments in $S$ is at least $4$. ",
        "title": "Searching in trees with monotonic query times",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13748",
        "abstract_url": "http://arxiv.org/abs/2401.13748",
        "authors": [
            {
                "last_name": "Cil",
                "first_name": "Erdem Eray"
            },
            {
                "last_name": "Schmalen",
                "first_name": "Laurent"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  In this paper, we present a novel log-log domain sum-product algorithm (SPA) for decoding low-density parity-check (LDPC) codes in continuous-variable quantum key distribution (CV-QKD) systems. This algorithm reduces the fractional bit width of decoder messages, leading to a smaller memory footprint and a lower resource consumption in hardware implementation. We also provide practical insights for fixed-point arithmetic and compare our algorithm with the conventional SPA in terms of performance and complexity. Our results show that our algorithm achieves comparable or better decoding accuracy than the conventional SPA while saving at least $25\\%$ of the fractional bit width. ",
        "title": "Log-Log Domain Sum-Product Algorithm for Information Reconciliation in  Continuous-Variable Quantum Key Distribution",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13751",
        "abstract_url": "http://arxiv.org/abs/2401.13751",
        "authors": [
            {
                "last_name": "Meyers",
                "first_name": "Charles"
            },
            {
                "last_name": "Sedghpour",
                "first_name": "Mohammad Reza Saleh"
            },
            {
                "last_name": "L\u00f6fstedt",
                "first_name": "Tommy"
            },
            {
                "last_name": "Elmroth",
                "first_name": "Erik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CV"
        ],
        "abstract": "  Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a function of time and relate that to a novel metric that allows us to quickly determine whether or not the cost of training a model outweighs the cost of attacking it. Using this approach, we are able to approximate the expected failure rate using a small number of specially crafted samples rather than increasingly larger benchmark datasets. We demonstrate the efficacy of this technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit floating-point numbers, various data pre-processing techniques, and several attacks on five configurations of the ResNet model. Then, using empirical measurements, we examine the various trade-offs between cost, robustness, latency, and reliability to find that larger models do not significantly aid in adversarial robustness despite costing significantly more to train. ",
        "title": "A Systematic Approach to Robustness Modelling for Deep Convolutional  Neural Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13752",
        "abstract_url": "http://arxiv.org/abs/2401.13752",
        "authors": [
            {
                "last_name": "Chockler",
                "first_name": "Hana"
            },
            {
                "last_name": "Halpern",
                "first_name": "Joseph Y."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We focus on explaining image classifiers, taking the work of Mothilal et al. [2021] (MMTS) as our point of departure. We observe that, although MMTS claim to be using the definition of explanation proposed by Halpern [2016], they do not quite do so. Roughly speaking, Halpern's definition has a necessity clause and a sufficiency clause. MMTS replace the necessity clause by a requirement that, as we show, implies it. Halpern's definition also allows agents to restrict the set of options considered. While these difference may seem minor, as we show, they can have a nontrivial impact on explanations. We also show that, essentially without change, Halpern's definition can handle two issues that have proved difficult for other approaches: explanations of absence (when, for example, an image classifier for tumors outputs \"no tumor\") and explanations of rare events (such as tumors). ",
        "title": "Explaining Image Classifiers",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13754",
        "abstract_url": "http://arxiv.org/abs/2401.13754",
        "authors": [
            {
                "last_name": "Kalantzis",
                "first_name": "Vassilis"
            },
            {
                "last_name": "Squillante",
                "first_name": "Mark S."
            },
            {
                "last_name": "Ubaru",
                "first_name": "Shashanka"
            },
            {
                "last_name": "Gokmen",
                "first_name": "Tayfun"
            },
            {
                "last_name": "Wu",
                "first_name": "Chai Wah"
            },
            {
                "last_name": "Gupta",
                "first_name": "Anshul"
            },
            {
                "last_name": "Avron",
                "first_name": "Haim"
            },
            {
                "last_name": "Nowicki",
                "first_name": "Tomasz"
            },
            {
                "last_name": "Rasch",
                "first_name": "Malte"
            },
            {
                "last_name": "Onen",
                "first_name": "Murat"
            },
            {
                "last_name": "Marrero",
                "first_name": "Vanessa Lopez"
            },
            {
                "last_name": "Leobandung",
                "first_name": "Effendi"
            },
            {
                "last_name": "Kohda",
                "first_name": "Yasuteru"
            },
            {
                "last_name": "Haensch",
                "first_name": "Wilfried"
            },
            {
                "last_name": "Horesh",
                "first_name": "Lior"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Numerical computation is essential to many areas of artificial intelligence (AI), whose computing demands continue to grow dramatically, yet their continued scaling is jeopardized by the slowdown in Moore's law. Multi-function multi-way analog (MFMWA) technology, a computing architecture comprising arrays of memristors supporting in-memory computation of matrix operations, can offer tremendous improvements in computation and energy, but at the expense of inherent unpredictability and noise. We devise novel randomized algorithms tailored to MFMWA architectures that mitigate the detrimental impact of imperfect analog computations while realizing their potential benefits across various areas of AI, such as applications in computer vision. Through analysis, measurements from analog devices, and simulations of larger systems, we demonstrate orders of magnitude reduction in both computation and energy with accuracy similar to digital computers. ",
        "title": "Multi-Function Multi-Way Analog Technology for Sustainable Machine  Intelligence Computation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13756",
        "abstract_url": "http://arxiv.org/abs/2401.13756",
        "authors": [
            {
                "last_name": "Al-Ars",
                "first_name": "Zaid"
            },
            {
                "last_name": "Agba",
                "first_name": "Obinna"
            },
            {
                "last_name": "Guo",
                "first_name": "Zhuoran"
            },
            {
                "last_name": "Boerkamp",
                "first_name": "Christiaan"
            },
            {
                "last_name": "Jaber",
                "first_name": "Ziyaad"
            },
            {
                "last_name": "Jaber",
                "first_name": "Tareq"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper offers a systematic method for creating medical knowledge-grounded patient records for use in activities involving differential diagnosis. Additionally, an assessment of machine learning models that can differentiate between various conditions based on given symptoms is also provided. We use a public disease-symptom data source called SymCat in combination with Synthea to construct the patients records. In order to increase the expressive nature of the synthetic data, we use a medically-standardized symptom modeling method called NLICE to augment the synthetic data with additional contextual information for each condition. In addition, Naive Bayes and Random Forest models are evaluated and compared on the synthetic data. The paper shows how to successfully construct SymCat-based and NLICE-based datasets. We also show results for the effectiveness of using the datasets to train predictive disease models. The SymCat-based dataset is able to train a Naive Bayes and Random Forest model yielding a 58.8% and 57.1% Top-1 accuracy score, respectively. In contrast, the NLICE-based dataset improves the results, with a Top-1 accuracy of 82.0% and Top-5 accuracy values of more than 90% for both models. Our proposed data generation approach solves a major barrier to the application of artificial intelligence methods in the healthcare domain. Our novel NLICE symptom modeling approach addresses the incomplete and insufficient information problem in the current binary symptom representation approach. The NLICE code is open sourced at https://github.com/guozhuoran918/NLICE. ",
        "title": "NLICE: Synthetic Medical Record Generation for Effective Primary  Healthcare Differential Diagnosis",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13758",
        "abstract_url": "http://arxiv.org/abs/2401.13758",
        "authors": [
            {
                "last_name": "Richardson",
                "first_name": "Thomas S."
            },
            {
                "last_name": "Robins",
                "first_name": "James M."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this note we give proofs for results relating to the Instrumental Variable (IV) model with binary response $Y$ and binary treatment $X$, but with an instrument $Z$ that takes $K$ states that were originally stated in Richardson & Robins (2014), \"ACE Bounds; SEMS with Equilibrium Conditions,\" arXiv:1410.0470. ",
        "title": "Assumptions and Bounds in the Instrumental Variable Model",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13761",
        "abstract_url": "http://arxiv.org/abs/2401.13761",
        "authors": [
            {
                "last_name": "del-Pino-L\u00f3pez",
                "first_name": "Juan Carlos"
            },
            {
                "last_name": "Cruz-Romero",
                "first_name": "Pedro"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Due to recent advances, the numerical analysis of submarine three-core armored cables can nowadays be developed through the finite element method (FEM) in a small slice of the cable. This strongly reduces the computational burden and simulation time. However, the performance of this ultra-shortened 3D-FEM model is still to be fully assessed with experimental measurements. This paper focuses on this validation for an extensive variety of situations through the experimental measurements available in the specialized literature for up to 10 actual cables. In particular, it deals not only with relevant calculations at power frequency, like the series resistance and inductive reactance or the induced sheath current, but also with other aspects never analyzed before through 3D-FEM simulations, such as the zero sequence impedance, the magnetic field distribution around the power cable, as well as side effects due to the nonlinear properties of the armor wires. All this considering different armoring and sheath bonding configurations. Results show a very good agreement between measured and computed values, presenting the ultra-shortened 3D-FEM model as a suitable tool for the analysis and design of three-core armored cables, and opening the possibility to reduce the need of extensive experimental tests in the design stage of new cables. ",
        "title": "Experimental validation of ultra-shortened 3D finite element  electromagnetic modeling of three-core armored cables at power frequency",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13762",
        "abstract_url": "http://arxiv.org/abs/2401.13762",
        "authors": [
            {
                "last_name": "Leeman",
                "first_name": "Antoine P."
            },
            {
                "last_name": "K\u00f6hler",
                "first_name": "Johannes"
            },
            {
                "last_name": "Messerer",
                "first_name": "Florian"
            },
            {
                "last_name": "Lahr",
                "first_name": "Amon"
            },
            {
                "last_name": "Diehl",
                "first_name": "Moritz"
            },
            {
                "last_name": "Zeilinger",
                "first_name": "Melanie N."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  System Level Synthesis (SLS) enables improved robust MPC formulations by allowing for joint optimization of the nominal trajectory and controller. This paper introduces a tailored algorithm for solving the corresponding disturbance feedback optimization problem. The proposed algorithm builds on a recently proposed joint optimization scheme and iterates between optimizing the controller and the nominal trajectory while converging q-linearly to an optimal solution. We show that the controller optimization can be solved through Riccati recursions leading to a horizon-length, state, and input scalability of $\\mathcal{O}(N^2 ( n_x^3 + n_u ^3 ) )$ for each iterate. On a numerical example, the proposed algorithm exhibits computational speedups of order $10$ to $10^3$ compared to general-purpose commercial solvers. ",
        "title": "Fast System Level Synthesis: Robust Model Predictive Control using  Riccati Recursions",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13766",
        "abstract_url": "http://arxiv.org/abs/2401.13766",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Hu"
            },
            {
                "last_name": "Siniscalchi",
                "first_name": "Sabato Marco"
            },
            {
                "last_name": "Lee",
                "first_name": "Chin-Hui"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  In this work, we aim to establish a Bayesian adaptive learning framework by focusing on estimating latent variables in deep neural network (DNN) models. Latent variables indeed encode both transferable distributional information and structural relationships. Thus the distributions of the source latent variables (prior) can be combined with the knowledge learned from the target data (likelihood) to yield the distributions of the target latent variables (posterior) with the goal of addressing acoustic mismatches between training and testing conditions. The prior knowledge transfer is accomplished through Variational Bayes (VB). In addition, we also investigate Maximum a Posteriori (MAP) based Bayesian adaptation. Experimental results on device adaptation in acoustic scene classification show that our proposed approaches can obtain good improvements on target devices, and consistently outperforms other cut-edging algorithms. ",
        "title": "Bayesian adaptive learning to latent variables via Variational Bayes and  Maximum a Posteriori",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13769",
        "abstract_url": "http://arxiv.org/abs/2401.13769",
        "authors": [
            {
                "last_name": "Karaaslanli",
                "first_name": "Abdullah"
            },
            {
                "last_name": "Aviyente",
                "first_name": "Selin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph topology inference, i.e., learning graphs from a given set of nodal observations, is a significant task in many application domains. Existing approaches are mostly limited to learning a single graph assuming that the observed data is homogeneous. This is problematic because many modern datasets are heterogeneous or mixed and involve multiple related graphs, i.e., multiview graphs. Recent work proposing to learn multiview graphs ensures the similarity of learned view graphs through pairwise regularization, where each pair of views is encouraged to have similar structures. However, this approach cannot infer the shared structure across views. In this work, we propose an alternative method based on consensus regularization, where views are ensured to be similar through a learned consensus graph representing the common structure of the views. In particular, we propose an optimization problem, where graph data is assumed to be smooth over the multiview graph and the topology of the individual views and that of the consensus graph are learned, simultaneously. Our optimization problem is designed to be general in the sense that different regularization functions can be used depending on what the shared structure across views is. Moreover, we propose two regularization functions that extend fused and group graphical lasso to consensus based regularization. Proposed multiview graph learning is evaluated on simulated data and shown to have better performance than existing methods. It is also employed to infer the functional brain connectivity networks of multiple subjects from their electroencephalogram (EEG) recordings. The proposed method reveals the structure shared by subjects as well as the characteristics unique to each subject. ",
        "title": "Multiview Graph Learning with Consensus Graph",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13770",
        "abstract_url": "http://arxiv.org/abs/2401.13770",
        "authors": [
            {
                "last_name": "Jha",
                "first_name": "Piyush"
            },
            {
                "last_name": "Li",
                "first_name": "Zhengyu"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhengyang"
            },
            {
                "last_name": "Bright",
                "first_name": "Curtis"
            },
            {
                "last_name": "Ganesh",
                "first_name": "Vijay"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS) based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving challenging combinatorial problems. Despite the tremendous success of CnC solvers in solving a variety of hard combinatorial problems, the lookahead cubing techniques at the heart of CnC have not evolved much for many years. Part of the reason is the sheer difficulty of coming up with new cubing techniques that are both low-cost and effective in partitioning input formulas into sub-formulas, such that the overall runtime is minimized.   Lookahead cubing techniques used by current state-of-the-art CnC solvers, such as March, keep their cubing costs low by constraining the search for the optimal splitting variables. By contrast, our key innovation is a deductively-driven MCTS-based lookahead cubing technique, that performs a deeper heuristic search to find effective cubes, while keeping the cubing cost low. We perform an extensive comparison of AlphaMapleSAT against the March CnC solver on challenging combinatorial problems such as the minimum Kochen-Specker and Ramsey problems. We also perform ablation studies to verify the efficacy of the MCTS heuristic search for the cubing problem. Results show up to 2.3x speedup in parallel (and up to 27x in sequential) elapsed real time. ",
        "title": "AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard  Combinatorial Problems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13773",
        "abstract_url": "http://arxiv.org/abs/2401.13773",
        "authors": [
            {
                "last_name": "Prasad",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Vitercik",
                "first_name": "Ellen"
            },
            {
                "last_name": "Balcan",
                "first_name": "Maria-Florina"
            },
            {
                "last_name": "Sandholm",
                "first_name": "Tuomas"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM",
            "DS"
        ],
        "abstract": "  Sequence-independent lifting is a procedure for strengthening valid inequalities of an integer program. We generalize the sequence-independent lifting method of Gu, Nemhauser, and Savelsbergh (GNS lifting) for cover inequalities and correct an error in their proposed generalization. We obtain a new sequence-independent lifting technique -- piecewise-constant (PC) lifting -- with a number of interesting properties. We derive a broad set of sufficient conditions under which PC lifting is facet defining. To our knowledge, this is the first characterization of facet-defining sequence-independent liftings that are efficiently computable from the underlying cover. Finally, we demonstrate via experiments that PC lifting can be a useful alternative to GNS lifting. We test our new lifting techniques atop a number of novel cover cut generation routines, which prove to be effective in experiments with CPLEX. ",
        "title": "New Sequence-Independent Lifting Techniques for Cutting Planes and When  They Induce Facets",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13775",
        "abstract_url": "http://arxiv.org/abs/2401.13775",
        "authors": [
            {
                "last_name": "Warnick",
                "first_name": "Karl F."
            },
            {
                "last_name": "Broyde",
                "first_name": "Frederic"
            },
            {
                "last_name": "Jelinek",
                "first_name": "Lukas"
            },
            {
                "last_name": "Capek",
                "first_name": "Miloslav"
            },
            {
                "last_name": "Clavelier",
                "first_name": "Evelyne"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  It is well known that reciprocal antenna systems have a symmetric impedance matrix. What is less well understood is how the system reciprocity manifests in the bidirectionally transferred powers with a beamformed system such as a massive multiple input multiple output (MIMO) array. To answer this question, we connect four disparate ideas, Lorentz reciprocity, the Friis transmission formula, noise-based active antenna parameters, and the active antenna available power. This results in an unnamed power gain that is connected with available gain and transducer gain but is unmentioned in the theory of two-port amplifiers. This quantity is symmetric under link direction reversal in the near field, as well as the far field, and generalizes the Friis transmission formula to beamformed multiport antenna systems in an arbitrary reciprocal propagation environment. ",
        "title": "The Friis Transmission Formula, Active Antenna Available Power,  Reciprocity in Multiantenna Systems, and the Unnamed Power Gain",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13779",
        "abstract_url": "http://arxiv.org/abs/2401.13779",
        "authors": [
            {
                "last_name": "Herrera",
                "first_name": "Daniel P\u00e9rez"
            },
            {
                "last_name": "Chen",
                "first_name": "Zheng"
            },
            {
                "last_name": "Larsson",
                "first_name": "Erik G."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "DC",
            "LG"
        ],
        "abstract": "  Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely adopted algorithm for decentralized training of machine learning models across networked agents. A crucial part of D-SGD is the consensus-based model averaging, which heavily relies on information exchange and fusion among the nodes. Specifically, for consensus averaging over wireless networks, communication coordination is necessary to determine when and how a node can access the channel and transmit (or receive) information to (or from) its neighbors. In this work, we propose $\\texttt{BASS}$, a broadcast-based subgraph sampling method designed to accelerate the convergence of D-SGD while considering the actual communication cost per iteration. $\\texttt{BASS}$ creates a set of mixing matrix candidates that represent sparser subgraphs of the base topology. In each consensus iteration, one mixing matrix is sampled, leading to a specific scheduling decision that activates multiple collision-free subsets of nodes. The sampling occurs in a probabilistic manner, and the elements of the mixing matrices, along with their sampling probabilities, are jointly optimized. Simulation results demonstrate that $\\texttt{BASS}$ enables faster convergence with fewer transmission slots compared to existing link-based scheduling methods. In conclusion, the inherent broadcasting nature of wireless channels offers intrinsic advantages in accelerating the convergence of decentralized optimization and learning. ",
        "title": "Faster Convergence with Less Communication: Broadcast-Based Subgraph  Sampling for Decentralized Learning over Wireless Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13782",
        "abstract_url": "http://arxiv.org/abs/2401.13782",
        "authors": [
            {
                "last_name": "Weissburg",
                "first_name": "Iain Xie"
            },
            {
                "last_name": "Arora",
                "first_name": "Mehir"
            },
            {
                "last_name": "Pan",
                "first_name": "Liangming"
            },
            {
                "last_name": "Wang",
                "first_name": "William Yang"
            }
        ],
        "primary_category": "DL",
        "categories": [
            "DL",
            "CL",
            "CV",
            "LG",
            "SI"
        ],
        "abstract": "  As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital academic landscape. ",
        "title": "Tweets to Citations: Unveiling the Impact of Social Media Influencers on  AI Research Visibility",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13784",
        "abstract_url": "http://arxiv.org/abs/2401.13784",
        "authors": [
            {
                "last_name": "Narayanan",
                "first_name": "Sriram"
            },
            {
                "last_name": "Mohamed",
                "first_name": "Mohamed Naveed Gul"
            },
            {
                "last_name": "Nayak",
                "first_name": "Indranil"
            },
            {
                "last_name": "Chakravorty",
                "first_name": "Suman"
            },
            {
                "last_name": "Kumar",
                "first_name": "Mrinal"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper discusses the predictive capability of Dynamic Mode Decomposition (DMD) in the context of orbital mechanics. The focus is specifically on the Hankel variant of DMD which uses a stacked set of time-delayed observations for system identification and subsequent prediction. A theory on the minimum number of time delays required for accurate reconstruction of periodic trajectories of nonlinear systems is presented and corroborated using experimental analysis. In addition, the window size for training and prediction regions, respectively, is presented. The need for a meticulous approach while using DMD is emphasized by drawing comparisons between its performance on two candidate satellites, the ISS and MOLNIYA-3-50. ",
        "title": "On the Predictive Capability of Dynamic Mode Decomposition for Nonlinear  Periodic Systems with Focus on Orbital Mechanics",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13785",
        "abstract_url": "http://arxiv.org/abs/2401.13785",
        "authors": [
            {
                "last_name": "Silva",
                "first_name": "Sathira"
            },
            {
                "last_name": "Wannigama",
                "first_name": "Savindu Bhashitha"
            },
            {
                "last_name": "Ragel",
                "first_name": "Roshan"
            },
            {
                "last_name": "Jayatilaka",
                "first_name": "Gihan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Holistic understanding and reasoning in 3D scenes play a vital role in the success of autonomous driving systems. The evolution of 3D semantic occupancy prediction as a pretraining task for autonomous driving and robotic downstream tasks captures finer 3D details compared to methods like 3D detection. Existing approaches predominantly focus on spatial cues, often overlooking temporal cues. Query-based methods tend to converge on computationally intensive Voxel representation for encoding 3D scene information. This study introduces S2TPVFormer, an extension of TPVFormer, utilizing a spatiotemporal transformer architecture for coherent 3D semantic occupancy prediction. Emphasizing the importance of spatiotemporal cues in 3D scene perception, particularly in 3D semantic occupancy prediction, our work explores the less-explored realm of temporal cues. Leveraging Tri-Perspective View (TPV) representation, our spatiotemporal encoder generates temporally rich embeddings, improving prediction coherence while maintaining computational efficiency. To achieve this, we propose a novel Temporal Cross-View Hybrid Attention (TCVHA) mechanism, facilitating effective spatiotemporal information exchange across TPV views. Experimental evaluations on the nuScenes dataset demonstrate a substantial 3.1% improvement in mean Intersection over Union (mIoU) for 3D Semantic Occupancy compared to TPVFormer, confirming the effectiveness of the proposed S2TPVFormer in enhancing 3D scene perception. ",
        "title": "S2TPVFormer: Spatio-Temporal Tri-Perspective View for temporally  coherent 3D Semantic Occupancy Prediction",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13786",
        "abstract_url": "http://arxiv.org/abs/2401.13786",
        "authors": [
            {
                "last_name": "Lichy",
                "first_name": "Daniel"
            },
            {
                "last_name": "Su",
                "first_name": "Hang"
            },
            {
                "last_name": "Badki",
                "first_name": "Abhishek"
            },
            {
                "last_name": "Kautz",
                "first_name": "Jan"
            },
            {
                "last_name": "Gallo",
                "first_name": "Orazio"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Wide field-of-view (FoV) cameras efficiently capture large portions of the scene, which makes them attractive in multiple domains, such as automotive and robotics. For such applications, estimating depth from multiple images is a critical task, and therefore, a large amount of ground truth (GT) data is available. Unfortunately, most of the GT data is for pinhole cameras, making it impossible to properly train depth estimation models for large-FoV cameras. We propose the first method to train a stereo depth estimation model on the widely available pinhole data, and to generalize it to data captured with larger FoVs. Our intuition is simple: We warp the training data to a canonical, large-FoV representation and augment it to allow a single network to reason about diverse types of distortions that otherwise would prevent generalization. We show strong generalization ability of our approach on both indoor and outdoor datasets, which was not possible with previous methods. ",
        "title": "FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset  Generalization",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13789",
        "abstract_url": "http://arxiv.org/abs/2401.13789",
        "authors": [
            {
                "last_name": "Stricker",
                "first_name": "Armand"
            },
            {
                "last_name": "Paroubek",
                "first_name": "Patrick"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In current text-based task-oriented dialogue (TOD) systems, user emotion detection (ED) is often overlooked or is typically treated as a separate and independent task, requiring additional training. In contrast, our work demonstrates that seamlessly unifying ED and TOD modeling brings about mutual benefits, and is therefore an alternative to be considered. Our method consists in augmenting SimpleToD, an end-to-end TOD system, by extending belief state tracking to include ED, relying on a single language model. We evaluate our approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ annotated with emotions. Our results reveal a general increase in performance for ED and task results. Our findings also indicate that user emotions provide useful contextual conditioning for system responses, and can be leveraged to further refine responses in terms of empathy. ",
        "title": "A Unified Approach to Emotion Detection and Task-Oriented Dialogue  Modeling",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13790",
        "abstract_url": "http://arxiv.org/abs/2401.13790",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Lie-Liang"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The principle of orthogonal time-frequency-space (OTFS) signaling is firstly analyzed, followed by explaining that OTFS embeds another signaling scheme referred to as orthogonal short-time Fourier (OSTF). Then, the relationship among OTFS, OSTF, orthogonal frequency-division multiplexing (OFDM) and single-carrier frequency-division multiple-access (SC-FDMA) is explored, demonstrating that OSTF/OTFS are fundamentally the extensions of OFDM/SC-FDMA from one-dimensional (1D) signaling to two-dimensional (2D) signaling. Hence, the characteristics and performance of OSTF/OTFS schemes can be perceived from the well-understood OFDM/SC-FDMA schemes. Accordingly, the advantages and disadvantages of OSTF/OTFS are discussed. Furthermore, from the principles of OFDM/SC-FDMA, the multiuser multiplexing in OSTF/OTFS systems is analyzed with respect to uplink and downlink, respectively. Added on this, a range of generalized multiplexing schemes are presented, whose characteristics are briefly analyzed. ",
        "title": "Orthogonal Time-Frequency-Space (OTFS) and Related Signaling",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13792",
        "abstract_url": "http://arxiv.org/abs/2401.13792",
        "authors": [
            {
                "last_name": "Lahham",
                "first_name": "Saria Al"
            },
            {
                "last_name": "Wu",
                "first_name": "Di"
            },
            {
                "last_name": "Hossain",
                "first_name": "Ekram"
            },
            {
                "last_name": "Liu",
                "first_name": "Xue"
            },
            {
                "last_name": "Dudek",
                "first_name": "Gregory"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The ever-increasing demand for data services and the proliferation of user equipment (UE) have resulted in a significant rise in the volume of mobile traffic. Moreover, in multi-band networks, non-uniform traffic distribution among different operational bands can lead to congestion, which can adversely impact the user's quality of experience. Load balancing is a critical aspect of network optimization, where it ensures that the traffic is evenly distributed among different bands, avoiding congestion and ensuring better user experience. Traditional load balancing approaches rely only on the band channel quality as a load indicator and to move UEs between bands, which disregards the UE's demands and the band resource, and hence, leading to a suboptimal balancing and utilization of resources. To address this challenge, we propose an event-based algorithm, in which we model the load balancing problem as a multi-objective stochastic optimization, and assign UEs to bands in a probabilistic manner. The goal is to evenly distribute traffic across available bands according to their resources, while maintaining minimal number of inter-frequency handovers to avoid the signaling overhead and the interruption time. Simulation results show that the proposed algorithm enhances the network's performance and outperforms traditional load balancing approaches in terms of throughput and interruption time. ",
        "title": "Probabilistic Mobility Load Balancing for Multi-band 5G and Beyond  Networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13794",
        "abstract_url": "http://arxiv.org/abs/2401.13794",
        "authors": [
            {
                "last_name": "Ismaeel",
                "first_name": "Ayad Ghany"
            },
            {
                "last_name": "Janardhanan",
                "first_name": "Krishnadas"
            },
            {
                "last_name": "Sankar",
                "first_name": "Manishankar"
            },
            {
                "last_name": "Natarajan",
                "first_name": "Yuvaraj"
            },
            {
                "last_name": "Mahmood",
                "first_name": "Sarmad Nozad"
            },
            {
                "last_name": "Alani",
                "first_name": "Sameer"
            },
            {
                "last_name": "Shather",
                "first_name": "Akram H."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper examines the use of deep recurrent neural networks to classify traffic patterns in smart cities. We propose a novel approach to traffic pattern classification based on deep recurrent neural networks, which can effectively capture traffic patterns' dynamic and sequential features. The proposed model combines convolutional and recurrent layers to extract features from traffic pattern data and a SoftMax layer to classify traffic patterns. Experimental results show that the proposed model outperforms existing methods regarding accuracy, precision, recall, and F1 score. Furthermore, we provide an in depth analysis of the results and discuss the implications of the proposed model for smart cities. The results show that the proposed model can accurately classify traffic patterns in smart cities with a precision of as high as 95%. The proposed model is evaluated on a real world traffic pattern dataset and compared with existing classification methods. ",
        "title": "Traffic Pattern Classification in Smart Cities Using Deep Recurrent  Neural Network",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13795",
        "abstract_url": "http://arxiv.org/abs/2401.13795",
        "authors": [
            {
                "last_name": "Seyfioglu",
                "first_name": "Mehmet Saygin"
            },
            {
                "last_name": "Bouyarmane",
                "first_name": "Karim"
            },
            {
                "last_name": "Kumar",
                "first_name": "Suren"
            },
            {
                "last_name": "Tavanaei",
                "first_name": "Amir"
            },
            {
                "last_name": "Tutar",
                "first_name": "Ismail B."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  As online shopping is growing, the ability for buyers to virtually visualize products in their settings-a phenomenon we define as \"Virtual Try-All\"-has become crucial. Recent diffusion models inherently contain a world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We present \"Diffuse to Choose,\" a novel diffusion-based image-conditioned inpainting model that efficiently balances fast inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as few-shot diffusion personalization algorithms like DreamPaint. ",
        "title": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent  Diffusion Models for Virtual Try-All",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13796",
        "abstract_url": "http://arxiv.org/abs/2401.13796",
        "authors": [
            {
                "last_name": "Apicella",
                "first_name": "Andrea"
            },
            {
                "last_name": "Isgr\u00f2",
                "first_name": "Francesco"
            },
            {
                "last_name": "Prevete",
                "first_name": "Roberto"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a \"push the button\" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning, and compares standard inductive ML with transductive ML frameworks. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications. ",
        "title": "Don't Push the Button! Exploring Data Leakage Risks in Machine Learning  and Transfer Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13799",
        "abstract_url": "http://arxiv.org/abs/2401.13799",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yuling"
            },
            {
                "last_name": "Zhu",
                "first_name": "Xiuqi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xiaomu"
            },
            {
                "last_name": "Yao",
                "first_name": "Bingsheng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Kai"
            },
            {
                "last_name": "Wang",
                "first_name": "Dakuo"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiaju"
            },
            {
                "last_name": "He",
                "first_name": "Liang"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "HC"
        ],
        "abstract": "  The proliferation of Information and Communication Technologies (ICTs) has shown great promise in addressing educational challenges facing rural areas. However, the complex rural context poses significant challenges to the effective utilization of these technologies. This paper examines the empirical integration of live-streaming-based remote classrooms (LSRC) through a qualitative study in rural China. Our findings suggest that while LSRC enables rural students equal access to high-quality educational resources, its practical integration faces numerous challenges. In particular, we emphasize the crucial role of local teachers in addressing these challenges, ultimately achieving the desired improvement of students' learning outcomes. We also examine the impact of LSRC on the original rural education ecosystem. Building upon our findings, we call for a reconsideration of interaction paradigms and evaluation systems of ICT-mediated rural education, emphasizing the significance of rural teachers. We conclude by discussing the implications for future ICT-mediated technology interventions in rural settings. ",
        "title": "Who Changed the Destiny of Rural Students, and How?: Unpacking  ICT-Mediated Remote Education in Rural China",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13800",
        "abstract_url": "http://arxiv.org/abs/2401.13800",
        "authors": [
            {
                "last_name": "Sadek",
                "first_name": "Assem"
            },
            {
                "last_name": "Bono",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Chidlovskii",
                "first_name": "Boris"
            },
            {
                "last_name": "Baskurt",
                "first_name": "Atilla"
            },
            {
                "last_name": "Wolf",
                "first_name": "Christian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.   In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task. ",
        "title": "Multi-Object Navigation in real environments using hybrid policies",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13801",
        "abstract_url": "http://arxiv.org/abs/2401.13801",
        "authors": [
            {
                "last_name": "Padisala",
                "first_name": "Shanthan Kumar"
            },
            {
                "last_name": "Vyas",
                "first_name": "Shashank Dhananjay"
            },
            {
                "last_name": "Dey",
                "first_name": "Satadru"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Technological advancements like the Internet of Things (IoT) have facilitated data exchange across various platforms. This data exchange across various platforms has transformed the traditional battery system into a cyber physical system. Such connectivity makes modern cyber physical battery systems vulnerable to cyber threats where a cyber attacker can manipulate sensing and actuation signals to bring the battery system into an unsafe operating condition. Hence, it is essential to build resilience in modern cyber physical battery systems (CPBS) under cyber attacks. The first step of building such resilience is to analyze potential adversarial behavior, that is, how the adversaries can inject attacks into the battery systems. However, it has been found that in this under-explored area of battery cyber physical security, such an adversarial threat model has not been studied in a systematic manner. In this study, we address this gap and explore adversarial attack generation policies based on optimal control framework. The framework is developed by performing theoretical analysis, which is subsequently supported by evaluation with experimental data generated from a commercial battery cell. ",
        "title": "Exploring Adversarial Threat Models in Cyber Physical Battery Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13802",
        "abstract_url": "http://arxiv.org/abs/2401.13802",
        "authors": [
            {
                "last_name": "Khajezade",
                "first_name": "Mohamad"
            },
            {
                "last_name": "Wu",
                "first_name": "Jie"
            },
            {
                "last_name": "Fard",
                "first_name": "Fatemeh Hendijani"
            },
            {
                "last_name": "Rodr\u00edguez-P\u00e9rez",
                "first_name": "Gema"
            },
            {
                "last_name": "Shehata",
                "first_name": "Mohamed Sami"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "LG"
        ],
        "abstract": "  Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \\textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \\textcolor{black}{then} conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. %\\textbf{Results:} ChatGPT surpasses the baselines in cross-language CCD \\textcolor{black}{attaining an F1-score of 0.877 } and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, \\textcolor{black}{with an F1-score of 0.878}. Also, the \\textcolor{black}{prompt and the} difficulty level of the problems has an impact on the performance of ChatGPT. \\textcolor{black}{Finally,} we provide insights and future directions based on our initial analysis ",
        "title": "Investigating the Efficacy of Large Language Models for Code Clone  Detection",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13803",
        "abstract_url": "http://arxiv.org/abs/2401.13803",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yongtao"
            },
            {
                "last_name": "Checa",
                "first_name": "Marti"
            },
            {
                "last_name": "Vasudevan",
                "first_name": "Rama K."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  With the advent of large language models (LLMs), in both the open source and proprietary domains, attention is turning to how to exploit such artificial intelligence (AI) systems in assisting complex scientific tasks, such as material synthesis, characterization, analysis and discovery. Here, we explore the utility of LLM, particularly ChatGPT4, in combination with application program interfaces (APIs) in tasks of experimental design, programming workflows, and data analysis in scanning probe microscopy, using both in-house developed API and API given by a commercial vendor for instrument control. We find that the LLM can be especially useful in converting ideations of experimental workflows to executable code on microscope APIs. Beyond code generation, we find that the GPT4 is capable of analyzing microscopy images in a generic sense. At the same time, we find that GPT4 suffers from inability to extend beyond basic analyses or more in-depth technical experimental design. We argue that a LLM specifically fine-tuned for individual scientific domains can potentially be a better language interface for converting scientific ideations from human experts to executable workflows, such a synergy between human expertise and LLM efficiency in experimentation can open new door for accelerating scientific research, enabling effective experimental protocols archive and sharing in scientific community. ",
        "title": "Synergizing Human Expertise and AI Efficiency with Language Model for  Microscopy Operation and Automated Experiment Design",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13804",
        "abstract_url": "http://arxiv.org/abs/2401.13804",
        "authors": [
            {
                "last_name": "Sun",
                "first_name": "Yuling"
            },
            {
                "last_name": "Liu",
                "first_name": "Jiali"
            },
            {
                "last_name": "Yao",
                "first_name": "Bingsheng"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiaju"
            },
            {
                "last_name": "Wang",
                "first_name": "Dakuo"
            },
            {
                "last_name": "Ma",
                "first_name": "Xiaojuan"
            },
            {
                "last_name": "Lu",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "Xu",
                "first_name": "Ying"
            },
            {
                "last_name": "He",
                "first_name": "Liang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  Interactive storytelling is vital for preschooler development. While children's interactive partners have traditionally been their parents and teachers, recent advances in artificial intelligence (AI) have sparked a surge of AI-based storytelling technologies. As these technologies become increasingly ubiquitous in preschoolers' lives, questions arise regarding how they function in practical storytelling scenarios and, in particular, how parents, the most critical stakeholders, experience and perceive these technologies. This paper investigates these questions through a qualitative study with 17 parents of children aged 3-6. Our findings suggest that even though AI-based storytelling technologies provide more immersive and engaging interaction, they still cannot meet parents' expectations due to a series of interactive, functional, and algorithmic challenges. We elaborate on these challenges and discuss the possible implications of future AI-based storytelling technologies for preschoolers. We conclude by highlighting the design implications for future AI-based storytelling technologies. ",
        "title": "Exploring Parent's Needs for Children-Centered AI to Support  Preschoolers' Storytelling and Reading Activities",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13805",
        "abstract_url": "http://arxiv.org/abs/2401.13805",
        "authors": [
            {
                "last_name": "Nwaoha",
                "first_name": "Fabian"
            },
            {
                "last_name": "Gaffar",
                "first_name": "Ziyad"
            },
            {
                "last_name": "Chun",
                "first_name": "Ho Joon"
            },
            {
                "last_name": "Sokolova",
                "first_name": "Marina"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI",
            "IR"
        ],
        "abstract": "  In this study, we analyze texts of Reddit posts written by students of four major Canadian universities. We gauge the emotional tone and uncover prevailing themes and discussions through longitudinal topic modeling of posts textual data. Our study focuses on four years, 2020-2023, covering COVID-19 pandemic and after pandemic years. Our results highlight a gradual uptick in discussions related to mental health. ",
        "title": "Longitudinal Sentiment Topic Modelling of Reddit Posts",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13807",
        "abstract_url": "http://arxiv.org/abs/2401.13807",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Daniel Bochen"
            },
            {
                "last_name": "Ping",
                "first_name": "Shuohao"
            },
            {
                "last_name": "Cong",
                "first_name": "Jason"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET"
        ],
        "abstract": "  Reducing control complexity is essential for achieving large-scale quantum computing, particularly on platforms operating in cryogenic environments. Wiring each qubit to a room-temperature control poses a challenge, as this approach would surpass the thermal budget in the foreseeable future. An essential tradeoff becomes evident: reducing control knobs compromises the ability to independently address each qubit. Recent progress in neutral atom-based platforms suggests that rectangular addressing may strike a balance between control granularity and flexibility for 2D qubit arrays. This scheme allows addressing qubits on the intersections of a set of rows and columns each time. While quadratically reducing controls, it may necessitate more depth. We formulate the depth-optimal rectangular addressing problem as exact binary matrix factorization, an NP-hard problem also appearing in communication complexity and combinatorial optimization. We introduce a satisfiability modulo theories-based solver for this problem, and a heuristic, row packing, performing close to the optimal solver on various benchmarks. Furthermore, we discuss rectangular addressing in the context of fault-tolerant quantum computing, leveraging a natural two-level structure. ",
        "title": "Depth-Optimal Addressing of 2D Qubit Array with 1D Controls Based on  Exact Binary Matrix Factorization",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13810",
        "abstract_url": "http://arxiv.org/abs/2401.13810",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xuchao"
            },
            {
                "last_name": "Ghosh",
                "first_name": "Supriyo"
            },
            {
                "last_name": "Bansal",
                "first_name": "Chetan"
            },
            {
                "last_name": "Wang",
                "first_name": "Rujia"
            },
            {
                "last_name": "Ma",
                "first_name": "Minghua"
            },
            {
                "last_name": "Kang",
                "first_name": "Yu"
            },
            {
                "last_name": "Rajmohan",
                "first_name": "Saravan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "SE"
        ],
        "abstract": "  Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model's immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model. ",
        "title": "Automated Root Causing of Cloud Incidents using In-Context Learning with  GPT-4",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13812",
        "abstract_url": "http://arxiv.org/abs/2401.13812",
        "authors": [
            {
                "last_name": "Scarsini",
                "first_name": "Marco"
            },
            {
                "last_name": "Shmaya",
                "first_name": "Eran"
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT"
        ],
        "abstract": "  We consider an M/M/1 queueing model where customers can strategically decide whether to join the queue or balk and when to renege. We characterize the class of queueing regimes such that, for any parameters of the model, the socially efficient behavior is an equilibrium outcome. ",
        "title": "Optimal Queueing Regimes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13815",
        "abstract_url": "http://arxiv.org/abs/2401.13815",
        "authors": [
            {
                "last_name": "Collins",
                "first_name": "Brandon"
            },
            {
                "last_name": "Xu",
                "first_name": "Shouhuai"
            },
            {
                "last_name": "Brown",
                "first_name": "Philip N."
            }
        ],
        "primary_category": "GT",
        "categories": [
            "GT",
            "CR"
        ],
        "abstract": "  The discipline of game theory was introduced in the context of economics, and has been applied to study cyber attacker and defender behaviors. While adaptions have been made to accommodate features in the cyber domain, these studies are inherently limited by the root of game theory in economic systems where players (i.e., agents) may be selfish but not malicious. In this SoK, we systematize the major cybersecurity problems that have been studied with the game-theoretic approach, the assumptions that have been made, the models and solution concepts that have been proposed. The systematization leads to a characterization of the technical gaps that must be addressed in order to make game-theoretic cybersecurity models truly useful. We explore bridges to address them. ",
        "title": "SoK: Game-Theoretic Cybersecurity: Assumptions, Models, Gaps, and  Bridges",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13819",
        "abstract_url": "http://arxiv.org/abs/2401.13819",
        "authors": [
            {
                "last_name": "Anand",
                "first_name": "Aditya"
            },
            {
                "last_name": "Lee",
                "first_name": "Euiwoong"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  Given a metric space $(V, d)$ along with an integer $k$, the $k$-Median problem asks to open $k$ centers $C \\subseteq V$ to minimize $\\sum_{v \\in V} d(v, C)$, where $d(v, C) := \\min_{c \\in C} d(v, c)$. While the best-known approximation ratio of $2.613$ holds for the more general supplier version where an additional set $F \\subseteq V$ is given with the restriction $C \\subseteq F$, the best known hardness for these two versions are $1+1/e \\approx 1.36$ and $1+2/e \\approx 1.73$ respectively, using the same reduction from Max $k$-Coverage. We prove the following two results separating them.   First, we show a $1.546$-parameterized approximation algorithm that runs in time $f(k) n^{O(1)}$. Since $1+2/e$ is proved to be the optimal approximation ratio for the supplier version in the parameterized setting, this result separates the original $k$-Median from the supplier version.   Next, we prove a $1.416$-hardness for polynomial-time algorithms assuming the Unique Games Conjecture. This is achieved via a new fine-grained hardness of Max-$k$-Coverage for small set sizes.   Our upper bound and lower bound are derived from almost the same expression, with the only difference coming from the well-known separation between the powers of LP and SDP on (hypergraph) vertex cover. ",
        "title": "Separating $k$-Median from the Supplier Version",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13822",
        "abstract_url": "http://arxiv.org/abs/2401.13822",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Liang",
                "first_name": "Weixin"
            },
            {
                "last_name": "Zou",
                "first_name": "James"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face -- one of the largest platforms for sharing and collaborating on ML models and datasets -- as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, while the Considerations for Using the Data section receives the lowest proportion of content. (3) By analyzing the subsections within each section and utilizing topic modeling to identify key topics, we uncover what is discussed in each section, and underscore significant themes encompassing both technical and social impacts, as well as limitations within the Considerations for Using the Data section. (4) Our findings also highlight the need for improved accessibility and reproducibility of datasets in the Usage sections. (5) In addition, our human annotation evaluation emphasizes the pivotal role of comprehensive dataset content in shaping individuals' perceptions of a dataset card's overall quality. Overall, our study offers a unique perspective on analyzing dataset documentation through large-scale data science analysis and underlines the need for more thorough dataset documentation in machine learning research. ",
        "title": "Navigating Dataset Documentations in AI: A Large-Scale Analysis of  Dataset Cards on Hugging Face",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13823",
        "abstract_url": "http://arxiv.org/abs/2401.13823",
        "authors": [
            {
                "last_name": "Boratto",
                "first_name": "Ludovico"
            },
            {
                "last_name": "Fenu",
                "first_name": "Gianni"
            },
            {
                "last_name": "Fabbri",
                "first_name": "Francesco"
            },
            {
                "last_name": "Marras",
                "first_name": "Mirko"
            },
            {
                "last_name": "Medda",
                "first_name": "Giacomo"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Efforts in the recommendation community are shifting from the sole emphasis on utility to considering beyond-utility factors, such as fairness and robustness. Robustness of recommendation models is typically linked to their ability to maintain the original utility when subjected to attacks. Limited research has explored the robustness of a recommendation model in terms of fairness, e.g., the parity in performance across groups, under attack scenarios. In this paper, we aim to assess the robustness of graph-based recommender systems concerning fairness, when exposed to attacks based on edge-level perturbations. To this end, we considered four different fairness operationalizations, including both consumer and provider perspectives. Experiments on three datasets shed light on the impact of perturbations on the targeted fairness notion, uncovering key shortcomings in existing evaluation protocols for robustness. As an example, we observed perturbations affect consumer fairness on a higher extent than provider fairness, with alarming unfairness for the former. Source code: https://github.com/jackmedda/CPFairRobust ",
        "title": "Robustness in Fairness against Edge-level Perturbations in GNN-based  Recommendation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13827",
        "abstract_url": "http://arxiv.org/abs/2401.13827",
        "authors": [
            {
                "last_name": "Eldeeb",
                "first_name": "Eslam"
            },
            {
                "last_name": "Shehab",
                "first_name": "Mohammad"
            },
            {
                "last_name": "Alves",
                "first_name": "Hirley"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NI"
        ],
        "abstract": "  The age of information (AoI) is used to measure the freshness of the data. In IoT networks, the traditional resource management schemes rely on a message exchange between the devices and the base station (BS) before communication which causes high AoI, high energy consumption, and low reliability. Unmanned aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the AoI, energy-saving, and throughput improvement. In this paper, we present a novel learning-based framework that estimates the traffic arrival of IoT devices based on Markovian events. The learning proceeds to optimize the trajectory of multiple UAVs and their scheduling policy. First, the BS predicts the future traffic of the devices. We compare two traffic predictors: the forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we propose a deep reinforcement learning (DRL) approach to optimize the optimal policy of each UAV. Finally, we manipulate the optimum reward function for the proposed DRL approach. Simulation results show that the proposed algorithm outperforms the random-walk (RW) baseline model regarding the AoI, scheduling accuracy, and transmission power. ",
        "title": "Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink  in Markovian IoT Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13832",
        "abstract_url": "http://arxiv.org/abs/2401.13832",
        "authors": [
            {
                "last_name": "Kuznetsova",
                "first_name": "Elizaveta"
            },
            {
                "last_name": "Makhortykh",
                "first_name": "Mykola"
            },
            {
                "last_name": "Sydorova",
                "first_name": "Maryna"
            },
            {
                "last_name": "Urman",
                "first_name": "Aleksandra"
            },
            {
                "last_name": "Vitulano",
                "first_name": "Ilaria"
            },
            {
                "last_name": "Stolze",
                "first_name": "Martha"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR",
            "CY"
        ],
        "abstract": "  The growing volume of online content prompts the need for adopting algorithmic systems of information curation. These systems range from web search engines to recommender systems and are integral for helping users stay informed about important societal developments. However, unlike journalistic editing the algorithmic information curation systems (AICSs) are known to be subject to different forms of malperformance which make them vulnerable to possible manipulation. The risk of manipulation is particularly prominent in the case when AICSs have to deal with information about false claims that underpin propaganda campaigns of authoritarian regimes. Using as a case study of the Russian disinformation campaign concerning the US biolabs in Ukraine, we investigate how one of the most commonly used forms of AICSs - i.e. web search engines - curate misinformation-related content. For this aim, we conduct virtual agent-based algorithm audits of Google, Bing, and Yandex search outputs in June 2022. Our findings highlight the troubling performance of search engines. Even though some search engines, like Google, were less likely to return misinformation results, across all languages and locations, the three search engines still mentioned or promoted a considerable share of false content (33% on Google; 44% on Bing, and 70% on Yandex). We also find significant disparities in misinformation exposure based on the language of search, with all search engines presenting a higher number of false stories in Russian. Location matters as well with users from Germany being more likely to be exposed to search results promoting false information. These observations stress the possibility of AICSs being vulnerable to manipulation, in particular in the case of the unfolding propaganda campaigns, and underline the importance of monitoring performance of these systems to prevent it. ",
        "title": "Algorithmically Curated Lies: How Search Engines Handle Misinformation  about US Biolabs in Ukraine",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13835",
        "abstract_url": "http://arxiv.org/abs/2401.13835",
        "authors": [
            {
                "last_name": "Steyvers",
                "first_name": "Mark"
            },
            {
                "last_name": "Tejeda",
                "first_name": "Heliodoro"
            },
            {
                "last_name": "Kumar",
                "first_name": "Aakriti"
            },
            {
                "last_name": "Belem",
                "first_name": "Catarina"
            },
            {
                "last_name": "Karny",
                "first_name": "Sheer"
            },
            {
                "last_name": "Hu",
                "first_name": "Xinyue"
            },
            {
                "last_name": "Mayer",
                "first_name": "Lukas"
            },
            {
                "last_name": "Smyth",
                "first_name": "Padhraic"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "HC"
        ],
        "abstract": "  For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential. ",
        "title": "The Calibration Gap between Model and Human Confidence in Large Language  Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13836",
        "abstract_url": "http://arxiv.org/abs/2401.13836",
        "authors": [
            {
                "last_name": "Lawrence",
                "first_name": "Nathan P."
            },
            {
                "last_name": "Damarla",
                "first_name": "Seshu Kumar"
            },
            {
                "last_name": "Kim",
                "first_name": "Jong Woo"
            },
            {
                "last_name": "Tulsyan",
                "first_name": "Aditya"
            },
            {
                "last_name": "Amjad",
                "first_name": "Faraz"
            },
            {
                "last_name": "Wang",
                "first_name": "Kai"
            },
            {
                "last_name": "Chachuat",
                "first_name": "Benoit"
            },
            {
                "last_name": "Lee",
                "first_name": "Jong Min"
            },
            {
                "last_name": "Huang",
                "first_name": "Biao"
            },
            {
                "last_name": "Gopaluni",
                "first_name": "R. Bhushan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  With the rise of deep learning, there has been renewed interest within the process industries to utilize data on large-scale nonlinear sensing and control problems. We identify key statistical and machine learning techniques that have seen practical success in the process industries. To do so, we start with hybrid modeling to provide a methodological framework underlying core application areas: soft sensing, process optimization, and control. Soft sensing contains a wealth of industrial applications of statistical and machine learning methods. We quantitatively identify research trends, allowing insight into the most successful techniques in practice.   We consider two distinct flavors for data-driven optimization and control: hybrid modeling in conjunction with mathematical programming techniques and reinforcement learning. Throughout these application areas, we discuss their respective industrial requirements and challenges.   A common challenge is the interpretability and efficiency of purely data-driven methods. This suggests a need to carefully balance deep learning techniques with domain knowledge. As a result, we highlight ways prior knowledge may be integrated into industrial machine learning applications. The treatment of methods, problems, and applications presented here is poised to inform and inspire practitioners and researchers to develop impactful data-driven sensing, optimization, and control solutions in the process industries. ",
        "title": "Machine learning for industrial sensing and control: A survey and  practical perspective",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13837",
        "abstract_url": "http://arxiv.org/abs/2401.13837",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Mingxuan"
            },
            {
                "last_name": "Roy",
                "first_name": "Subhankar"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjing"
            },
            {
                "last_name": "Zhong",
                "first_name": "Zhun"
            },
            {
                "last_name": "Sebe",
                "first_name": "Nicu"
            },
            {
                "last_name": "Ricci",
                "first_name": "Elisa"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Identifying subordinate-level categories from images is a longstanding task in computer vision and is referred to as fine-grained visual recognition (FGVR). It has tremendous significance in real-world applications since an average layperson does not excel at differentiating species of birds or mushrooms due to subtle differences among the species. A major bottleneck in developing FGVR systems is caused by the need of high-quality paired expert annotations. To circumvent the need of expert knowledge we propose Fine-grained Semantic Category Reasoning (FineR) that internally leverages the world knowledge of large language models (LLMs) as a proxy in order to reason about fine-grained category names. In detail, to bridge the modality gap between images and LLM, we extract part-level visual attributes from images as text and feed that information to a LLM. Based on the visual attributes and its internal world knowledge the LLM reasons about the subordinate-level category names. Our training-free FineR outperforms several state-of-the-art FGVR and language and vision assistant models and shows promise in working in the wild and in new domains where gathering expert annotation is arduous. ",
        "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13839",
        "abstract_url": "http://arxiv.org/abs/2401.13839",
        "authors": [
            {
                "last_name": "Kowalik",
                "first_name": "\u0141ukasz"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In this paper we show that every graph $G$ of bounded maximum average degree ${\\rm mad}(G)$ and with maximum degree $\\Delta$ can be edge-colored using the optimal number of $\\Delta$ colors in quasilinear expected time, whenever $\\Delta\\ge 2{\\rm mad}(G)$. The maximum average degree is within a multiplicative constant of other popular graph sparsity parameters like arboricity, degeneracy or maximum density. Our algorithm extends previous results of Chrobak and Nishizeki [J. Algorithms, 1990] and Bhattacharya, Costa, Panski and Solomon [arXiv, 2023]. ",
        "title": "Edge-coloring sparse graphs with $\\Delta$ colors in quasilinear time",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13842",
        "abstract_url": "http://arxiv.org/abs/2401.13842",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Pan"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In this paper, we propose an online-matching-based model to tackle the two fundamental issues, matching and pricing, existing in a wide range of real-world gig platforms, including ride-hailing (matching riders and drivers), crowdsourcing markets (pairing workers and tasks), and online recommendations (offering items to customers). Our model assumes the arriving distributions of dynamic agents (e.g., riders, workers, and buyers) are accessible in advance, and they can change over time, which is referred to as \\emph{Known Heterogeneous Distributions} (KHD).   In this paper, we initiate variance analysis for online matching algorithms under KHD. Unlike the popular competitive-ratio (CR) metric, the variance of online algorithms' performance is rarely studied due to inherent technical challenges, though it is well linked to robustness. We focus on two natural parameterized sampling policies, denoted by $\\mathsf{ATT}(\\gamma)$ and $\\mathsf{SAMP}(\\gamma)$, which appear as foundational bedrock in online algorithm design. We offer rigorous competitive ratio (CR) and variance analyses for both policies. Specifically, we show that $\\mathsf{ATT}(\\gamma)$ with $\\gamma \\in [0,1/2]$ achieves a CR of $\\gamma$ and a variance of $\\gamma \\cdot (1-\\gamma) \\cdot B$ on the total number of matches with $B$ being the total matching capacity. In contrast, $\\mathsf{SAMP}(\\gamma)$ with $\\gamma \\in [0,1]$ accomplishes a CR of $\\gamma (1-\\gamma)$ and a variance of $\\bar{\\gamma} (1-\\bar{\\gamma})\\cdot B$ with $\\bar{\\gamma}=\\min(\\gamma,1/2)$. All CR and variance analyses are tight and unconditional of any benchmark. As a byproduct, we prove that $\\mathsf{ATT}(\\gamma=1/2)$ achieves an optimal CR of $1/2$. ",
        "title": "Tight Competitive and Variance Analyses of Matching Policies in Gig  Platforms",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13843",
        "abstract_url": "http://arxiv.org/abs/2401.13843",
        "authors": [
            {
                "last_name": "Fazekas",
                "first_name": "Attila"
            },
            {
                "last_name": "Kovacs",
                "first_name": "Gyorgy"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  K-fold cross-validation is a widely used tool for assessing classifier performance. The reproducibility crisis faced by artificial intelligence partly results from the irreproducibility of reported k-fold cross-validation-based performance scores. Recently, we introduced numerical techniques to test the consistency of claimed performance scores and experimental setups. In a crucial use case, the method relies on the combinatorial enumeration of all k-fold configurations, for which we proposed an algorithm in the binary classification case. ",
        "title": "Enumerating the k-fold configurations in multi-class classification  problems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13848",
        "abstract_url": "http://arxiv.org/abs/2401.13848",
        "authors": [
            {
                "last_name": "Alekszejenk\u00f3",
                "first_name": "Levente"
            },
            {
                "last_name": "Dobrowiecki",
                "first_name": "Tadeusz"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact.   In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model it with a non-IID (independent and identically distributed) dataset to evaluate the capabilities of the proposed system in terms of performance and privacy. Results indicate that the proposed FL scheme improves learning performance and prevents eavesdropping at the aggregator server side. ",
        "title": "A V2X-based Privacy Preserving Federated Measuring and Learning System",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13849",
        "abstract_url": "http://arxiv.org/abs/2401.13849",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Haorui"
            },
            {
                "last_name": "Zhang",
                "first_name": "Rongzhi"
            },
            {
                "last_name": "Li",
                "first_name": "Yinghao"
            },
            {
                "last_name": "Kong",
                "first_name": "Lingkai"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Yuchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiusi"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the student model begins making inferences, TPD requires no further intervention from the teacher LLM or humans. Through extensive experiments across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared to standard chain-of-thought prompting, TPD significantly improves the student model's performance, achieving $6.2\\%$ improvement on average. ",
        "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery  and Guidance",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13850",
        "abstract_url": "http://arxiv.org/abs/2401.13850",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Nayoung"
            },
            {
                "last_name": "Cohen",
                "first_name": "Myke C."
            },
            {
                "last_name": "Ba",
                "first_name": "Yang"
            },
            {
                "last_name": "Pan",
                "first_name": "Anna"
            },
            {
                "last_name": "Bhatti",
                "first_name": "Shawaiz"
            },
            {
                "last_name": "Salehi",
                "first_name": "Pouria"
            },
            {
                "last_name": "Sung",
                "first_name": "James"
            },
            {
                "last_name": "Blasch",
                "first_name": "Erik"
            },
            {
                "last_name": "Mancenido",
                "first_name": "Michelle V."
            },
            {
                "last_name": "Chiou",
                "first_name": "Erin K."
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Designing for AI trustworthiness is challenging, with a lack of practical guidance despite extensive literature on trust. The Multisource AI Scorecard Table (MAST), a checklist rating system, addresses this gap in designing and evaluating AI-enabled decision support systems. We propose the Principled Approach for Designing Trustable Human-centered AI systems using MAST Methodology (PADTHAI-MM), a nine-step framework what we demonstrate through the iterative design of a text analysis platform called the REporting Assistant for Defense and Intelligence Tasks (READIT). We designed two versions of READIT, high-MAST including AI context and explanations, and low-MAST resembling a \"black box\" type system. Participant feedback and state-of-the-art AI knowledge was integrated in the design process, leading to a redesigned prototype tested by participants in an intelligence reporting task. Results show that MAST-guided design can improve trust perceptions, and that MAST criteria can be linked to performance, process, and purpose information, providing a practical and theory-informed basis for AI system design. ",
        "title": "PADTHAI-MM: A Principled Approach for Designing Trustable,  Human-centered AI systems using the MAST Methodology",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13851",
        "abstract_url": "http://arxiv.org/abs/2401.13851",
        "authors": [
            {
                "last_name": "Arora",
                "first_name": "Akshit"
            },
            {
                "last_name": "Badlani",
                "first_name": "Rohan"
            },
            {
                "last_name": "Kim",
                "first_name": "Sungwon"
            },
            {
                "last_name": "Valle",
                "first_name": "Rafael"
            },
            {
                "last_name": "Catanzaro",
                "first_name": "Bryan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  In this paper, we describe the TTS models developed by NVIDIA for the MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024 Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by training additionally on 5 minutes of target speaker data. In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets. We use HiFi-GAN vocoders for all submissions. RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS) of 3.62. ",
        "title": "Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice  cloning to Indic Languages",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13853",
        "abstract_url": "http://arxiv.org/abs/2401.13853",
        "authors": [
            {
                "last_name": "Carmichael",
                "first_name": "Spencer"
            },
            {
                "last_name": "Buchan",
                "first_name": "Austin"
            },
            {
                "last_name": "Ramanagopal",
                "first_name": "Mani"
            },
            {
                "last_name": "Ravi",
                "first_name": "Radhika"
            },
            {
                "last_name": "Vasudevan",
                "first_name": "Ram"
            },
            {
                "last_name": "Skinner",
                "first_name": "Katherine A."
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  Conventional cameras employed in autonomous vehicle (AV) systems support many perception tasks, but are challenged by low-light or high dynamic range scenes, adverse weather, and fast motion. Novel sensors, such as event and thermal cameras, offer capabilities with the potential to address these scenarios, but they remain to be fully exploited. This paper introduces the Novel Sensors for Autonomous Vehicle Perception (NSAVP) dataset to facilitate future research on this topic. The dataset was captured with a platform including stereo event, thermal, monochrome, and RGB cameras as well as a high precision navigation system providing ground truth poses. The data was collected by repeatedly driving two ~8 km routes and includes varied lighting conditions and opposing viewpoint perspectives. We provide benchmarking experiments on the task of place recognition to demonstrate challenges and opportunities for novel sensors to enhance critical AV perception tasks. To our knowledge, the NSAVP dataset is the first to include stereo thermal cameras together with stereo event and monochrome cameras. The dataset and supporting software suite is available at: https://umautobots.github.io/nsavp ",
        "title": "Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13854",
        "abstract_url": "http://arxiv.org/abs/2401.13854",
        "authors": [
            {
                "last_name": "Pu",
                "first_name": "Jiameng"
            },
            {
                "last_name": "Takhirov",
                "first_name": "Zafar"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR"
        ],
        "abstract": "  This report summarizes all the MIA experiments (Membership Inference Attacks) of the Embedding Attack Project, including threat models, experimental setup, experimental results, findings and discussion. Current results cover the evaluation of two main MIA strategies (loss-based and embedding-based MIAs) on 6 AI models ranging from Computer Vision to Language Modelling. There are two ongoing experiments on MIA defense and neighborhood-comparison embedding attacks. These are ongoing projects.   The current work on MIA and PIA can be summarized into six conclusions: (1) Amount of overfitting is directly proportional to model's vulnerability; (2) early embedding layers in the model are less susceptible to privacy leaks; (3) Deeper model layers contain more membership information; (4) Models are more vulnerable to MIA if both embeddings and corresponding training labels are compromised; (5) it is possible to use pseudo-labels to increase the MIA success; and (6) although MIA and PIA success rates are proportional, reducing the MIA does not necessarily reduce the PIA. ",
        "title": "Embedding Attack Project (Work Report)",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13856",
        "abstract_url": "http://arxiv.org/abs/2401.13856",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Dat"
            },
            {
                "last_name": "Mejri",
                "first_name": "Nesryne"
            },
            {
                "last_name": "Singh",
                "first_name": "Inder Pal"
            },
            {
                "last_name": "Kuleshova",
                "first_name": "Polina"
            },
            {
                "last_name": "Astrid",
                "first_name": "Marcella"
            },
            {
                "last_name": "Kacem",
                "first_name": "Anis"
            },
            {
                "last_name": "Ghorbel",
                "first_name": "Enjie"
            },
            {
                "last_name": "Aouada",
                "first_name": "Djamila"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces a novel approach for high-quality deepfake detection called Localized Artifact Attention Network (LAA-Net). Existing methods for high-quality deepfake detection are mainly based on a supervised binary classifier coupled with an implicit attention mechanism. As a result, they do not generalize well to unseen manipulations. To handle this issue, two main contributions are made. First, an explicit attention mechanism within a multi-task learning framework is proposed. By combining heatmap-based and self-consistency attention strategies, LAA-Net is forced to focus on a few small artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid Network (E-FPN) is proposed as a simple and effective mechanism for spreading discriminative low-level features into the final feature output, with the advantage of limiting redundancy. Experiments performed on several benchmarks show the superiority of our approach in terms of Area Under the Curve (AUC) and Average Precision (AP). The code will be released soon. ",
        "title": "LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes  Detection",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13858",
        "abstract_url": "http://arxiv.org/abs/2401.13858",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Gang"
            },
            {
                "last_name": "Xu",
                "first_name": "Jiaxin"
            },
            {
                "last_name": "Luo",
                "first_name": "Tengfei"
            },
            {
                "last_name": "Jiang",
                "first_name": "Meng"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. An inverse polymer design task for gas separation with feedback from domain experts further demonstrates its practical utility. ",
        "title": "Inverse Molecular Design with Multi-Conditional Diffusion Guidance",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13865",
        "abstract_url": "http://arxiv.org/abs/2401.13865",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Suneung"
            },
            {
                "last_name": "Nam",
                "first_name": "Woo-Jeoung"
            },
            {
                "last_name": "Lee",
                "first_name": "Seong-Whan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model. ",
        "title": "Appearance Debiased Gaze Estimation via Stochastic Subject-Wise  Adversarial Learning",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13867",
        "abstract_url": "http://arxiv.org/abs/2401.13867",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Yifan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaoyu"
            },
            {
                "last_name": "Jin",
                "first_name": "Qiao"
            },
            {
                "last_name": "Huang",
                "first_name": "Furong"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhiyong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all patients. ",
        "title": "Unmasking and Quantifying Racial Bias of Large Language Models in  Medical Report Generation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13868",
        "abstract_url": "http://arxiv.org/abs/2401.13868",
        "authors": [
            {
                "last_name": "Kobayashi",
                "first_name": "Hiroki"
            },
            {
                "last_name": "Nomura",
                "first_name": "Katsuya"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yuqing"
            },
            {
                "last_name": "Tanaka",
                "first_name": "Masato"
            },
            {
                "last_name": "Kawamoto",
                "first_name": "Atsushi"
            },
            {
                "last_name": "Nomura",
                "first_name": "Tsuyoshi"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  This paper proposes a level set-based method for optimizing shell structures with large design changes in shape and topology. Conventional shell optimization methods, whether parametric or nonparametric, often only allow limited design changes in shape. In the proposed method, the shell structure is defined as the isosurface of a level set function. The level set function is iteratively updated based on the shape sensitivity on the surface mesh. Therefore, the proposed method can represent an arbitrary manifold surface while dealing with topological changes, for example, from a spherical surface to a toroidal surface. We applied the proposed method to the mean compliance minimization problems of 3D shell structural designs for dome, bending plate and cantilever beam examples to demonstrate its efficacy of the proposed method. ",
        "title": "Shell topology optimization based on level set method",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13870",
        "abstract_url": "http://arxiv.org/abs/2401.13870",
        "authors": [
            {
                "last_name": "Luo",
                "first_name": "Sichun"
            },
            {
                "last_name": "Yao",
                "first_name": "Yuxuan"
            },
            {
                "last_name": "He",
                "first_name": "Bowei"
            },
            {
                "last_name": "Huang",
                "first_name": "Yinya"
            },
            {
                "last_name": "Zhou",
                "first_name": "Aojun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xinyi"
            },
            {
                "last_name": "Xiao",
                "first_name": "Yuanzhang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Mingjie"
            },
            {
                "last_name": "Song",
                "first_name": "Linqi"
            }
        ],
        "primary_category": "IR",
        "categories": [
            "IR"
        ],
        "abstract": "  Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance.   In this paper, we introduce a general and model-agnostic framework known as \\textbf{L}arge \\textbf{la}nguage model with \\textbf{m}utual augmentation and \\textbf{a}daptive aggregation for \\textbf{Rec}ommendation (\\textbf{Llama4Rec}). Llama4Rec synergistically combines conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent outperformance of baseline methods and significant improvements in recommendation performance. ",
        "title": "Integrating Large Language Models into Recommendation via Mutual  Augmentation and Adaptive Aggregation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13872",
        "abstract_url": "http://arxiv.org/abs/2401.13872",
        "authors": [
            {
                "last_name": "Jo",
                "first_name": "Hayoung"
            },
            {
                "last_name": "Lee",
                "first_name": "Seong-Whan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  With the rapid advancement in cyber-physical systems, the increasing number of sensors has significantly complicated manual monitoring of system states. Consequently, graph-based time-series anomaly detection methods have gained attention due to their ability to explicitly represent relationships between sensors. However, these methods often apply a uniform source node representation across all connected target nodes, even when updating different target node representations. Moreover, the graph attention mechanism, commonly used to infer unknown graph structures, could constrain the diversity of source node representations. In this paper, we introduce the Edge Conditional Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge conditional node update module, dynamically transforms source node representations based on connected edges to represent target nodes aptly. We validate performance on three real-world datasets: SWaT, WADI, and PSM. Our model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively, compared to best F1 baseline models. ",
        "title": "Edge Conditional Node Update Graph Neural Network for Multi-variate Time  Series Anomaly Detection",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13875",
        "abstract_url": "http://arxiv.org/abs/2401.13875",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Huy"
            },
            {
                "last_name": "Akbarian",
                "first_name": "Pedram"
            },
            {
                "last_name": "Ho",
                "first_name": "Nhat"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Dense-to-sparse gating mixture of experts (MoE) has recently become an effective alternative to a well-known sparse MoE. Rather than fixing the number of activated experts as in the latter model, which could limit the investigation of potential experts, the former model utilizes the temperature to control the softmax weight distribution and the sparsity of the MoE during training in order to stabilize the expert specialization. Nevertheless, while there are previous attempts to theoretically comprehend the sparse MoE, a comprehensive analysis of the dense-to-sparse gating MoE has remained elusive. Therefore, we aim to explore the impacts of the dense-to-sparse gate on the maximum likelihood estimation under the Gaussian MoE in this paper. We demonstrate that due to interactions between the temperature and other model parameters via some partial differential equations, the convergence rates of parameter estimations are slower than any polynomial rates, and could be as slow as $\\mathcal{O}(1/\\log(n))$, where $n$ denotes the sample size. To address this issue, we propose using a novel activation dense-to-sparse gate, which routes the output of a linear layer to an activation function before delivering them to the softmax function. By imposing linearly independence conditions on the activation function and its derivatives, we show that the parameter estimation rates are significantly improved to polynomial rates. ",
        "title": "Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13877",
        "abstract_url": "http://arxiv.org/abs/2401.13877",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Tengfei"
            },
            {
                "last_name": "Lu",
                "first_name": "Fucheng"
            },
            {
                "last_name": "Qin",
                "first_name": "Jintao"
            },
            {
                "last_name": "Huang",
                "first_name": "Taosheng"
            },
            {
                "last_name": "Kong",
                "first_name": "Hui"
            },
            {
                "last_name": "Shen",
                "first_name": "Ping"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "RO"
        ],
        "abstract": "  Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment. ",
        "title": "AscDAMs: Advanced SLAM-based channel detection and mapping system",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13882",
        "abstract_url": "http://arxiv.org/abs/2401.13882",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Yongqing"
            },
            {
                "last_name": "Li",
                "first_name": "Yong"
            },
            {
                "last_name": "Quek",
                "first_name": "Tony Q. S."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  As a critical technology for next-generation communication networks, integrated sensing and communication (ISAC) aims to achieve the harmonious coexistence of communication and sensing. The degrees-of-freedom (DoF) of ISAC is limited due to multiple performance metrics used for communication and sensing. Reconfigurable Intelligent Surfaces (RIS) composed of metamaterials can enhance the DoF in the spatial domain of ISAC systems. However, the availability of perfect Channel State Information (CSI) is a prerequisite for the gain brought by RIS, which is not realistic in practical environments. Therefore, under the imperfect CSI condition, we propose a decomposition-based large deviation inequality approach to eliminate the impact of CSI error on communication rate and sensing Cram\\'er-Rao bound (CRB). Then, an alternating optimization (AO) algorithm based on semi-definite relaxation (SDR) and gradient extrapolated majorization-maximization (GEMM) is proposed to solve the transmit beamforming and discrete RIS beamforming problems. We also analyze the complexity and convergence of the proposed algorithm. Simulation results show that the proposed algorithms can effectively eliminate the influence of CSI error and have good convergence performance. Notably, when CSI error exists, the gain brought by RIS will decrease with the increase of the number of RIS elements. Finally, we summarize and outline future research directions. ",
        "title": "Robust Transmission Design for RIS-Assisted Integrated Sensing and  Communication Systems",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13883",
        "abstract_url": "http://arxiv.org/abs/2401.13883",
        "authors": [
            {
                "last_name": "Kuroiwa",
                "first_name": "Ryo"
            },
            {
                "last_name": "Beck",
                "first_name": "J. Christopher"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and both MIP and CP in seven. ",
        "title": "Domain-Independent Dynamic Programming",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13884",
        "abstract_url": "http://arxiv.org/abs/2401.13884",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yixuan"
            },
            {
                "last_name": "Xie",
                "first_name": "Qiaomin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows the application of Richardson-Romberg (RR) extrapolation technique to construct a new estimate that is provably closer to the optimal Q function. Numerical results corroborate our theoretical finding on the improvement of the RR extrapolation method. ",
        "title": "Constant Stepsize Q-learning: Distributional Convergence, Bias and  Extrapolation",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13887",
        "abstract_url": "http://arxiv.org/abs/2401.13887",
        "authors": [
            {
                "last_name": "Sushil",
                "first_name": "Madhumita"
            },
            {
                "last_name": "Zack",
                "first_name": "Travis"
            },
            {
                "last_name": "Mandair",
                "first_name": "Divneet"
            },
            {
                "last_name": "Zheng",
                "first_name": "Zhiwei"
            },
            {
                "last_name": "Wali",
                "first_name": "Ahmed"
            },
            {
                "last_name": "Yu",
                "first_name": "Yan-Ning"
            },
            {
                "last_name": "Quan",
                "first_name": "Yuwei"
            },
            {
                "last_name": "Butte",
                "first_name": "Atul J."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the differences were more prominent. Frequent sources of GPT-4 errors included inferences from multiple samples and complex task design. On complex tasks where large annotated datasets cannot be easily collected, LLMs can reduce the burden of large-scale data labeling. However, if the use of LLMs is prohibitive, the use of simpler supervised models with large annotated datasets can provide comparable results. LLMs demonstrated the potential to speed up the execution of clinical NLP studies by reducing the need for curating large annotated datasets. This may result in an increase in the utilization of NLP-based variables and outcomes in observational clinical studies. ",
        "title": "A comparative study of zero-shot inference with large language models  and supervised modeling in breast cancer pathology classification",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13888",
        "abstract_url": "http://arxiv.org/abs/2401.13888",
        "authors": [
            {
                "last_name": "Xi",
                "first_name": "Zeyu"
            },
            {
                "last_name": "Shi",
                "first_name": "Ge"
            },
            {
                "last_name": "Wu",
                "first_name": "Lifang"
            },
            {
                "last_name": "Li",
                "first_name": "Xuefen"
            },
            {
                "last_name": "Yan",
                "first_name": "Junchi"
            },
            {
                "last_name": "Wang",
                "first_name": "Liang"
            },
            {
                "last_name": "Liu",
                "first_name": "Zilin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Despite the recent emergence of video captioning models, how to generate the text description with specific entity names and fine-grained actions is far from being solved, which however has great applications such as basketball live text broadcast. In this paper, a new multimodal knowledge supported basketball benchmark for video captioning is proposed. Specifically, we construct a Multimodal Basketball Game Knowledge Graph (MbgKG) to provide knowledge beyond videos. Then, a Multimodal Basketball Game Video Captioning (MbgVC) dataset that contains 9 types of fine-grained shooting events and 286 players' knowledge (i.e., images and names) is constructed based on MbgKG. We develop a novel framework in the encoder-decoder form named Entity-Aware Captioner (EAC) for basketball live text broadcast. The temporal information in video is encoded by introducing the bi-directional GRU (Bi-GRU) module. And the multi-head self-attention module is utilized to model the relationships among the players and select the key players. Besides, we propose a new performance evaluation metric named Game Description Score (GDS), which measures not only the linguistic performance but also the accuracy of the names prediction. Extensive experiments on MbgVC dataset demonstrate that EAC effectively leverages external knowledge and outperforms advanced video captioning models. The proposed benchmark and corresponding codes will be publicly available soon. ",
        "title": "Knowledge Graph Supported Benchmark and Video Captioning for Basketball",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13891",
        "abstract_url": "http://arxiv.org/abs/2401.13891",
        "authors": [
            {
                "last_name": "s",
                "first_name": "Harini"
            },
            {
                "last_name": "M",
                "first_name": "Manoj G"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech. ",
        "title": "Text to speech synthesis",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13898",
        "abstract_url": "http://arxiv.org/abs/2401.13898",
        "authors": [
            {
                "last_name": "Le",
                "first_name": "Huy Q."
            },
            {
                "last_name": "Thwal",
                "first_name": "Chu Myaet"
            },
            {
                "last_name": "Qiao",
                "first_name": "Yu"
            },
            {
                "last_name": "Tun",
                "first_name": "Ye Lin"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Minh N. H."
            },
            {
                "last_name": "Hong",
                "first_name": "Choong Seon"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Multimodal federated learning (MFL) has emerged as a decentralized machine learning paradigm, allowing multiple clients with different modalities to collaborate on training a machine learning model across diverse data sources without sharing their private data. However, challenges, such as data heterogeneity and severely missing modalities, pose crucial hindrances to the robustness of MFL, significantly impacting the performance of global model. The absence of a modality introduces misalignment during the local training phase, stemming from zero-filling in the case of clients with missing modalities. Consequently, achieving robust generalization in global model becomes imperative, especially when dealing with clients that have incomplete data. In this paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a novel approach for MFL under severely missing modalities by conducting the complete prototypes to provide diverse modality knowledge in modality-shared level with the cross-modal regularization and modality-specific level with cross-modal contrastive mechanism. Additionally, our approach introduces the cross-modal alignment to provide regularization for modality-specific features, thereby enhancing overall performance, particularly in scenarios involving severely missing modalities. Through extensive experiments on three multimodal datasets, we demonstrate the effectiveness of MFCPL in mitigating these challenges and improving the overall performance. ",
        "title": "Cross-Modal Prototype based Multimodal Federated Learning under Severely  Missing Modality",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13903",
        "abstract_url": "http://arxiv.org/abs/2401.13903",
        "authors": [
            {
                "last_name": "Bennie",
                "first_name": "Callum"
            },
            {
                "last_name": "Casey",
                "first_name": "Bridget"
            },
            {
                "last_name": "Paris",
                "first_name": "Cecile"
            },
            {
                "last_name": "Kulic",
                "first_name": "Dana"
            },
            {
                "last_name": "Tidd",
                "first_name": "Brendan"
            },
            {
                "last_name": "Lawrance",
                "first_name": "Nicholas"
            },
            {
                "last_name": "Pitt",
                "first_name": "Alex"
            },
            {
                "last_name": "Talbot",
                "first_name": "Fletcher"
            },
            {
                "last_name": "Williams",
                "first_name": "Jason"
            },
            {
                "last_name": "Howard",
                "first_name": "David"
            },
            {
                "last_name": "Sikka",
                "first_name": "Pavan"
            },
            {
                "last_name": "Senaratne",
                "first_name": "Hashini"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  This article presents an implementation of a natural-language speech interface and a haptic feedback interface that enables a human supervisor to provide guidance to, request information, and receive status updates from a Spot robot. We provide insights gained during preliminary user testing of the interface in a realistic robot exploration scenario. ",
        "title": "Alternative Interfaces for Human-initiated Natural Language  Communication and Robot-initiated Haptic Feedback: Towards Better Situational  Awareness in Human-Robot Collaboration",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13904",
        "abstract_url": "http://arxiv.org/abs/2401.13904",
        "authors": [
            {
                "last_name": "Lou",
                "first_name": "Siyu"
            },
            {
                "last_name": "Liu",
                "first_name": "Chengchun"
            },
            {
                "last_name": "Chen",
                "first_name": "Yuntian"
            },
            {
                "last_name": "Mo",
                "first_name": "Fanyang"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DB"
        ],
        "abstract": "  Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior. ",
        "title": "Empowering Machines to Think Like Chemists: Unveiling Molecular  Structure-Polarity Relationships with Hierarchical Symbolic Regression",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13905",
        "abstract_url": "http://arxiv.org/abs/2401.13905",
        "authors": [
            {
                "last_name": "Sirin",
                "first_name": "Hale"
            },
            {
                "last_name": "Lippincott",
                "first_name": "Tom"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  We present a novel combination of dynamic embedded topic models and change-point detection to explore diachronic change of lexical semantic modality in classical and early Christian Latin. We demonstrate several methods for finding and characterizing patterns in the output, and relating them to traditional scholarship in Comparative Literature and Classics. This simple approach to unsupervised models of semantic change can be applied to any suitable corpus, and we conclude with future directions and refinements aiming to allow noisier, less-curated materials to meet that threshold. ",
        "title": "Dynamic embedded topic models and change-point detection for exploring  literary-historical hypotheses",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13907",
        "abstract_url": "http://arxiv.org/abs/2401.13907",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Han"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Researchers recently found out that sometimes language models achieve high accuracy on benchmark data set, but they can not generalize very well with even little changes to the original data set. This is sometimes due to data artifacts, model is learning the spurious correlation between tokens and labels, instead of the semantics and logic. In this work, we analyzed SNLI data and visualized such spurious correlations. We proposed an adaptive up-sampling algorithm to correct the data artifacts, which is simple and effective, and does not need human edits or annotation. We did an experiment applying the algorithm to fix the data artifacts in SNLI data and the model trained with corrected data performed significantly better than the model trained with raw SNLI data, overall, as well as on the subset we corrected. ",
        "title": "No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data  Artifacts",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13912",
        "abstract_url": "http://arxiv.org/abs/2401.13912",
        "authors": [
            {
                "last_name": "Miller",
                "first_name": "John A."
            },
            {
                "last_name": "Aldosari",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Saeed",
                "first_name": "Farah"
            },
            {
                "last_name": "Barna",
                "first_name": "Nasid Habib"
            },
            {
                "last_name": "Rana",
                "first_name": "Subas"
            },
            {
                "last_name": "Arpinar",
                "first_name": "I. Budak"
            },
            {
                "last_name": "Liu",
                "first_name": "Ninghao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided. ",
        "title": "A Survey of Deep Learning and Foundation Models for Time Series  Forecasting",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13913",
        "abstract_url": "http://arxiv.org/abs/2401.13913",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zixiao"
            },
            {
                "last_name": "Qiao",
                "first_name": "Dong"
            },
            {
                "last_name": "Fan",
                "first_name": "Jicong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency. ",
        "title": "Spectral Clustering for Discrete Distributions",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13919",
        "abstract_url": "http://arxiv.org/abs/2401.13919",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Hongliang"
            },
            {
                "last_name": "Yao",
                "first_name": "Wenlin"
            },
            {
                "last_name": "Ma",
                "first_name": "Kaixin"
            },
            {
                "last_name": "Yu",
                "first_name": "Wenhao"
            },
            {
                "last_name": "Dai",
                "first_name": "Yong"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongming"
            },
            {
                "last_name": "Lan",
                "first_name": "Zhenzhong"
            },
            {
                "last_name": "Yu",
                "first_name": "Dong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager in practical applications. We found that our proposed automatic evaluation achieves 85.3% agreement with human judgment, paving the way for further development of web agents in a real-world setting. ",
        "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal  Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13920",
        "abstract_url": "http://arxiv.org/abs/2401.13920",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jing"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhijie"
            },
            {
                "last_name": "He",
                "first_name": "Xuan"
            },
            {
                "last_name": "Zeng",
                "first_name": "Li"
            },
            {
                "last_name": "Lin",
                "first_name": "Yi"
            },
            {
                "last_name": "Li",
                "first_name": "Entong"
            },
            {
                "last_name": "Zheng",
                "first_name": "Binfan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Rongqian"
            },
            {
                "last_name": "Chen",
                "first_name": "Xin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We port these modifications on the PanGu-Sigma model based on the MindSpore framework with multi-level routing and conduct experiments on Ascend clusters. The experiment results demonstrate that the proposed LocMoE reduces training time per epoch by 12.68% to 22.24% compared to classical routers, such as hash router and switch router, without impacting the model accuracy. ",
        "title": "LocMoE: A Low-overhead MoE for Large Language Model Training",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13921",
        "abstract_url": "http://arxiv.org/abs/2401.13921",
        "authors": [
            {
                "last_name": "Jung",
                "first_name": "Sunghee"
            },
            {
                "last_name": "Jang",
                "first_name": "Won"
            },
            {
                "last_name": "Yoon",
                "first_name": "Jaesam"
            },
            {
                "last_name": "Kim",
                "first_name": "Bongwan"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Although numerous recent studies have suggested new frameworks for zero-shot TTS using large-scale, real-world data, studies that focus on the intelligibility of zero-shot TTS are relatively scarce. Zero-shot TTS demands additional efforts to ensure clear pronunciation and speech quality due to its inherent requirement of replacing a core parameter (speaker embedding or acoustic prompt) with a new one at the inference stage. In this study, we propose a zero-shot TTS model focused on intelligibility, which we refer to as Intelli-Z. Intelli-Z learns speaker embeddings by using multi-speaker TTS as its teacher and is trained with a cycle-consistency loss to include mismatched text-speech pairs for training. Additionally, it selectively aggregates speaker embeddings along the temporal dimension to minimize the interference of the text content of reference speech at the inference stage. We substantiate the effectiveness of the proposed methods with an ablation study. The Mean Opinion Score (MOS) increases by 9% for unseen speakers when the first two methods are ap- plied, and it further improves by 16% when selective temporal aggregation is applied. ",
        "title": "Intelli-Z: Toward Intelligible Zero-Shot TTS",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13922",
        "abstract_url": "http://arxiv.org/abs/2401.13922",
        "authors": [
            {
                "last_name": "Saber",
                "first_name": "Hamid"
            },
            {
                "last_name": "Hatami",
                "first_name": "Homayoon"
            },
            {
                "last_name": "Bae",
                "first_name": "Jung Hyun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Polar codes are the first class of structured channel codes that achieve the symmetric capacity of binary channels with efficient encoding and decoding. In 2019, Arikan proposed a new polar coding scheme referred to as polarization-adjusted convolutional (PAC)} codes. In contrast to polar codes, PAC codes precode the information word using a convolutional code prior to polar encoding. This results in material coding gain over polar code under Fano sequential decoding as well as successive cancellation list (SCL) decoding. Given the advantages of SCL decoding over Fano decoding in certain scenarios such as low-SNR regime or where a constraint on the worst case decoding latency exists, in this paper, we focus on SCL decoding and present a simplified SCL (SSCL) decoding algorithm for PAC codes. SSCL decoding of PAC codes reduces the decoding latency by identifying special nodes in the decoding tree and processing them at the intermediate stages of the graph. Our simulation results show that the performance of PAC codes under SSCL decoding is almost similar to the SCL decoding while having lower decoding latency. ",
        "title": "Simplified Successive Cancellation List Decoding of PAC Codes",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13923",
        "abstract_url": "http://arxiv.org/abs/2401.13923",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Sihang"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Luo",
                "first_name": "Yanchen"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiang"
            },
            {
                "last_name": "He",
                "first_name": "Xiangnan"
            },
            {
                "last_name": "Kawaguchi",
                "first_name": "Kenji"
            },
            {
                "last_name": "Chua",
                "first_name": "Tat-Seng"
            },
            {
                "last_name": "Tian",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "IR"
        ],
        "abstract": "  Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties. ",
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13924",
        "abstract_url": "http://arxiv.org/abs/2401.13924",
        "authors": [
            {
                "last_name": "Kirinuki",
                "first_name": "Hiroyuki"
            },
            {
                "last_name": "Tanno",
                "first_name": "Haruto"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  In recent years, large language models (LLMs), such as ChatGPT, have been pivotal in advancing various artificial intelligence applications, including natural language processing and software engineering. A promising yet underexplored area is utilizing LLMs in software testing, particularly in black-box testing. This paper explores the test cases devised by ChatGPT in comparison to those created by human participants. In this study, ChatGPT (GPT-4) and four participants each created black-box test cases for three applications based on specifications written by the authors. The goal was to evaluate the real-world applicability of the proposed test cases, identify potential shortcomings, and comprehend how ChatGPT could enhance human testing strategies. ChatGPT can generate test cases that generally match or slightly surpass those created by human participants in terms of test viewpoint coverage. Additionally, our experiments demonstrated that when ChatGPT cooperates with humans, it can cover considerably more test viewpoints than each can achieve alone, suggesting that collaboration between humans and ChatGPT may be more effective than human pairs working together. Nevertheless, we noticed that the test cases generated by ChatGPT have certain issues that require addressing before use. ",
        "title": "ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13926",
        "abstract_url": "http://arxiv.org/abs/2401.13926",
        "authors": [
            {
                "last_name": "\u015awirydowicz",
                "first_name": "Kasia"
            },
            {
                "last_name": "Koukpaizan",
                "first_name": "Nicholson"
            },
            {
                "last_name": "Alam",
                "first_name": "Maksudul"
            },
            {
                "last_name": "Regev",
                "first_name": "Shaked"
            },
            {
                "last_name": "Saunders",
                "first_name": "Michael"
            },
            {
                "last_name": "Pele\u0161",
                "first_name": "Slaven"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Linear solvers are major computational bottlenecks in a wide range of decision support and optimization computations. The challenges become even more pronounced on heterogeneous hardware, where traditional sparse numerical linear algebra methods are often inefficient. For example, methods for solving ill-conditioned linear systems have relied on conditional branching, which degrades performance on hardware accelerators such as graphical processing units (GPUs). To improve the efficiency of solving ill-conditioned systems, our computational strategy separates computations that are efficient on GPUs from those that need to run on traditional central processing units (CPUs). Our strategy maximizes the reuse of expensive CPU computations. Iterative methods, which thus far have not been broadly used for ill-conditioned linear systems, play an important role in our approach. In particular, we extend ideas from [1] to implement iterative refinement using inexact LU factors and flexible generalized minimal residual (FGMRES), with the aim of efficient performance on GPUs. We focus on solutions that are effective within broader application contexts, and discuss how early performance tests could be improved to be more predictive of the performance in a realistic environment ",
        "title": "Iterative Methods in GPU-Resident Linear Solvers for Nonlinear  Constrained Optimization",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13927",
        "abstract_url": "http://arxiv.org/abs/2401.13927",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Yepeng"
            },
            {
                "last_name": "Bu",
                "first_name": "Yuheng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of \\emph{un-watermarked} LLMs while maintaining security even under various attacks. ",
        "title": "Adaptive Text Watermark for Large Language Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13928",
        "abstract_url": "http://arxiv.org/abs/2401.13928",
        "authors": [
            {
                "last_name": "Jayasuriya",
                "first_name": "Namal"
            },
            {
                "last_name": "Guo",
                "first_name": "Yi"
            },
            {
                "last_name": "Hu",
                "first_name": "Wen"
            },
            {
                "last_name": "Ghannoum",
                "first_name": "Oula"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  Future food security is a major concern of the 21st century with the growing global population and climate changes. In addressing these challenges, protected cropping ensures food production year-round and increases crop production per land area by controlling environment conditions. Maintaining the growth and health of crops in these facilities is essential to ensure optimum food production. However, this is a laborious work and is currently done manually. Image-based non-destructive plant phenotyping is an emerging research area that reduces the skilled labour cost while enhancing the monitoring of crop growth, health, and identifying phenotype-genotype relations for plant breeding. With the proliferations of protected infrastructures and targeted plants, different technologies and sensor setups are needed for image-based crop monitoring. Conveyor-type plant-to-sensor systems, bench-top or gantry-based systems are commonly found in research facilities focussing on phenotyping of small, relatively short, or movable model plants. This review examines the literature on crop monitoring and phenotyping platforms in both field and protected facilities and explains different camera technologies and their ability to extract different plant traits. The review highlights the future research directions of image-based monitoring of commercial scale protected crops where crops can be relatively tall or vertically supported under semi controlled environments, which presents new challenges and is rarely covered in the literature. ",
        "title": "Image based Crop Monitoring Technologies in Protected Horticulture: A  Review",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13929",
        "abstract_url": "http://arxiv.org/abs/2401.13929",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Xingche"
            },
            {
                "last_name": "Zeng",
                "first_name": "Donglin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yuanjia"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for analyzing reward-based decision-making. Our model accommodates learning strategy switching between two distinct approaches under a hidden Markov model (HMM): subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient EM algorithm for parameter estimation and employ a nonparametric bootstrap for inference. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task. ",
        "title": "Reinforcement Learning with Hidden Markov Models for Discovering  Decision-Making Dynamics",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13931",
        "abstract_url": "http://arxiv.org/abs/2401.13931",
        "authors": [
            {
                "last_name": "Azghadi",
                "first_name": "Mostafa Rahimi"
            },
            {
                "last_name": "Olsen",
                "first_name": "Alex"
            },
            {
                "last_name": "Wood",
                "first_name": "Jake"
            },
            {
                "last_name": "Saleh",
                "first_name": "Alzayat"
            },
            {
                "last_name": "Calvert",
                "first_name": "Brendan"
            },
            {
                "last_name": "Granshaw",
                "first_name": "Terry"
            },
            {
                "last_name": "Fillols",
                "first_name": "Emilie"
            },
            {
                "last_name": "Philippa",
                "first_name": "Bronson"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Precise robotic weed control plays an essential role in precision agriculture. It can help significantly reduce the environmental impact of herbicides while reducing weed management costs for farmers. In this paper, we demonstrate that a custom-designed robotic spot spraying tool based on computer vision and deep learning can significantly reduce herbicide usage on sugarcane farms. We present results from field trials that compare robotic spot spraying against industry-standard broadcast spraying, by measuring the weed control efficacy, the reduction in herbicide usage, and the water quality improvements in irrigation runoff. The average results across 25 hectares of field trials show that spot spraying on sugarcane farms is 97% as effective as broadcast spraying and reduces herbicide usage by 35%, proportionally to the weed density. For specific trial strips with lower weed pressure, spot spraying reduced herbicide usage by up to 65%. Water quality measurements of irrigation-induced runoff, three to six days after spraying, showed reductions in the mean concentration and mean load of herbicides of 39% and 54%, respectively, compared to broadcast spraying. These promising results reveal the capability of spot spraying technology to reduce herbicide usage on sugarcane farms without impacting weed control and potentially providing sustained water quality benefits. ",
        "title": "Precise Robotic Weed Spot-Spraying for Reduced Herbicide Usage and  Improved Environmental Outcomes -- A Real-World Case Study",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13934",
        "abstract_url": "http://arxiv.org/abs/2401.13934",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Tao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yinuo"
            },
            {
                "last_name": "Meng",
                "first_name": "Cai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deformable image registration is an essential approach for medical image analysis.This paper introduces MambaMorph, an innovative multi-modality deformable registration network, specifically designed for Magnetic Resonance (MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with its Mamba-based registration module and a contrastive feature learning approach, addressing the prevalent challenges in multi-modality registration. The network leverages Mamba blocks for efficient long-range modeling and high-dimensional data processing, coupled with a feature extractor that learns fine-grained features for enhanced registration accuracy. Experimental results showcase MambaMorph's superior performance over existing methods in MR-CT registration, underlining its potential in clinical applications. This work underscores the significance of feature learning in multi-modality registration and positions MambaMorph as a trailblazing solution in this field. The code for MambaMorph is available at: https://github.com/Guo-Stone/MambaMorph. ",
        "title": "MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for  Deformable MR-CT Registration",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13935",
        "abstract_url": "http://arxiv.org/abs/2401.13935",
        "authors": [
            {
                "last_name": "Bynum",
                "first_name": "Lucius E. J."
            },
            {
                "last_name": "Loftus",
                "first_name": "Joshua R."
            },
            {
                "last_name": "Stoyanovich",
                "first_name": "Julia"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  Counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions -- like interventions on race -- may not translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions. ",
        "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13936",
        "abstract_url": "http://arxiv.org/abs/2401.13936",
        "authors": [
            {
                "last_name": "Yun",
                "first_name": "Sinwoong"
            },
            {
                "last_name": "Kim",
                "first_name": "Dongsun"
            },
            {
                "last_name": "Park",
                "first_name": "Chanwon"
            },
            {
                "last_name": "Lee",
                "first_name": "Jemin"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  As the demand on artificial intelligence (AI)-based applications increases, the freshness of sensed data becomes crucial in the wireless sensor networks. Since those applications require a large amount of computation for processing the sensed data, it is essential to offload the computation load to the edge computing (EC) server. In this paper, we propose the sensing and computing decision (SCD) algorithms for data freshness in the EC-enabled wireless sensor networks. We define the {\\eta}-coverage probability to show the probability of maintaining fresh data for more than {\\eta} ratio of the network, where the spatial-temporal correlation of information is considered. We then propose the probability-based SCD for the single pre-charged sensor case with providing the optimal point after deriving the {\\eta}-coverage probability. We also propose the reinforcement learning (RL)- based SCD by training the SCD policy of sensors for both the single pre-charged and multiple energy harvesting (EH) sensor cases, to make a real-time decision based on its observation. Our simulation results verify the performance of the proposed algorithms under various environment settings, and show that the RL-based SCD algorithm achieves higher performance compared to baseline algorithms for both the single pre-charged sensor and multiple EH sensor cases. ",
        "title": "Learning-based sensing and computing decision for data freshness in edge  computing-enabled networks",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13937",
        "abstract_url": "http://arxiv.org/abs/2401.13937",
        "authors": [
            {
                "last_name": "Truong",
                "first_name": "Quang-Trung"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Duc Thanh"
            },
            {
                "last_name": "Hua",
                "first_name": "Binh-Son"
            },
            {
                "last_name": "Yeung",
                "first_name": "Sai-Kit"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage. ",
        "title": "Self-supervised Video Object Segmentation with Distillation Learning of  Deformable Attention",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13940",
        "abstract_url": "http://arxiv.org/abs/2401.13940",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yuxia"
            },
            {
                "last_name": "Qin",
                "first_name": "Mian"
            },
            {
                "last_name": "Stol",
                "first_name": "Klaas-Jan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Minghui"
            },
            {
                "last_name": "Liu",
                "first_name": "Hui"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  It is now commonplace for organizations to pay developers to work on specific open source software (OSS) projects to pursue their business goals. Such paid developers work alongside voluntary contributors, but given the different motivations of these two groups of developers, conflict may arise, which may pose a threat to a project's sustainability. This paper presents an empirical study of paid developers and volunteers in Rust, a popular open source programming language project. Rust is a particularly interesting case given considerable concerns about corporate participation. We compare volunteers and paid developers through contribution characteristics and long-term participation, and solicit volunteers' perceptions on paid developers. We find that core paid developers tend to contribute more frequently; commits contributed by one-time paid developers have bigger sizes; peripheral paid developers implement more features; and being paid plays a positive role in becoming a long-term contributor. We also find that volunteers do have some prejudices against paid developers. This study suggests that the dichotomous view of paid vs. volunteer developers is too simplistic and that further subgroups can be identified. Companies should become more sensitive to how they engage with OSS communities, in certain ways as suggested by this study. ",
        "title": "How Are Paid and Volunteer Open Source Developers Different? A Study of  the Rust Project",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13941",
        "abstract_url": "http://arxiv.org/abs/2401.13941",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Quan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Xuanyi"
            },
            {
                "last_name": "Li",
                "first_name": "Dannuo"
            },
            {
                "last_name": "Yeow",
                "first_name": "Raye Chen-Hua"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Soft electrohydraulic actuators known as HASEL actuators have attracted widespread research interest due to their outstanding dynamic performance and high output power. However, the displacement of electrohydraulic actuators usually declines with time under constant DC voltage, which hampers its prospective application. A mathematical model is firstly established to not only explain the decrease in displacement under DC voltage but also predict the relatively stable displacement with oscillation under AC square wave voltage. The mathematical model is validated since the actual displacement confirms the trend observed by our model. To smooth the displacement oscillation introduced by AC voltage, a serial elastic component is incorporated to form a SE-HASEL actuator. A feedback control with a proportion-integration algorithm enables the SE-HASEL actuator to eliminate the obstinate displacement hysteresis. Our results revealed that, through our methodology, the SE-HASEL actuator can give stable and smooth displacement and is capable of absorbing external impact disturbance simultaneously. A rotary joint based on the SE-HASEL actuator is developed to reflect its possibility to generate a common rotary motion for wide robotic applications. More importantly, this paper also proposes a highly accurate needle biopsy robot that can be utilized in MRI-guide surgical procedures. Overall, we have achieved AC-driven series elastic electrohydraulic actuators that can exhibit stable and smooth displacement output. ",
        "title": "AC-Driven Series Elastic Electrohydraulic Actuator for Stable and Smooth  Displacement Output",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13942",
        "abstract_url": "http://arxiv.org/abs/2401.13942",
        "authors": [
            {
                "last_name": "Bai",
                "first_name": "Yalong"
            },
            {
                "last_name": "Zhou",
                "first_name": "Mohan"
            },
            {
                "last_name": "Yang",
                "first_name": "Qing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The ability to fine-tune generative models for text-to-image generation tasks is crucial, particularly facing the complexity involved in accurately interpreting and visualizing textual inputs. While LoRA is efficient for language model adaptation, it often falls short in text-to-image tasks due to the intricate demands of image generation, such as accommodating a broad spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a specialized fine-tuning approach tailored for text-to-image models. StyleInject comprises multiple parallel low-rank parameter matrices, maintaining the diversity of visual features. It dynamically adapts to varying styles by adjusting the variance of visual features based on the characteristics of the input signal. This approach significantly minimizes the impact on the original model's text-image alignment capabilities while adeptly adapting to various styles in transfer learning. StyleInject proves particularly effective in learning from and enhancing a range of advanced, community-fine-tuned generative models. Our comprehensive experiments, including both small-sample and large-scale data fine-tuning as well as base model distillation, show that StyleInject surpasses traditional LoRA in both text-image semantic consistency and human preference evaluation, all while ensuring greater parameter efficiency. ",
        "title": "StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion  Models",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13945",
        "abstract_url": "http://arxiv.org/abs/2401.13945",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Tong"
            },
            {
                "last_name": "Huang",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Du",
                "first_name": "Yu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Weihao"
            },
            {
                "last_name": "Shi",
                "first_name": "Luping"
            },
            {
                "last_name": "Zhao",
                "first_name": "Rong"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY",
            "CE",
            "MA"
        ],
        "abstract": "  Given the escalating intricacy and multifaceted nature of contemporary social systems, manually generating solutions to address pertinent social issues has become a formidable task. In response to this challenge, the rapid development of artificial intelligence has spurred the exploration of computational methodologies aimed at automatically generating solutions. However, current methods for auto-generation of solutions mainly concentrate on local social regulations that pertain to specific scenarios. Here, we report an automatic social operating system (ASOS) designed for general social solution generation, which is built upon agent-based models, enabling both global and local analyses and regulations of social problems across spatial and temporal dimensions. ASOS adopts a hypergraph with extensible social semantics for a comprehensive and structured representation of social dynamics. It also incorporates a generalized protocol for standardized hypergraph operations and a symbolic hybrid framework that delivers interpretable solutions, yielding a balance between regulatory efficacy and function viability. To demonstrate the effectiveness of ASOS, we apply it to the domain of averting extreme events within international oil futures markets. By generating a new trading role supplemented by new mechanisms, ASOS can adeptly discern precarious market conditions and make front-running interventions for non-profit purposes. This study demonstrates that ASOS provides an efficient and systematic approach for generating solutions for enhancing our society. ",
        "title": "General Automatic Solution Generation of Social Problems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13947",
        "abstract_url": "http://arxiv.org/abs/2401.13947",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "Chen"
            },
            {
                "last_name": "Liu",
                "first_name": "Andrew L."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "MA"
        ],
        "abstract": "  Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations. ",
        "title": "Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy  Trading",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13950",
        "abstract_url": "http://arxiv.org/abs/2401.13950",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Vitaliy"
            },
            {
                "last_name": "Jung",
                "first_name": "Gunho"
            },
            {
                "last_name": "Lee",
                "first_name": "Seong-Whan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Many multi-object tracking (MOT) approaches, which employ the Kalman Filter as a motion predictor, assume constant velocity and Gaussian-distributed filtering noises. These assumptions render the Kalman Filter-based trackers effective in linear motion scenarios. However, these linear assumptions serve as a key limitation when estimating future object locations within scenarios involving non-linear motion and occlusions. To address this issue, we propose a motion-based MOT approach with an adaptable motion predictor, called AM-SORT, which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension of the SORT-series trackers that supersedes the Kalman Filter with the transformer architecture as a motion predictor. We introduce a historical trajectory embedding that empowers the transformer to extract spatio-temporal features from a sequence of bounding boxes. AM-SORT achieves competitive performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1 and 55.6 HOTA. We conduct extensive experiments to demonstrate the effectiveness of our method in predicting non-linear movement under occlusions. ",
        "title": "AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding  for Multi-Object Tracking",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13952",
        "abstract_url": "http://arxiv.org/abs/2401.13952",
        "authors": [
            {
                "last_name": "Pan",
                "first_name": "Mingen"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  An algorithm is developed to gradually relax the Differential Privacy (DP) guarantee of a randomized response. The output from each relaxation maintains the same probability distribution as a standard randomized response with the equivalent DP guarantee, ensuring identical utility as the standard approach. The entire relaxation process is proven to have the same DP guarantee as the most recent relaxed guarantee.   The DP relaxation algorithm is adaptable to any Local Differential Privacy (LDP) mechanisms relying on randomized response. It has been seamlessly integrated into RAPPOR, an LDP crowdsourcing string-collecting tool, to optimize the utility of estimating the frequency of collected data. Additionally, it facilitates the relaxation of the DP guarantee for mean estimation based on randomized response. Finally, numerical experiments have been conducted to validate the utility and DP guarantee of the algorithm. ",
        "title": "Randomized Response with Gradual Release of Privacy Budget",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13956",
        "abstract_url": "http://arxiv.org/abs/2401.13956",
        "authors": [
            {
                "last_name": "Ma",
                "first_name": "Xuanchao"
            },
            {
                "last_name": "Wu",
                "first_name": "Zehan"
            },
            {
                "last_name": "Liu",
                "first_name": "Hongyan"
            },
            {
                "last_name": "Zhou",
                "first_name": "Chengxu"
            },
            {
                "last_name": "Gu",
                "first_name": "Ke"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent years have witnessed a broader range of applications of image processing technologies in multiple industrial processes, such as smoke detection, security monitoring, and workpiece inspection. Different kinds of distortion types and levels must be introduced into an image during the processes of acquisition, compression, transmission, storage, and display, which might heavily degrade the image quality and thus strongly reduce the final display effect and clarity. To verify the reliability of existing image quality assessment methods, we establish a new industrial process image database (IPID), which contains 3000 distorted images generated by applying different levels of distortion types to each of the 50 source images. We conduct the subjective test on the aforementioned 3000 images to collect their subjective quality ratings in a well-suited laboratory environment. Finally, we perform comparison experiments on IPID database to investigate the performance of some objective image quality assessment algorithms. The experimental results show that the state-of-the-art image quality assessment methods have difficulty in predicting the quality of images that contain multiple distortion types. ",
        "title": "A New Image Quality Database for Multiple Industrial Processes",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13957",
        "abstract_url": "http://arxiv.org/abs/2401.13957",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Tangyou"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiaoyi"
            },
            {
                "last_name": "Katupitiya",
                "first_name": "Jay"
            },
            {
                "last_name": "Wang",
                "first_name": "Jiaole"
            },
            {
                "last_name": "Wu",
                "first_name": "Liao"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "HC"
        ],
        "abstract": "  A common limitation of autonomous tissue manipulation in robotic minimally invasive surgery (MIS) is the absence of force sensing and control at the tool level. Recently, our team has developed haptics-enabled forceps that can simultaneously measure the grasping and pulling forces during tissue manipulation. Based on this design, here we further present a method to automate tissue traction with controlled grasping and pulling forces. Specifically, the grasping stage relies on a controlled grasping force, while the pulling stage is under the guidance of a controlled pulling force. Notably, during the pulling process, the simultaneous control of both grasping and pulling forces is also enabled for more precise tissue traction, achieved through force decoupling. The force controller is built upon a static model of tissue manipulation, considering the interaction between the haptics-enabled forceps and soft tissue. The efficacy of this force control approach is validated through a series of experiments comparing targeted, estimated, and actual reference forces. To verify the feasibility of the proposed method in surgical applications, various tissue resections are conducted on ex vivo tissues employing a dual-arm robotic setup. Finally, we discuss the benefits of multi-force control in tissue traction, evidenced through comparative analyses of various ex vivo tissue resections. The results affirm the feasibility of implementing automatic tissue traction using micro-sized forceps with multi-force control, suggesting its potential to promote autonomous MIS. A video demonstrating the experiments can be found at https://youtu.be/8fe8o8IFrjE. ",
        "title": "Automatic Tissue Traction with Haptics-Enabled Forceps for Minimally  Invasive Surgery",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13959",
        "abstract_url": "http://arxiv.org/abs/2401.13959",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Henan"
            },
            {
                "last_name": "Pan",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Feng",
                "first_name": "Runsen"
            },
            {
                "last_name": "Guo",
                "first_name": "Zongyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhibo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This document is an expanded version of a one-page abstract originally presented at the 2024 Data Compression Conference. It describes our proposed method for the video track of the Challenge on Learned Image Compression (CLIC) 2024. Our scheme follows the typical hybrid coding framework with some novel techniques. Firstly, we adopt Spynet network to produce accurate motion vectors for motion estimation. Secondly, we introduce the context mining scheme with conditional frame coding to fully exploit the spatial-temporal information. As for the low target bitrates given by CLIC, we integrate spatial-temporal super-resolution modules to improve rate-distortion performance. Our team name is IMCLVC. ",
        "title": "Conditional Neural Video Coding with Spatial-Temporal Super-Resolution",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13961",
        "abstract_url": "http://arxiv.org/abs/2401.13961",
        "authors": [
            {
                "last_name": "Wan",
                "first_name": "Jia"
            },
            {
                "last_name": "Li",
                "first_name": "Wanhua"
            },
            {
                "last_name": "Banerjee",
                "first_name": "Atmadeep"
            },
            {
                "last_name": "Adhinarta",
                "first_name": "Jason Ken"
            },
            {
                "last_name": "Sjostedt",
                "first_name": "Evelina"
            },
            {
                "last_name": "Wu",
                "first_name": "Jingpeng"
            },
            {
                "last_name": "Lichtman",
                "first_name": "Jeff"
            },
            {
                "last_name": "Pfister",
                "first_name": "Hanspeter"
            },
            {
                "last_name": "Wei",
                "first_name": "Donglai"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this paper, we address a significant gap in the field of neuroimaging by introducing the largest-to-date public benchmark, BvEM, designed specifically for cortical blood vessel segmentation in Volume Electron Microscopy (VEM) images. The intricate relationship between cerebral blood vessels and neural function underscores the vital role of vascular analysis in understanding brain health. While imaging techniques at macro and mesoscales have garnered substantial attention and resources, the microscale VEM imaging, capable of revealing intricate vascular details, has lacked the necessary benchmarking infrastructure. As researchers delve deeper into the microscale intricacies of cerebral vasculature, our BvEM benchmark represents a critical step toward unraveling the mysteries of neurovascular coupling and its impact on brain function and pathology. The BvEM dataset is based on VEM image volumes from three mammal species: adult mouse, macaque, and human. We standardized the resolution, addressed imaging variations, and meticulously annotated blood vessels through semi-automatic, manual, and quality control processes, ensuring high-quality 3D segmentation. Furthermore, we developed a zero-shot cortical blood vessel segmentation method named TriSAM, which leverages the powerful segmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to 3D volume segmentation, TriSAM employs a multi-seed tracking framework, leveraging the reliability of certain image planes for tracking while using others to identify potential turning points. This approach, consisting of Tri-Plane selection, SAM-based tracking, and recursive redirection, effectively achieves long-term 3D blood vessel segmentation without model training or fine-tuning. Experimental results show that TriSAM achieved superior performances on the BvEM benchmark across three species. ",
        "title": "TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation  in VEM images",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13964",
        "abstract_url": "http://arxiv.org/abs/2401.13964",
        "authors": [
            {
                "last_name": "Lu",
                "first_name": "Yifan"
            },
            {
                "last_name": "Hu",
                "first_name": "Yue"
            },
            {
                "last_name": "Zhong",
                "first_name": "Yiqi"
            },
            {
                "last_name": "Wang",
                "first_name": "Dequan"
            },
            {
                "last_name": "Chen",
                "first_name": "Siheng"
            },
            {
                "last_name": "Wang",
                "first_name": "Yanfeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. It also protects new agents' model details from disclosure since the training can be conducted by the agent owner locally. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types. Code and data are available at: https://github.com/yifanlu0227/HEAL. ",
        "title": "An Extensible Framework for Open Heterogeneous Collaborative Perception",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13965",
        "abstract_url": "http://arxiv.org/abs/2401.13965",
        "authors": [
            {
                "last_name": "Khan",
                "first_name": "Adnan"
            },
            {
                "last_name": "Shaaban",
                "first_name": "Mai A."
            },
            {
                "last_name": "Khan",
                "first_name": "Muhammad Haris"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Beyond attaining domain generalization (DG), visual recognition models should also be data-efficient during learning by leveraging limited labels. We study the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial for real-world applications like automated healthcare. SSDG requires learning a cross-domain generalizable model when the given training data is only partially labelled. Empirical investigations reveal that the DG methods tend to underperform in SSDG settings, likely because they are unable to exploit the unlabelled data. Semi-supervised learning (SSL) shows improved but still inferior results compared to fully-supervised learning. A key challenge, faced by the best-performing SSL-based SSDG methods, is selecting accurate pseudo-labels under multiple domain shifts and reducing overfitting to source domains under limited labels. In this work, we propose new SSDG approach, which utilizes a novel uncertainty-guided pseudo-labelling with model averaging (UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to improve pseudo-labelling selection, addressing poor model calibration under multi-source unlabelled data. The UPL technique, enhanced by our novel model averaging (MA) strategy, mitigates overfitting to source domains with limited labels. Extensive experiments on key representative DG datasets suggest that our method demonstrates effectiveness against existing methods. Our code and chosen labelled data seeds are available on GitHub: https://github.com/Adnan-Khan7/UPLM ",
        "title": "Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised  Domain Generalization",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13967",
        "abstract_url": "http://arxiv.org/abs/2401.13967",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Nianxiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Junxi"
            },
            {
                "last_name": "Wang",
                "first_name": "Huairui"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhenzhong"
            }
        ],
        "primary_category": "MM",
        "categories": [
            "MM"
        ],
        "abstract": "  In this paper, we extend our prior research named DKIC and propose the perceptual-oriented learned image compression method, PO-DKIC. Specifically, DKIC adopts a dynamic kernel-based dynamic residual block group to enhance the transform coding and an asymmetric space-channel context entropy model to facilitate the estimation of gaussian parameters. Based on DKIC, PO-DKIC introduces PatchGAN and LPIPS loss to enhance visual quality. Furthermore, to maximize the overall perceptual quality under a rate constraint, we formulate this challenge into a constrained programming problem and use the Linear Integer Programming method for resolution. The experiments demonstrate that our proposed method can generate realistic images with richer textures and finer details when compared to state-of-the-art image compression techniques. ",
        "title": "Perceptual-oriented Learned Image Compression with Dynamic Kernel",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13968",
        "abstract_url": "http://arxiv.org/abs/2401.13968",
        "authors": [
            {
                "last_name": "Ma'sum",
                "first_name": "Muhammad Anwar"
            },
            {
                "last_name": "Sarkar",
                "first_name": "MD Rasel"
            },
            {
                "last_name": "Pratama",
                "first_name": "Mahardhika"
            },
            {
                "last_name": "Ramasamy",
                "first_name": "Savitha"
            },
            {
                "last_name": "Anavatti",
                "first_name": "Sreenatha"
            },
            {
                "last_name": "Liu",
                "first_name": "Lin"
            },
            {
                "last_name": "Habibullah",
                "first_name": ""
            },
            {
                "last_name": "Kowalczyk",
                "first_name": "Ryszard"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANTRA are publicly available in \\url{https://github.com/anwarmaxsum/MANTRA}. ",
        "title": "Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13970",
        "abstract_url": "http://arxiv.org/abs/2401.13970",
        "authors": [
            {
                "last_name": "Desai",
                "first_name": "Smit"
            },
            {
                "last_name": "Wei",
                "first_name": "Christina"
            },
            {
                "last_name": "Sin",
                "first_name": "Jaisie"
            },
            {
                "last_name": "Dubiel",
                "first_name": "Mateusz"
            },
            {
                "last_name": "Zargham",
                "first_name": "Nima"
            },
            {
                "last_name": "Ahire",
                "first_name": "Shashank"
            },
            {
                "last_name": "Porcheron",
                "first_name": "Martin"
            },
            {
                "last_name": "Kuzminykh",
                "first_name": "Anastasia"
            },
            {
                "last_name": "Lee",
                "first_name": "Minha"
            },
            {
                "last_name": "Candello",
                "first_name": "Heloisa"
            },
            {
                "last_name": "Fischer",
                "first_name": "Joel"
            },
            {
                "last_name": "Munteanu",
                "first_name": "Cosmin"
            },
            {
                "last_name": "Cowan",
                "first_name": "Benjamin R"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Conversational user interfaces (CUIs) have become an everyday technology for people the world over, as well as a booming area of research. Advances in voice synthesis and the emergence of chatbots powered by large language models (LLMs), notably ChatGPT, have pushed CUIs to the forefront of human-computer interaction (HCI) research and practice. Now that these technologies enable an elemental level of usability and user experience (UX), we must turn our attention to higher-order human factors: trust and reliance. In this workshop, we aim to bring together a multidisciplinary group of researchers and practitioners invested in the next phase of CUI design. Through keynotes, presentations, and breakout sessions, we will share our knowledge, identify cutting-edge resources, and fortify an international network of CUI scholars. In particular, we will engage with the complexity of trust and reliance as attitudes and behaviours that emerge when people interact with conversational agents. ",
        "title": "CUI@CHI 2024: Building Trust in CUIs-From Design to Deployment",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13971",
        "abstract_url": "http://arxiv.org/abs/2401.13971",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Wenzhi"
            },
            {
                "last_name": "Deng",
                "first_name": "Qi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper considers stochastic weakly convex optimization without the standard Lipschitz continuity assumption. Based on new adaptive regularization (stepsize) strategies, we show that a wide class of stochastic algorithms, including the stochastic subgradient method, preserve the $\\mathcal{O} ( 1 / \\sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on rather weak assumptions: the Lipschitz parameter can be either bounded by a general growth function of $\\|x\\|$ or locally estimated through independent random samples. ",
        "title": "Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13973",
        "abstract_url": "http://arxiv.org/abs/2401.13973",
        "authors": [
            {
                "last_name": "Miyajima",
                "first_name": "Ken"
            },
            {
                "last_name": "Yamada",
                "first_name": "Takayuki"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  In this study, we proposed a design methodology for a piezoelectric energy-harvesting device optimized for maximal power generation at a designated frequency using topology optimization. The proposed methodology emphasizes the design of a unimorph-type piezoelectric energy harvester, wherein a piezoelectric film is affixed to a singular side of a silicon cantilever beam. Both the substrate and the piezoelectric film components underwent concurrent optimization. Constraints were imposed to ensure that the resultant design is amenable to microfabrication, with specific emphasis on the etchability of piezoelectric energy harvesters. Several numerical examples were provided to validate the efficacy of the proposed method. The results showed that the proposed method derives both the substrate and piezoelectric designs that maximize the electromechanical coupling coefficient and allows the eigenfrequency of the device and minimum output voltage to be set to the desired values. Furthermore, the proposed method can provide solutions that satisfy the cross-sectional shape, substrate-depend, and minimum output voltage constraints. The solutions obtained by the proposed method are manufacturable in the field of microfabrication. ",
        "title": "Optimal design of unimorph-type cantilevered piezoelectric energy  harvesters using level set-based topology optimization by considering  manufacturability",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13974",
        "abstract_url": "http://arxiv.org/abs/2401.13974",
        "authors": [
            {
                "last_name": "Purushwalkam",
                "first_name": "Senthil"
            },
            {
                "last_name": "Gokul",
                "first_name": "Akash"
            },
            {
                "last_name": "Joty",
                "first_name": "Shafiq"
            },
            {
                "last_name": "Naik",
                "first_name": "Nikhil"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "GR"
        ],
        "abstract": "  Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.   The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts. ",
        "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation  Capabilities in Pretrained Diffusion Models",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13976",
        "abstract_url": "http://arxiv.org/abs/2401.13976",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Wei"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuqi"
            },
            {
                "last_name": "Ma",
                "first_name": "De"
            },
            {
                "last_name": "Zheng",
                "first_name": "Qian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superiority of our method over state-of-the-art methods.Code is available at https://github.com/SnailForce/SIM-Net. ",
        "title": "Learning to Manipulate Artistic Images",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13977",
        "abstract_url": "http://arxiv.org/abs/2401.13977",
        "authors": [
            {
                "last_name": "Ghosh",
                "first_name": "Tanmay"
            },
            {
                "last_name": "Nagaraj",
                "first_name": "Nithin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The decision making involved behind the mode choice is critical for transportation planning. While statistical learning techniques like discrete choice models have been used traditionally, machine learning (ML) models have gained traction recently among the transportation planners due to their higher predictive performance. However, the black box nature of ML models pose significant interpretability challenges, limiting their practical application in decision and policy making. This study utilised a dataset of $1350$ households belonging to low and low-middle income bracket in the city of Bengaluru to investigate mode choice decision making behaviour using Multinomial logit model and ML classifiers like decision trees, random forests, extreme gradient boosting and support vector machines. In terms of accuracy, random forest model performed the best ($0.788$ on training data and $0.605$ on testing data) compared to all the other models. This research has adopted modern interpretability techniques like feature importance and individual conditional expectation plots to explain the decision making behaviour using ML models. A higher travel costs significantly reduce the predicted probability of bus usage compared to other modes (a $0.66\\%$ and $0.34\\%$ reduction using Random Forests and XGBoost model for $10\\%$ increase in travel cost). However, reducing travel time by $10\\%$ increases the preference for the metro ($0.16\\%$ in Random Forests and 0.42% in XGBoost). This research augments the ongoing research on mode choice analysis using machine learning techniques, which would help in improving the understanding of the performance of these models with real-world data in terms of both accuracy and interpretability. ",
        "title": "Evaluating the Determinants of Mode Choice Using Statistical and Machine  Learning Techniques in the Indian Megacity of Bengaluru",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13979",
        "abstract_url": "http://arxiv.org/abs/2401.13979",
        "authors": [
            {
                "last_name": "Mohammadshahi",
                "first_name": "Alireza"
            },
            {
                "last_name": "Shaikh",
                "first_name": "Ali"
            },
            {
                "last_name": "Yazdani",
                "first_name": "Majid"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator nearly matches GPT4's performance at half the cost and even exceeds GPT4's results with a 25% cost reduction. These findings illustrate the potential of our architecture in creating state-of-the-art and cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve superior performance outcomes. ",
        "title": "Leeroo Orchestrator: Elevating LLMs Performance Through Model  Integration",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13980",
        "abstract_url": "http://arxiv.org/abs/2401.13980",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Weixuan"
            },
            {
                "last_name": "Shao",
                "first_name": "Shuo"
            },
            {
                "last_name": "Yang",
                "first_name": "Qianqian"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhaoyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ping"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper addresses the challenge of achieving information-theoretic security in semantic communication (SeCom) over a wiretap channel, where a legitimate receiver coexists with an eavesdropper experiencing a poorer channel condition. Despite previous efforts to secure SeCom against eavesdroppers, achieving information-theoretic security in such schemes remains an open issue. In this work, we propose a secure digital SeCom approach based on superposition codes, aiming to attain nearly information-theoretic security. Our proposed method involves associating semantic information with satellite constellation points within a double-layered constellation map, where cloud center constellation points are randomly selected. By carefully allocating power between these two layers of constellation, we ensure that the symbol error probability (SEP) of the eavesdropper decoding satellite constellation points is nearly equivalent to random guessing, while maintaining a low SEP for the legitimate receiver to successfully decode the semantic information. Simulation results showcase that the Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) for the eavesdropper's reconstructed data, using our proposed method, can range from decoding Gaussian-distributed random noise to approaching the variance of the data. This validates the ability of our method to achieve nearly information-theoretic security, demonstrating superior data security compared to benchmark methods. ",
        "title": "A Nearly Information Theoretically Secure Approach for Semantic  Communications over Wiretap Channel",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13985",
        "abstract_url": "http://arxiv.org/abs/2401.13985",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Yuwen"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present new convergence estimates of generalized empirical interpolation methods in terms of the entropy numbers of the parametrized function class. Our analysis is transparent and leads to sharper convergence rates than the classical analysis via the Kolmogorov n-width. In addition, we also derive novel entropy-based convergence estimates of the Chebyshev greedy algorithm for sparse n-term nonlinear approximation of a target function. This also improves classical convergence analysis when corresponding entropy numbers decay fast enough. ",
        "title": "A new analysis of empirical interpolation methods and Chebyshev greedy  algorithms",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13986",
        "abstract_url": "http://arxiv.org/abs/2401.13986",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Yanda"
            },
            {
                "last_name": "Singh",
                "first_name": "Chandan"
            },
            {
                "last_name": "Liu",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Zuo",
                "first_name": "Simiao"
            },
            {
                "last_name": "Yu",
                "first_name": "Bin"
            },
            {
                "last_name": "He",
                "first_name": "He"
            },
            {
                "last_name": "Gao",
                "first_name": "Jianfeng"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation \"all birds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile answer \"no\" to the related question \"Can penguins fly?\". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out-of-distribution datasets not seen during finetuning (+4.5% relative). Code is available at https://github.com/yandachen/explanation-consistency-finetuning . ",
        "title": "Towards Consistent Natural-Language Explanations via  Explanation-Consistency Finetuning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13987",
        "abstract_url": "http://arxiv.org/abs/2401.13987",
        "authors": [
            {
                "last_name": "Paeedeh",
                "first_name": "Naeem"
            },
            {
                "last_name": "Pratama",
                "first_name": "Mahardhika"
            },
            {
                "last_name": "Ma'sum",
                "first_name": "Muhammad Anwar"
            },
            {
                "last_name": "Mayer",
                "first_name": "Wolfgang"
            },
            {
                "last_name": "Cao",
                "first_name": "Zehong"
            },
            {
                "last_name": "Kowlczyk",
                "first_name": "Ryszard"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins. ",
        "title": "Cross-Domain Few-Shot Learning via Adaptive Transformer Networks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13990",
        "abstract_url": "http://arxiv.org/abs/2401.13990",
        "authors": [
            {
                "last_name": "Shoaib",
                "first_name": "Mohamed R."
            },
            {
                "last_name": "Emara",
                "first_name": "Heba M."
            },
            {
                "last_name": "Zhao",
                "first_name": "Jun"
            },
            {
                "last_name": "El-Shafai",
                "first_name": "Walid"
            },
            {
                "last_name": "Soliman",
                "first_name": "Naglaa F."
            },
            {
                "last_name": "Mubarak",
                "first_name": "Ahmed S."
            },
            {
                "last_name": "Omer",
                "first_name": "Osama A."
            },
            {
                "last_name": "El-Samie",
                "first_name": "Fathi E. Abd"
            },
            {
                "last_name": "Esmaiel",
                "first_name": "Hamada"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Diabetic retinopathy (DR) is a significant cause of vision impairment, emphasizing the critical need for early detection and timely intervention to avert visual deterioration. Diagnosing DR is inherently complex, as it necessitates the meticulous examination of intricate retinal images by experienced specialists. This makes the early diagnosis of DR essential for effective treatment and the prevention of eventual blindness. Traditional diagnostic methods, relying on human interpretation of these medical images, face challenges in terms of accuracy and efficiency. In the present research, we introduce a novel method that offers superior precision in DR diagnosis, compared to these traditional methods, by employing advanced deep learning techniques. Central to this approach is the concept of transfer learning. This entails using pre-existing, well-established models, specifically InceptionResNetv2 and Inceptionv3, to extract features and fine-tune select layers to cater to the unique requirements of this specific diagnostic task. Concurrently, we also present a newly devised model, DiaCNN, which is tailored for the classification of eye diseases. To validate the efficacy of the proposed methodology, we leveraged the Ocular Disease Intelligent Recognition (ODIR) dataset, which comprises eight different eye disease categories. The results were promising. The InceptionResNetv2 model, incorporating transfer learning, registered an impressive 97.5% accuracy in both the training and testing phases. Its counterpart, the Inceptionv3 model, achieved an even more commendable 99.7% accuracy during training, and 97.5% during testing. Remarkably, the DiaCNN model showcased unparalleled precision, achieving 100% accuracy in training and 98.3\\% in testing. ",
        "title": "Deep Learning Innovations in Diagnosing Diabetic Retinopathy: The  Potential of Transfer Learning and the DiaCNN Model",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13992",
        "abstract_url": "http://arxiv.org/abs/2401.13992",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zhen"
            },
            {
                "last_name": "Li",
                "first_name": "Yuelei"
            },
            {
                "last_name": "Wan",
                "first_name": "Jia"
            },
            {
                "last_name": "Vasconcelos",
                "first_name": "Nuno"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Crowd counting is an important problem in computer vision due to its wide range of applications in image understanding. Currently, this problem is typically addressed using deep learning approaches, such as Convolutional Neural Networks (CNNs) and Transformers. However, deep networks are data-driven and are prone to overfitting, especially when the available labeled crowd dataset is limited. To overcome this limitation, we have designed a pipeline that utilizes a diffusion model to generate extensive training data. We are the first to generate images conditioned on a location dot map (a binary dot map that specifies the location of human heads) with a diffusion model. We are also the first to use these diverse synthetic data to augment the crowd counting models. Our proposed smoothed density map input for ControlNet significantly improves ControlNet's performance in generating crowds in the correct locations. Also, Our proposed counting loss for the diffusion model effectively minimizes the discrepancies between the location dot map and the crowd images generated. Additionally, our innovative guidance sampling further directs the diffusion process toward regions where the generated crowd images align most accurately with the location dot map. Collectively, we have enhanced ControlNet's ability to generate specified objects from a location dot map, which can be used for data augmentation in various counting problems. Moreover, our framework is versatile and can be easily adapted to all kinds of counting problems. Extensive experiments demonstrate that our framework improves the counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS datasets, showcasing its effectiveness. ",
        "title": "Diffusion-based Data Augmentation for Object Counting Problems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13996",
        "abstract_url": "http://arxiv.org/abs/2401.13996",
        "authors": [
            {
                "last_name": "Qian",
                "first_name": "Cheng"
            },
            {
                "last_name": "Liang",
                "first_name": "Shihao"
            },
            {
                "last_name": "Qin",
                "first_name": "Yujia"
            },
            {
                "last_name": "Ye",
                "first_name": "Yining"
            },
            {
                "last_name": "Cong",
                "first_name": "Xin"
            },
            {
                "last_name": "Lin",
                "first_name": "Yankai"
            },
            {
                "last_name": "Wu",
                "first_name": "Yesai"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhiyuan"
            },
            {
                "last_name": "Sun",
                "first_name": "Maosong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full autonomy. ",
        "title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent  Self-Evolution",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.13998",
        "abstract_url": "http://arxiv.org/abs/2401.13998",
        "authors": [
            {
                "last_name": "Gan",
                "first_name": "Haitao"
            },
            {
                "last_name": "Fu",
                "first_name": "Lingchao"
            },
            {
                "last_name": "Zhou",
                "first_name": "Ran"
            },
            {
                "last_name": "Gan",
                "first_name": "Weiyan"
            },
            {
                "last_name": "Wang",
                "first_name": "Furong"
            },
            {
                "last_name": "Wu",
                "first_name": "Xiaoyan"
            },
            {
                "last_name": "Yang",
                "first_name": "Zhi"
            },
            {
                "last_name": "Huang",
                "first_name": "Zhongwei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  The classification of carotid artery ultrasound images is a crucial means for diagnosing carotid plaques, holding significant clinical relevance for predicting the risk of stroke. Recent research suggests that utilizing plaque segmentation as an auxiliary task for classification can enhance performance by leveraging the correlation between segmentation and classification tasks. However, this approach relies on obtaining a substantial amount of challenging-to-acquire segmentation annotations. This paper proposes a novel weakly supervised auxiliary task learning network model (WAL-Net) to explore the interdependence between carotid plaque classification and segmentation tasks. The plaque classification task is primary task, while the plaque segmentation task serves as an auxiliary task, providing valuable information to enhance the performance of the primary task. Weakly supervised learning is adopted in the auxiliary task to completely break away from the dependence on segmentation annotations. Experiments and evaluations are conducted on a dataset comprising 1270 carotid plaque ultrasound images from Wuhan University Zhongnan Hospital. Results indicate that the proposed method achieved an approximately 1.3% improvement in carotid plaque classification accuracy compared to the baseline network. Specifically, the accuracy of mixed-echoic plaques classification increased by approximately 3.3%, demonstrating the effectiveness of our approach. ",
        "title": "WAL-Net: Weakly supervised auxiliary task learning network for carotid  plaques classification",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14000",
        "abstract_url": "http://arxiv.org/abs/2401.14000",
        "authors": [
            {
                "last_name": "Feng",
                "first_name": "K. J. Kevin"
            },
            {
                "last_name": "Koo",
                "first_name": "Xander"
            },
            {
                "last_name": "Tan",
                "first_name": "Lawrence"
            },
            {
                "last_name": "Bruckman",
                "first_name": "Amy"
            },
            {
                "last_name": "McDonald",
                "first_name": "David W."
            },
            {
                "last_name": "Zhang",
                "first_name": "Amy X."
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Social media feeds are deeply personal spaces that reflect individual values and preferences. However, top-down, platform-wide content algorithms can reduce users' sense of agency and fail to account for nuanced experiences and values. Drawing on the paradigm of interactive machine teaching (IMT), an interaction framework for non-expert algorithmic adaptation, we map out a design space for teachable social media feed experiences to empower agential, personalized feed curation. To do so, we conducted a think-aloud study (N=24) featuring four social media platforms -- Instagram, Mastodon, TikTok, and Twitter -- to understand key signals users leveraged to determine the value of a post in their feed. We synthesized users' signals into taxonomies that, when combined with user interviews, inform five design principles that extend IMT into the social media setting. We finally embodied our principles into three feed designs that we present as sensitizing concepts for teachable feed experiences moving forward. ",
        "title": "Mapping the Design Space of Teachable Social Media Feed Experiences",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14003",
        "abstract_url": "http://arxiv.org/abs/2401.14003",
        "authors": [
            {
                "last_name": "Do",
                "first_name": "Quyet V."
            },
            {
                "last_name": "Fang",
                "first_name": "Tianqing"
            },
            {
                "last_name": "Diao",
                "first_name": "Shizhe"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhaowei"
            },
            {
                "last_name": "Song",
                "first_name": "Yangqiu"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge. Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning. One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (Bengio et al., 2021). To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints. When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints. The acquired constraint-checking result is then aggregated with the output of the main prompting technique to produce the final output. Experimental results on CSKB Reasoning benchmarks demonstrate the effectiveness of our method by bringing consistent improvements over all prompting methods. Codes and data are available at \\url{https://github.com/HKUST-KnowComp/ConstraintChecker}. ",
        "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on  Commonsense Knowledge Bases",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14005",
        "abstract_url": "http://arxiv.org/abs/2401.14005",
        "authors": [
            {
                "last_name": "Yigit",
                "first_name": "Yagmur"
            },
            {
                "last_name": "Panitsas",
                "first_name": "Ioannis"
            },
            {
                "last_name": "Maglaras",
                "first_name": "Leandros"
            },
            {
                "last_name": "Tassiulas",
                "first_name": "Leandros"
            },
            {
                "last_name": "Canberk",
                "first_name": "Berk"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  The rapid evolution of Vehicular Ad-hoc NETworks (VANETs) has ushered in a transformative era for intelligent transportation systems (ITS), significantly enhancing road safety and vehicular communication. However, the intricate and dynamic nature of VANETs presents formidable challenges, particularly in vehicle-to-infrastructure (V2I) communications. Roadside Units (RSUs), integral components of VANETs, are increasingly susceptible to cyberattacks, such as jamming and distributed denial-of-service (DDoS) attacks. These vulnerabilities pose grave risks to road safety, potentially leading to traffic congestion and vehicle malfunctions. Current approaches often struggle to effectively merge digital twin technology with Artificial Intelligence (AI) models to boost security and sustainability. Our study introduces an innovative cyber-twin framework tailored to enhance the security of RSUs in VANETs. This framework uniquely combines digital twin technology with cutting-edge AI to offer a real-time, dynamic representation of RSUs. This allows for detailed monitoring and efficient detection of threats, significantly strengthening RSU security in VANETs. Moreover, our framework makes a notable contribution to eco-friendly communication by improving the computational efficiency of RSUs, leading to increased energy efficiency and extended hardware durability. Our results show a considerable enhancement in resource management and attack detection, surpassing the performance of existing solutions. In particular, the cyber-twin framework showed a substantial reduction in RSU load and an optimal balance between resource consumption and high attack detection efficiency, with a defined twinning rate range of seventy-six to ninety per cent. These advancements underscore our commitment to developing sustainable, secure, and resilient vehicular communication systems for the future of smart cities. ",
        "title": "Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for  Vehicular Ad-Hoc Networks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14007",
        "abstract_url": "http://arxiv.org/abs/2401.14007",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Daxin"
            },
            {
                "last_name": "Bai",
                "first_name": "Yuanchao"
            },
            {
                "last_name": "Wang",
                "first_name": "Kai"
            },
            {
                "last_name": "Jiang",
                "first_name": "Junjun"
            },
            {
                "last_name": "Liu",
                "first_name": "Xianming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recent advancements in neural compression have surpassed traditional codecs in PSNR and MS-SSIM measurements. However, at low bit-rates, these methods can introduce visually displeasing artifacts, such as blurring, color shifting, and texture loss, thereby compromising perceptual quality of images. To address these issues, this study presents an enhanced neural compression method designed for optimal visual fidelity. We have trained our model with a sophisticated semantic ensemble loss, integrating Charbonnier loss, perceptual loss, style loss, and a non-binary adversarial loss, to enhance the perceptual quality of image reconstructions. Additionally, we have implemented a latent refinement process to generate content-aware latent codes. These codes adhere to bit-rate constraints, balance the trade-off between distortion and fidelity, and prioritize bit allocation to regions of greater importance. Our empirical findings demonstrate that this approach significantly improves the statistical fidelity of neural image compression. On CLIC2024 validation set, our approach achieves a 62% bitrate saving compared to MS-ILLM under FID metric. ",
        "title": "Semantic Ensemble Loss and Latent Refinement for High-Fidelity Neural  Image Compression",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14008",
        "abstract_url": "http://arxiv.org/abs/2401.14008",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Wu",
                "first_name": "Yongpeng"
            },
            {
                "last_name": "An",
                "first_name": "Jianping"
            },
            {
                "last_name": "Ng",
                "first_name": "Derrick Wing Kwan"
            },
            {
                "last_name": "Xing",
                "first_name": "Chengwen"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenjun"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper investigates the unsourced random access (URA) problem with a massive multiple-input multiple-output receiver that serves wireless devices in the near-field of radiation. We employ an uncoupled transmission protocol without appending redundancies to the slot-wise encoded messages. To exploit the channel sparsity for block length reduction while facing the collapsed sparse structure in the angular domain of near-field channels, we propose a sparse channel sampling method that divides the angle-distance (polar) domain based on the maximum permissible coherence. Decoding starts with retrieving active codewords and channels from each slot. We address the issue by leveraging the structured channel sparsity in the spatial and polar domains and propose a novel turbo-based recovery algorithm. Furthermore, we investigate an off-grid compressed sensing method to refine discretely estimated channel parameters over the continuum that improves the detection performance. Afterward, without the assistance of redundancies, we recouple the separated messages according to the similarity of the users' channel information and propose a modified K-medoids method to handle the constraints and collisions involved in channel clustering. Simulations reveal that via exploiting the channel sparsity, the proposed URA scheme achieves high spectral efficiency and surpasses existing multi-slot-based schemes. Moreover, with more measurements provided by the overcomplete channel sampling, the near-field-suited scheme outperforms its counterpart of the far-field. ",
        "title": "Massive Unsourced Random Access for Near-Field Communications",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14009",
        "abstract_url": "http://arxiv.org/abs/2401.14009",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Yuxia"
            },
            {
                "last_name": "Fang",
                "first_name": "Yuan"
            },
            {
                "last_name": "Liao",
                "first_name": "Lizi"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  Dynamic graph modeling is crucial for understanding complex structures in web graphs, spanning applications in social networks, recommender systems, and more. Most existing methods primarily emphasize structural dependencies and their temporal changes. However, these approaches often overlook detailed temporal aspects or struggle with long-term dependencies. Furthermore, many solutions overly complicate the process by emphasizing intricate module designs to capture dynamic evolutions. In this work, we harness the strength of the Transformer's self-attention mechanism, known for adeptly handling long-range dependencies in sequence modeling. Our approach offers a simple Transformer model tailored for dynamic graph modeling without complex modifications. We re-conceptualize dynamic graphs as a sequence modeling challenge and introduce an innovative temporal alignment technique. This technique not only captures the inherent temporal evolution patterns within dynamic graphs but also streamlines the modeling process of their evolution. As a result, our method becomes versatile, catering to an array of applications. Our model's effectiveness is underscored through rigorous experiments on four real-world datasets from various sectors, solidifying its potential in dynamic graph modeling. ",
        "title": "On the Feasibility of Simple Transformer for Dynamic Graph Modeling",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14010",
        "abstract_url": "http://arxiv.org/abs/2401.14010",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Yi"
            },
            {
                "last_name": "Cao",
                "first_name": "Shixiong"
            },
            {
                "last_name": "Shi",
                "first_name": "Yang"
            },
            {
                "last_name": "Chen",
                "first_name": "Qing"
            },
            {
                "last_name": "Xu",
                "first_name": "Ke"
            },
            {
                "last_name": "Cao",
                "first_name": "Nan"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Narrative visualization effectively transforms data into engaging stories, making complex information accessible to a broad audience. Large models, essential for narrative visualization, inherently facilitate this process through their superior ability to handle natural language queries and answers, generate cohesive narratives, and enhance visual communication. Inspired by previous work in narrative visualization and recent advances in large models, we synthesized potential tasks and opportunities for large models at various stages of narrative visualization. In our study, we surveyed 79 papers to explore the role of large models in automating narrative visualization creation. We propose a comprehensive pipeline that leverages large models for crafting narrative visualization, categorizing the reviewed literature into four essential phases: Data, Narration, Visualization, and Presentation. Additionally, we identify ten specific tasks where large models are applied across these stages. This study maps out the landscape of challenges and opportunities in the LM4NV process, providing insightful directions for future research and valuable guidance for scholars in the field. ",
        "title": "Leveraging Large Models for Crafting Narrative Visualization: A Survey",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14011",
        "abstract_url": "http://arxiv.org/abs/2401.14011",
        "authors": [
            {
                "last_name": "He",
                "first_name": "Zheqi"
            },
            {
                "last_name": "Wu",
                "first_name": "Xinya"
            },
            {
                "last_name": "Zhou",
                "first_name": "Pengfei"
            },
            {
                "last_name": "Xuan",
                "first_name": "Richeng"
            },
            {
                "last_name": "Liu",
                "first_name": "Guang"
            },
            {
                "last_name": "Yang",
                "first_name": "Xi"
            },
            {
                "last_name": "Zhu",
                "first_name": "Qiannan"
            },
            {
                "last_name": "Huang",
                "first_name": "Hua"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "MM"
        ],
        "abstract": "  Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strategy aims to reduce position bias, minimize the influence of randomness on correctness, and perform a quantitative analysis of position bias. We evaluate seven open-source MLLMs along with GPT4-V, Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a significant challenge to the recent MLLMs. ",
        "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question  Understanding and Reasoning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14012",
        "abstract_url": "http://arxiv.org/abs/2401.14012",
        "authors": [
            {
                "last_name": "Giudici",
                "first_name": "Paolo"
            },
            {
                "last_name": "Raffinetti",
                "first_name": "Emanuela"
            },
            {
                "last_name": "Toscani",
                "first_name": "Giuseppe"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Inequality measures are quantitative measures that take values in the unit interval, with a zero value characterizing perfect equality. Although originally proposed to measure economic inequalities, they can be applied to several other situations, in which one is interested in the mutual variability between a set of observations, rather than in their deviations from the mean. While unidimensional measures of inequality, such as the Gini index, are widely known and employed, multidimensional measures, such as Lorenz Zonoids, are difficult to interpret and computationally expensive and, for these reasons, are not much well known. To overcome the problem, in this paper we propose a new scaling invariant multidimensional inequality index, based on the Fourier transform, which exhibits a number of interesting properties, and whose application to the multidimensional case is rather straightforward to calculate and interpret. ",
        "title": "Measuring multidimensional inequality: a new proposal based on the  Fourier transform",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14013",
        "abstract_url": "http://arxiv.org/abs/2401.14013",
        "authors": [
            {
                "last_name": "Hu",
                "first_name": "Bin-Bin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hai-Tao"
            },
            {
                "last_name": "Yao",
                "first_name": "Weijia"
            },
            {
                "last_name": "Sun",
                "first_name": "Zhiyong"
            },
            {
                "last_name": "Cao",
                "first_name": "Ming"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  We design a distributed coordinated guiding vector field (CGVF) for a group of robots to achieve ordering-flexible motion coordination while maneuvering on a desired two-dimensional (2D) surface. The CGVF is characterized by three terms, i.e., a convergence term to drive the robots to converge to the desired surface, a propagation term to provide a traversing direction for maneuvering on the desired surface, and a coordinated term to achieve the surface motion coordination with an arbitrary ordering of the robotic group. By setting the surface parameters as additional virtual coordinates, the proposed approach eliminates the potential singularity of the CGVF and enables both the global convergence to the desired surface and the maneuvering on the surface from all possible initial conditions. The ordering-flexible surface motion coordination is realized by each robot to share with its neighbors only two virtual coordinates, i.e. that of a given target and that of its own, which reduces the communication and computation cost in multi-robot surface navigation. Finally, the effectiveness of the CGVF is substantiated by extensive numerical simulations. ",
        "title": "Coordinated Guiding Vector Field Design for Ordering-Flexible  Multi-Robot Surface Navigation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14014",
        "abstract_url": "http://arxiv.org/abs/2401.14014",
        "authors": [
            {
                "last_name": "Morinaga",
                "first_name": "Daiki"
            },
            {
                "last_name": "Akimoto",
                "first_name": "Youhei"
            }
        ],
        "primary_category": "NE",
        "categories": [
            "NE"
        ],
        "abstract": "  In black-box optimization, noise in the objective function is inevitable. Noise disrupts the ranking of candidate solutions in comparison-based optimization, possibly deteriorating the search performance compared with a noiseless scenario. Explicit averaging takes the sample average of noisy objective function values and is widely used as a simple and versatile noise-handling technique. Although it is suitable for various applications, it is ineffective if the mean is not finite. We theoretically reveal that explicit averaging has a negative effect on the estimation of ground-truth rankings when assuming stably distributed noise without a finite mean. Alternatively, sign averaging is proposed as a simple but robust noise-handling technique. We theoretically prove that the sign averaging estimates the order of the medians of the noisy objective function values of a pair of points with arbitrarily high probability as the number of samples increases. Its advantages over explicit averaging and its robustness are also confirmed through numerical experiments. ",
        "title": "Theoretical Analysis of Explicit Averaging and Novel Sign Averaging in  Comparison-Based Search",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14016",
        "abstract_url": "http://arxiv.org/abs/2401.14016",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Jiuzhou"
            },
            {
                "last_name": "Buntine",
                "first_name": "Wray"
            },
            {
                "last_name": "Shareghi",
                "first_name": "Ehsan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty. ",
        "title": "Towards Uncertainty-Aware Language Agent",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14019",
        "abstract_url": "http://arxiv.org/abs/2401.14019",
        "authors": [
            {
                "last_name": "Bandel",
                "first_name": "Elron"
            },
            {
                "last_name": "Perlitz",
                "first_name": "Yotam"
            },
            {
                "last_name": "Venezian",
                "first_name": "Elad"
            },
            {
                "last_name": "Friedman-Melamed",
                "first_name": "Roni"
            },
            {
                "last_name": "Arviv",
                "first_name": "Ofir"
            },
            {
                "last_name": "Orbach",
                "first_name": "Matan"
            },
            {
                "last_name": "Don-Yehyia",
                "first_name": "Shachar"
            },
            {
                "last_name": "Sheinwald",
                "first_name": "Dafna"
            },
            {
                "last_name": "Gera",
                "first_name": "Ariel"
            },
            {
                "last_name": "Choshen",
                "first_name": "Leshem"
            },
            {
                "last_name": "Shmueli-Scheuer",
                "first_name": "Michal"
            },
            {
                "last_name": "Katz",
                "first_name": "Yoav"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond being a tool, Unitxt is a community-driven platform, empowering users to build, share, and advance their pipelines collaboratively. Join the Unitxt community at https://github.com/IBM/unitxt! ",
        "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation  for Generative AI",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14021",
        "abstract_url": "http://arxiv.org/abs/2401.14021",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Zhihao"
            },
            {
                "last_name": "Zhu",
                "first_name": "Alan"
            },
            {
                "last_name": "Yang",
                "first_name": "Lijie"
            },
            {
                "last_name": "Xu",
                "first_name": "Yihua"
            },
            {
                "last_name": "Li",
                "first_name": "Lanting"
            },
            {
                "last_name": "Phothilimthana",
                "first_name": "Phitchaya Mangpo"
            },
            {
                "last_name": "Jia",
                "first_name": "Zhihao"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL",
            "IR"
        ],
        "abstract": "  Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline. ",
        "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14024",
        "abstract_url": "http://arxiv.org/abs/2401.14024",
        "authors": [
            {
                "last_name": "Peng",
                "first_name": "Haiyang"
            },
            {
                "last_name": "Zhan",
                "first_name": "Yi"
            },
            {
                "last_name": "Wang",
                "first_name": "Benkang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hongtao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In High-definition (HD) maps, lane elements constitute the majority of components and demand stringent localization requirements to ensure safe vehicle navigation. Vision lane detection with LiDAR position assignment is a prevalent method to acquire initial lanes for HD maps. However, due to incorrect vision detection and coarse camera-LiDAR calibration, initial lanes may deviate from their true positions within an uncertain range. To mitigate the need for manual lane correction, we propose a patch-wise lane correction network (PLCNet) to automatically correct the positions of initial lane points in local LiDAR images that are transformed from point clouds. PLCNet first extracts multi-scale image features and crops patch (ROI) features centered at each initial lane point. By applying ROIAlign, the fix-sized ROI features are flattened into 1D features. Then, a 1D lane attention module is devised to compute instance-level lane features with adaptive weights. Finally, lane correction offsets are inferred by a multi-layer perceptron and used to correct the initial lane positions. Considering practical applications, our automatic method supports merging local corrected lanes into global corrected lanes. Through extensive experiments on a self-built dataset, we demonstrate that PLCNet achieves fast and effective initial lane correction. ",
        "title": "PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction  in High-definition Maps",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14025",
        "abstract_url": "http://arxiv.org/abs/2401.14025",
        "authors": [
            {
                "last_name": "Ozan",
                "first_name": "\u015e\u00fckr\u00fc"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recent studies in DNA sequence classification have leveraged sophisticated machine learning techniques, achieving notable accuracy in categorizing complex genomic data. Among these, methods such as k-mer counting have proven effective in distinguishing sequences from varied species like chimpanzees, dogs, and humans, becoming a staple in contemporary genomic research. However, these approaches often demand extensive computational resources, posing a challenge in terms of scalability and efficiency. Addressing this issue, our study introduces a novel adaptation of Jiang et al.'s compressor-based, parameter-free classification method, specifically tailored for DNA sequence analysis. This innovative approach utilizes a variety of compression algorithms, such as Gzip, Brotli, and LZMA, to efficiently process and classify genomic sequences. Not only does this method align with the current state-of-the-art in terms of accuracy, but it also offers a more resource-efficient alternative to traditional machine learning methods. Our comprehensive evaluation demonstrates the proposed method's effectiveness in accurately classifying DNA sequences from multiple species. We present a detailed analysis of the performance of each algorithm used, highlighting the strengths and limitations of our approach in various genomic contexts. Furthermore, we discuss the broader implications of our findings for bioinformatics, particularly in genomic data processing and analysis. The results of our study pave the way for more efficient and scalable DNA sequence classification methods, offering significant potential for advancements in genomic research and applications. ",
        "title": "DNA Sequence Classification with Compressors",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14027",
        "abstract_url": "http://arxiv.org/abs/2401.14027",
        "authors": [
            {
                "last_name": "Du",
                "first_name": "Mengyao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Miao"
            },
            {
                "last_name": "Pu",
                "first_name": "Yuwen"
            },
            {
                "last_name": "Xu",
                "first_name": "Kai"
            },
            {
                "last_name": "Ji",
                "first_name": "Shouling"
            },
            {
                "last_name": "Yin",
                "first_name": "Quanjun"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  To tackle the scarcity and privacy issues associated with domain-specific datasets, the integration of federated learning in conjunction with fine-tuning has emerged as a practical solution. However, our findings reveal that federated learning has the risk of skewing fine-tuning features and compromising the out-of-distribution robustness of the model. By introducing three robustness indicators and conducting experiments across diverse robust datasets, we elucidate these phenomena by scrutinizing the diversity, transferability, and deviation within the model feature space. To mitigate the negative impact of federated learning on model robustness, we introduce GNP, a \\underline{G}eneral \\underline{N}oisy \\underline{P}rojection-based robust algorithm, ensuring no deterioration of accuracy on the target distribution. Specifically, the key strategy for enhancing model robustness entails the transfer of robustness from the pre-trained model to the fine-tuned model, coupled with adding a small amount of Gaussian noise to augment the representative capacity of the model. Comprehensive experimental results demonstrate that our approach markedly enhances the robustness across diverse scenarios, encompassing various parameter-efficient fine-tuning methods and confronting different levels of data heterogeneity. ",
        "title": "The Risk of Federated Learning to Skew Fine-Tuning Features and  Underperform Out-of-Distribution Robustness",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14028",
        "abstract_url": "http://arxiv.org/abs/2401.14028",
        "authors": [
            {
                "last_name": "Poda",
                "first_name": "Veronica"
            },
            {
                "last_name": "Matias",
                "first_name": "Catherine"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  We conducted a comparative analysis of the performance of modularity-based methods for clustering nodes in binary hypergraphs. Statistical analysis and node clustering in hypergraphs constitute an emerging topic suffering from a lack of standardization. In contrast to the case of graphs, the concept of nodes' community in hypergraphs is not unique and encompasses various distinct situations. To address this, we begin by presenting, within a unified framework, the various hypergraph modularity criteria proposed in the literature, emphasizing their differences and respective focuses. Subsequently, we provide an overview of the state-of-the-art codes available to maximize hypergraph modularities for detecting node communities in binary hypergraphs. Through exploration of various simulation settings with controlled ground truth clustering, we offer a comparison of these methods using different quality measures, including true clustering recovery, running time, (local) maximization of the objective, and the number of clusters detected. Our contribution marks the first attempt to clarify the advantages and drawbacks of these newly available methods. This effort lays the foundation for a better understanding of the primary objectives of modularity-based node clustering methods for binary hypergraphs. ",
        "title": "Comparison of modularity-based approaches for nodes clustering in binary  hypergraphs",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14029",
        "abstract_url": "http://arxiv.org/abs/2401.14029",
        "authors": [
            {
                "last_name": "D\u00f6rfler",
                "first_name": "Florian"
            },
            {
                "last_name": "He",
                "first_name": "Zhiyu"
            },
            {
                "last_name": "Belgioioso",
                "first_name": "Giuseppe"
            },
            {
                "last_name": "Bolognani",
                "first_name": "Saverio"
            },
            {
                "last_name": "Lygeros",
                "first_name": "John"
            },
            {
                "last_name": "Muehlebach",
                "first_name": "Michael"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Traditionally, numerical algorithms are seen as isolated pieces of code confined to an {\\em in silico} existence. However, this perspective is not appropriate for many modern computational approaches in control, learning, or optimization, wherein {\\em in vivo} algorithms interact with their environment. Examples of such {\\em open} include various real-time optimization-based control strategies, reinforcement learning, decision-making architectures, online optimization, and many more. Further, even {\\em closed} algorithms in learning or optimization are increasingly abstracted in block diagrams with interacting dynamic modules and pipelines. In this opinion paper, we state our vision on a to-be-cultivated {\\em systems theory of algorithms} and argue in favour of viewing algorithms as open dynamical systems interacting with other algorithms, physical systems, humans, or databases. Remarkably, the manifold tools developed under the umbrella of systems theory also provide valuable insights into this burgeoning paradigm shift and its accompanying challenges in the algorithmic world. We survey various instances where the principles of algorithmic systems theory are being developed and outline pertinent modeling, analysis, and design challenges. ",
        "title": "Towards a Systems Theory of Algorithms",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14031",
        "abstract_url": "http://arxiv.org/abs/2401.14031",
        "authors": [
            {
                "last_name": "Kuvshinova",
                "first_name": "Kseniia"
            },
            {
                "last_name": "Tsymboi",
                "first_name": "Olga"
            },
            {
                "last_name": "Oseledets",
                "first_name": "Ivan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CR",
            "CV"
        ],
        "abstract": "  The research in the field of adversarial attacks and models' vulnerability is one of the fundamental directions in modern machine learning. Recent studies reveal the vulnerability phenomenon, and understanding the mechanisms behind this is essential for improving neural network characteristics and interpretability. In this paper, we propose a novel sparse universal white-box adversarial attack. Our approach is based on truncated power iteration providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian matrices. Using the ImageNet benchmark validation subset, we analyze the proposed method in various settings, achieving results comparable to dense baselines with more than a 50% fooling rate while damaging only 5% of pixels and utilizing 256 samples for perturbation fitting. We also show that our algorithm admits higher attack magnitude without affecting the human ability to solve the task. Furthermore, we investigate that the constructed perturbations are highly transferable among different models without significantly decreasing the fooling rate. Our findings demonstrate the vulnerability of state-of-the-art models to sparse attacks and highlight the importance of developing robust machine learning systems. ",
        "title": "Sparse and Transferable Universal Singular Vectors Attack",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14032",
        "abstract_url": "http://arxiv.org/abs/2401.14032",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Butian"
            },
            {
                "last_name": "Li",
                "first_name": "Zhuo"
            },
            {
                "last_name": "Li",
                "first_name": "Zhen"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information ",
        "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D  Reconstruction Dataset Using Gaussian Splatting",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14033",
        "abstract_url": "http://arxiv.org/abs/2401.14033",
        "authors": [
            {
                "last_name": "Pauli",
                "first_name": "Patricia"
            },
            {
                "last_name": "Havens",
                "first_name": "Aaron"
            },
            {
                "last_name": "Araujo",
                "first_name": "Alexandre"
            },
            {
                "last_name": "Garg",
                "first_name": "Siddharth"
            },
            {
                "last_name": "Khorrami",
                "first_name": "Farshad"
            },
            {
                "last_name": "Allg\u00f6wer",
                "first_name": "Frank"
            },
            {
                "last_name": "Hu",
                "first_name": "Bin"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach (Fazlyab et al., 2019) has received much attention and provides the least conservative Lipschitz upper bounds that can be computed with polynomial time guarantees. However, one main restriction of LipSDP is that its formulation requires the activation functions to be slope-restricted on $[0,1]$, preventing its further use for more general activation functions such as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for example as residual ReLU networks. However, a direct application of LipSDP to the resultant residual ReLU networks is conservative and even fails in recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our paper bridges this gap and extends LipSDP beyond slope-restricted activation functions. To this end, we provide novel quadratic constraints for GroupSort, MaxMin, and Householder activations via leveraging their underlying properties such as sum preservation. Our proposed analysis is general and provides a unified approach for estimating $\\ell_2$ and $\\ell_\\infty$ Lipschitz bounds for a rich class of neural network architectures, including non-residual and residual neural networks and implicit models, with GroupSort, MaxMin, and Householder activations. Finally, we illustrate the utility of our approach with a variety of experiments and show that our proposed SDPs generate less conservative Lipschitz bounds in comparison to existing approaches. ",
        "title": "Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted  Activations",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14034",
        "abstract_url": "http://arxiv.org/abs/2401.14034",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Chuankun"
            },
            {
                "last_name": "Li",
                "first_name": "Shuai"
            },
            {
                "last_name": "Gao",
                "first_name": "Yanbo"
            },
            {
                "last_name": "Chen",
                "first_name": "Ping"
            },
            {
                "last_name": "Li",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Wanqing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Unsupervised skeleton based action recognition has achieved remarkable progress recently. Existing unsupervised learning methods suffer from severe overfitting problem, and thus small networks are used, significantly reducing the representation capability. To address this problem, the overfitting mechanism behind the unsupervised learning for skeleton based action recognition is first investigated. It is observed that the skeleton is already a relatively high-level and low-dimension feature, but not in the same manifold as the features for action recognition. Simply applying the existing unsupervised learning method may tend to produce features that discriminate the different samples instead of action classes, resulting in the overfitting problem. To solve this problem, this paper presents an Unsupervised spatial-temporal Feature Enrichment and Fidelity Preservation framework (U-FEFP) to generate rich distributed features that contain all the information of the skeleton sequence. A spatial-temporal feature transformation subnetwork is developed using spatial-temporal graph convolutional network and graph convolutional gate recurrent unit network as the basic feature extraction network. The unsupervised Bootstrap Your Own Latent based learning is used to generate rich distributed features and the unsupervised pretext task based learning is used to preserve the information of the skeleton sequence. The two unsupervised learning ways are collaborated as U-FEFP to produce robust and discriminative representations. Experimental results on three widely used benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate that the proposed U-FEFP achieves the best performance compared with the state-of-the-art unsupervised learning methods. t-SNE illustrations further validate that U-FEFP can learn more discriminative features for unsupervised skeleton based action recognition. ",
        "title": "Unsupervised Spatial-Temporal Feature Enrichment and Fidelity  Preservation Network for Skeleton based Action Recognition",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14036",
        "abstract_url": "http://arxiv.org/abs/2401.14036",
        "authors": [
            {
                "last_name": "Xie",
                "first_name": "Jiu-Cheng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jun"
            },
            {
                "last_name": "Wang",
                "first_name": "Wenqing"
            },
            {
                "last_name": "Xu",
                "first_name": "Feng"
            },
            {
                "last_name": "Gao",
                "first_name": "Hao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Face aging has received continuous research attention over the past two decades. Although previous works on this topic have achieved impressive success, two longstanding problems remain unsettled: 1) generating diverse and plausible facial aging patterns at the target age stage; 2) measuring the rationality of identity variation between the original portrait and its syntheses with age progression or regression. In this paper, we introduce DLAT + , the first algorithm that can realize Diverse and Lifespan Age Transformation on human faces, where the diversity jointly manifests in the transformation of facial textures and shapes. Apart from the diversity mechanism embedded in the model, multiple consistency restrictions are leveraged to keep it away from counterfactual aging syntheses. Moreover, we propose a new metric to assess the rationality of Identity Deviation under Age Gaps (IDAG) between the input face and its series of age-transformed generations, which is based on statistical laws summarized from plenty of genuine face-aging data. Extensive experimental results demonstrate the uniqueness and effectiveness of our method in synthesizing diverse and perceptually reasonable faces across the whole lifetime. ",
        "title": "Diverse and Lifespan Facial Age Transformation Synthesis with Identity  Variation Rationality Metric",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14038",
        "abstract_url": "http://arxiv.org/abs/2401.14038",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Hai-Xin"
            },
            {
                "last_name": "Huang",
                "first_name": "Dong"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Deep clustering has gained significant attention due to its capability in learning clustering-friendly representations without labeled data. However, previous deep clustering methods tend to treat all samples equally, which neglect the variance in the latent distribution and the varying difficulty in classifying or clustering different samples. To address this, this paper proposes a novel end-to-end deep clustering method with diffused sampling and hardness-aware self-distillation (HaDis). Specifically, we first align one view of instances with another view via diffused sampling alignment (DSA), which helps improve the intra-cluster compactness. To alleviate the sampling bias, we present the hardness-aware self-distillation (HSD) mechanism to mine the hardest positive and negative samples and adaptively adjust their weights in a self-distillation fashion, which is able to deal with the potential imbalance in sample contributions during optimization. Further, the prototypical contrastive learning is incorporated to simultaneously enhance the inter-cluster separability and intra-cluster compactness. Experimental results on five challenging image datasets demonstrate the superior clustering performance of our HaDis method over the state-of-the-art. Source code is available at https://github.com/Regan-Zhang/HaDis. ",
        "title": "Deep Clustering with Diffused Sampling and Hardness-aware  Self-distillation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14040",
        "abstract_url": "http://arxiv.org/abs/2401.14040",
        "authors": [
            {
                "last_name": "Periti",
                "first_name": "Francesco"
            },
            {
                "last_name": "Dubossarsky",
                "first_name": "Haim"
            },
            {
                "last_name": "Tahmasebi",
                "first_name": "Nina"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes. ",
        "title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14043",
        "abstract_url": "http://arxiv.org/abs/2401.14043",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Haochen"
            },
            {
                "last_name": "Leung",
                "first_name": "Jonathan"
            },
            {
                "last_name": "Shen",
                "first_name": "Zhiqi"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering. ",
        "title": "Towards Goal-oriented Large Language Model Prompting: A Survey",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14047",
        "abstract_url": "http://arxiv.org/abs/2401.14047",
        "authors": [
            {
                "last_name": "Adler",
                "first_name": "Rasmus"
            },
            {
                "last_name": "Elberzhager",
                "first_name": "Frank"
            },
            {
                "last_name": "Baldauf",
                "first_name": "Florian"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Engineering a sustainable world requires to consider various systems that interact with each other. These systems include ecological systems, economical systems, social systems and tech-nical systems. They are loosely coupled, geographically distributed, evolve permanently and generate emergent behavior. As these are characteristics of systems of systems (SoS), we discuss the engi-neering of a sustainable world from a SoS engineering perspective. We studied SoS engineering in context of a research project, which aims at political recommendations and a research roadmap for engineering dynamic SoS. The project included an exhaustive literature review, interviews and work-shops with representatives from industry and academia from different application domains. Based on these results and observations, we will discuss how suitable the current state-of-the-art in SoS engi-neering is in order to engineer sustainability. Sustainability was a major driver for SoS engineering in all domains, but we argue that the current scope of SoS engineering is too limited in order to engineer sustainability. Further, we argue that mastering dynamics in this larger scope is essential to engineer sustainability and that this is accompanied by dynamic adaptation of technological SoS. ",
        "title": "Engineering a sustainable world by enhancing the scope of systems of  systems engineering and mastering dynamics",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14051",
        "abstract_url": "http://arxiv.org/abs/2401.14051",
        "authors": [
            {
                "last_name": "Fang",
                "first_name": "Shun"
            },
            {
                "last_name": "Feng",
                "first_name": "Xing"
            },
            {
                "last_name": "Cui",
                "first_name": "Ming"
            }
        ],
        "primary_category": "GR",
        "categories": [
            "GR",
            "CV"
        ],
        "abstract": "  We propose a neural network-based real-time volume rendering method for realistic and efficient rendering of volumetric media. The traditional volume rendering method uses path tracing to solve the radiation transfer equation, which requires a huge amount of calculation and cannot achieve real-time rendering. Therefore, this paper uses neural networks to simulate the iterative integration process of solving the radiative transfer equation to speed up the volume rendering of volume media. Specifically, the paper first performs data processing on the volume medium to generate a variety of sampling features, including density features, transmittance features and phase features. The hierarchical transmittance fields are fed into a 3D-CNN network to compute more important transmittance features. Secondly, the diffuse reflection sampling template and the highlight sampling template are used to layer the three types of sampling features into the network. This method can pay more attention to light scattering, highlights and shadows, and then select important channel features through the attention module. Finally, the scattering distribution of the center points of all sampling templates is predicted through the backbone neural network. This method can achieve realistic volumetric media rendering effects and greatly increase the rendering speed while maintaining rendering quality, which is of great significance for real-time rendering applications. Experimental results indicate that our method outperforms previous methods. ",
        "title": "A real-time rendering method for high albedo anisotropic materials with  multiple scattering",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14055",
        "abstract_url": "http://arxiv.org/abs/2401.14055",
        "authors": [
            {
                "last_name": "Ruiz-Hernandez",
                "first_name": "Diego"
            },
            {
                "last_name": "Pinar-P\u00e9rez",
                "first_name": "Jes\u00fas Mar\u00eda"
            },
            {
                "last_name": "Delgado-G\u00f3mez",
                "first_name": "David"
            }
        ],
        "primary_category": "DM",
        "categories": [
            "DM"
        ],
        "abstract": "  In this paper we address the problem of allocating the efforts of a collection of repairmen to a number of deteriorating machines in order to reduce operation costs and to mitigate the cost (and likelihood) of unexpected failures. Notwithstanding these preventive maintenance interventions are aimed at returning the machine to a so-called as-good-as-new state, unforeseeable factors may imply that maintenance interventions are not perfect and the machine is only returned to an earlier (uncertain) state of wear. The problem is modelled as a restless bandit problem and an index policy for the sequential allocation of maintenance tasks is proposed. A series of numerical experiments shows the strong performance of the proposed policy. Moreover, the methodology is of interest in the general context of dynamic resource allocation and restless bandit problems, as well as being useful in the particular imperfect maintenance model described. ",
        "title": "Multi-machine preventative maintenance scheduling with imperfect  interventions: a restless bandit approach",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14056",
        "abstract_url": "http://arxiv.org/abs/2401.14056",
        "authors": [
            {
                "last_name": "Zandberg",
                "first_name": "Koen"
            },
            {
                "last_name": "Gulati",
                "first_name": "Mayank"
            },
            {
                "last_name": "Wunder",
                "first_name": "Gerhard"
            },
            {
                "last_name": "Baccelli",
                "first_name": "Emmanuel"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  The typical federated learning workflow requires communication between a central server and a large set of clients synchronizing model parameters between each other. The current frameworks use communication protocols not suitable for resource-constrained devices and are either hard to deploy or require high-throughput links not available on these devices. In this paper, we present a generic message framework using CBOR for communication with existing federated learning frameworks optimised for use with resource-constrained devices and low power and lossy network links. We evaluate the resulting message sizes against JSON serialized messages where compare both with model parameters resulting in optimal and worst case serialization length, and with a real-world LeNet-5 model. Our benchmarks show that with our approach, messages are up to 75 % smaller in size when compared to the JSON alternative. ",
        "title": "Model CBOR Serialization for Federated Learning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14057",
        "abstract_url": "http://arxiv.org/abs/2401.14057",
        "authors": [
            {
                "last_name": "Rinaldo",
                "first_name": "Jarrad"
            },
            {
                "last_name": "Kuhlmann",
                "first_name": "Levin"
            },
            {
                "last_name": "Friedman",
                "first_name": "Jason"
            },
            {
                "last_name": "Kowadlo",
                "first_name": "Gideon"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "LG",
            "NE"
        ],
        "abstract": "  Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. The models were trained and tested on two tasks common in the human motor control literature: the random reach task, suited to the dominant system, a model with better coordination, and the hold position task, suited to the non-dominant system, a model with more stable movement. Each system out-performed the non-favoured system in its preferred task. For both tasks, a bilateral model outperforms the 'non-preferred' hand, and is as good or better than the 'preferred' hand. The Corpus Callosum tends to improve performance, but not always for the specialised models. ",
        "title": "Left/Right Brain, human motor control and the implications for robotics",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14060",
        "abstract_url": "http://arxiv.org/abs/2401.14060",
        "authors": [
            {
                "last_name": "Filtser",
                "first_name": "Arnold"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "CG"
        ],
        "abstract": "  Given a metric space $(X,d_X)$, a $(\\beta,s,\\Delta)$-sparse cover is a collection of clusters $\\mathcal{C}\\subseteq P(X)$ with diameter at most $\\Delta$, such that for every point $x\\in X$, the ball $B_X(x,\\frac\\Delta\\beta)$ is fully contained in some cluster $C\\in \\mathcal{C}$, and $x$ belongs to at most $s$ clusters in $\\mathcal{C}$. Our main contribution is to show that the shortest path metric of every $K_r$-minor free graphs admits $(O(r),O(r^2),\\Delta)$-sparse cover, and for every $\\epsilon>0$, $(4+\\epsilon,O(\\frac1\\epsilon)^r,\\Delta)$-sparse cover (for arbitrary $\\Delta>0$). We then use this sparse cover to show that every $K_r$-minor free graph embeds into $\\ell_\\infty^{\\tilde{O}(\\frac1\\epsilon)^{r+1}\\cdot\\log n}$ with distortion $3+\\eps$ (resp. into $\\ell_\\infty^{\\tilde{O}(r^2)\\cdot\\log n}$ with distortion $O(r)$). Further, we provide applications of these sparse covers into padded decompositions, sparse partitions, universal TSP / Steiner tree, oblivious buy at bulk, name independent routing, and path reporting distance oracles. ",
        "title": "On Sparse Covers of Minor Free Graphs, Low Dimensional Metric  Embeddings, and other applications",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14065",
        "abstract_url": "http://arxiv.org/abs/2401.14065",
        "authors": [
            {
                "last_name": "Malik",
                "first_name": "Hasmat"
            },
            {
                "last_name": "Yadav",
                "first_name": "Amit Kumar"
            },
            {
                "last_name": "M\u00e1rquez",
                "first_name": "Fausto Pedro Garc\u00eda"
            },
            {
                "last_name": "Pinar-P\u00e9rez",
                "first_name": "Jes\u00fas Mar\u00eda"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Wind power generated by wind has non-schedule nature due to stochastic nature of meteorological variable. Hence energy business and control of wind power generation requires prediction of wind speed (WS) from few seconds to different time steps in advance. To deal with prediction shortcomings, various WS prediction methods have been used. Predictive data mining offers variety of methods for WS predictions where artificial neural network (ANN) is one of the reliable and accurate methods. It is observed from the result of this study that ANN gives better accuracy in comparison conventional model. The accuracy of WS prediction models is found to be dependent on input parameters and architecture type algorithms utilized. So the selection of most relevant input parameters is important research area in WS predicton field. The objective of the paper is twofold: first extensive review of ANN for wind power and WS prediction is carried out. Discussion and analysis of feature selection using Relief Algorithm (RA) in WS prediction are considered for different Indian sites. RA identify atmospheric pressure, solar radiation and relative humidity are relevant input variables. Based on relevant input variables Cascade ANN model is developed and prediction accuracy is evaluated. It is found that root mean square error (RMSE) for comparison between predicted and measured WS for training and testing wind speed are found to be 1.44 m/s and 1.49 m/s respectively. The developed cascade ANN model can be used to predict wind speed for sites where there are not WS measuring instruments are installed in India. ",
        "title": "Novel application of Relief Algorithm in cascaded artificial neural  network to predict wind speed for wind power resource assessment in India",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14066",
        "abstract_url": "http://arxiv.org/abs/2401.14066",
        "authors": [
            {
                "last_name": "Huang",
                "first_name": "Nisha"
            },
            {
                "last_name": "Dong",
                "first_name": "Weiming"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yuxin"
            },
            {
                "last_name": "Tang",
                "first_name": "Fan"
            },
            {
                "last_name": "Li",
                "first_name": "Ronghui"
            },
            {
                "last_name": "Ma",
                "first_name": "Chongyang"
            },
            {
                "last_name": "Li",
                "first_name": "Xiu"
            },
            {
                "last_name": "Xu",
                "first_name": "Changsheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. This allows for the precise manipulation of image style and content while maintaining the integrity of the original model parameters. Rigorous qualitative and quantitative evaluations underscore that CreativeSynth excels in enhancing artistic images' fidelity and preserves their innate aesthetic essence. By bridging the gap between generative models and artistic finesse, CreativeSynth becomes a custom digital palette. ",
        "title": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on  Multimodal Diffusion",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14067",
        "abstract_url": "http://arxiv.org/abs/2401.14067",
        "authors": [
            {
                "last_name": "Althabiti",
                "first_name": "Saud"
            },
            {
                "last_name": "Alsalka",
                "first_name": "Mohammad Ammar"
            },
            {
                "last_name": "Atwell",
                "first_name": "Eric"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This paper introduces Ta'keed, an explainable Arabic automatic fact-checking system. While existing research often focuses on classifying claims as \"True\" or \"False,\" there is a limited exploration of generating explanations for claim credibility, particularly in Arabic. Ta'keed addresses this gap by assessing claim truthfulness based on retrieved snippets, utilizing two main components: information retrieval and LLM-based claim verification. We compiled the ArFactEx, a testing gold-labelled dataset with manually justified references, to evaluate the system. The initial model achieved a promising F1 score of 0.72 in the classification task. Meanwhile, the system's generated explanations are compared with gold-standard explanations syntactically and semantically. The study recommends evaluating using semantic similarities, resulting in an average cosine similarity score of 0.76. Additionally, we explored the impact of varying snippet quantities on claim classification accuracy, revealing a potential correlation, with the model using the top seven hits outperforming others with an F1 score of 0.77. ",
        "title": "Ta'keed: The First Generative Fact-Checking System for Arabic Claims",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14069",
        "abstract_url": "http://arxiv.org/abs/2401.14069",
        "authors": [
            {
                "last_name": "Zhu",
                "first_name": "Huminhao"
            },
            {
                "last_name": "Wang",
                "first_name": "Fangyikang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Chao"
            },
            {
                "last_name": "Zhao",
                "first_name": "Hanbin"
            },
            {
                "last_name": "Qian",
                "first_name": "Hui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution. We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. To further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++ model is devised, which first follows the Sinkhorn flow to approach the image manifold quickly ($\\le 5$ NFEs) and then refines the samples along a simple straight flow. Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed methods. ",
        "title": "Neural Sinkhorn Gradient Flow",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14074",
        "abstract_url": "http://arxiv.org/abs/2401.14074",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Y."
            },
            {
                "last_name": "Lin",
                "first_name": "L."
            },
            {
                "last_name": "Wong",
                "first_name": "K. K. Y."
            },
            {
                "last_name": "Tang",
                "first_name": "X."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on three medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods ",
        "title": "ProCNS: Progressive Prototype Calibration and Noise Suppression for  Weakly-Supervised Medical Image Segmentation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14076",
        "abstract_url": "http://arxiv.org/abs/2401.14076",
        "authors": [
            {
                "last_name": "Shamsazad",
                "first_name": "Shida"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  In this paper, we present a novel ciphertext-policy attribute based encryption (CP-ABE) scheme that offers a flexible access structure. Our proposed scheme incorporates an access tree as its access control policy, enabling fine-grained access control over encrypted data. The security of our scheme is provable under the hardness assumption of the decisional Ring-Learning with Errors (R-LWE) problem, ensuring robust protection against unauthorized access. CP-ABE is a cryptographic technique that allows data owners to encrypt their data with access policies defined in terms of attributes. Only users possessing the required attributes can decrypt and access the encrypted data. Our scheme extends the capabilities of CP-ABE by introducing a flexible access structure based on an access tree. This structure enables more complex and customizable access policies, accommodating a wider range of real-world scenarios. To ensure the security of our scheme, we rely on the decisional R-LWE problem, a well-established hardness assumption in cryptography. By proving the security of our scheme under this assumption, we provide a strong guarantee of protection against potential attacks. Furthermore, our proposed scheme operates in the standard model, which means it does not rely on any additional assumptions or idealized cryptographic primitives. This enhances the practicality and applicability of our scheme, making it suitable for real-world deployment. We evaluate the performance and efficiency of our scheme through extensive simulations and comparisons with existing CP-ABE schemes. The results demonstrate the effectiveness and scalability of our proposed approach, highlighting its potential for secure and flexible data access control in various domains. ",
        "title": "Quantum Resistant Ciphertext-Policy Attribute-Based Encryption Scheme  with Flexible Access Structure",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14077",
        "abstract_url": "http://arxiv.org/abs/2401.14077",
        "authors": [
            {
                "last_name": "Vera-Vald\u00e9s",
                "first_name": "J. Eduardo"
            }
        ],
        "primary_category": "MS",
        "categories": [
            "MS"
        ],
        "abstract": "  LongMemory.jl is a package for time series long memory modelling in Julia. The package provides functions to generate long memory, estimate model parameters, and forecast. Generating methods include fractional differencing, stochastic error duration, and cross-sectional aggregation. Estimators include the classic ones used to estimate the Hurst effect, those inspired by log-periodogram regression, and parametric ones. Forecasting is provided for all parametric estimators. Moreover, the package adds plotting capabilities to illustrate long memory dynamics and forecasting. This article presents the theoretical developments for long memory modelling, show examples using the data included with the package, and compares the properties of LongMemory.jl with current alternatives, including benchmarks. For some of the theoretical developments, LongMemory.jl provides the first publicly available implementation in any programming language. A notable feature of this package is that all functions are implemented in the same programming language, taking advantage of the ease of use and speed provided by Julia. Therefore, all code is accessible to the user. Multiple dispatch, a novel feature of the language, is used to speed computations and provide consistent calls to related methods. The package is related to the R packages LongMemoryTS and fracdiff. ",
        "title": "LongMemory.jl: Generating, Estimating, and Forecasting Long Memory  Models in Julia",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14078",
        "abstract_url": "http://arxiv.org/abs/2401.14078",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Binh Vinh Duc"
            },
            {
                "last_name": "Moere",
                "first_name": "Andrew Vande"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "RO"
        ],
        "abstract": "  A typical open-plan office layout is unable to optimally host multiple collocated work activities, personal needs, and situational events, as its space exerts a range of environmental demands on workers in terms of maintaining their acoustic, visual or privacy comfort. As we hypothesise that these demands could be coped by optimising the environmental resources of the architectural layout, we deployed a mobile robotic partition that autonomously manoeuvres between predetermined locations. During a five-weeks in-the-wild study within a real-world open-plan office, we studied how 13 workers adopted four distinct adaptation strategies when sharing the spatiotemporal control of the robotic partition. Based on their logged and self-reported reasoning, we present six initiation regulating factors that determine the appropriateness of each adaptation strategy. This study thus contributes to how future human-building interaction could autonomously improve the experience, comfort, performance, and even the health and wellbeing of multiple workers that share the same workplace. ",
        "title": "The Adaptive Architectural Layout: How the Control of a Semi-Autonomous  Mobile Robotic Partition was Shared to Mediate the Environmental Demands and  Resources of an Open-Plan Office",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14079",
        "abstract_url": "http://arxiv.org/abs/2401.14079",
        "authors": [
            {
                "last_name": "Eisenreich",
                "first_name": "Tobias"
            },
            {
                "last_name": "Speth",
                "first_name": "Sandro"
            },
            {
                "last_name": "Wagner",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. To evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies. ",
        "title": "From Requirements to Architecture: An AI-Based Journey to  Semi-Automatically Generate Software Architectures",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14081",
        "abstract_url": "http://arxiv.org/abs/2401.14081",
        "authors": [
            {
                "last_name": "Taheri",
                "first_name": "Tayebeh"
            },
            {
                "last_name": "Aghaei",
                "first_name": "Alireza Afzal"
            },
            {
                "last_name": "Parand",
                "first_name": "Kourosh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  This paper presents a novel operational matrix method to accelerate the training of fractional Physics-Informed Neural Networks (fPINNs). Our approach involves a non-uniform discretization of the fractional Caputo operator, facilitating swift computation of fractional derivatives within Caputo-type fractional differential problems with $0<\\alpha<1$. In this methodology, the operational matrix is precomputed, and during the training phase, automatic differentiation is replaced with a matrix-vector product. While our methodology is compatible with any network, we particularly highlight its successful implementation in PINNs, emphasizing the enhanced accuracy achieved when utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates Legendre polynomials into the PINN structure, providing a significant boost in accuracy. The effectiveness of our proposed method is validated across diverse differential equations, including Delay Differential Equations (DDEs) and Systems of Differential Algebraic Equations (DAEs). To demonstrate its versatility, we extend the application of the method to systems of differential equations, specifically addressing nonlinear Pantograph fractional-order DDEs/DAEs. The results are supported by a comprehensive analysis of numerical outcomes. ",
        "title": "Accelerating Fractional PINNs using Operational Matrices of Derivative",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14085",
        "abstract_url": "http://arxiv.org/abs/2401.14085",
        "authors": [
            {
                "last_name": "Blair",
                "first_name": "Aidan"
            },
            {
                "last_name": "Gostar",
                "first_name": "Amirali Khodadadian"
            },
            {
                "last_name": "Bab-Hadiashar",
                "first_name": "Alireza"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaodong"
            },
            {
                "last_name": "Hoseinnezhad",
                "first_name": "Reza"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Tracking multiple targets in dynamic environments using distributed sensor networks is a challenging problem that has received significant attention in recent years. In such scenarios, the network of sensors must coordinate their actions to estimate the locations and trajectories of multiple targets accurately. Multi-sensor control methods can improve the performance of these networks by enabling efficient utilization of resources and enhancing the accuracy of the estimated target states. This paper proposes two novel multi-sensor control methods that utilize the Random Finite Set (RFS) framework to address this problem. Our methods improve computational tractability and enable fully distributed control, making them suitable for real-time applications. ",
        "title": "Enhanced Multi-Target Tracking in Dynamic Environments: Distributed  Control Methods Within the Random Finite Set Framework",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14086",
        "abstract_url": "http://arxiv.org/abs/2401.14086",
        "authors": [
            {
                "last_name": "Nemecek",
                "first_name": "Jiri"
            },
            {
                "last_name": "Pevny",
                "first_name": "Tomas"
            },
            {
                "last_name": "Marecek",
                "first_name": "Jakub"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where \"distance from the sample\" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is provided. ",
        "title": "Generating Likely Counterfactuals Using Sum-Product Networks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14088",
        "abstract_url": "http://arxiv.org/abs/2401.14088",
        "authors": [
            {
                "last_name": "Schlett",
                "first_name": "Torsten"
            },
            {
                "last_name": "Rathgeb",
                "first_name": "Christian"
            },
            {
                "last_name": "Tapia",
                "first_name": "Juan"
            },
            {
                "last_name": "Busch",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Various face image datasets intended for facial biometrics research were created via web-scraping, i.e. the collection of images publicly available on the internet. This work presents an approach to detect both exactly and nearly identical face image duplicates, using file and image hashes. The approach is extended through the use of face image preprocessing. Additional steps based on face recognition and face image quality assessment models reduce false positives, and facilitate the deduplication of the face images both for intra- and inter-subject duplicate sets. The presented approach is applied to five datasets, namely LFW, TinyFace, Adience, CASIA-WebFace, and C-MS-Celeb (a cleaned MS-Celeb-1M variant). Duplicates are detected within every dataset, with hundreds to hundreds of thousands of duplicates for all except LFW. Face recognition and quality assessment experiments indicate a minor impact on the results through the duplicate removal. The final deduplication data is publicly available. ",
        "title": "Double Trouble? Impact and Detection of Duplicates in Face Image  Datasets",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14089",
        "abstract_url": "http://arxiv.org/abs/2401.14089",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Ren-Xin"
            },
            {
                "last_name": "Shi",
                "first_name": "Jinjing"
            },
            {
                "last_name": "Li",
                "first_name": "Xuelong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy in discerning the significance of quantum data, resulting in diminished efficacy when handling extensive quantum datasets. Hard Attention Mechanism (HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters the substantial challenge of non-differentiability, consequently constraining its extensive applicability. In response to the dilemma of HAM and QML, a Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed. Notably, the FO is designed to surmount the non-differentiable issue by executing the activation or masking of Discrete Primitives (DPs) with Flexible Control (FC) to weave various discrete destinies. Based on this, such discrete choice can be visualized with a specially defined Quantum Hard Attention Score (QHAS). Furthermore, a trainable ADO is devised to boost the generality and flexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network (GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST binary classification. Experimental findings demonstrate that GQHAN adeptly surmounts the non-differentiability hurdle, surpassing the efficacy of extant quantum soft self-attention mechanisms in accuracies and learning ability. In noise experiments, GQHAN is robuster to bit-flip noise in accuracy and amplitude damping noise in learning performance. Predictably, the proposal of GQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for future quantum computers to process large-scale data, and promotes the development of quantum computer vision. ",
        "title": "GQHAN: A Grover-inspired Quantum Hard Attention Network",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14090",
        "abstract_url": "http://arxiv.org/abs/2401.14090",
        "authors": [
            {
                "last_name": "Teuwen",
                "first_name": "Koen T. W."
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            "SE"
        ],
        "abstract": "  Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion pool and the linear opinion pool. An experimental validation suggests that the modular approach does not result in decreased performance and can even enhance precision and recall compared to monolithic alternatives. The results also suggest that the Pairing Aggregator can improve precision over the linear and logarithmic opinion pools. Furthermore, the improved k-accuracy in the experiment suggests that forensic experts can leverage the resulting PMF during their manual attribution processes to enhance their efficiency. ",
        "title": "A Modular Approach to Automatic Cyber Threat Attribution using Opinion  Pools",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14093",
        "abstract_url": "http://arxiv.org/abs/2401.14093",
        "authors": [
            {
                "last_name": "Poenaru-Olaru",
                "first_name": "Lorena"
            },
            {
                "last_name": "Cruz",
                "first_name": "Luis"
            },
            {
                "last_name": "Rellermeyer",
                "first_name": "Jan"
            },
            {
                "last_name": "van Deursen",
                "first_name": "Arie"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LG"
        ],
        "abstract": "  Due to the continuous change in operational data, AIOps solutions suffer from performance degradation over time. Although periodic retraining is the state-of-the-art technique to preserve the failure prediction AIOps models' performance over time, this technique requires a considerable amount of labeled data to retrain. In AIOps obtaining label data is expensive since it requires the availability of domain experts to intensively annotate it. In this paper, we present McUDI, a model-centric unsupervised degradation indicator that is capable of detecting the exact moment the AIOps model requires retraining as a result of changes in data. We further show how employing McUDI in the maintenance pipeline of AIOps solutions can reduce the number of samples that require annotations with 30k for job failure prediction and 260k for disk failure prediction while achieving similar performance with periodic retraining. ",
        "title": "McUDI: Model-Centric Unsupervised Degradation Indicator for Failure  Prediction AIOps Solutions",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14095",
        "abstract_url": "http://arxiv.org/abs/2401.14095",
        "authors": [
            {
                "last_name": "Yue",
                "first_name": "Mingtao"
            },
            {
                "last_name": "Sayuda",
                "first_name": "Tomomi"
            },
            {
                "last_name": "Pennington",
                "first_name": "Miles"
            },
            {
                "last_name": "Sugano",
                "first_name": "Yusuke"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Appearance-based gaze estimation, which uses only a regular camera to estimate human gaze, is important in various application fields. While the technique faces data bias issues, data collection protocol is often demanding, and collecting data from a wide range of participants is difficult. It is an important challenge to design opportunities that allow a diverse range of people to participate while ensuring the quality of the training data. To tackle this challenge, we introduce a novel gamified approach for collecting training data. In this game, two players communicate words via eye gaze through a transparent letter board. Images captured during gameplay serve as valuable training data for gaze estimation models. The game is designed as a physical installation that involves communication between players, and it is expected to attract the interest of diverse participants. We assess the game's significance on data quality and user experience through a comparative user study. ",
        "title": "Evaluating User Experience and Data Quality in a Gamified Data  Collection for Appearance-Based Gaze Estimation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14098",
        "abstract_url": "http://arxiv.org/abs/2401.14098",
        "authors": [
            {
                "last_name": "Kundu",
                "first_name": "Suparna"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Siddhartha"
            },
            {
                "last_name": "Saha",
                "first_name": "Sayandeep"
            },
            {
                "last_name": "Karmakar",
                "first_name": "Angshuman"
            },
            {
                "last_name": "Mukhopadhyay",
                "first_name": "Debdeep"
            },
            {
                "last_name": "Verbauwhede",
                "first_name": "Ingrid"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  Post-quantum cryptographic (PQC) algorithms, especially those based on the learning with errors (LWE) problem, have been subjected to several physical attacks in the recent past. Although the attacks broadly belong to two classes - passive side-channel attacks and active fault attacks, the attack strategies vary significantly due to the inherent complexities of such algorithms. Exploring further attack surfaces is, therefore, an important step for eventually securing the deployment of these algorithms. Also, it is important to test the robustness of the already proposed countermeasures in this regard. In this work, we propose a new fault attack on side-channel secure masked implementation of LWE-based key-encapsulation mechanisms (KEMs) exploiting fault propagation. The attack typically originates due to an algorithmic modification widely used to enable masking, namely the Arithmetic-to-Boolean (A2B) conversion. We exploit the data dependency of the adder carry chain in A2B and extract sensitive information, albeit masking (of arbitrary order) being present. As a practical demonstration of the exploitability of this information leakage, we show key recovery attacks of Kyber, although the leakage also exists for other schemes like Saber. The attack on Kyber targets the decapsulation module and utilizes Belief Propagation (BP) for key recovery. To the best of our knowledge, it is the first attack exploiting an algorithmic component introduced to ease masking rather than only exploiting the randomness introduced by masking to obtain desired faults (as done by Delvaux). Finally, we performed both simulated and electromagnetic (EM) fault-based practical validation of the attack for an open-source first-order secure Kyber implementation running on an STM32 platform. ",
        "title": "Carry Your Fault: A Fault Propagation Attack on Side-Channel Protected  LWE-based KEM",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14100",
        "abstract_url": "http://arxiv.org/abs/2401.14100",
        "authors": [
            {
                "last_name": "Heinrich",
                "first_name": "Stefan"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Recently the adaption problem of Information-Based Complexity (IBC) for linear problems in the randomized setting was solved in Heinrich (J. Complexity 82, 2024, 101821). Several papers treating further aspects of this problem followed. However, all examples obtained so far were vector-valued. In this paper we settle the scalar-valued case. We study the complexity of mean computation in finite dimensional sequence spaces with mixed $L_p^N$ norms. We determine the $n$-th minimal errors in the randomized adaptive and non-adaptive setting. It turns out that among the problems considered there are examples where adaptive and non-adaptive $n$-th minimal errors deviate by a power of $n$. The gap can be (up to log factors) of the order $n^{1/4}$. We also show how to turn such results into infinite dimensional examples with suitable deviation for all $n$ simultaneously. ",
        "title": "Randomized Complexity of Mean Computation and the Adaption Problem",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14106",
        "abstract_url": "http://arxiv.org/abs/2401.14106",
        "authors": [
            {
                "last_name": "Buchholtz",
                "first_name": "Ulrik"
            },
            {
                "last_name": "de Jong",
                "first_name": "Tom"
            },
            {
                "last_name": "Rijke",
                "first_name": "Egbert"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  We characterize the epimorphisms in homotopy type theory (HoTT) as the fiberwise acyclic maps and develop a type-theoretic treatment of acyclic maps and types in the context of synthetic homotopy theory. We present examples and applications in group theory, such as the acyclicity of the Higman group, through the identification of groups with $0$-connected, pointed $1$-types. Many of our results are formalized as part of the agda-unimath library. ",
        "title": "Epimorphisms and Acyclic Types in Univalent Mathematics",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14107",
        "abstract_url": "http://arxiv.org/abs/2401.14107",
        "authors": [
            {
                "last_name": "Saeed",
                "first_name": "Aaqib"
            },
            {
                "last_name": "Spathis",
                "first_name": "Dimitris"
            },
            {
                "last_name": "Oh",
                "first_name": "Jungwoo"
            },
            {
                "last_name": "Choi",
                "first_name": "Edward"
            },
            {
                "last_name": "Etemad",
                "first_name": "Ali"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and compare it against eight competitive baselines designed to deal with noisy labels. We show that FHLR achieves significantly better performance when learning from noisy labels and achieves state-of-the-art by a large margin, with up to 19% accuracy improvement under symmetric and asymmetric noise. Notably, we find that FHLR is particularly robust to increased label noise, unlike prior works that suffer from severe performance degradation. Our work not only achieves better generalization in high-stakes health sensing benchmarks but also sheds light on how noise affects commonly-used models. ",
        "title": "Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14109",
        "abstract_url": "http://arxiv.org/abs/2401.14109",
        "authors": [
            {
                "last_name": "Tomut",
                "first_name": "Andrei"
            },
            {
                "last_name": "Jahromi",
                "first_name": "Saeed S."
            },
            {
                "last_name": "Singh",
                "first_name": "Sukhbinder"
            },
            {
                "last_name": "Ishtiaq",
                "first_name": "Faysal"
            },
            {
                "last_name": "Mu\u00f1oz",
                "first_name": "Cesar"
            },
            {
                "last_name": "Bajaj",
                "first_name": "Prabdeep Singh"
            },
            {
                "last_name": "Elborady",
                "first_name": "Ali"
            },
            {
                "last_name": "del Bimbo",
                "first_name": "Gianni"
            },
            {
                "last_name": "Alizadeh",
                "first_name": "Mehrazin"
            },
            {
                "last_name": "Montero",
                "first_name": "David"
            },
            {
                "last_name": "Martin-Ramiro",
                "first_name": "Pablo"
            },
            {
                "last_name": "Ibrahim",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Alaoui",
                "first_name": "Oussama Tahiri"
            },
            {
                "last_name": "Malcolm",
                "first_name": "John"
            },
            {
                "last_name": "Mugel",
                "first_name": "Samuel"
            },
            {
                "last_name": "Orus",
                "first_name": "Roman"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined and interpretable model compression. Our method is versatile and can be implemented with - or on top of - other compression techniques. As a benchmark, we demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model to only $30\\%$ of its original size while recovering over $90\\%$ of the original accuracy after a brief distributed retraining. ",
        "title": "CompactifAI: Extreme Compression of Large Language Models using  Quantum-Inspired Tensor Networks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14110",
        "abstract_url": "http://arxiv.org/abs/2401.14110",
        "authors": [
            {
                "last_name": "Blumenfeld",
                "first_name": "Yaniv"
            },
            {
                "last_name": "Hubara",
                "first_name": "Itay"
            },
            {
                "last_name": "Soudry",
                "first_name": "Daniel"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "AR"
        ],
        "abstract": "  The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy. ",
        "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width  Accumulators",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14111",
        "abstract_url": "http://arxiv.org/abs/2401.14111",
        "authors": [
            {
                "last_name": "Mishra",
                "first_name": "Rameshwar"
            },
            {
                "last_name": "Subramanyam",
                "first_name": "A V"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset. ",
        "title": "Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph  Conditioning in Diffusion Models",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14112",
        "abstract_url": "http://arxiv.org/abs/2401.14112",
        "authors": [
            {
                "last_name": "Xia",
                "first_name": "Haojun"
            },
            {
                "last_name": "Zheng",
                "first_name": "Zhen"
            },
            {
                "last_name": "Wu",
                "first_name": "Xiaoxia"
            },
            {
                "last_name": "Chen",
                "first_name": "Shiyang"
            },
            {
                "last_name": "Yao",
                "first_name": "Zhewei"
            },
            {
                "last_name": "Youn",
                "first_name": "Stephen"
            },
            {
                "last_name": "Bakhtiari",
                "first_name": "Arash"
            },
            {
                "last_name": "Wyatt",
                "first_name": "Michael"
            },
            {
                "last_name": "Zhuang",
                "first_name": "Donglin"
            },
            {
                "last_name": "Zhou",
                "first_name": "Zhongzhu"
            },
            {
                "last_name": "Ruwase",
                "first_name": "Olatunji"
            },
            {
                "last_name": "He",
                "first_name": "Yuxiong"
            },
            {
                "last_name": "Song",
                "first_name": "Shuaiwen Leon"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "AR"
        ],
        "abstract": "  Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline. The source code will be publicly available soon. ",
        "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric  Algorithm-System Co-Design",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14113",
        "abstract_url": "http://arxiv.org/abs/2401.14113",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Xiaobao"
            },
            {
                "last_name": "Pan",
                "first_name": "Fengjun"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Thong"
            },
            {
                "last_name": "Feng",
                "first_name": "Yichao"
            },
            {
                "last_name": "Liu",
                "first_name": "Chaoqun"
            },
            {
                "last_name": "Nguyen",
                "first_name": "Cong-Duy"
            },
            {
                "last_name": "Luu",
                "first_name": "Anh Tuan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstrate that our method surpasses state-of-the-art baselines, effectively improving the affinity, rationality, and diversity of hierarchical topic modeling with better performance on downstream tasks. ",
        "title": "On the Affinity, Rationality, and Diversity of Hierarchical Topic  Modeling",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14115",
        "abstract_url": "http://arxiv.org/abs/2401.14115",
        "authors": [
            {
                "last_name": "Kuang",
                "first_name": "Jian"
            },
            {
                "last_name": "Li",
                "first_name": "Wenjing"
            },
            {
                "last_name": "Li",
                "first_name": "Fang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Jun"
            },
            {
                "last_name": "Wu",
                "first_name": "Zhongcheng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Distracted driver activity recognition plays a critical role in risk aversion-particularly beneficial in intelligent transportation systems. However, most existing methods make use of only the video from a single view and the difficulty-inconsistent issue is neglected. Different from them, in this work, we propose a novel MultI-camera Feature Integration (MIFI) approach for 3D distracted driver activity recognition by jointly modeling the data from different camera views and explicitly re-weighting examples based on their degree of difficulty. Our contributions are two-fold: (1) We propose a simple but effective multi-camera feature integration framework and provide three types of feature fusion techniques. (2) To address the difficulty-inconsistent problem in distracted driver activity recognition, a periodic learning method, named example re-weighting that can jointly learn the easy and hard samples, is presented. The experimental results on the 3MDAD dataset demonstrate that the proposed MIFI can consistently boost performance compared to single-view models. ",
        "title": "MIFI: MultI-camera Feature Integration for Roust 3D Distracted Driver  Activity Recognition",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14117",
        "abstract_url": "http://arxiv.org/abs/2401.14117",
        "authors": [
            {
                "last_name": "Nakasato",
                "first_name": "Naohito"
            },
            {
                "last_name": "Murakami",
                "first_name": "Yuki"
            },
            {
                "last_name": "Kono",
                "first_name": "Fumiya"
            },
            {
                "last_name": "Nakata",
                "first_name": "Maho"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC",
            "AR",
            "MS"
        ],
        "abstract": "  We present an evaluation of 32-bit POSIT arithmetic through its implementation as accelerators on FPGAs and GPUs. POSIT, a floating-point number format, adaptively changes the size of its fractional part. We developed hardware designs for FPGAs and software for GPUs to accelerate linear algebra operations using Posit(32,2) arithmetic. Our FPGA- and GPU-based accelerators in Posit(32,2) arithmetic significantly accelerated the Cholesky and LU decomposition algorithms for dense matrices. In terms of numerical accuracy, Posit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the standard 32-bit format, especially when the norm of the elements of the input matrix is close to 1. Evaluating power consumption, we observed that the power efficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for the LU decomposition in Posit(32,2) arithmetic. The power efficiency of the latest GPUs as accelerators of Posit(32,2) arithmetic is better than that of the evaluated FPGA chip. ",
        "title": "Evaluation of POSIT Arithmetic with Accelerators",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14121",
        "abstract_url": "http://arxiv.org/abs/2401.14121",
        "authors": [
            {
                "last_name": "Nie",
                "first_name": "Yongwei"
            },
            {
                "last_name": "Fan",
                "first_name": "Mingxian"
            },
            {
                "last_name": "Long",
                "first_name": "Chengjiang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Qing"
            },
            {
                "last_name": "Zhu",
                "first_name": "Jian"
            },
            {
                "last_name": "Xu",
                "first_name": "Xuemiao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We propose a novel optimization-based human mesh recovery method from a single image. Given a test exemplar, previous approaches optimize the pre-trained regression network to minimize the 2D re-projection loss, which however suffer from over-/under-fitting problems. This is because the ``exemplar optimization'' at testing time has too weak relation to the pre-training process, and the exemplar optimization loss function is different from the training loss function. (1) We incorporate exemplar optimization into the training stage. During training, our method first executes exemplar optimization and subsequently proceeds with training-time optimization. The exemplar optimization may run into a wrong direction, while the subsequent training optimization serves to correct the deviation. Involved in training, the exemplar optimization learns to adapt its behavior to training data, thereby acquires generalibility to test exemplars. (2) We devise a dual-network architecture to convey the novel training paradigm, which is composed of a main regression network and an auxiliary network, in which we can formulate the exemplar optimization loss function in the same form as the training loss function. This further enhances the compatibility between the exemplar and training optimizations. Experiments demonstrate that our exemplar optimization after the novel training scheme significantly outperforms state-of-the-art approaches. ",
        "title": "Incorporating Exemplar Optimization into Training with Dual Networks for  Human Mesh Recovery",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14126",
        "abstract_url": "http://arxiv.org/abs/2401.14126",
        "authors": [
            {
                "last_name": "Breuvart",
                "first_name": "Flavien"
            },
            {
                "last_name": "Olimpieri",
                "first_name": "Federico"
            }
        ],
        "primary_category": "LO",
        "categories": [
            "LO"
        ],
        "abstract": "  Indexed Linear Logic has been introduced by Ehrhard and Bucciarelli, it can be seen as a logical presentation of non-idempotent intersection types extended through the relational semantics to the full linear logic. We introduce an idempotent variant of Indexed Linear Logic. We give a fine-grained reformulation of the syntax by exposing implicit parameters and by unifying several operations on formulae via the notion of base change. Idempotency is achieved by means of an appropriate subtyping relation. We carry on an in-depth study of indLL as a logic, showing how it determines a refinement of classical linear logic and establishing a terminating cut-elimination procedure. Cut-elimination is proved to be confluent up to an appropriate congruence induced by the subtyping relation. ",
        "title": "An Indexed Linear Logic for Idempotent Intersection Types (Long version)",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14129",
        "abstract_url": "http://arxiv.org/abs/2401.14129",
        "authors": [
            {
                "last_name": "Zhao",
                "first_name": "Boqun"
            },
            {
                "last_name": "Ouyang",
                "first_name": "Chongjun"
            },
            {
                "last_name": "Zhang",
                "first_name": "Xingqi"
            },
            {
                "last_name": "Liu",
                "first_name": "Yuanwei"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  A near-field holographic multiple-input multiple-output (MIMO) based integrated sensing and communications (ISAC) framework is proposed for both downlink and uplink scenarios, where spherical wave-based model is considered to capture the characteristics of the near field. The coupling effect introduced by the densely spaced antennas of the holographic MIMO are characterized by spatially correlated Rayleigh fading. Based on the proposed framework, by considering both instantaneous channel state information (CSI) and statistical CSI, closed-form expressions are derived for sensing rates (SRs), communication rates (CRs), and outage probabilities under different ISAC designs. Further insights are gained by examining high signal-to-noise ratio slopes and diversity orders. Specifically, 1) for the downlink case, a sensing-centric (S-C) design and a communications-centric (C-C) design are investigated based on different beamforming strategies, and a Pareto optimal design is proposed to characterize the attainable SR-CR region; and 2) for the uplink case, the S-C design and the C-C design are distinguished by the interference cancellation order of the communication signal and the sensing signal, and the rate region is obtained through a time-sharing strategy. Numerical results reveal that the proposed ISAC system achieves more extensive rate regions than the conventional frequency-division sensing and communications system, highlighting its superior performance. ",
        "title": "Performance Analysis for Near-Field ISAC: A Holographic MIMO Design",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14130",
        "abstract_url": "http://arxiv.org/abs/2401.14130",
        "authors": [
            {
                "last_name": "Lin",
                "first_name": "Yihao"
            },
            {
                "last_name": "Li",
                "first_name": "Ximeng"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yan"
            },
            {
                "last_name": "Tang",
                "first_name": "Jinshan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy. ",
        "title": "Attention-based Efficient Classification for 3D MRI Image of Alzheimer's  Disease",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14131",
        "abstract_url": "http://arxiv.org/abs/2401.14131",
        "authors": [
            {
                "last_name": "Andersdotter",
                "first_name": "Emma"
            },
            {
                "last_name": "Ohlsson",
                "first_name": "Fredrik"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this paper we develop a manifestly geometric framework for equivariant manifold neural ordinary differential equations (NODEs), and use it to analyse their modelling capabilities for symmetric data. First, we consider the action of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence between equivariance of vector fields, symmetries of the corresponding Cauchy problems, and equivariance of the associated NODEs. We also propose a novel formulation of the equivariant NODEs in terms of the differential invariants of the action of $G$ on $M$, based on Lie theory for symmetries of differential equations, which provides an efficient parameterisation of the space of equivariant vector fields in a way that is agnostic to both the manifold $M$ and the symmetry group $G$. Second, we construct augmented manifold NODEs, through embeddings into equivariant flows, and show that they are universal approximators of equivariant diffeomorphisms on any path-connected $M$. Furthermore, we show that the augmented NODEs can be incorporated in the geometric framework and parameterised using higher order differential invariants. Finally, we consider the induced action of $G$ on different fields on $M$ and show how it can be used to generalise previous work, on, e.g., continuous normalizing flows, to equivariant models in any geometry. ",
        "title": "Equivariant Manifold Neural ODEs and Differential Invariants",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14132",
        "abstract_url": "http://arxiv.org/abs/2401.14132",
        "authors": [
            {
                "last_name": "Min",
                "first_name": "Chulhong"
            },
            {
                "last_name": "Yi",
                "first_name": "Juheon"
            },
            {
                "last_name": "Acer",
                "first_name": "Utku Gunay"
            },
            {
                "last_name": "Kawsar",
                "first_name": "Fahim"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "DC"
        ],
        "abstract": "  Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis. However, existing visual analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures. In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras. We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras. We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities. Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to the state-of-the-art), while achieving comparable tracking quality. ",
        "title": "Enabling Cross-Camera Collaboration for Video Analytics on Distributed  Smart Cameras",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14135",
        "abstract_url": "http://arxiv.org/abs/2401.14135",
        "authors": [
            {
                "last_name": "Barman",
                "first_name": "Amit"
            },
            {
                "last_name": "Roy",
                "first_name": "Devangan"
            },
            {
                "last_name": "Paul",
                "first_name": "Debapriya"
            },
            {
                "last_name": "Dutta",
                "first_name": "Indranil"
            },
            {
                "last_name": "Guha",
                "first_name": "Shouvik Kumar"
            },
            {
                "last_name": "Karmakar",
                "first_name": "Samir"
            },
            {
                "last_name": "Naskar",
                "first_name": "Sudip Kumar"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "CY",
            "LG"
        ],
        "abstract": "  There is an evident lack of implementation of Machine Learning (ML) in the legal domain in India, and any research that does take place in this domain is usually based on data from the higher courts of law and works with English data. The lower courts and data from the different regional languages of India are often overlooked. In this paper, we deploy a Convolutional Neural Network (CNN) architecture on a corpus of Hindi legal documents. We perform a bail Prediction task with the help of a CNN model and achieve an overall accuracy of 93\\% which is an improvement on the benchmark accuracy, set by Kapoor et al. (2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh. ",
        "title": "Convolutional Neural Networks can achieve binary bail judgement  classification",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14136",
        "abstract_url": "http://arxiv.org/abs/2401.14136",
        "authors": [
            {
                "last_name": "Lohesara",
                "first_name": "Fatemeh Ghorbani"
            },
            {
                "last_name": "Egiazarian",
                "first_name": "Karen"
            },
            {
                "last_name": "Knorr",
                "first_name": "Sebastian"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Head-mounted displays (HMDs) serve as indispensable devices for observing extended reality (XR) environments and virtual content. However, HMDs present an obstacle to external recording techniques as they block the upper face of the user. This limitation significantly affects social XR applications, specifically teleconferencing, where facial features and eye gaze information play a vital role in creating an immersive user experience. In this study, we propose a new network for expression-aware video inpainting for HMD removal (EVI-HRnet) based on generative adversarial networks (GANs). Our model effectively fills in missing information with regard to facial landmarks and a single occlusion-free reference image of the user. The framework and its components ensure the preservation of the user's identity across frames using the reference frame. To further improve the level of realism of the inpainted output, we introduce a novel facial expression recognition (FER) loss function for emotion preservation. Our results demonstrate the remarkable capability of the proposed framework to remove HMDs from facial videos while maintaining the subject's facial expression and identity. Moreover, the outputs exhibit temporal consistency along the inpainted frames. This lightweight framework presents a practical approach for HMD occlusion removal, with the potential to enhance various collaborative XR applications without the need for additional hardware. ",
        "title": "Expression-aware video inpainting for HMD removal in XR applications",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14141",
        "abstract_url": "http://arxiv.org/abs/2401.14141",
        "authors": [
            {
                "last_name": "Qayyum",
                "first_name": "Hina"
            },
            {
                "last_name": "Ikram",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Zhao",
                "first_name": "Benjamin Zi Hao"
            },
            {
                "last_name": "Wood",
                "first_name": "Ian D."
            },
            {
                "last_name": "Kourtellis",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Kaafar",
                "first_name": "Mohamed Ali"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In the pursuit of bolstering user safety, social media platforms deploy active moderation strategies, including content removal and user suspension. These measures target users engaged in discussions marked by hate speech or toxicity, often linked to specific keywords or hashtags. Nonetheless, the increasing prevalence of toxicity indicates that certain users adeptly circumvent these measures. This study examines consistently toxic users on Twitter (rebranded as X) Rather than relying on traditional methods based on specific topics or hashtags, we employ a novel approach based on patterns of toxic tweets, yielding deeper insights into their behavior. We analyzed 38 million tweets from the timelines of 12,148 Twitter users and identified the top 1,457 users who consistently exhibit toxic behavior, relying on metrics like the Gini index and Toxicity score. By comparing their posting patterns to those of non-consistently toxic users, we have uncovered distinctive temporal patterns, including contiguous activity spans, inter-tweet intervals (referred to as 'Burstiness'), and churn analysis. These findings provide strong evidence for the existence of a unique tweeting pattern associated with toxic behavior on Twitter. Crucially, our methodology transcends Twitter and can be adapted to various social media platforms, facilitating the identification of consistently toxic users based on their posting behavior. This research contributes to ongoing efforts to combat online toxicity and offers insights for refining moderation strategies in the digital realm. We are committed to open research and will provide our code and data to the research community. ",
        "title": "Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14142",
        "abstract_url": "http://arxiv.org/abs/2401.14142",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Xinyue"
            },
            {
                "last_name": "Qin",
                "first_name": "Yi"
            },
            {
                "last_name": "Mi",
                "first_name": "Lu"
            },
            {
                "last_name": "Wang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Xiaomeng"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not help correct highly correlated concepts (e.g., \"yellow belly\"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label \"Kentucky Warbler\" and a concept \"black bill\", what is the probability that the model correctly predicts another concept \"black crown\"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations. Empirical results show that our approach outperforms the state-of-the-art on real-world datasets. ",
        "title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept  Intervention, and Conditional Interpretations",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14147",
        "abstract_url": "http://arxiv.org/abs/2401.14147",
        "authors": [
            {
                "last_name": "Grimmeisen",
                "first_name": "Philipp"
            },
            {
                "last_name": "Sautter",
                "first_name": "Friedrich"
            },
            {
                "last_name": "Morozov",
                "first_name": "Andrey"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  AI-controlled robotic systems pose a risk to human workers and the environment. Classical risk assessment methods cannot adequately describe such black box systems. Therefore, new methods for a dynamic risk assessment of such AI-controlled systems are required. In this paper, we introduce the concept of a new dynamic risk assessment approach for AI-controlled robotic systems. The approach pipelines five blocks: (i) a Data Logging that logs the data of the given simulation, (ii) a Skill Detection that automatically detects the executed skills with a deep learning technique, (iii) a Behavioral Analysis that creates the behavioral profile of the robotic systems, (iv) a Risk Model Generation that automatically transforms the behavioral profile and risk data containing the failure probabilities of robotic hardware components into advanced hybrid risk models, and (v) Risk Model Solvers for the numerical evaluation of the generated hybrid risk models.   Keywords: Dynamic Risk Assessment, Hybrid Risk Models, M2M Transformation, ROS, AI-Controlled Robotic Systems, Deep Learning, Reinforcement Learning ",
        "title": "Concept: Dynamic Risk Assessment for AI-Controlled Robotic Systems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14148",
        "abstract_url": "http://arxiv.org/abs/2401.14148",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Zhenbin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lei"
            },
            {
                "last_name": "Wang",
                "first_name": "Lituan"
            },
            {
                "last_name": "Zhu",
                "first_name": "Minjuan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data distribution when transferring knowledge from multiple labeled source domains to an unlabeled target domain. However, existing MSDA techniques assume target domain images are available, yet overlook image-rich semantic information. Consequently, an open question is whether MSDA can be guided solely by textual cues in the absence of target domain images. By employing a multimodal model with a joint image and language embedding space, we propose a novel language-guided MSDA approach, termed LanDA, based on optimal transfer theory, which facilitates the transfer of multiple source domains to a new target domain, requiring only a textual description of the target domain without needing even a single target domain image, while retaining task-relevant information. We present extensive experiments across different transfer scenarios using a suite of relevant benchmarks, demonstrating that LanDA outperforms standard fine-tuning and ensemble approaches in both target and source domains. ",
        "title": "LanDA: Language-Guided Multi-Source Domain Adaptation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14149",
        "abstract_url": "http://arxiv.org/abs/2401.14149",
        "authors": [
            {
                "last_name": "K\u00fcsters",
                "first_name": "Aaron"
            },
            {
                "last_name": "van der Aalst",
                "first_name": "Wil M. P."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  The most commonly used open-source process mining software tools today are ProM and PM4Py, written in Java and Python, respectively. Such high-level, often interpreted, programming languages trade off performance with memory safety and ease-of-use. In contrast, traditional compiled languages, like C or C++, can achieve top performance but often suffer from instability related to unsafe memory management. Lately, Rust emerged as a highly performant, compiled programming language with inherent memory safety. In this paper, we describe our approach to developing a shared process mining library in Rust with bindings to both Java and Python, allowing full integration into the existing ecosystems, like ProM and PM4Py. By facilitating interoperability, our methodology enables researchers or industry to develop novel algorithms in Rust once and make them accessible to the entire community while also achieving superior performance. ",
        "title": "Developing a High-Performance Process Mining Library with Java and  Python Bindings in Rust",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14151",
        "abstract_url": "http://arxiv.org/abs/2401.14151",
        "authors": [
            {
                "last_name": "Tan",
                "first_name": "Weihao"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wentao"
            },
            {
                "last_name": "Liu",
                "first_name": "Shanqi"
            },
            {
                "last_name": "Zheng",
                "first_name": "Longtao"
            },
            {
                "last_name": "Wang",
                "first_name": "Xinrun"
            },
            {
                "last_name": "An",
                "first_name": "Bo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning. ",
        "title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied  Environments via Reinforcement Learning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14153",
        "abstract_url": "http://arxiv.org/abs/2401.14153",
        "authors": [
            {
                "last_name": "Carbo",
                "first_name": "J."
            },
            {
                "last_name": "Sanchez",
                "first_name": "N."
            },
            {
                "last_name": "Molina",
                "first_name": "J. M."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this paper an agent-based simulation is developed in order to evaluate an AmI scenario based on agents. Many AmI applications are implemented through agents but they are not compared to any other existing alternative in order to evaluate the relative benefits of using them. The proposal simulation environment developed in Netlogo analyse such benefits using two evaluation criteria: First, measuring agent satisfaction of different types of desires along the execution. Second, measuring time savings obtained through a correct use of context information.   So, here, a previously suggested agent architecture, an ontology and a 12-steps protocol to provide AmI services in airports, is evaluated using a NetLogo simulation environment. The present work uses a NetLogo model considering scalability problems of this application domain but using FIPA and BDI extensions to be coherent with our previous works and our previous JADE implementation of them.   The NetLogo model presented simulates an airport with agent users passing through several zones located in a specific order in a map: passport controls, check-in counters of airline companies, boarding gates, different types of shopping. Although initial data in simulations are generated randomly, and the model is just an approximation of real-world airports, the definition of this case of use of Ambient Intelligence through NetLogo agents opens an interesting way to evaluate the benefits of using Ambient Intelligence, which is a significant contribution to the final development of them. ",
        "title": "Agent-based Simulation with Netlogo to Evaluate AmI Scenarios",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14155",
        "abstract_url": "http://arxiv.org/abs/2401.14155",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Yuan"
            },
            {
                "last_name": "Wang",
                "first_name": "Xiang"
            },
            {
                "last_name": "He",
                "first_name": "Xiangnan"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhenguang"
            },
            {
                "last_name": "Feng",
                "first_name": "Huamin"
            },
            {
                "last_name": "Zhang",
                "first_name": "Yongdong"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.   This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophily. We tease out the anomaly features on which we constrain to mitigate the effect of heterophilous neighbors and make them invariant. We term our proposed framework as Graph Decomposition Network (GDN). Extensive experiments are conducted on two benchmark datasets, and the proposed framework achieves a remarkable performance boost in GAD, especially in an SDS environment where anomalies have largely different structural distribution across training and testing environments. Codes are open-sourced in https://github.com/blacksingular/wsdm_GDN. ",
        "title": "Alleviating Structural Distribution Shift in Graph Anomaly Detection",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14159",
        "abstract_url": "http://arxiv.org/abs/2401.14159",
        "authors": [
            {
                "last_name": "Ren",
                "first_name": "Tianhe"
            },
            {
                "last_name": "Liu",
                "first_name": "Shilong"
            },
            {
                "last_name": "Zeng",
                "first_name": "Ailing"
            },
            {
                "last_name": "Lin",
                "first_name": "Jing"
            },
            {
                "last_name": "Li",
                "first_name": "Kunchang"
            },
            {
                "last_name": "Cao",
                "first_name": "He"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiayu"
            },
            {
                "last_name": "Huang",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Chen",
                "first_name": "Yukang"
            },
            {
                "last_name": "Yan",
                "first_name": "Feng"
            },
            {
                "last_name": "Zeng",
                "first_name": "Zhaoyang"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Li",
                "first_name": "Feng"
            },
            {
                "last_name": "Yang",
                "first_name": "Jie"
            },
            {
                "last_name": "Li",
                "first_name": "Hongyang"
            },
            {
                "last_name": "Jiang",
                "first_name": "Qing"
            },
            {
                "last_name": "Zhang",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models. ",
        "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14160",
        "abstract_url": "http://arxiv.org/abs/2401.14160",
        "authors": [
            {
                "last_name": "Niu",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ping"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  Semantic communication initiates a new direction for future communication. In this paper, we aim to establish a systematic framework of semantic information theory (SIT). First, we propose a semantic communication model and define the synonymous mapping to indicate the critical relationship between semantic information and syntactic information. Based on this core concept, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems of SIT, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of information theory are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory. In summary, the theoretic framework proposed in this paper is a natural extension of classic information theory and may reveal great performance potential for future communication. ",
        "title": "A Mathematical Theory of Semantic Communication: Overview",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14163",
        "abstract_url": "http://arxiv.org/abs/2401.14163",
        "authors": [
            {
                "last_name": "Gu",
                "first_name": "Shanshan"
            },
            {
                "last_name": "Zhai",
                "first_name": "Qilong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In this article, we decrease the degree of the polynomials on the boundary of the weak functions and modify the definition of the weak laplacian which are introduced in \\cite{BiharmonicSFWG} to use the SFWG method for the biharmonic equation. Then we propose the relevant numerical format and obtain the optimal order of error estimates in $H^2$ and $L^2$ norms. Finally, we confirm the estimates using numerical experiments. ",
        "title": "The stabilizer-free weak Galerkin finite element method for the  Biharmonic equation using polynomials of reduced order",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14166",
        "abstract_url": "http://arxiv.org/abs/2401.14166",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Jiangmeng"
            },
            {
                "last_name": "Song",
                "first_name": "Fei"
            },
            {
                "last_name": "Jin",
                "first_name": "Yifan"
            },
            {
                "last_name": "Qiang",
                "first_name": "Wenwen"
            },
            {
                "last_name": "Zheng",
                "first_name": "Changwen"
            },
            {
                "last_name": "Sun",
                "first_name": "Fuchun"
            },
            {
                "last_name": "Xiong",
                "first_name": "Hui"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs. Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks. ",
        "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on  Few-shot Inference via Debiased Domain Abstraction",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14168",
        "abstract_url": "http://arxiv.org/abs/2401.14168",
        "authors": [
            {
                "last_name": "Yang",
                "first_name": "Yijun"
            },
            {
                "last_name": "Xing",
                "first_name": "Zhaohu"
            },
            {
                "last_name": "Zhu",
                "first_name": "Lei"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Traditional convolutional neural networks have a limited receptive field while transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity. Such the bottleneck poses a significant challenge when processing long video sequences in video analysis tasks. Very recently, the state space models (SSMs) with efficient hardware-aware designs, famous by Mamba, have exhibited impressive achievements in long sequence modeling, which facilitates the development of deep neural networks on many vision tasks. To better capture available cues in video frames, this paper presents a generic Video Vision Mamba-based framework for medical video object segmentation tasks, named Vivim. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales by our designed Temporal Mamba Block. Compared to existing video-level Transformer-based methods, our model maintains excellent segmentation results with better speed performance. Extensive experiments on the breast US dataset demonstrate the effectiveness and efficiency of our Vivim. The code for Vivim is available at: https://github.com/scott-yjyang/Vivim. ",
        "title": "Vivim: a Video Vision Mamba for Medical Video Object Segmentation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14169",
        "abstract_url": "http://arxiv.org/abs/2401.14169",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Huifang"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  We present a finite volume method preserving the invariant region property (IRP) for the reaction-diffusion systems with quasimonotone functions, including nondecreasing, decreasing, and mixed quasimonotone systems. The diffusion terms and time derivatives are discretized by a finite volume method satisfying the discrete maximum principle (DMP) and the backward Euler method, respectively. The discretization leads to an implicit and nonlinear scheme, and it is proved to preserve the invariant region property unconditionally. We construct an iterative algorithm and prove the invariant region property ar each iteration step. Numerical examples are shown to confirm the accuracy and invariant region property of our scheme. ",
        "title": "A finite volume method preserving the invariant region property for the  quasimonotone reaction-diffusion systems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14171",
        "abstract_url": "http://arxiv.org/abs/2401.14171",
        "authors": [
            {
                "last_name": "Perlo",
                "first_name": "Daniele"
            },
            {
                "last_name": "Kanli",
                "first_name": "Georgia"
            },
            {
                "last_name": "Boudissa",
                "first_name": "Selma"
            },
            {
                "last_name": "Keunen",
                "first_name": "Olivier"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This research paper presents a novel approach to the prediction of hypoxia in brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia, a condition characterized by low oxygen levels, is a common feature of malignant brain tumors associated with poor prognosis. Fluoromisonidazole Positron Emission Tomography (FMISO PET) is a well-established method for detecting hypoxia in vivo, but it is expensive and not widely available. Our study proposes the use of MRI, a more accessible and cost-effective imaging modality, to predict FMISO PET signals. We investigate deep learning models (DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and FMISO PET images from patients with brain tumors. Our trained models effectively learn the complex relationships between the MRI features and the corresponding FMISO PET signals, thereby enabling the prediction of hypoxia from MRI scans alone. The results show a strong correlation between the predicted and actual FMISO PET signals, with an overall PSNR score above 29.6 and a SSIM score greater than 0.94, confirming MRI as a promising option for hypoxia prediction in brain tumors. This approach could significantly improve the accessibility of hypoxia detection in clinical settings, with the potential for more timely and targeted treatments. ",
        "title": "Predicting Hypoxia in Brain Tumors from Multiparametric MRI",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14173",
        "abstract_url": "http://arxiv.org/abs/2401.14173",
        "authors": [
            {
                "last_name": "Dinc",
                "first_name": "Niyazi Ulas"
            },
            {
                "last_name": "Yildirim",
                "first_name": "Mustafa"
            },
            {
                "last_name": "Moser",
                "first_name": "Christophe"
            },
            {
                "last_name": "Psaltis",
                "first_name": "Demetri"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI"
        ],
        "abstract": "  Artificial Intelligence (AI) demands large data flows within datacenters, heavily relying on multicasting data transfers. As AI models scale, the requirement for high-bandwidth and low-latency networking compounds. The common use of electrical packet switching faces limitations due to its optical-electrical-optical conversion bottleneck. Optical switches, while bandwidth-agnostic and low-latency, suffer from having only unicast or non-scalable multicasting capability. This paper introduces an optical switching technique addressing the scalable multicasting challenge. Our approach enables arbitrarily programmable simultaneous unicast and multicast connectivity, eliminating the need for optical splitters that hinder scalability due to optical power loss. We use phase modulation in multiple planes, tailored to implement any multicast connectivity map. Using phase modulation enables wavelength selectivity on top of spatial selectivity, resulting in an optical switch that implements space-wavelength routing. We conducted simulations and experiments to validate our approach. Our results affirm the concept's feasibility and effectiveness, as a multicasting switch. ",
        "title": "Multicasting Optical Reconfigurable Switch",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14174",
        "abstract_url": "http://arxiv.org/abs/2401.14174",
        "authors": [
            {
                "last_name": "Brand",
                "first_name": "Cornelius"
            },
            {
                "last_name": "Ganian",
                "first_name": "Robert"
            },
            {
                "last_name": "Inerney",
                "first_name": "Fionn Mc"
            },
            {
                "last_name": "Wietheger",
                "first_name": "Simon"
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  We study the complexity-theoretic boundaries of tractability for three classical problems in the context of Hierarchical Task Network Planning: the validation of a provided plan, whether an executable plan exists, and whether a given state can be reached by some plan. We show that all three problems can be solved in polynomial time on primitive task networks of constant partial order width (and a generalization thereof), whereas for the latter two problems this holds only under a provably necessary restriction to the state space. Next, we obtain an algorithmic meta-theorem along with corresponding lower bounds to identify tight conditions under which general polynomial-time solvability results can be lifted from primitive to general task networks. Finally, we enrich our investigation by analyzing the parameterized complexity of the three considered problems, and show that (1) fixed-parameter tractability for all three problems can be achieved by replacing the partial order width with the vertex cover number of the network as the parameter, and (2) other classical graph-theoretic parameters of the network (including treewidth, treedepth, and the aforementioned partial order width) do not yield fixed-parameter tractability for any of the three problems. ",
        "title": "The Boundaries of Tractability in Hierarchical Task Network Planning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14176",
        "abstract_url": "http://arxiv.org/abs/2401.14176",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Beiqi"
            },
            {
                "last_name": "Liang",
                "first_name": "Peng"
            },
            {
                "last_name": "Feng",
                "first_name": "Qiong"
            },
            {
                "last_name": "Fu",
                "first_name": "Yujia"
            },
            {
                "last_name": "Li",
                "first_name": "Zengyang"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing Python code smells generated by Copilot itself. Besides, the effectiveness of Copilot Chat in fixing these smells can be improved with the provision of more detailed prompts. However, using Copilot Chat to fix these smells might introduce new code smells. ",
        "title": "Copilot Refinement: Addressing Code Smells in Copilot-Generated Python  Code",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14183",
        "abstract_url": "http://arxiv.org/abs/2401.14183",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Liming"
            },
            {
                "last_name": "Mak",
                "first_name": "Stephen"
            },
            {
                "last_name": "Proselkov",
                "first_name": "Yaniv"
            },
            {
                "last_name": "Brintrup",
                "first_name": "Alexandra"
            }
        ],
        "primary_category": "MA",
        "categories": [
            "MA"
        ],
        "abstract": "  Recent global disruptions, such as the pandemic and geopolitical conflicts, have profoundly exposed vulnerabilities in traditional supply chains, requiring exploration of more resilient alternatives. Autonomous supply chains (ASCs) have emerged as a potential solution, offering increased visibility, flexibility, and resilience in turbulent trade environments. Despite discussions in industry and academia over several years, ASCs lack well-established theoretical foundations. This paper addresses this research gap by presenting a formal definition of ASC along with its defining characteristics and auxiliary concepts. We propose a layered conceptual framework called the MIISI model. An illustrative case study focusing on the meat supply chain demonstrates an initial ASC implementation based on this conceptual model. Additionally, we introduce a seven-level supply chain autonomy reference model, delineating a trajectory towards achieving a full supply chain autonomy. Recognising that this work represents an initial endeavour, we emphasise the need for continued exploration in this emerging domain. We anticipate that this work will stimulate further research, both theoretical and technical, and contribute to the continual evolution of ASCs. ",
        "title": "Towards Autonomous Supply Chains: Definition, Characteristics,  Conceptual Framework, and Autonomy Levels",
        "date": "2023-10-13",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14184",
        "abstract_url": "http://arxiv.org/abs/2401.14184",
        "authors": [
            {
                "last_name": "Kurmukova",
                "first_name": "Anastasiia"
            },
            {
                "last_name": "Gunduz",
                "first_name": "Deniz"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "LG"
        ],
        "abstract": "  This paper introduces a novel approach called \"friendly attack\" aimed at enhancing the performance of error correction channel codes. Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance. By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint. The perturbation design is accomplished by a modified iterative fast gradient method. This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations. Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes. We demonstrate that the proposed friendly attack method can improve the reliability across different channels, modulations, codes, and decoders. This method allows us to increase the reliability of communication with a legacy receiver by simply modifying the transmitted codeword appropriately. ",
        "title": "Friendly Attacks to Improve Channel Coding Reliability",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14185",
        "abstract_url": "http://arxiv.org/abs/2401.14185",
        "authors": [
            {
                "last_name": "Pegg",
                "first_name": "Samuel"
            },
            {
                "last_name": "Li",
                "first_name": "Kai"
            },
            {
                "last_name": "Hu",
                "first_name": "Xiaolin"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies. Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance. In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method. TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\\% across all performance metrics compared with the previous SOTA method CTCNet. Remarkably, these results are achieved using fewer parameters and only 28\\% of the multiply-accumulate operations (MACs) of CTCNet. In essence, our method presents a highly effective and efficient solution to the challenges of speech separation within the audio-visual domain, making significant strides in harnessing visual information optimally. ",
        "title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down  Fusion",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14192",
        "abstract_url": "http://arxiv.org/abs/2401.14192",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Lei"
            },
            {
                "last_name": "Yu",
                "first_name": "Shuo"
            },
            {
                "last_name": "Wang",
                "first_name": "Runze"
            },
            {
                "last_name": "Ma",
                "first_name": "Zhenxun"
            },
            {
                "last_name": "Shen",
                "first_name": "Yanming"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CL"
        ],
        "abstract": "  While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diverse spatial-temporal benchmark datasets show that STG-LLM successfully unlocks LLM potential for spatial-temporal forecasting. Remarkably, our approach achieves competitive performance on par with dedicated SOTA methods. ",
        "title": "How Can Large Language Models Understand Spatial-Temporal Data?",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14193",
        "abstract_url": "http://arxiv.org/abs/2401.14193",
        "authors": [
            {
                "last_name": "Heinlein",
                "first_name": "Lukas"
            },
            {
                "last_name": "Maron",
                "first_name": "Roman C."
            },
            {
                "last_name": "Hekler",
                "first_name": "Achim"
            },
            {
                "last_name": "Haggenm\u00fcller",
                "first_name": "Sarah"
            },
            {
                "last_name": "Wies",
                "first_name": "Christoph"
            },
            {
                "last_name": "Utikal",
                "first_name": "Jochen S."
            },
            {
                "last_name": "Meier",
                "first_name": "Friedegund"
            },
            {
                "last_name": "Hobelsberger",
                "first_name": "Sarah"
            },
            {
                "last_name": "Gellrich",
                "first_name": "Frank F."
            },
            {
                "last_name": "Sergon",
                "first_name": "Mildred"
            },
            {
                "last_name": "Hauschild",
                "first_name": "Axel"
            },
            {
                "last_name": "French",
                "first_name": "Lars E."
            },
            {
                "last_name": "Heinzerling",
                "first_name": "Lucie"
            },
            {
                "last_name": "Schlager",
                "first_name": "Justin G."
            },
            {
                "last_name": "Ghoreschi",
                "first_name": "Kamran"
            },
            {
                "last_name": "Schlaak",
                "first_name": "Max"
            },
            {
                "last_name": "Hilke",
                "first_name": "Franz J."
            },
            {
                "last_name": "Poch",
                "first_name": "Gabriela"
            },
            {
                "last_name": "Korsing",
                "first_name": "S\u00f6ren"
            },
            {
                "last_name": "Berking",
                "first_name": "Carola"
            },
            {
                "last_name": "Heppt",
                "first_name": "Markus V."
            },
            {
                "last_name": "Erdmann",
                "first_name": "Michael"
            },
            {
                "last_name": "Haferkamp",
                "first_name": "Sebastian"
            },
            {
                "last_name": "Drexler",
                "first_name": "Konstantin"
            },
            {
                "last_name": "Schadendorf",
                "first_name": "Dirk"
            },
            {
                "last_name": "Sondermann",
                "first_name": "Wiebke"
            },
            {
                "last_name": "Goebeler",
                "first_name": "Matthias"
            },
            {
                "last_name": "Schilling",
                "first_name": "Bastian"
            },
            {
                "last_name": "Krieghoff-Henning",
                "first_name": "Eva"
            },
            {
                "last_name": "Brinker",
                "first_name": "Titus J."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Early detection of melanoma, a potentially lethal type of skin cancer with high prevalence worldwide, improves patient prognosis. In retrospective studies, artificial intelligence (AI) has proven to be helpful for enhancing melanoma detection. However, there are few prospective studies confirming these promising results. Existing studies are limited by low sample sizes, too homogenous datasets, or lack of inclusion of rare melanoma subtypes, preventing a fair and thorough evaluation of AI and its generalizability, a crucial aspect for its application in the clinical setting. Therefore, we assessed 'All Data are Ext' (ADAE), an established open-source ensemble algorithm for detecting melanomas, by comparing its diagnostic accuracy to that of dermatologists on a prospectively collected, external, heterogeneous test set comprising eight distinct hospitals, four different camera setups, rare melanoma subtypes, and special anatomical sites. We advanced the algorithm with real test-time augmentation (R-TTA, i.e. providing real photographs of lesions taken from multiple angles and averaging the predictions), and evaluated its generalization capabilities. Overall, the AI showed higher balanced accuracy than dermatologists (0.798, 95% confidence interval (CI) 0.779-0.814 vs. 0.781, 95% CI 0.760-0.802; p<0.001), obtaining a higher sensitivity (0.921, 95% CI 0.900- 0.942 vs. 0.734, 95% CI 0.701-0.770; p<0.001) at the cost of a lower specificity (0.673, 95% CI 0.641-0.702 vs. 0.828, 95% CI 0.804-0.852; p<0.001). As the algorithm exhibited a significant performance advantage on our heterogeneous dataset exclusively comprising melanoma-suspicious lesions, AI may offer the potential to support dermatologists particularly in diagnosing challenging cases. ",
        "title": "Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from  a Prospective Multicenter Study",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14194",
        "abstract_url": "http://arxiv.org/abs/2401.14194",
        "authors": [
            {
                "last_name": "Ravaut",
                "first_name": "Mathieu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hao"
            },
            {
                "last_name": "Xu",
                "first_name": "Lu"
            },
            {
                "last_name": "Sun",
                "first_name": "Aixin"
            },
            {
                "last_name": "Liu",
                "first_name": "Yong"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on two benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of PECRS on recommendation and conversation. Our code is available at: https://github.com/Ravoxsg/efficient_unified_crs. ",
        "title": "Parameter-Efficient Conversational Recommender System as a Language  Processing Task",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14196",
        "abstract_url": "http://arxiv.org/abs/2401.14196",
        "authors": [
            {
                "last_name": "Guo",
                "first_name": "Daya"
            },
            {
                "last_name": "Zhu",
                "first_name": "Qihao"
            },
            {
                "last_name": "Yang",
                "first_name": "Dejian"
            },
            {
                "last_name": "Xie",
                "first_name": "Zhenda"
            },
            {
                "last_name": "Dong",
                "first_name": "Kai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wentao"
            },
            {
                "last_name": "Chen",
                "first_name": "Guanting"
            },
            {
                "last_name": "Bi",
                "first_name": "Xiao"
            },
            {
                "last_name": "Wu",
                "first_name": "Y."
            },
            {
                "last_name": "Li",
                "first_name": "Y. K."
            },
            {
                "last_name": "Luo",
                "first_name": "Fuli"
            },
            {
                "last_name": "Xiong",
                "first_name": "Yingfei"
            },
            {
                "last_name": "Liang",
                "first_name": "Wenfeng"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "CL",
            "LG"
        ],
        "abstract": "  The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use. ",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The  Rise of Code Intelligence",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14199",
        "abstract_url": "http://arxiv.org/abs/2401.14199",
        "authors": [
            {
                "last_name": "Su",
                "first_name": "Junwei"
            },
            {
                "last_name": "Wu",
                "first_name": "Shan"
            },
            {
                "last_name": "Li",
                "first_name": "Jinhui"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies. ",
        "title": "MTRGL:Effective Temporal Correlation Discerning through Multi-modal  Temporal Relational Graph Learning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14202",
        "abstract_url": "http://arxiv.org/abs/2401.14202",
        "authors": [
            {
                "last_name": "Nitzsche",
                "first_name": "Marius"
            },
            {
                "last_name": "Hahn",
                "first_name": "Bernadette N"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  In Magnetic Particle Imaging (MPI), it is typically assumed that the studied specimen is stationary during the data acquisition. In practical applications however, the searched-for 3D distribution of the magnetic nanoparticles might show a dynamic behavior, caused by e.g. breathing or movement of the blood. Neglecting those dynamics during the reconstruction step results in motion artifacts and a reduced image quality.   This article addresses the challenge of capturing high quality images in the presence of motion. A promising technique provides the Regularized Sequential Subspace Optimization (RESESOP) algorithm, which takes dynamics as model inexactness into account, significantly improving reconstruction compared to standard static algorithms like regularized Kaczmarz. Notably, this algorithm operates with minimal prior information and the method allows for subframe reconstruction, making it suitable for scenarios with rapid particle movement. The performance of the proposed method is demonstrated on both simulated and real data sets. ",
        "title": "Dynamic image reconstruction in MPI with RESESOP-Kaczmarz",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14203",
        "abstract_url": "http://arxiv.org/abs/2401.14203",
        "authors": [
            {
                "last_name": "Nguyen",
                "first_name": "Thanh Luan"
            },
            {
                "last_name": "Kaddoum",
                "first_name": "Georges"
            },
            {
                "last_name": "Do",
                "first_name": "Tri Nhu"
            },
            {
                "last_name": "Haas",
                "first_name": "Zygmunt J."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper studies the statistical characterization of ground-to-UAV (G2A) and reconfigurable intelligent surface (RIS)-assisted UAV-to-ground (A2G) communications in terrestrial and non-terrestrial networks under the impact of channel aging. We first model the G2A and A2G signal-to-noise ratios as non-central complex Gaussian quadratic random variables (RVs) and derive their exact probability density functions, offering a unique characterization for the A2G SNR as the product of two scaled non-central chi-square RVs. Moreover, we also find that, for a large number of RIS elements, the RIS-assisted A2G channel can be characterized as a single Rician fading channel. Our results reveal the presence of channel hardening in A2G communication under low UAV speeds, where we derive the maximum target spectral efficiency (SE) for a system to maintain a consistent required outage level. Meanwhile, high UAV speeds, exceeding 50 m/s, lead to a significant performance degradation, which cannot be mitigated by increasing the number of RIS elements. ",
        "title": "Statistical Characterization of RIS-assisted UAV Communications in  Terrestrial and Non-Terrestrial Networks Under Channel Aging",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14206",
        "abstract_url": "http://arxiv.org/abs/2401.14206",
        "authors": [
            {
                "last_name": "Perlo",
                "first_name": "Daniele"
            },
            {
                "last_name": "Berton",
                "first_name": "Luca"
            },
            {
                "last_name": "Delpiano",
                "first_name": "Alessia"
            },
            {
                "last_name": "Menchini",
                "first_name": "Francesca"
            },
            {
                "last_name": "Tibaldi",
                "first_name": "Stefano"
            },
            {
                "last_name": "Grosso",
                "first_name": "Marco"
            },
            {
                "last_name": "Fonio",
                "first_name": "Paolo"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  The liver is the most involved organ by distant metastasis in colon-rectal cancer (CRC) patients and it comes necessary to be aware of the mutational status of the lesions to correctly design the best individual treatment. So far, efforts have been made in order to develop non-invasive and real-time methods that permit the analysis of the whole tumor, using new artificial intelligence tools to analyze the tumor's image obtained by Computed Tomography (CT) scan. In order to address the current medical workflow, that is biopsy analysis-based, we propose the first DeepLearning-based exploration, to our knowledge, of such classification approach from the patient medical imaging. We propose i) a solid pipeline for managing undersized datasets of available CT scans and ii) a baseline study for genomics mutation diagnosis support for preemptive patient follow-up. Our method is able to identify CRC RAS mutation family from CT images with 0.73 F1 score. ",
        "title": "Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation  classification",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14210",
        "abstract_url": "http://arxiv.org/abs/2401.14210",
        "authors": [
            {
                "last_name": "Dahal",
                "first_name": "Ashok"
            },
            {
                "last_name": "Huser",
                "first_name": "Rapha\u00ebl"
            },
            {
                "last_name": "Lombardo",
                "first_name": "Luigi"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple return periods. We also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century. Our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner. Geomorphologically, we find that under both climate change scenarios (SSP245 and SSP885), landslide hazard is likely to increase up to two times on average in the lower Himalayan regions while remaining the same in the middle Himalayan region whilst decreasing slightly in the upper Himalayan region areas. ",
        "title": "At the junction between deep learning and statistics of extremes:  formalizing the landslide hazard definition",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14211",
        "abstract_url": "http://arxiv.org/abs/2401.14211",
        "authors": [
            {
                "last_name": "Saeed",
                "first_name": "Vasileios Tsouvalas. Aaqib"
            },
            {
                "last_name": "Ozcelebi",
                "first_name": "Tanir"
            },
            {
                "last_name": "Meratnia",
                "first_name": "Nirvana"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to baselines in terms of communication costs and inference speed. We will make our implementation public upon acceptance. ",
        "title": "Communication-Efficient Federated Learning through Adaptive Weight  Clustering and Server-Side Distillation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14212",
        "abstract_url": "http://arxiv.org/abs/2401.14212",
        "authors": [
            {
                "last_name": "Nuyts",
                "first_name": "Wolf"
            },
            {
                "last_name": "Cartuyvels",
                "first_name": "Ruben"
            },
            {
                "last_name": "Moens",
                "first_name": "Marie-Francine"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text. The loss has the potential to be used in other generation tasks where a tree-like structure underlies the conditioning modality. Code, trained models and the USCOCO evaluation set will be made available via github. ",
        "title": "Explicitly Representing Syntax Improves Sentence-to-layout Prediction of  Unexpected Situations",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14214",
        "abstract_url": "http://arxiv.org/abs/2401.14214",
        "authors": [
            {
                "last_name": "Kougang-Yombi",
                "first_name": "Donald"
            },
            {
                "last_name": "H\u0105z\u0142a",
                "first_name": "Jan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This paper introduces a quantitative generalization of the ``more capable'' comparison of broadcast channels, which is termed ``more capable with advantage''. Some basic properties are demonstrated (including tensorization on product channels), and a characterisation is given for the cases of Binary Symmetric Channel (BSC) and Binary Erasure Channel (BEC).   It is then applied to two problems. First, a list decoding bound on the BSC is given that applies to transitive codes that achieve capacity on the BEC. Second, new lower bounds on entropy rates of binary hidden Markov processes are derived. ",
        "title": "A Quantitative Version of More Capable Channel Comparison",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14215",
        "abstract_url": "http://arxiv.org/abs/2401.14215",
        "authors": [
            {
                "last_name": "Kim",
                "first_name": "Hana"
            },
            {
                "last_name": "Ong",
                "first_name": "Kai Tzu-iunn"
            },
            {
                "last_name": "Kim",
                "first_name": "Seoyeon"
            },
            {
                "last_name": "Lee",
                "first_name": "Dongha"
            },
            {
                "last_name": "Yeo",
                "first_name": "Jinyoung"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/. ",
        "title": "Commonsense-augmented Memory Construction and Management in Long-term  Conversations via Context-aware Persona Refinement",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14216",
        "abstract_url": "http://arxiv.org/abs/2401.14216",
        "authors": [
            {
                "last_name": "Puluckul",
                "first_name": "Priyesh Pappinisseri"
            },
            {
                "last_name": "Weyn",
                "first_name": "Maarten"
            }
        ],
        "primary_category": "AR",
        "categories": [
            "AR"
        ],
        "abstract": "  This paper presents InfiniteEn, a multi-source energy harvesting platform designed for the Internet of Batteryless Things (IoBT). InfiniteEn incorporates an efficient energy combiner to combine energy from different harvesting sources. The energy combiner uses capacitor-to-capacitor energy transfer to combine energy from multiple sources and achieves a nominal efficiency of 88\\%. In addition to multiplexing different sources, the energy combiner facilitates the estimation of the harvesting rate and the calibration of the capacity of the energy buffer. The energy storage architecture of InfiniteEn employs an array of storage buffers that can be configured on demand to cope with varying energy harvesting rates and load's energy requirements. To address the challenge of tracking the energy state of batteryless devices with minimum energy overhead, this work introduces the concept of a Load Monitoring Module (LMM). InfiniteEn is a load-agnostic platform, meaning that it does not require any prior knowledge of the energy profile of the load to track its energy states. The LMM assists InfiniteEn in tracking the energy state of the load and dynamically modifying the storage buffers to meet the load's energy requirements. Furthermore, the module can detect and signal any abnormalities in the energy consumption pattern of the load caused by a hardware or software defect. Experiments demonstrate that LMM has a response time of less than 11 ms to energy state changes. ",
        "title": "InfiniteEn: A Multi-Source Energy Harvesting System with Load Monitoring  Module for Batteryless Internet of Things",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14226",
        "abstract_url": "http://arxiv.org/abs/2401.14226",
        "authors": [
            {
                "last_name": "Han",
                "first_name": "Shuai"
            },
            {
                "last_name": "Dastani",
                "first_name": "Mehdi"
            },
            {
                "last_name": "Wang",
                "first_name": "Shihan"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our algorithm in a variety of sparse-reward environments. The experiment results show that our approach significantly outperforms the state-of-art baselines as the difficulty of the task increases. ",
        "title": "Sample Efficient Reinforcement Learning by Automatically Learning to  Compose Subtasks",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14228",
        "abstract_url": "http://arxiv.org/abs/2401.14228",
        "authors": [
            {
                "last_name": "Sabry",
                "first_name": "Mohammed"
            },
            {
                "last_name": "Belz",
                "first_name": "Anya"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but that there are interesting performance differences between the four PEFT techniques. We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models. ",
        "title": "Assessing the Portability of Parameter Matrices Trained by  Parameter-Efficient Finetuning Methods",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14231",
        "abstract_url": "http://arxiv.org/abs/2401.14231",
        "authors": [
            {
                "last_name": "Krenn",
                "first_name": "Daniel"
            },
            {
                "last_name": "Shallit",
                "first_name": "Jeffrey"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "DM"
        ],
        "abstract": "  Drawing inspiration from a recent paper of Heuberger, Krenn, and Lipnik, we define the class of strongly k-recursive sequences. We show that every k-automatic sequence is strongly $k$-recursive, therefore k-recursive, and discuss that the converse is not true.   We also show that the class of strongly k-recursive sequences is a proper subclass of the class of k-regular sequences, and we present some explicit examples. We then extend the proof techniques to answer the same question for the class of k-recursive sequences. ",
        "title": "Strongly k-recursive sequences",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14232",
        "abstract_url": "http://arxiv.org/abs/2401.14232",
        "authors": [
            {
                "last_name": "Salek",
                "first_name": "M Sabbir"
            },
            {
                "last_name": "Mamun",
                "first_name": "Abdullah Al"
            },
            {
                "last_name": "Chowdhury",
                "first_name": "Mashrur"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CR",
            "LG"
        ],
        "abstract": "  This study developed a generative adversarial network (GAN)-based defense method for traffic sign classification in an autonomous vehicle (AV), referred to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i) assuming zero knowledge of adversarial attack models and samples and (ii) providing consistently high traffic sign classification performance under various adversarial attack types. The AR-GAN classification system consists of a generator that denoises an image by reconstruction, and a classifier that classifies the reconstructed image. The authors have tested the AR-GAN under no-attack and under various adversarial attacks, such as Fast Gradient Sign Method (FGSM), DeepFool, Carlini and Wagner (C&W), and Projected Gradient Descent (PGD). The authors considered two forms of these attacks, i.e., (i) black-box attacks (assuming the attackers possess no prior knowledge of the classifier), and (ii) white-box attacks (assuming the attackers possess full knowledge of the classifier). The classification performance of the AR-GAN was compared with several benchmark adversarial defense methods. The results showed that both the AR-GAN and the benchmark defense methods are resilient against black-box attacks and could achieve similar classification performance to that of the unperturbed images. However, for all the white-box attacks considered in this study, the AR-GAN method outperformed the benchmark defense methods. In addition, the AR-GAN was able to maintain its high classification performance under varied white-box adversarial perturbation magnitudes, whereas the performance of the other defense methods dropped abruptly at increased perturbation magnitudes. ",
        "title": "AR-GAN: Generative Adversarial Network-Based Defense Method Against  Adversarial Attacks on the Traffic Sign Classification System of Autonomous  Vehicles",
        "date": "2023-12-31",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14236",
        "abstract_url": "http://arxiv.org/abs/2401.14236",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Haixia"
            },
            {
                "last_name": "Brailsford",
                "first_name": "Tim"
            },
            {
                "last_name": "Goulding",
                "first_name": "James"
            },
            {
                "last_name": "Smith",
                "first_name": "Gavin"
            },
            {
                "last_name": "Bull",
                "first_name": "Larry"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper investigates how adjustments to deep learning architectures impact model performance in image classification. Small-scale experiments generate initial insights although the trends observed are not consistent with the entire dataset. Filtering operations in the image processing pipeline are crucial, with image filtering before pre-processing yielding better results. The choice and order of layers as well as filter placement significantly impact model performance. This study provides valuable insights into optimizing deep learning models, with potential avenues for future research including collaborative platforms. ",
        "title": "Exploring the Unexplored: Understanding the Impact of Layer Adjustments  on Image Classification",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14240",
        "abstract_url": "http://arxiv.org/abs/2401.14240",
        "authors": [
            {
                "last_name": "Kimera",
                "first_name": "Richard"
            },
            {
                "last_name": "Rim",
                "first_name": "Daniela N."
            },
            {
                "last_name": "Kirabira",
                "first_name": "Joseph"
            },
            {
                "last_name": "Udomah",
                "first_name": "Ubong Godwin"
            },
            {
                "last_name": "Choi",
                "first_name": "Heeyoul"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  Depression is a global burden and one of the most challenging mental health conditions to control. Experts can detect its severity early using the Beck Depression Inventory (BDI) questionnaire, administer appropriate medication to patients, and impede its progression. Due to the fear of potential stigmatization, many patients turn to social media platforms like Reddit for advice and assistance at various stages of their journey. This research extracts text from Reddit to facilitate the diagnostic process. It employs a proposed labeling approach to categorize the text and subsequently fine-tunes the Longformer model. The model's performance is compared against baseline models, including Naive Bayes, Random Forest, Support Vector Machines, and Gradient Boosting. Our findings reveal that the Longformer model outperforms the baseline models in both English (48%) and Luganda (45%) languages on a custom-made dataset. ",
        "title": "Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer  Models for Classifying Depression Severity in English and Luganda",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14241",
        "abstract_url": "http://arxiv.org/abs/2401.14241",
        "authors": [
            {
                "last_name": "Kamatsuka",
                "first_name": "Akira"
            },
            {
                "last_name": "Ishikawa",
                "first_name": "Yuki"
            },
            {
                "last_name": "Kazama",
                "first_name": "Koki"
            },
            {
                "last_name": "Yoshida",
                "first_name": "Takahiro"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  The Arimoto capacity and Sibson capacity, which are based on the Arimoto and Sibson mutual information (MI) of order {\\alpha}, respectively, are well-known generalizations of the channel capacity C. In this study, we derive novel alternating optimization algorithms for computing these capacities by providing new max characterizations of the Arimoto MI and Sibson MI. Moreover, we prove that all iterative algorithms for computing these capacities are equivalent under appropriate conditions imposed on their initial distributions ",
        "title": "New Algorithms for Computing Sibson Capacity and Arimoto Capacity",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14242",
        "abstract_url": "http://arxiv.org/abs/2401.14242",
        "authors": [
            {
                "last_name": "Li",
                "first_name": "Wei"
            },
            {
                "last_name": "Zan",
                "first_name": "Daoguang"
            },
            {
                "last_name": "Guan",
                "first_name": "Bei"
            },
            {
                "last_name": "Yu",
                "first_name": "Ailun"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiaolin"
            },
            {
                "last_name": "Wang",
                "first_name": "Yongji"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Code large language models (Code LLMs) have demonstrated remarkable performance in code generation. Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention. To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user's natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement. This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools. To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages. Extensive experimental results demonstrate the effectiveness of our proposed framework. ",
        "title": "Improving Natural Language Capability of Code Large Language Model",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14244",
        "abstract_url": "http://arxiv.org/abs/2401.14244",
        "authors": [
            {
                "last_name": "Ferreira",
                "first_name": "David R."
            },
            {
                "last_name": "Mendes",
                "first_name": "Alexandra"
            },
            {
                "last_name": "Ferreira",
                "first_name": "Jo\u00e3o F."
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LO",
            "PL"
        ],
        "abstract": "  Formal contracts and assertions are effective methods to enhance software quality by enforcing preconditions, postconditions, and invariants. Previous research has demonstrated the value of contracts in traditional software development contexts. However, the adoption and impact of contracts in the context of mobile application development, particularly of Android applications, remain unexplored.   To address this, we present the first large-scale empirical study on the presence and use of contracts in Android applications, written in Java or Kotlin. We consider different types of contract elements divided into five categories: conditional runtime exceptions, APIs, annotations, assertions, and other. We analyzed 2,390 Android applications from the F-Droid repository and processed more than 51,749 KLOC to determine 1) how and to what extent contracts are used, 2) how contract usage evolves, and 3) whether contracts are used safely in the context of program evolution and inheritance. Our findings include: 1) although most applications do not specify contracts, annotation-based approaches are the most popular among practitioners; 2) applications that use contracts continue to use them in later versions, but the number of methods increases at a higher rate than the number of contracts; and 3) there are many potentially unsafe specification changes when applications evolve and in subtyping relationships, which indicates a lack of specification stability. Our findings show that it would be desirable to have libraries that standardize contract specifications in Java and Kotlin, and tools that aid practitioners in writing stronger contracts and in detecting contract violations in the context of program evolution and inheritance. ",
        "title": "Contract Usage and Evolution in Android Mobile Applications",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14248",
        "abstract_url": "http://arxiv.org/abs/2401.14248",
        "authors": [
            {
                "last_name": "Xu",
                "first_name": "Kesi"
            },
            {
                "last_name": "Goetz",
                "first_name": "Lea"
            },
            {
                "last_name": "Rajpoot",
                "first_name": "Nasir"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Pre-trained on a large and diverse dataset, the segment anything model (SAM) is the first promptable foundation model in computer vision aiming at object segmentation tasks. In this work, we evaluate SAM for the task of nuclear instance segmentation performance with zero-shot learning and finetuning. We compare SAM with other representative methods in nuclear instance segmentation, especially in the context of model generalisability. To achieve automatic nuclear instance segmentation, we propose using a nuclei detection model to provide bounding boxes or central points of nu-clei as visual prompts for SAM in generating nuclear instance masks from histology images. ",
        "title": "On generalisability of segment anything model for nuclear instance  segmentation in histology images",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14250",
        "abstract_url": "http://arxiv.org/abs/2401.14250",
        "authors": [
            {
                "last_name": "Casamitjana",
                "first_name": "Adria"
            },
            {
                "last_name": "Iglesias",
                "first_name": "Juan Eugenio"
            },
            {
                "last_name": "Tudela",
                "first_name": "Raul"
            },
            {
                "last_name": "Ninerola-Baizan",
                "first_name": "Aida"
            },
            {
                "last_name": "Sala-Llonch",
                "first_name": "Roser"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  We present a pipeline for unbiased and robust multimodal registration of neuroimaging modalities with minimal pre-processing. While typical multimodal studies need to use multiple independent processing pipelines, with diverse options and hyperparameters, we propose a single and structured framework to jointly process different image modalities. The use of state-of-the-art learning-based techniques enables fast inferences, which makes the presented method suitable for large-scale and/or multi-cohort datasets with a diverse number of modalities per session. The pipeline currently works with structural MRI, resting state fMRI and amyloid PET images. We show the predictive power of the derived biomarkers using in a case-control study and study the cross-modal relationship between different image modalities. The code can be found in https: //github.com/acasamitjana/JUMP. ",
        "title": "JUMP: A joint multimodal registration pipeline for neuroimaging with  minimal preprocessing",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14252",
        "abstract_url": "http://arxiv.org/abs/2401.14252",
        "authors": [
            {
                "last_name": "Qayyum",
                "first_name": "Hina"
            },
            {
                "last_name": "Ikram",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Zhao",
                "first_name": "Benjamin Zi Hao"
            },
            {
                "last_name": "Wood",
                "first_name": "an D."
            },
            {
                "last_name": "Kourtellis",
                "first_name": "Nicolas"
            },
            {
                "last_name": "Kaafar",
                "first_name": "Mohamed Ali"
            }
        ],
        "primary_category": "CY",
        "categories": [
            "CY"
        ],
        "abstract": "  The argument for persistent social media influence campaigns, often funded by malicious entities, is gaining traction. These entities utilize instrumented profiles to disseminate divisive content and disinformation, shaping public perception. Despite ample evidence of these instrumented profiles, few identification methods exist to locate them in the wild. To evade detection and appear genuine, small clusters of instrumented profiles engage in unrelated discussions, diverting attention from their true goals. This strategic thematic diversity conceals their selective polarity towards certain topics and fosters public trust.   This study aims to characterize profiles potentially used for influence operations, termed 'on-mission profiles,' relying solely on thematic content diversity within unlabeled data. Distinguishing this work is its focus on content volume and toxicity towards specific themes. Longitudinal data from 138K Twitter or X, profiles and 293M tweets enables profiling based on theme diversity. High thematic diversity groups predominantly produce toxic content concerning specific themes, like politics, health, and news classifying them as 'on-mission' profiles.   Using the identified ``on-mission\" profiles, we design a classifier for unseen, unlabeled data. Employing a linear SVM model, we train and test it on an 80/20% split of the most diverse profiles. The classifier achieves a flawless 100% accuracy, facilitating the discovery of previously unknown ``on-mission\" profiles in the wild. ",
        "title": "On mission Twitter Profiles: A Study of Selective Toxic Behavior",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14255",
        "abstract_url": "http://arxiv.org/abs/2401.14255",
        "authors": [
            {
                "last_name": "Hasan",
                "first_name": "Yumnah"
            },
            {
                "last_name": "de Lima",
                "first_name": "Allan"
            },
            {
                "last_name": "Amerehi",
                "first_name": "Fatemeh"
            },
            {
                "last_name": "de Bulnes",
                "first_name": "Darian Reyes Fernandez"
            },
            {
                "last_name": "Healy",
                "first_name": "Patrick"
            },
            {
                "last_name": "Ryan",
                "first_name": "Conor"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "NE"
        ],
        "abstract": "  Medical imaging diagnosis increasingly relies on Machine Learning (ML) models. This is a task that is often hampered by severely imbalanced datasets, where positive cases can be quite rare. Their use is further compromised by their limited interpretability, which is becoming increasingly important. While post-hoc interpretability techniques such as SHAP and LIME have been used with some success on so-called black box models, the use of inherently understandable models makes such endeavors more fruitful. This paper addresses these issues by demonstrating how a relatively new synthetic data generation technique, STEM, can be used to produce data to train models produced by Grammatical Evolution (GE) that are inherently understandable. STEM is a recently introduced combination of the Synthetic Minority Oversampling Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously been successfully used to tackle both between class and within class imbalance issues. We test our technique on the Digital Database for Screening Mammography (DDSM) and the Wisconsin Breast Cancer (WBC) datasets and compare Area Under the Curve (AUC) results with an ensemble of the top three performing classifiers from a set of eight standard ML classifiers with varying degrees of interpretability. We demonstrate that the GE-derived models present the best AUC while still maintaining interpretable solutions. ",
        "title": "Interpretable Solutions for Breast Cancer Diagnosis with Grammatical  Evolution and Data Augmentation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14256",
        "abstract_url": "http://arxiv.org/abs/2401.14256",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Cheng"
            },
            {
                "last_name": "Kyathanahally",
                "first_name": "Sreenath"
            },
            {
                "last_name": "Reyes",
                "first_name": "Marta"
            },
            {
                "last_name": "Merkli",
                "first_name": "Stefanie"
            },
            {
                "last_name": "Merz",
                "first_name": "Ewa"
            },
            {
                "last_name": "Francazi",
                "first_name": "Emanuele"
            },
            {
                "last_name": "Hoege",
                "first_name": "Marvin"
            },
            {
                "last_name": "Pomati",
                "first_name": "Francesco"
            },
            {
                "last_name": "Baity-Jesi",
                "first_name": "Marco"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD degradation compared to nominal test performance, (ii) conducting a diagnostic analysis of degradation causes, and (iii) providing solutions. We find that ensembles of BEiT vision transformers, with targeted augmentations addressing OOD robustness, geometric ensembling, and rotation-based test-time augmentation, constitute the most robust model, which we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated on container classes. Moreover, it exhibits lower sensitivity to dataset shift, and reproduces well the plankton abundances. Our proposed pipeline is applicable to generic plankton classifiers, contingent on the availability of suitable test cells. By identifying critical shortcomings and offering practical procedures to fortify models against dataset shift, our study contributes to the development of more reliable plankton classification technologies. ",
        "title": "Producing Plankton Classifiers that are Robust to Dataset Shift",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14257",
        "abstract_url": "http://arxiv.org/abs/2401.14257",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Minglin"
            },
            {
                "last_name": "Wang",
                "first_name": "Longguang"
            },
            {
                "last_name": "Yuan",
                "first_name": "Weihao"
            },
            {
                "last_name": "Wang",
                "first_name": "Yukun"
            },
            {
                "last_name": "Sheng",
                "first_name": "Zhe"
            },
            {
                "last_name": "He",
                "first_name": "Yisheng"
            },
            {
                "last_name": "Dong",
                "first_name": "Zilong"
            },
            {
                "last_name": "Bo",
                "first_name": "Liefeng"
            },
            {
                "last_name": "Guo",
                "first_name": "Yulan"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment. ",
        "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14263",
        "abstract_url": "http://arxiv.org/abs/2401.14263",
        "authors": [
            {
                "last_name": "Ruiz-Gonzalez",
                "first_name": "Antonio"
            },
            {
                "last_name": "Meco-Gutierrez",
                "first_name": "Mario"
            },
            {
                "last_name": "Heredia-Larrubia",
                "first_name": "Juan-Ramon"
            },
            {
                "last_name": "Perez-Hidalgo",
                "first_name": "Francisco"
            },
            {
                "last_name": "Vargas-Merino",
                "first_name": "Francisco"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  A new carrier-based pulse-width modulation (PWM) technique to control power inverters is presented in this paper. To generate the output waveform, this technique compares a harmonic-injection modulating wave and a frequency-modulated triangular carrier wave. The instantaneous frequency for the carrier wave is adjusted according to a periodic function synchronized with the fundamental term of the modulating wave. The main motivation for using this technique compared to a classic PWM sinusoidal technique revolves around the reduction of total harmonic distortion, the reduction of the distortion factor and the shift of temporal harmonics to higher frequencies for any modulation frequency order. Experimental results show that it is possible to optimize the time harmonics generated to minimize vibrations produced by an induction motor when it is fed with a DC/AC converter controlled by the proposed control strategy. This is made possible by using a control parameter that modifies the instantaneous frequency of the carrier wave without modifying the number of pulses per period of the modulating wave, i. e. the mean value of the carrier wave frequency. The proposed technique is applied to an open loop-controlled inverter that operates an induction motor, helping to reduce the vibration levels produced. ",
        "title": "Pulse width modulation technique with harmonic injection in the  modulating wave and discontinuous frequency modulation for the carrier wave  to reduce vibrations in asynchronous machines",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14265",
        "abstract_url": "http://arxiv.org/abs/2401.14265",
        "authors": [
            {
                "last_name": "Wu",
                "first_name": "Jyun-Sian"
            },
            {
                "last_name": "Lin",
                "first_name": "Pin-Hsun"
            },
            {
                "last_name": "Mross",
                "first_name": "Marcel A."
            },
            {
                "last_name": "Jorswieck",
                "first_name": "Eduard A."
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  This work considers an asynchronous $\\textsf{K}_a$-active-user unsourced multiple access channel (AUMAC) with the worst-case asynchronicity. The transmitted messages must be decoded within $n$ channel uses, while some codewords are not completely received due to asynchronicities. We consider a constraint of the largest allowed delay of the transmission. The AUMAC lacks the permutation-invariant property of the synchronous UMAC since different permutations of the same codewords with a fixed asynchronicity are distinguishable. Hence, the analyses require calculating all $2^{\\textsf{K}_a}-1$ combinations of erroneously decoded messages. Moreover, transmitters cannot adapt the corresponding codebooks according to asynchronicity due to a lack of information on asynchronicities. To overcome this challenge, a uniform bound of the per-user probability of error (PUPE) is derived by investigating the worst-case of the asynchronous patterns with the delay constraint. Numerical results show the trade-off between the energy-per-bit and the number of active users for different delay constraints. In addition, although the asynchronous transmission reduces interference, the required energy-per-bit increases as the receiver decodes with incompletely received codewords, compared to the synchronous case. ",
        "title": "Worst-Case Per-User Error Bound for Asynchronous Unsourced Multiple  Access",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14267",
        "abstract_url": "http://arxiv.org/abs/2401.14267",
        "authors": [
            {
                "last_name": "Muller",
                "first_name": "Lyle"
            },
            {
                "last_name": "Churchland",
                "first_name": "Patricia S."
            },
            {
                "last_name": "Sejnowski",
                "first_name": "Terrence J."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long \"encoding vector\" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, \"self-attention\" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in transformers. ",
        "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across  Time",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14268",
        "abstract_url": "http://arxiv.org/abs/2401.14268",
        "authors": [
            {
                "last_name": "Vu",
                "first_name": "Minh Duc"
            },
            {
                "last_name": "Wang",
                "first_name": "Han"
            },
            {
                "last_name": "Li",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Chen",
                "first_name": "Jieshan"
            },
            {
                "last_name": "Zhao",
                "first_name": "Shengdong"
            },
            {
                "last_name": "Xing",
                "first_name": "Zhenchang"
            },
            {
                "last_name": "Chen",
                "first_name": "Chunyang"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC"
        ],
        "abstract": "  Virtual assistants have the potential to play an important role in helping users achieves different tasks. However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions. Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices. GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion. The system continually learns from historical user commands to automate subsequent usages, further enhancing execution efficiency. Our experiments affirm GptVoiceTasker's exceptional command interpretation abilities and the precision of its task automation module. In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback. We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency. ",
        "title": "GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14269",
        "abstract_url": "http://arxiv.org/abs/2401.14269",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Heming"
            },
            {
                "last_name": "Healy",
                "first_name": "Eric W."
            },
            {
                "last_name": "Wang",
                "first_name": "DeLiang"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Speech super-resolution (SR) is the task that restores high-resolution speech from low-resolution input. Existing models employ simulated data and constrained experimental settings, which limit generalization to real-world SR. Predictive models are known to perform well in fixed experimental settings, but can introduce artifacts in adverse conditions. On the other hand, generative models learn the distribution of target data and have a better capacity to perform well on unseen conditions. In this study, we propose a novel two-stage approach that combines the strengths of predictive and generative models. Specifically, we employ a diffusion-based model that is conditioned on the output of a predictive model. Our experiments demonstrate that the model significantly outperforms single-stage counterparts and existing strong baselines on benchmark SR datasets. Furthermore, we introduce a repainting technique during the inference of the diffusion process, enabling the proposed model to regenerate high-frequency components even in mismatched conditions. An additional contribution is the collection of and evaluation on real SR recordings, using the same microphone at different native sampling rates. We make this dataset freely accessible, to accelerate progress towards real-world speech super-resolution. ",
        "title": "Combined Generative and Predictive Modeling for Speech Super-resolution",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14270",
        "abstract_url": "http://arxiv.org/abs/2401.14270",
        "authors": [
            {
                "last_name": "Rosenkranz",
                "first_name": "Max"
            },
            {
                "last_name": "Kalina",
                "first_name": "Karl A."
            },
            {
                "last_name": "Brummund",
                "first_name": "J\u00f6rg"
            },
            {
                "last_name": "Sun",
                "first_name": "WaiChing"
            },
            {
                "last_name": "K\u00e4stner",
                "first_name": "Markus"
            }
        ],
        "primary_category": "CE",
        "categories": [
            "CE"
        ],
        "abstract": "  We present an approach for the data-driven modeling of nonlinear viscoelastic materials at small strains which is based on physics-augmented neural networks (NNs) and requires only stress and strain paths for training. The model is built on the concept of generalized standard materials and is therefore thermodynamically consistent by construction. It consists of a free energy and a dissipation potential, which can be either expressed by the components of their tensor arguments or by a suitable set of invariants. The two potentials are described by fully/partially input convex neural networks. For training of the NN model by paths of stress and strain, an efficient and flexible training method based on a recurrent cell, particularly a long short-term memory cell, is developed to automatically generate the internal variable(s) during the training process. The proposed method is benchmarked and thoroughly compared with existing approaches. These include a method that obtains the internal variable by integrating the evolution equation over the entire sequence, while the other method uses an an auxiliary feedforward neural network for the internal variable(s). Databases for training are generated by using a conventional nonlinear viscoelastic reference model, where 3D and 2D plane strain data with either ideal or noisy stresses are generated. The coordinate-based and the invariant-based formulation are compared and the advantages of the latter are demonstrated. Afterwards, the invariant-based model is calibrated by applying the three training methods using ideal or noisy stress data. All methods yield good results, but differ in computation time and usability for large data sets. The presented training method based on a recurrent cell turns out to be particularly robust and widely applicable and thus represents a promising approach for the calibration of other types of models as well. ",
        "title": "Viscoelasticty with physics-augmented neural networks: Model formulation  and training methods without prescribed internal variables",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14271",
        "abstract_url": "http://arxiv.org/abs/2401.14271",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Wangyou"
            },
            {
                "last_name": "Jung",
                "first_name": "Jee-weon"
            },
            {
                "last_name": "Watanabe",
                "first_name": "Shinji"
            },
            {
                "last_name": "Qian",
                "first_name": "Yanmin"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Building a single universal speech enhancement (SE) system that can handle arbitrary input is a demanded but underexplored research topic. Towards this ultimate goal, one direction is to build a single model that handles diverse audio duration, sampling frequencies, and microphone variations in noisy and reverberant scenarios, which we define here as \"input condition invariant SE\". Such a model was recently proposed showing promising performance; however, its multi-channel performance degraded severely in real conditions. In this paper we propose novel architectures to improve the input condition invariant SE model so that performance in simulated conditions remains competitive while real condition degradation is much mitigated. For this purpose, we redesign the key components that comprise such a system. First, we identify that the channel-modeling module's generalization to unseen scenarios can be sub-optimal and redesign this module. We further introduce a two-stage training strategy to enhance training efficiency. Second, we propose two novel dual-path time-frequency blocks, demonstrating superior performance with fewer parameters and computational costs compared to the existing method. All proposals combined, experiments on various public datasets validate the efficacy of the proposed model, with significantly improved performance on real conditions. Recipe with full model details is released at https://github.com/espnet/espnet. ",
        "title": "Improving Design of Input Condition Invariant Speech Enhancement",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14272",
        "abstract_url": "http://arxiv.org/abs/2401.14272",
        "authors": [
            {
                "last_name": "Izzard",
                "first_name": "Robert G."
            },
            {
                "last_name": "Hendriks",
                "first_name": "David D."
            },
            {
                "last_name": "Nemergut",
                "first_name": "Daniel P."
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  A common requirement in science is to store and share large sets of simulation data in an efficient, nested, flexible and human-readable way. Such datasets contain number counts and distributions, i.e. histograms and maps, of arbitrary dimension and variable type, e.g. floating-point number, integer or character string. Modern high-level programming languages like Perl and Python have associated arrays, knowns as dictionaries or hashes, respectively, to fulfil this storage need. Low-level languages used more commonly for fast computational simulations, such as C and Fortran, lack this functionality. We present libcdict, a C dictionary library, to solve this problem. Libcdict provides C and Fortran application programming interfaces (APIs) to native dictionaries, called cdicts, and functions for cdicts to load and save these as JSON and hence for easy interpretation in other software and languages like Perl, Python and R. ",
        "title": "libcdict: fast dictionaries in C",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14276",
        "abstract_url": "http://arxiv.org/abs/2401.14276",
        "authors": [
            {
                "last_name": "Pedrosa",
                "first_name": "Matheus V. A."
            },
            {
                "last_name": "Scheffe",
                "first_name": "Patrick"
            },
            {
                "last_name": "Alrifaee",
                "first_name": "Bassam"
            },
            {
                "last_name": "Fla\u00dfkamp",
                "first_name": "Kathrin"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  Trajectory planning for autonomous cars can be addressed by primitive-based methods, which encode nonlinear dynamical system behavior into automata. In this paper, we focus on optimal trajectory planning. Since, typically, multiple criteria have to be taken into account, multiobjective optimization problems have to be solved. For the resulting Pareto-optimal motion primitives, we introduce a universal automaton, which can be reduced or reconfigured according to prioritized criteria during planning. We evaluate a corresponding multi-vehicle planning scenario with both simulations and laboratory experiments. ",
        "title": "Optimization-based motion primitive automata for autonomous driving",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14277",
        "abstract_url": "http://arxiv.org/abs/2401.14277",
        "authors": [
            {
                "last_name": "Mazooji",
                "first_name": "Kayvon"
            },
            {
                "last_name": "Shomorony",
                "first_name": "Ilan"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT",
            "DS"
        ],
        "abstract": "  In the trace reconstruction problem, one observes the output of passing a binary string $s \\in \\{0,1\\}^n$ through a deletion channel $T$ times and wishes to recover $s$ from the resulting $T$ \"traces.\" Most of the literature has focused on characterizing the hardness of this problem in terms of the number of traces $T$ needed for perfect reconstruction either in the worst case or in the average case (over input sequences $s$). In this paper, we propose an alternative, instance-based approach to the problem. We define the \"Levenshtein difficulty\" of a problem instance $(s,T)$ as the probability that the resulting traces do not provide enough information for correct recovery with full certainty. One can then try to characterize, for a specific $s$, how $T$ needs to scale in order for the Levenshtein difficulty to go to zero, and seek reconstruction algorithms that match this scaling for each $s$. For a class of binary strings with alternating long runs, we precisely characterize the scaling of $T$ for which the Levenshtein difficulty goes to zero. For this class, we also prove that a simple \"Las Vegas algorithm\" has an error probability that decays to zero with the same rate as that with which the Levenshtein difficulty tends to zero. ",
        "title": "An Instance-Based Approach to the Trace Reconstruction Problem",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14278",
        "abstract_url": "http://arxiv.org/abs/2401.14278",
        "authors": [
            {
                "last_name": "Neiheiser",
                "first_name": "Ray"
            },
            {
                "last_name": "Babaei",
                "first_name": "Arman"
            },
            {
                "last_name": "Alexopoulos",
                "first_name": "Giannis"
            },
            {
                "last_name": "Kogias",
                "first_name": "Marios"
            },
            {
                "last_name": "Kogias",
                "first_name": "Eleftherios Kokoris"
            }
        ],
        "primary_category": "DC",
        "categories": [
            "DC"
        ],
        "abstract": "  Blockchain performance has historically faced challenges posed by the throughput limitations of consensus algorithms. Recent breakthroughs in research have successfully alleviated these constraints by introducing a modular architecture that decouples consensus from execution. The move toward independent optimization of the consensus layer has shifted attention to the execution layer.   While concurrent transaction execution is a promising solution for increasing throughput, practical challenges persist. Its effectiveness varies based on the workloads, and the associated increased hardware requirements raise concerns about undesirable centralization. This increased requirement results in full nodes and stragglers synchronizing from signed checkpoints, decreasing the trustless nature of blockchain systems.   In response to these challenges, this paper introduces Chiron, a system designed to extract execution hints for the acceleration of straggling and full nodes. Notably, Chiron achieves this without compromising the security of the system or introducing overhead on the critical path of consensus. Evaluation results demonstrate a notable speedup of up to 30%, effectively addressing the gap between theoretical research and practical deployment. The quantification of this speedup is achieved through realistic blockchain benchmarks derived from a comprehensive analysis of Ethereum and Solana workloads, constituting an independent contribution. ",
        "title": "CHIRON: Accelerating Node Synchronization without Security Trade-offs in  Distributed Ledgers",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14279",
        "abstract_url": "http://arxiv.org/abs/2401.14279",
        "authors": [
            {
                "last_name": "Kabir",
                "first_name": "Azmain"
            },
            {
                "last_name": "Wang",
                "first_name": "Shaowei"
            },
            {
                "last_name": "Tian",
                "first_name": "Yuan"
            },
            {
                "last_name": "Tse-Hsun",
                "first_name": ""
            },
            {
                "last_name": "Chen",
                "first_name": ""
            },
            {
                "last_name": "Asaduzzaman",
                "first_name": "Muhammad"
            },
            {
                "last_name": "Zhang",
                "first_name": "Wenbin"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Technical question and answering (Q&A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through collaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C on a widely used benchmark called StatType-SO against the SOTA approach SnR. Compared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a 39.3% improvement. On average, ZS4C can infer more accurate import statements than SnR, with an improvement of 6.6% in the F1. ",
        "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code  Snippets using ChatGPT",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14280",
        "abstract_url": "http://arxiv.org/abs/2401.14280",
        "authors": [
            {
                "last_name": "Husain",
                "first_name": "Jaavid Aktar"
            },
            {
                "last_name": "Dabre",
                "first_name": "Raj"
            },
            {
                "last_name": "Kumar",
                "first_name": "Aswanth"
            },
            {
                "last_name": "Puduppully",
                "first_name": "Ratish"
            },
            {
                "last_name": "Kunchukuttan",
                "first_name": "Anoop"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks. ",
        "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large  Language Models models via Romanization",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14283",
        "abstract_url": "http://arxiv.org/abs/2401.14283",
        "authors": [
            {
                "last_name": "Gupta",
                "first_name": "Pritha"
            },
            {
                "last_name": "Wever",
                "first_name": "Marcel"
            },
            {
                "last_name": "H\u00fcllermeier",
                "first_name": "Eyke"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns. IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation. Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL. We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predictor. As the Bayes predictor is typically unknown in practice, we propose to approximate it with the help of automated machine learning (AutoML). First, we compare our MI estimation approaches against current baselines, using synthetic data sets generated using the multivariate normal (MVN) distribution with known MI. Second, we introduce a cut-off technique using one-sided statistical tests to detect IL, employing the Holm-Bonferroni correction to increase confidence in detection decisions. Our study evaluates IL detection performance on real-world data sets, highlighting the effectiveness of the Bayes predictor's log-loss estimation, and finds our proposed method to effectively estimate MI on synthetic data sets and thus detect ILs accurately. ",
        "title": "Information Leakage Detection through Approximate Bayes-optimal  Prediction",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14284",
        "abstract_url": "http://arxiv.org/abs/2401.14284",
        "authors": [
            {
                "last_name": "Birillo",
                "first_name": "Anastasiia"
            },
            {
                "last_name": "Tigina",
                "first_name": "Maria"
            },
            {
                "last_name": "Kurbatova",
                "first_name": "Zarina"
            },
            {
                "last_name": "Potriasaeva",
                "first_name": "Anna"
            },
            {
                "last_name": "Vlasov",
                "first_name": "Ilya"
            },
            {
                "last_name": "Ovchinnikov",
                "first_name": "Valerii"
            },
            {
                "last_name": "Gerasimov",
                "first_name": "Igor"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  In this work, we introduce a novel approach to programming education - in-IDE courses implemented for IntelliJ-based IDEs via the JetBrains Academy Plugin. The primary objective of this approach is to address the challenge of familiarizing students with industrial technologies by moving all theory and practical materials to a professional IDE. This approach allows students to immediately use modern industrial tools as they are fully integrated into the learning process. We have already applied this approach in over 40 courses, and it successfully educates students across diverse topics such as Plugin Development, Algorithms, Data Analysis, and Language mastery in various programming languages, including Kotlin, Java, C++, and Python. Along with the paper, we are providing the community not only with a new way of learning and a set of ready-made courses but also a collection of helpful resources to assist educators in getting started with the plugin. Finally, we describe in detail an IDE plugin development course that demonstrates how the in-IDE approach covers complex topics easily. ",
        "title": "Bridging Education and Development: IDEs as Interactive Learning  Platforms",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14285",
        "abstract_url": "http://arxiv.org/abs/2401.14285",
        "authors": [
            {
                "last_name": "Zhou",
                "first_name": "Bo"
            },
            {
                "last_name": "Hou",
                "first_name": "Jun"
            },
            {
                "last_name": "Chen",
                "first_name": "Tianqi"
            },
            {
                "last_name": "Zhou",
                "first_name": "Yinchi"
            },
            {
                "last_name": "Chen",
                "first_name": "Xiongchao"
            },
            {
                "last_name": "Xie",
                "first_name": "Huidong"
            },
            {
                "last_name": "Liu",
                "first_name": "Qiong"
            },
            {
                "last_name": "Guo",
                "first_name": "Xueqi"
            },
            {
                "last_name": "Tsai",
                "first_name": "Yu-Jung"
            },
            {
                "last_name": "Panin",
                "first_name": "Vladimir Y."
            },
            {
                "last_name": "Toyonaga",
                "first_name": "Takuya"
            },
            {
                "last_name": "Duncan",
                "first_name": "James S."
            },
            {
                "last_name": "Liu",
                "first_name": "Chi"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging. However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses. To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET. First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level. Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation. The integration of OUR-Net and PPGM within a cascade framework enables iterative refinement of $\\mu$-map generation, resulting in the production of high-quality $\\mu$-maps. Experimental results underscore the effectiveness of POUR-Net, showing it as a promising solution for accurate CT-free low-count PET attenuation correction, which also surpasses the performance of previous baseline methods. ",
        "title": "POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for  Low-Count PET Attenuation Map Generation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14286",
        "abstract_url": "http://arxiv.org/abs/2401.14286",
        "authors": [
            {
                "last_name": "Abel",
                "first_name": "Andreas"
            }
        ],
        "primary_category": "PL",
        "categories": [
            "PL",
            "LO"
        ],
        "abstract": "  McBride and Paterson introduced Applicative functors to Haskell, which are equivalent to the lax monoidal functors (with strength) of category theory. Applicative functors F are presented via idiomatic application $\\_\\circledast\\_ : F (A \\to B) \\to F A \\to F B$ and laws that are a bit hard to remember. Capriotti and Kaposi observed that applicative functors can be conceived as multifunctors, i.e., by a family liftA$_n$ : $(A_1 \\to ... \\to A_n \\to C) \\to F A_1 \\to ... \\to F A_n \\to F C$ of zipWith-like functions that generalize pure $(n=0)$, fmap $(n=1)$ and liftA2 $(n=2)$. This reduces the associated laws to just the first functor law and a uniform scheme of second (multi)functor laws, i.e., a composition law for liftA. In this note, we rigorously prove that applicative functors are in fact equivalent to multifunctors, by interderiving their laws. ",
        "title": "Equivalence of Applicative Functors and Multifunctors",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14289",
        "abstract_url": "http://arxiv.org/abs/2401.14289",
        "authors": [
            {
                "last_name": "Cuervo",
                "first_name": "Santiago"
            },
            {
                "last_name": "Marxer",
                "first_name": "Ricard"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD",
            "LG"
        ],
        "abstract": "  Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications. ",
        "title": "Speech foundation models on intelligibility prediction for  hearing-impaired listeners",
        "date": "2024-01-24",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14292",
        "abstract_url": "http://arxiv.org/abs/2401.14292",
        "authors": [
            {
                "last_name": "Rajendran",
                "first_name": "Vishnu"
            },
            {
                "last_name": "Parsons",
                "first_name": "Simon"
            },
            {
                "last_name": "E",
                "first_name": "Amir Ghalamzan"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO"
        ],
        "abstract": "  This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE < 0.8 N), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE < 0.3 mm). In conclusion, the AST skin, with its innovative design and modular architecture, successfully addresses the challenge of tactile feature estimation. The presented results showcase its ability to precisely estimate various tactile features, making it a practical and cost-effective solution for robotic applications. ",
        "title": "AST-2: Single and bi-layered 2-D acoustic soft tactile skin",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14295",
        "abstract_url": "http://arxiv.org/abs/2401.14295",
        "authors": [
            {
                "last_name": "Besta",
                "first_name": "Maciej"
            },
            {
                "last_name": "Memedi",
                "first_name": "Florim"
            },
            {
                "last_name": "Zhang",
                "first_name": "Zhenyu"
            },
            {
                "last_name": "Gerstenberger",
                "first_name": "Robert"
            },
            {
                "last_name": "Blach",
                "first_name": "Nils"
            },
            {
                "last_name": "Nyczyk",
                "first_name": "Piotr"
            },
            {
                "last_name": "Copik",
                "first_name": "Marcin"
            },
            {
                "last_name": "Kwa\u015bniewski",
                "first_name": "Grzegorz"
            },
            {
                "last_name": "M\u00fcller",
                "first_name": "J\u00fcrgen"
            },
            {
                "last_name": "Gianinazzi",
                "first_name": "Lukas"
            },
            {
                "last_name": "Kubicek",
                "first_name": "Ales"
            },
            {
                "last_name": "Niewiadomski",
                "first_name": "Hubert"
            },
            {
                "last_name": "Mutlu",
                "first_name": "Onur"
            },
            {
                "last_name": "Hoefler",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and others parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques. ",
        "title": "Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of  Thoughts",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14296",
        "abstract_url": "http://arxiv.org/abs/2401.14296",
        "authors": [
            {
                "last_name": "Tricomi",
                "first_name": "Pier Paolo"
            },
            {
                "last_name": "Pajola",
                "first_name": "Luca"
            },
            {
                "last_name": "Pasa",
                "first_name": "Luca"
            },
            {
                "last_name": "Conti",
                "first_name": "Mauro"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG",
            "SI"
        ],
        "abstract": "  In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. These publicly accessible playlists transcend the boundaries of mere musical preferences: they serve as sources of rich insights into users' attributes and identities. For example, the musical preferences of elderly individuals may lean more towards Frank Sinatra, while Billie Eilish remains a favored choice among teenagers. These playlists thus become windows into the diverse and evolving facets of one's musical identity.   In this work, we investigate the relationship between Spotify users' attributes and their public playlists. In particular, we focus on identifying recurring musical characteristics associated with users' individual attributes, such as demographics, habits, or personality traits. To this end, we conducted an online survey involving 739 Spotify users, yielding a dataset of 10,286 publicly shared playlists encompassing over 200,000 unique songs and 55,000 artists. Through extensive statistical analyses, we first assess a deep connection between a user's Spotify playlists and their real-life attributes. For instance, we found individuals high in openness often create playlists featuring a diverse array of artists, while female users prefer Pop and K-pop music genres. Building upon these observed associations, we create accurate predictive models for users' attributes, presenting a novel DeepSet application that outperforms baselines in most of these users' attributes. ",
        "title": "\"All of Me\": Mining Users' Attributes from their Public Spotify  Playlists",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14297",
        "abstract_url": "http://arxiv.org/abs/2401.14297",
        "authors": [
            {
                "last_name": "Ruiz-Gonzalez",
                "first_name": "Antonio"
            },
            {
                "last_name": "Meco-Gutierrez",
                "first_name": "Mario"
            },
            {
                "last_name": "Hidalgo",
                "first_name": "Francisco Perez-"
            },
            {
                "last_name": "Vargas-Merino",
                "first_name": "Francisco"
            },
            {
                "last_name": "Heredia-Larrubia",
                "first_name": "JuanR"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  A new, programmed pulse width modulation (PWM) technique to control power inverters, which uses a harmonic injection modulator and a frequency modulated triangular carrier, synchronized with the modulating signal is presented in this paper. The instantaneous carrier frequency is adjusted according to a periodic function synchronized with the fundamental term of the modulating signal, in order to maintain the average value of the instantaneous frequency as an odd positive integer multiple of 3, for each period of the modulating signal which is known as the average modulation order. The advantages of using the proposed technique over the conventional PWM techniques are the reduction in the total harmonic distortion and shift the frequency up of the temporal harmonics for any average modulation order. The experimental results show the viability of optimizing the time harmonics generated to minimize the vibrations in an induction motor or avoid the resonant frequencies.The mathematical formulation for the output modulated voltage is defined and the results are also checked experimentally and compared to a sinusoidal PWM technique ",
        "title": "PWM strategy with harmonics injection and modulated frequency triangular  carrier. A review",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14303",
        "abstract_url": "http://arxiv.org/abs/2401.14303",
        "authors": [
            {
                "last_name": "Cojocaru",
                "first_name": "Liliana"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL",
            "CC",
            "LO"
        ],
        "abstract": "  We deal with a normal form for context-free grammars, called Dyck normal form. This normal form is a syntactical restriction of the Chomsky normal form, in which the two nonterminals occurring on the right-hand side of a rule are paired nonterminals. This pairwise property, along with several other terminal rewriting conditions, makes it possible to define a homomorphism from Dyck words to words generated by a grammar in Dyck normal form. We prove that for each context-free language L, there exist an integer K and a homomorphism phi such that L=phi(D'_K), where D'_K is a subset of D_K and D_K is the one-sided Dyck language over K letters. As an application we give an alternative proof of the inclusion of the class of even linear languages in AC1. ",
        "title": "On Some Complexity Results for Even Linear Languages",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14304",
        "abstract_url": "http://arxiv.org/abs/2401.14304",
        "authors": [
            {
                "last_name": "Bae",
                "first_name": "Juho"
            },
            {
                "last_name": "Bai",
                "first_name": "Ji Hoon"
            },
            {
                "last_name": "Lee",
                "first_name": "Byung-Yoon"
            },
            {
                "last_name": "Lee",
                "first_name": "Jun-Yong"
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  This paper presents an enhanced direct-method-based approach for the real-time solution of optimal control problems to handle path constraints, such as obstacles. The principal contributions of this work are twofold: first, the existing methods for constructing reachability sets in the literature are extended to derive the envelope of these sets, which determines the region swept by all feasible trajectories between adjacent sample points. Second, we propose a novel method to guarantee constraint violation-free between discrete states in two dimensions through mesh refinement approach. To illustrate the effectiveness of the proposed methodology, numerical simulations are conducted on real-time path planning for fixed-wing unmanned aerial vehicles. ",
        "title": "Constraint-Aware Mesh Refinement Method by Reachability Set Envelope of  Curvature Bounded Paths",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14310",
        "abstract_url": "http://arxiv.org/abs/2401.14310",
        "authors": [
            {
                "last_name": "Saglio",
                "first_name": "Caterina Beatrice Leimer"
            },
            {
                "last_name": "Pagani",
                "first_name": "Stefano"
            },
            {
                "last_name": "Corti",
                "first_name": "Mattia"
            },
            {
                "last_name": "Antonietti",
                "first_name": "Paola F."
            }
        ],
        "primary_category": "",
        "categories": [],
        "abstract": "  Epilepsy is a clinical neurological disorder characterized by recurrent and spontaneous seizures consisting of abnormal high-frequency electrical activity in the brain. In this condition, the transmembrane potential dynamics are characterized by rapid and sharp wavefronts traveling along the heterogeneous and anisotropic conduction pathways of the brain. This work employs the monodomain model, coupled with specific neuronal ionic models characterizing ion concentration dynamics, to mathematically describe brain tissue electrophysiology in grey and white matter at the organ scale. This multiscale model is discretized in space with the high-order discontinuous Galerkin method on polygonal and polyhedral grids (PolyDG) and advanced in time with a Crank-Nicolson scheme. This ensures, on the one hand, efficient and accurate simulations of the high-frequency electrical activity that is responsible for epileptic seizure and, on the other hand, keeps reasonably low the computational costs by a suitable combination of high-order approximations and agglomerated polytopal meshes. We numerically investigate synthetic test cases on a two-dimensional heterogeneous squared domain discretized with a polygonal grid, and on a two-dimensional brainstem in a sagittal plane with an agglomerated polygonal grid that takes full advantage of the flexibility of the PolyDG approximation of the semidiscrete formulation. Finally, we provide a theoretical analysis of stability and an a-priori convergence analysis for a simplified mathematical problem. ",
        "title": "A high-order discontinuous Galerkin method for the numerical modeling of  epileptic seizures",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14314",
        "abstract_url": "http://arxiv.org/abs/2401.14314",
        "authors": [
            {
                "last_name": "Gao",
                "first_name": "Xinyu"
            },
            {
                "last_name": "Wang",
                "first_name": "Zhijie"
            },
            {
                "last_name": "Feng",
                "first_name": "Yang"
            },
            {
                "last_name": "Ma",
                "first_name": "Lei"
            },
            {
                "last_name": "Chen",
                "first_name": "Zhenyu"
            },
            {
                "last_name": "Xu",
                "first_name": "Baowen"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE"
        ],
        "abstract": "  Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms. With the continuous advancement in data-driven artificial intelligence (AI), MSF's potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems. Similar to traditional software, adequate testing is also required for AI-enabled MSF systems. Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-/point cloud-based object detection systems). There remains a lack of emphasis on generating multi-modal test cases for MSF systems. To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems. MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds. A fitness metric is designed to guide and boost the test generation process. We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases' realism, (2) fault detection capabilities, and (3) performance improvement. The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test. Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system's robustness. ",
        "title": "MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor  Fusion Perception Systems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14317",
        "abstract_url": "http://arxiv.org/abs/2401.14317",
        "authors": [
            {
                "last_name": "Brown",
                "first_name": "Adam"
            },
            {
                "last_name": "Laddha",
                "first_name": "Aditi"
            },
            {
                "last_name": "Singh",
                "first_name": "Mohit"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  In an instance of the minimum eigenvalue problem, we are given a collection of $n$ vectors $v_1,\\ldots, v_n \\subset {\\mathbb{R}^d}$, and the goal is to pick a subset $B\\subseteq [n]$ of given vectors to maximize the minimum eigenvalue of the matrix $\\sum_{i\\in B} v_i v_i^{\\top} $. Often, additional combinatorial constraints such as cardinality constraint $\\left(|B|\\leq k\\right)$ or matroid constraint ($B$ is a basis of a matroid defined on $[n]$) must be satisfied by the chosen set of vectors. The minimum eigenvalue problem with matroid constraints models a wide variety of problems including the Santa Clause problem, the E-design problem, and the constructive Kadison-Singer problem.   In this paper, we give a randomized algorithm that finds a set $B\\subseteq [n]$ subject to any matroid constraint whose minimum eigenvalue is at least $(1-\\epsilon)$ times the optimum, with high probability. The running time of the algorithm is $O\\left( n^{O(d\\log(d)/\\epsilon^2)}\\right)$. In particular, our results give a polynomial time asymptotic scheme when the dimension of the vectors is constant. Our algorithm uses a convex programming relaxation of the problem after guessing a rescaling which allows us to apply pipage rounding and matrix Chernoff inequalities to round to a good solution. The key new component is a structural lemma which enables us to \"guess'' the appropriate rescaling, which could be of independent interest. Our approach generalizes the approximation guarantee to monotone, homogeneous functions and as such we can maximize $\\det(\\sum_{i\\in B} v_i v_i^\\top)^{1/d}$, or minimize any norm of the eigenvalues of the matrix $\\left(\\sum_{i\\in B} v_i v_i^\\top\\right)^{-1} $, with the same running time under some mild assumptions. As a byproduct, we also get a simple algorithm for an algorithmic version of Kadison-Singer problem. ",
        "title": "Maximizing the Minimum Eigenvalue in Constant Dimension",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14319",
        "abstract_url": "http://arxiv.org/abs/2401.14319",
        "authors": [
            {
                "last_name": "Sela",
                "first_name": "Benjamin"
            },
            {
                "last_name": "Katz",
                "first_name": "Jonathan"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR"
        ],
        "abstract": "  We study the (quantum) security of pseudorandom generators (PRGs) constructed from random oracles. We prove a ``lifting theorem'' showing, roughly, that if such a PRG is unconditionally secure against classical adversaries making polynomially many queries to the random oracle, then it is also (unconditionally) secure against quantum adversaries in the same sense. As a result of independent interest, we also show that any pseudo-deterministic quantum-oracle algorithm (i.e., a quantum algorithm that with high probability returns the same value on repeated executions) can be simulated by a computationally unbounded but query bounded classical-oracle algorithm with only a polynomial blowup in the number of queries. This implies as a corollary that our lifting theorem holds even for PRGs that themselves make quantum queries to the random oracle. ",
        "title": "A Quantum \"Lifting Theorem\" for Constructions of Pseudorandom Generators  from Random Oracles",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14320",
        "abstract_url": "http://arxiv.org/abs/2401.14320",
        "authors": [
            {
                "last_name": "Lanzinger",
                "first_name": "Florian"
            },
            {
                "last_name": "Martin",
                "first_name": "Christian"
            },
            {
                "last_name": "Reiche",
                "first_name": "Frederik"
            },
            {
                "last_name": "Teuber",
                "first_name": "Samuel"
            },
            {
                "last_name": "Heinrich",
                "first_name": "Robert"
            },
            {
                "last_name": "Weigl",
                "first_name": "Alexander"
            }
        ],
        "primary_category": "SE",
        "categories": [
            "SE",
            "LO"
        ],
        "abstract": "  Most formal methods see the correctness of a software system as a binary decision. However, proving the correctness of complex systems completely is difficult because they are composed of multiple components, usage scenarios, and environments. We present QuAC, a modular approach for quantifying the correctness of service-oriented software systems by combining software architecture modeling with deductive verification. Our approach is based on a model of the service-oriented architecture and the probabilistic usage scenarios of the system. The correctness of a single service is approximated by a coverage region, which is a formula describing which inputs for that service are proven to not lead to an erroneous execution. The coverage regions can be determined by a combination of various analyses, e.g., formal verification, expert estimations, or testing. The coverage regions and the software model are then combined into a probabilistic program. From this, we can compute the probability that under a given usage profile no service is called outside its coverage region. If the coverage region is large enough, then instead of attempting to get 100% coverage, which may be prohibitively expensive, run-time verification or testing approaches may be used to deal with inputs outside the coverage region. We also present an implementation of QuAC for Java using the modeling tool Palladio and the deductive verification tool KeY. We demonstrate its usability by applying it to a software simulation of an energy system. ",
        "title": "Quantifying Software Correctness by Combining Architecture Modeling and  Formal Program Analysis",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14321",
        "abstract_url": "http://arxiv.org/abs/2401.14321",
        "authors": [
            {
                "last_name": "Du",
                "first_name": "Chenpeng"
            },
            {
                "last_name": "Guo",
                "first_name": "Yiwei"
            },
            {
                "last_name": "Wang",
                "first_name": "Hankun"
            },
            {
                "last_name": "Yang",
                "first_name": "Yifan"
            },
            {
                "last_name": "Niu",
                "first_name": "Zhikang"
            },
            {
                "last_name": "Wang",
                "first_name": "Shuai"
            },
            {
                "last_name": "Zhang",
                "first_name": "Hui"
            },
            {
                "last_name": "Chen",
                "first_name": "Xie"
            },
            {
                "last_name": "Yu",
                "first_name": "Kai"
            }
        ],
        "primary_category": "SD",
        "categories": [
            "SD"
        ],
        "abstract": "  Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and difficulty in stopping. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3\\% in the word error rate. Furthermore, the controllability of alignment in VALL-T during decoding facilitates the use of untranscribed speech prompts, even in unknown languages. It also enables the synthesis of lengthy speech by utilizing an aligned context window. ",
        "title": "VALL-T: Decoder-Only Generative Transducer for Robust and  Decoding-Controllable Text-to-Speech",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14322",
        "abstract_url": "http://arxiv.org/abs/2401.14322",
        "authors": [
            {
                "last_name": "Srinivasan",
                "first_name": "Hansa"
            },
            {
                "last_name": "Schumann",
                "first_name": "Candice"
            },
            {
                "last_name": "Sinha",
                "first_name": "Aradhana"
            },
            {
                "last_name": "Madras",
                "first_name": "David"
            },
            {
                "last_name": "Olanubi",
                "first_name": "Gbolahan Oluwafemi"
            },
            {
                "last_name": "Beutel",
                "first_name": "Alex"
            },
            {
                "last_name": "Ricco",
                "first_name": "Susanna"
            },
            {
                "last_name": "Chen",
                "first_name": "Jilin"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "CY"
        ],
        "abstract": "  Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers. We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner. The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm, is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire). PATHS is created in two stages. First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model. Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient. Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators. ",
        "title": "Generalized People Diversity: Learning a Human Perception-Aligned  Diversity Representation for People Images",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14323",
        "abstract_url": "http://arxiv.org/abs/2401.14323",
        "authors": [
            {
                "last_name": "Ezzine",
                "first_name": "Rami"
            },
            {
                "last_name": "Wiese",
                "first_name": "Moritz"
            },
            {
                "last_name": "Deppe",
                "first_name": "Christian"
            },
            {
                "last_name": "Boche",
                "first_name": "Holger"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We investigate the problem of generating common randomness (CR) from finite compound sources aided by unidirectional communication over rate-limited perfect channels. The two communicating parties, often referred to as terminals, observe independent and identically distributed (i.i.d.) samples of a finite compound source and aim to agree on a common random variable with a high probability for every possible realization of the source state. Both parties know the set of source states as well as their statistics. However, they are unaware of the actual realization of the source state. We establish a single-letter lower and upper bound on the compound CR capacity for the specified model. Furthermore, we present two special scenarios where the established bounds coincide. ",
        "title": "Common Randomness Generation from Finite Compound Sources",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14324",
        "abstract_url": "http://arxiv.org/abs/2401.14324",
        "authors": [
            {
                "last_name": "Dierl",
                "first_name": "Simon"
            },
            {
                "last_name": "Fiterau-Brostean",
                "first_name": "Paul"
            },
            {
                "last_name": "Howar",
                "first_name": "Falk"
            },
            {
                "last_name": "Jonsson",
                "first_name": "Bengt"
            },
            {
                "last_name": "Sagonas",
                "first_name": "Konstantinos"
            },
            {
                "last_name": "T\u00e5quist",
                "first_name": "Fredrik"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  Existing active automata learning (AAL) algorithms have demonstrated their potential in capturing the behavior of complex systems (e.g., in analyzing network protocol implementations). The most widely used AAL algorithms generate finite state machine models, such as Mealy machines. For many analysis tasks, however, it is crucial to generate richer classes of models that also show how relations between data parameters affect system behavior. Such models have shown potential to uncover critical bugs, but their learning algorithms do not scale beyond small and well curated experiments. In this paper, we present $SL^\\lambda$, an effective and scalable register automata (RA) learning algorithm that significantly reduces the number of tests required for inferring models. It achieves this by combining a tree-based cost-efficient data structure with mechanisms for computing short and restricted tests. We have implemented $SL^\\lambda$ as a new algorithm in RALib. We evaluate its performance by comparing it against $SL^*$, the current state-of-the-art RA learning algorithm, in a series of experiments, and show superior performance and substantial asymptotic improvements in bigger systems. ",
        "title": "Scalable Tree-based Register Automata Learning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14325",
        "abstract_url": "http://arxiv.org/abs/2401.14325",
        "authors": [
            {
                "last_name": "R\u00f6\u00dfle",
                "first_name": "Dominik"
            },
            {
                "last_name": "Gerner",
                "first_name": "Jeremias"
            },
            {
                "last_name": "Bogenberger",
                "first_name": "Klaus"
            },
            {
                "last_name": "Cremers",
                "first_name": "Daniel"
            },
            {
                "last_name": "Schmidtner",
                "first_name": "Stefanie"
            },
            {
                "last_name": "Sch\u00f6n",
                "first_name": "Torsten"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving. Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations. This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps. This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations. We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation. TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models. We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures. We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%. The code will be published on GitHub. ",
        "title": "Unlocking Past Information: Temporal Embeddings in Cooperative Bird's  Eye View Prediction",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14332",
        "abstract_url": "http://arxiv.org/abs/2401.14332",
        "authors": [
            {
                "last_name": "Safronov",
                "first_name": "Vadim"
            },
            {
                "last_name": "Mandalari",
                "first_name": "Anna Maria"
            },
            {
                "last_name": "Dubois",
                "first_name": "Daniel J."
            },
            {
                "last_name": "Choffnes",
                "first_name": "David"
            },
            {
                "last_name": "Haddadi",
                "first_name": "Hamed"
            }
        ],
        "primary_category": "CR",
        "categories": [
            "CR",
            "LG"
        ],
        "abstract": "  With an increasing number of Internet of Things (IoT) devices present in homes, there is a rise in the number of potential information leakage channels and their associated security threats and privacy risks. Despite a long history of attacks on IoT devices in unprotected home networks, the problem of accurate, rapid detection and prevention of such attacks remains open. Many existing IoT protection solutions are cloud-based, sometimes ineffective, and might share consumer data with unknown third parties. This paper investigates the potential for effective IoT threat detection locally, on a home router, using AI tools combined with classic rule-based traffic-filtering algorithms. Our results show that with a slight rise of router hardware resources caused by machine learning and traffic filtering logic, a typical home router instrumented with our solution is able to effectively detect risks and protect a typical home IoT network, equaling or outperforming existing popular solutions, without any effects on benign IoT functionality, and without relying on cloud services and third parties. ",
        "title": "SunBlock: Cloudless Protection for IoT Systems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14336",
        "abstract_url": "http://arxiv.org/abs/2401.14336",
        "authors": [
            {
                "last_name": "Liu",
                "first_name": "Dichao"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any additional overheads over the original backbone network. Combining the two frameworks, we obtain models that significantly exceed previous state-of-the-art methods in recognition accuracy on two widely-used, standard FGVR datasets, namely Stanford Cars, and CompCars, as well as three additional surveillance image-based vehicle-type classification datasets, namely Beijing Institute of Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images Dataset for Make Model Recognition (VIDMMR), without any additional overheads over the original backbone networks. The source code is available at https://github.com/Dichao-Liu/Anti-noise_FGVR ",
        "title": "Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for  Fine-grained Vehicle Recognition",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14340",
        "abstract_url": "http://arxiv.org/abs/2401.14340",
        "authors": [
            {
                "last_name": "Sevilla",
                "first_name": "Mart\u00edn"
            },
            {
                "last_name": "Marques",
                "first_name": "Antonio Garc\u00eda"
            },
            {
                "last_name": "Segarra",
                "first_name": "Santiago"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach. ",
        "title": "Estimation of partially known Gaussian graphical models with score-based  structural priors",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14341",
        "abstract_url": "http://arxiv.org/abs/2401.14341",
        "authors": [
            {
                "last_name": "Gabric",
                "first_name": "Daniel"
            },
            {
                "last_name": "Sawada",
                "first_name": "Joe"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS",
            "DM",
            "IT"
        ],
        "abstract": "  An orientable sequence of order $n$ is a cyclic binary sequence such that each length-$n$ substring appears at most once \\emph{in either direction}. Maximal length orientable sequences are known only for $n\\leq 7$, and a trivial upper bound on their length is $2^{n-1} - 2^{\\lfloor(n-1)/2\\rfloor}$. This paper presents the first efficient algorithm to construct orientable sequences with asymptotically optimal length; more specifically, our algorithm constructs orientable sequences via cycle-joining and a successor-rule approach requiring $O(n)$ time per symbol and $O(n)$ space. This answers a longstanding open question from Dai, Martin, Robshaw, Wild [Cryptography and Coding III (1993)]. Our sequences are applied to find new longest-known orientable sequences for $n\\leq 20$. ",
        "title": "Efficient Construction of Long Orientable Sequences",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14343",
        "abstract_url": "http://arxiv.org/abs/2401.14343",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Xuechen"
            },
            {
                "last_name": "Li",
                "first_name": "Mingchen"
            },
            {
                "last_name": "Chen",
                "first_name": "Jiasi"
            },
            {
                "last_name": "Thrampoulidis",
                "first_name": "Christos"
            },
            {
                "last_name": "Oymak",
                "first_name": "Samet"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "CY"
        ],
        "abstract": "  Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes. This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class. This way, optimization process better adapts to heterogeneities. CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show that CAP is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy. Finally, we evaluate CAP on problems with label noise as well as weighted test objectives to showcase how CAP can jointly adapt to different heterogeneities. ",
        "title": "Class-attribute Priors: Adapting Optimization to Heterogeneity and  Fairness Objective",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14347",
        "abstract_url": "http://arxiv.org/abs/2401.14347",
        "authors": [
            {
                "last_name": "Varley",
                "first_name": "Thomas F."
            },
            {
                "last_name": "Bongard",
                "first_name": "Joshua"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  There has recently been an explosion of interest in how \"higher-order\" structures emerge in complex systems. This \"emergent\" organization has been found in a variety of natural and artificial systems, although at present the field lacks a unified understanding of what the consequences of higher-order synergies and redundancies are for systems. Typical research treat the presence (or absence) of synergistic information as a dependent variable and report changes in the level of synergy in response to some change in the system. Here, we attempt to flip the script: rather than treating higher-order information as a dependent variable, we use evolutionary optimization to evolve boolean networks with significant higher-order redundancies, synergies, or statistical complexity. We then analyse these evolved populations of networks using established tools for characterizing discrete dynamics: the number of attractors, average transient length, and Derrida coefficient. We also assess the capacity of the systems to integrate information. We find that high-synergy systems are unstable and chaotic, but with a high capacity to integrate information. In contrast, evolved redundant systems are extremely stable, but have negligible capacity to integrate information. Finally, the complex systems that balance integration and segregation (known as Tononi-Sporns-Edelman complexity) show features of both chaosticity and stability, with a greater capacity to integrate information than the redundant systems while being more stable than the random and synergistic systems. We conclude that there may be a fundamental trade-off between the robustness of a systems dynamics and its capacity to integrate information (which inherently requires flexibility and sensitivity), and that certain kinds of complexity naturally balance this trade-off. ",
        "title": "Evolving higher-order synergies reveals a trade-off between stability  and information integration capacity in complex systems",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14349",
        "abstract_url": "http://arxiv.org/abs/2401.14349",
        "authors": [
            {
                "last_name": "Bono",
                "first_name": "Guillaume"
            },
            {
                "last_name": "Poirier",
                "first_name": "Herv\u00e9"
            },
            {
                "last_name": "Antsfeld",
                "first_name": "Leonid"
            },
            {
                "last_name": "Monaci",
                "first_name": "Gianluca"
            },
            {
                "last_name": "Chidlovskii",
                "first_name": "Boris"
            },
            {
                "last_name": "Wolf",
                "first_name": "Christian"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV"
        ],
        "abstract": "  In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work. ",
        "title": "Learning to navigate efficiently and precisely in real environments",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14350",
        "abstract_url": "http://arxiv.org/abs/2401.14350",
        "authors": [
            {
                "last_name": "Wala",
                "first_name": "Fatema Bannat"
            },
            {
                "last_name": "Kiran",
                "first_name": "Mariam"
            }
        ],
        "primary_category": "NI",
        "categories": [
            "NI",
            "CR"
        ],
        "abstract": "  This document provides an overview of 5G network security, describing various components of the 5G core network architecture and what kind of security services are offered by these 5G components. It also explores the potential security risks and vulnerabilities presented by the security architecture in 5G and recommends some of the best practices for the 5G network admins to consider while deploying a secure 5G network, based on the surveyed documents from the European government's efforts in commercializing the IoT devices and securing supply chain over 5G networks. ",
        "title": "5G Network Security Practices: An Overview and Survey",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14351",
        "abstract_url": "http://arxiv.org/abs/2401.14351",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Yao"
            },
            {
                "last_name": "Xue",
                "first_name": "Leyang"
            },
            {
                "last_name": "Huang",
                "first_name": "Yeqi"
            },
            {
                "last_name": "Brabete",
                "first_name": "Andrei-Octavian"
            },
            {
                "last_name": "Ustiugov",
                "first_name": "Dmitrii"
            },
            {
                "last_name": "Patel",
                "first_name": "Yuvraj"
            },
            {
                "last_name": "Mai",
                "first_name": "Luo"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "DC"
        ],
        "abstract": "  This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Our comprehensive experiments, which include microbenchmarks and real-world traces, show that ServerlessLLM surpasses state-of-the-art systems by 10 - 200X in latency performance when running various LLM inference workloads. ",
        "title": "ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language  Models",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14352",
        "abstract_url": "http://arxiv.org/abs/2401.14352",
        "authors": [
            {
                "last_name": "Tsoukanara",
                "first_name": "Evangelia"
            },
            {
                "last_name": "Koloniari",
                "first_name": "Georgia"
            },
            {
                "last_name": "Pitoura",
                "first_name": "Evaggelia"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  In this paper, we focus on temporal property graphs, that is, property graphs whose labeled nodes and edges as well as the values of the properties associated with them may change with time. For instance, consider a bibliographic network, with nodes representing authors and conferences with properties such as gender and location respectively, and edges representing collaboration between authors and publications in conferences. A key challenge in studying temporal graphs lies in detecting interesting events in their evolution, defined as time intervals of significant stability, growth, or shrinkage. To address this challenge, we build aggregated graphs, where nodes are grouped based on the values of their properties, and seek events at the aggregated level, for example, time intervals of significant growth in the collaborations between authors of the same gender. To locate such events, we propose a novel approach based on unified evolution skylines. A unified evolution skyline assesses the significance of an event in conjunction with the duration of the interval in which the event occurs. Significance is measured by a set of counts, where each count refers to the number of graph elements that remain stable, are created, or deleted, for a specific property value. For example, for property gender, we measure the number of female-female, female-male, and male-male collaborations. Lastly, we share experimental findings that highlight the efficiency and effectiveness of our approach. ",
        "title": "Skyline-based exploration of temporal property graphs",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14354",
        "abstract_url": "http://arxiv.org/abs/2401.14354",
        "authors": [
            {
                "last_name": "Wang",
                "first_name": "Jiaxu"
            },
            {
                "last_name": "Zhang",
                "first_name": "Ziyi"
            },
            {
                "last_name": "Xu",
                "first_name": "Renjing"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  This paper introduces a novel paradigm for the generalizable neural radiance field (NeRF). Previous generic NeRF methods combine multiview stereo techniques with image-based neural rendering for generalization, yielding impressive results, while suffering from three issues. First, occlusions often result in inconsistent feature matching. Then, they deliver distortions and artifacts in geometric discontinuities and locally sharp shapes due to their individual process of sampled points and rough feature aggregation. Third, their image-based representations experience severe degradations when source views are not near enough to the target view. To address challenges, we propose the first paradigm that constructs the generalizable neural field based on point-based rather than image-based rendering, which we call the Generalizable neural Point Field (GPF). Our approach explicitly models visibilities by geometric priors and augments them with neural features. We propose a novel nonuniform log sampling strategy to improve both rendering speed and reconstruction quality. Moreover, we present a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries. Besides, our representation can be easily manipulated. Experiments show that our model can deliver better geometries, view consistencies, and rendering quality than all counterparts and benchmarks on three datasets in both generalization and finetuning settings, preliminarily proving the potential of the new paradigm for generalizable NeRF. ",
        "title": "Learning Robust Generalizable Radiance Field with Visibility and Feature  Augmented Point Representation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14360",
        "abstract_url": "http://arxiv.org/abs/2401.14360",
        "authors": [
            {
                "last_name": "Elahi",
                "first_name": "Kazi Toufique"
            },
            {
                "last_name": "Rahman",
                "first_name": "Tasnuva Binte"
            },
            {
                "last_name": "Shahriar",
                "first_name": "Shakil"
            },
            {
                "last_name": "Sarker",
                "first_name": "Samir"
            },
            {
                "last_name": "Shawon",
                "first_name": "Md. Tanvir Rouf"
            },
            {
                "last_name": "Shahariar",
                "first_name": "G. M."
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature. Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts. In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts. At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis. Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons. The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reduction methods in future research endeavors. We have made the implementation and dataset presented in this paper publicly available at https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts ",
        "title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis  on Noisy Bengali Texts",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14361",
        "abstract_url": "http://arxiv.org/abs/2401.14361",
        "authors": [
            {
                "last_name": "Xue",
                "first_name": "Leyang"
            },
            {
                "last_name": "Fu",
                "first_name": "Yao"
            },
            {
                "last_name": "Lu",
                "first_name": "Zhan"
            },
            {
                "last_name": "Mai",
                "first_name": "Luo"
            },
            {
                "last_name": "Marina",
                "first_name": "Mahesh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG",
            "PF"
        ],
        "abstract": "  This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 - 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity ",
        "title": "MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE  Serving",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14362",
        "abstract_url": "http://arxiv.org/abs/2401.14362",
        "authors": [
            {
                "last_name": "Song",
                "first_name": "Inhwa"
            },
            {
                "last_name": "Pendse",
                "first_name": "Sachin R."
            },
            {
                "last_name": "Kumar",
                "first_name": "Neha"
            },
            {
                "last_name": "De Choudhury",
                "first_name": "Munmun"
            }
        ],
        "primary_category": "HC",
        "categories": [
            "HC",
            "CY"
        ],
        "abstract": "  People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethical and effective use of LLM chatbots and other AI mental health support tools in mental health care. ",
        "title": "The Typing Cure: Experiences with Large Language Model Chatbots for  Mental Health Support",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14367",
        "abstract_url": "http://arxiv.org/abs/2401.14367",
        "authors": [
            {
                "last_name": "Yehudai",
                "first_name": "Asaf"
            },
            {
                "last_name": "Carmeli",
                "first_name": "Boaz"
            },
            {
                "last_name": "Mass",
                "first_name": "Yosi"
            },
            {
                "last_name": "Arviv",
                "first_name": "Ofir"
            },
            {
                "last_name": "Mills",
                "first_name": "Nathaniel"
            },
            {
                "last_name": "Toledo",
                "first_name": "Assaf"
            },
            {
                "last_name": "Shnarch",
                "first_name": "Eyal"
            },
            {
                "last_name": "Choshen",
                "first_name": "Leshem"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains. ",
        "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14371",
        "abstract_url": "http://arxiv.org/abs/2401.14371",
        "authors": [
            {
                "last_name": "Picco",
                "first_name": "Enrico"
            },
            {
                "last_name": "Jaurigue",
                "first_name": "Lina"
            },
            {
                "last_name": "L\u00fcdge",
                "first_name": "Kathy"
            },
            {
                "last_name": "Massar",
                "first_name": "Serge"
            }
        ],
        "primary_category": "ET",
        "categories": [
            "ET",
            "NE"
        ],
        "abstract": "  We present an experimental validation of a recently proposed optimization technique for reservoir computing, using an optoelectronic setup. Reservoir computing is a robust framework for signal processing applications, and the development of efficient optimization approaches remains a key challenge. The technique we address leverages solely a delayed version of the input signal to identify the optimal operational region of the reservoir, simplifying the traditionally time-consuming task of hyperparameter tuning. We verify the effectiveness of this approach on different benchmark tasks and reservoir operating conditions. ",
        "title": "Efficient Optimisation of Physical Reservoir Computers using only a  Delayed Input",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14373",
        "abstract_url": "http://arxiv.org/abs/2401.14373",
        "authors": [
            {
                "last_name": "Uludo\u011fan",
                "first_name": "G\u00f6k\u00e7e"
            },
            {
                "last_name": "Balal",
                "first_name": "Zeynep Yirmibe\u015fo\u011flu"
            },
            {
                "last_name": "Akkurt",
                "first_name": "Furkan"
            },
            {
                "last_name": "T\u00fcrker",
                "first_name": "Melik\u015fah"
            },
            {
                "last_name": "G\u00fcng\u00f6r",
                "first_name": "Onur"
            },
            {
                "last_name": "\u00dcsk\u00fcdarl\u0131",
                "first_name": "Susan"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL",
            "LG"
        ],
        "abstract": "  The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA . ",
        "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced  Understanding and Generation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14375",
        "abstract_url": "http://arxiv.org/abs/2401.14375",
        "authors": [
            {
                "last_name": "Tsoukanara",
                "first_name": "Evangelia"
            },
            {
                "last_name": "Koloniari",
                "first_name": "Georgia"
            },
            {
                "last_name": "Pitoura",
                "first_name": "Evaggelia"
            }
        ],
        "primary_category": "SI",
        "categories": [
            "SI"
        ],
        "abstract": "  When the focus is on the relationships or interactions between entities, graphs offer an intuitive model for many real-world data. Such graphs are usually large and change over time, thus, requiring models and strategies that explore their evolution. We study the evolution of aggregated graphs and introduce the GraphTempo model that allows temporal and attribute aggregation not only on node level by grouping individual nodes, but on a pattern level as well, where subgraphs are grouped together. Furthermore, We propose an efficient strategy for exploring the evolution of the graph based on identifying time intervals of significant growth, shrinkage or stability. Finally, we evaluate the efficiency and effectiveness of the proposed approach using three real graphs. ",
        "title": "The GraphTempo Framework for Exploring the Evolution of a Graph through  Pattern Aggregation",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14377",
        "abstract_url": "http://arxiv.org/abs/2401.14377",
        "authors": [
            {
                "last_name": "Pshenitsyn",
                "first_name": "Tikhon"
            }
        ],
        "primary_category": "FL",
        "categories": [
            "FL"
        ],
        "abstract": "  We introduce bonding grammars, a graph grammar formalism developed to model DNA computation by means of graph transformations. It is a modification of fusion grammars introduced by Kreowski, Kuske and Lye in 2017. Bonding is a graph transformation that consists of merging two hyperedges into a single larger one. We show why bonding better reflects interaction between DNA molecules than fusion. We prove that bonding grammars naturally generalise regular sticker systems. We also study the relation between bonding grammars and hyperedge replacement grammars proving that each of these kinds of grammars generates a language the other one cannot generate. Finally, we prove that the membership problem for bonding grammars is NP-complete and, moreover, that some bonding grammar generates an NP-complete set. ",
        "title": "Bonding Grammars",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14379",
        "abstract_url": "http://arxiv.org/abs/2401.14379",
        "authors": [
            {
                "last_name": "Kapsalis",
                "first_name": "Timo"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design ",
        "title": "UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation  and Diffusion Models",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14381",
        "abstract_url": "http://arxiv.org/abs/2401.14381",
        "authors": [
            {
                "last_name": "Hanik",
                "first_name": "Martin"
            },
            {
                "last_name": "Steidl",
                "first_name": "Gabriele"
            },
            {
                "last_name": "von Tycowicz",
                "first_name": "Christoph"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  We propose two graph neural network layers for graphs with features in a Riemannian manifold. First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns. Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting. Both layers are equivariant with respect to node permutations and isometries of the feature manifold. These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks. Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers. ",
        "title": "Manifold GCN: Diffusion-based Convolutional Neural Network for  Manifold-valued Graphs",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14382",
        "abstract_url": "http://arxiv.org/abs/2401.14382",
        "authors": [
            {
                "last_name": "Taheri",
                "first_name": "Tayebeh"
            },
            {
                "last_name": "Aghaei",
                "first_name": "Alireza Afzal"
            },
            {
                "last_name": "Parand",
                "first_name": "Kourosh"
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The recent introduction of the Least-Squares Support Vector Regression (LS-SVR) algorithm for solving differential and integral equations has sparked interest. In this study, we expand the application of this algorithm to address systems of differential-algebraic equations (DAEs). Our work presents a novel approach to solving general DAEs in an operator format by establishing connections between the LS-SVR machine learning model, weighted residual methods, and Legendre orthogonal polynomials. To assess the effectiveness of our proposed method, we conduct simulations involving various DAE scenarios, such as nonlinear systems, fractional-order derivatives, integro-differential, and partial DAEs. Finally, we carry out comparisons between our proposed method and currently established state-of-the-art approaches, demonstrating its reliability and effectiveness. ",
        "title": "An Orthogonal Polynomial Kernel-Based Machine Learning Model for  Differential-Algebraic Equations",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14383",
        "abstract_url": "http://arxiv.org/abs/2401.14383",
        "authors": [
            {
                "last_name": "Sandhu",
                "first_name": "J. S."
            },
            {
                "last_name": "Shi",
                "first_name": "J."
            }
        ],
        "primary_category": "CC",
        "categories": [
            "CC"
        ],
        "abstract": "  We devise a parameterized family of distributions, the high-entropy step distributions (HES), which are expressive enough to capture near-optima of spherical spin glass models in the full Replica Symmetry Breaking (fRSB) regime and yet permit low-degree Sum-of-Squares (SoS) certificates that no such distribution can achieve value slightly larger than the true optimum. This yields a SoS optimization program and rounding scheme that attains near-optimal solutions for spherical spin glasses in the fRSB regime. In other regimes, the same results occur at the ALG value, which is a conjectured best-value attainable by any polynomial time algorithm. These SoS programs optimize over families of distributions of possible solutions, and circumvent the oft-cited impossibility of providing a low-degree SoS proof of concentration of measure by instead proving the same bounds only in expectation on solution distributions that can be produced by the chosen rounding algorithm. The new SoS hierarchy does not make any specific reference to the spherical spin glass problem, and we conjecture that it can be applied to a broad range of average-case problems to obtain value that is optimal among polynomial-time algorithms. We give evidence for this with examples of ensembles that provably fool certain local iterative algorithms but for which there is either proof or evidence that the SoS program is better. This opens the door to addressing a question posed by Barak about the possible optimality of SoS on average-case optimization problems, and by Schramm about reductions between different families of algorithms for average-case problems. In this paper, we give low-degree SoS proofs certifying key properties about HES distributions as well as the ALG threshold for spherical spin glasses. The rounding algorithm is introduced and analyzed in a companion paper. ",
        "title": "A Sum-of-Squares Hierarchy in the Absence of Pointwise Proofs I: Energy  Certificates",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14385",
        "abstract_url": "http://arxiv.org/abs/2401.14385",
        "authors": [
            {
                "last_name": "Bu",
                "first_name": "Kaifeng"
            },
            {
                "last_name": "Gu",
                "first_name": "Weichen"
            },
            {
                "last_name": "Jaffe",
                "first_name": "Arthur"
            }
        ],
        "primary_category": "IT",
        "categories": [
            "IT"
        ],
        "abstract": "  We establish an entropic, quantum central limit theorem and quantum inverse sumset theorem in discrete-variable quantum systems describing qudits or qubits. Both results are enabled by using our recently-discovered quantum convolution. We show that the exponential rate of convergence of the entropic central limit theorem is bounded by the magic gap. We also establish an ``quantum, entropic inverse sumset theorem,'' by introducing a quantum doubling constant. Furthermore, we introduce a ``quantum Ruzsa divergence'', and we pose a conjecture called ``convolutional strong subaddivity,'' which leads to the triangle inequality for the quantum Ruzsa divergence. A byproduct of this work is a magic measure to quantify the nonstabilizer nature of a state, based on the quantum Ruzsa divergence. ",
        "title": "Entropic Quantum Central Limit Theorem and Quantum Inverse Sumset  Theorem",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14387",
        "abstract_url": "http://arxiv.org/abs/2401.14387",
        "authors": [
            {
                "last_name": "Vorndran",
                "first_name": "Michael R. H."
            },
            {
                "last_name": "Roeck",
                "first_name": "Bernhard F."
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Generating sufficient labeled data is a significant hurdle in the efficient execution of deep learning projects, especially in uncharted territories of image segmentation where labeling demands extensive time, unlike classification tasks. Our study confronts this challenge, operating in an environment constrained by limited hardware resources and the lack of extensive datasets or pre-trained models. We introduce the novel use of Inconsistency Masks (IM) to effectively filter uncertainty in image-pseudo-label pairs, substantially elevating segmentation quality beyond traditional semi-supervised learning techniques. By integrating IM with other methods, we demonstrate remarkable binary segmentation performance on the ISIC 2018 dataset, starting with just 10% labeled data. Notably, three of our hybrid models outperform those trained on the fully labeled dataset. Our approach consistently achieves exceptional results across three additional datasets and shows further improvement when combined with other techniques. For comprehensive and robust evaluation, this paper includes an extensive analysis of prevalent semi-supervised learning strategies, all trained under identical starting conditions. The full code is available at: https://github.com/MichaelVorndran/InconsistencyMasks ",
        "title": "Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label  Pairs",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14388",
        "abstract_url": "http://arxiv.org/abs/2401.14388",
        "authors": [
            {
                "last_name": "Ozcan",
                "first_name": "Erhan Can"
            },
            {
                "last_name": "G\u00f6rg\u00fcl\u00fc",
                "first_name": "Berk"
            },
            {
                "last_name": "Baydogan",
                "first_name": "Mustafa G."
            },
            {
                "last_name": "Paschalidis",
                "first_name": "Ioannis Ch."
            }
        ],
        "primary_category": "LG",
        "categories": [
            "LG"
        ],
        "abstract": "  The most popular classification algorithms are designed to maximize classification accuracy during training. However, this strategy may fail in the presence of class imbalance since it is possible to train models with high accuracy by overfitting to the majority class. On the other hand, the Area Under the Curve (AUC) is a widely used metric to compare classification performance of different algorithms when there is a class imbalance, and various approaches focusing on the direct optimization of this metric during training have been proposed. Among them, SVM-based formulations are especially popular as this formulation allows incorporating different regularization strategies easily. In this work, we develop a prototype learning approach that relies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our algorithm learns simpler models by iteratively introducing cutting planes, thus overfitting is prevented in an unconventional way. Furthermore, it penalizes the changes in the weights at each iteration to avoid large jumps that might be observed in the test performance, thus facilitating a smooth learning process. Based on the experiments conducted on 73 binary classification datasets, our method yields the best test AUC in 25 datasets among its relevant competitors. ",
        "title": "Smooth Ranking SVM via Cutting-Plane Method",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14391",
        "abstract_url": "http://arxiv.org/abs/2401.14391",
        "authors": [
            {
                "last_name": "Fu",
                "first_name": "Letian"
            },
            {
                "last_name": "Lian",
                "first_name": "Long"
            },
            {
                "last_name": "Wang",
                "first_name": "Renhao"
            },
            {
                "last_name": "Shi",
                "first_name": "Baifeng"
            },
            {
                "last_name": "Wang",
                "first_name": "Xudong"
            },
            {
                "last_name": "Yala",
                "first_name": "Adam"
            },
            {
                "last_name": "Darrell",
                "first_name": "Trevor"
            },
            {
                "last_name": "Efros",
                "first_name": "Alexei A."
            },
            {
                "last_name": "Goldberg",
                "first_name": "Ken"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io ",
        "title": "Rethinking Patch Dependence for Masked Autoencoders",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14394",
        "abstract_url": "http://arxiv.org/abs/2401.14394",
        "authors": [
            {
                "last_name": "Bell",
                "first_name": "Tolson"
            },
            {
                "last_name": "Frieze",
                "first_name": "Alan"
            }
        ],
        "primary_category": "DS",
        "categories": [
            "DS"
        ],
        "abstract": "  The random walk $d$-ary cuckoo hashing algorithm was defined by Fotakis, Pagh, Sanders, and Spirakis to generalize and improve upon the standard cuckoo hashing algorithm of Pagh and Rodler. Random walk $d$-ary cuckoo hashing has low space overhead, guaranteed fast access, and fast in practice insertion time. In this paper, we give a theoretical insertion time bound for this algorithm. More precisely, for every $d\\ge 3$ hashes, let $c_d^*$ be the sharp threshold for the load factor at which a valid assignment of $cm$ objects to a hash table of size $m$ likely exists. We show that for any $d\\ge 4$ hashes and load factor $c<c_d^*$, the expectation of the random walk insertion time is $O(1)$, that is, a constant depending only on $d$ and $c$ but not $m$. ",
        "title": "O(1) Insertion for Random Walk d-ary Cuckoo Hashing up to the Load  Threshold",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14398",
        "abstract_url": "http://arxiv.org/abs/2401.14398",
        "authors": [
            {
                "last_name": "Ozguroglu",
                "first_name": "Ege"
            },
            {
                "last_name": "Liu",
                "first_name": "Ruoshi"
            },
            {
                "last_name": "Sur\u00eds",
                "first_name": "D\u00eddac"
            },
            {
                "last_name": "Chen",
                "first_name": "Dian"
            },
            {
                "last_name": "Dave",
                "first_name": "Achal"
            },
            {
                "last_name": "Tokmakov",
                "first_name": "Pavel"
            },
            {
                "last_name": "Vondrick",
                "first_name": "Carl"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions. ",
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14400",
        "abstract_url": "http://arxiv.org/abs/2401.14400",
        "authors": [
            {
                "last_name": "Vamvas",
                "first_name": "Jannis"
            },
            {
                "last_name": "Aepli",
                "first_name": "No\u00ebmi"
            },
            {
                "last_name": "Sennrich",
                "first_name": "Rico"
            }
        ],
        "primary_category": "CL",
        "categories": [
            "CL"
        ],
        "abstract": "  Creating neural text encoders for written Swiss German is challenging due to a dearth of training data combined with dialectal variation. In this paper, we build on several existing multilingual encoders and adapt them to Swiss German using continued pre-training. Evaluation on three diverse downstream tasks shows that simply adding a Swiss German adapter to a modular encoder achieves 97.5% of fully monolithic adaptation performance. We further find that for the task of retrieving Swiss German sentences given Standard German queries, adapting a character-level model is more effective than the other adaptation strategies. We release our code and the models trained for our experiments at https://github.com/ZurichNLP/swiss-german-text-encoders ",
        "title": "Modular Adaptation of Multilingual Encoders to Written Swiss German  Dialect",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14401",
        "abstract_url": "http://arxiv.org/abs/2401.14401",
        "authors": [
            {
                "last_name": "Conti",
                "first_name": "Andrea"
            },
            {
                "last_name": "Poggi",
                "first_name": "Matteo"
            },
            {
                "last_name": "Cambareri",
                "first_name": "Valerio"
            },
            {
                "last_name": "Mattoccia",
                "first_name": "Stefano"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV"
        ],
        "abstract": "  Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios -- e.g., outdoor 3D reconstruction from video sequences -- therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing RAMDepth, an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. Additional material can be found on our project page https://andreaconti.github.io/projects/range_agnostic_multi_view_depth. ",
        "title": "Range-Agnostic Multi-View Depth Estimation With Keyframe Selection",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14403",
        "abstract_url": "http://arxiv.org/abs/2401.14403",
        "authors": [
            {
                "last_name": "Xiong",
                "first_name": "Haoyu"
            },
            {
                "last_name": "Mendonca",
                "first_name": "Russell"
            },
            {
                "last_name": "Shaw",
                "first_name": "Kenneth"
            },
            {
                "last_name": "Pathak",
                "first_name": "Deepak"
            }
        ],
        "primary_category": "RO",
        "categories": [
            "RO",
            "CV",
            "LG"
        ],
        "abstract": "  Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation. Video results at https://open-world-mobilemanip.github.io/ ",
        "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14404",
        "abstract_url": "http://arxiv.org/abs/2401.14404",
        "authors": [
            {
                "last_name": "Chen",
                "first_name": "Xinlei"
            },
            {
                "last_name": "Liu",
                "first_name": "Zhuang"
            },
            {
                "last_name": "Xie",
                "first_name": "Saining"
            },
            {
                "last_name": "He",
                "first_name": "Kaiming"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning. ",
        "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
        "date": "2024-01-25",
        "group": "cs"
    },
    {
        "identifier": "oai:arXiv.org:2401.14405",
        "abstract_url": "http://arxiv.org/abs/2401.14405",
        "authors": [
            {
                "last_name": "Zhang",
                "first_name": "Yiyuan"
            },
            {
                "last_name": "Ding",
                "first_name": "Xiaohan"
            },
            {
                "last_name": "Gong",
                "first_name": "Kaixiong"
            },
            {
                "last_name": "Ge",
                "first_name": "Yixiao"
            },
            {
                "last_name": "Shan",
                "first_name": "Ying"
            },
            {
                "last_name": "Yue",
                "first_name": "Xiangyu"
            }
        ],
        "primary_category": "CV",
        "categories": [
            "CV",
            "LG"
        ],
        "abstract": "  We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT. ",
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other  Modalities",
        "date": "2024-01-25",
        "group": "cs"
    }
]